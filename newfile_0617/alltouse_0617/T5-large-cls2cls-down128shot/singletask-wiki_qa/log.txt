05/26/2022 22:46:54 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/26/2022 22:46:54 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-wiki_qa
05/26/2022 22:46:54 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/26/2022 22:46:54 - INFO - __main__ - models/T5-large-cls2cls-down128shot/singletask-wiki_qa
05/26/2022 22:46:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/26/2022 22:46:55 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/26/2022 22:46:55 - INFO - __main__ - args.device: cuda:0
05/26/2022 22:46:55 - INFO - __main__ - Using 2 gpus
05/26/2022 22:46:55 - INFO - __main__ - args.device: cuda:1
05/26/2022 22:46:55 - INFO - __main__ - Using 2 gpus
05/26/2022 22:46:55 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/26/2022 22:46:55 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/26/2022 22:47:00 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.5, bsz=8 ...
05/26/2022 22:47:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 22:47:00 - INFO - __main__ - Printing 3 examples
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 22:47:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 22:47:00 - INFO - __main__ - Printing 3 examples
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 22:47:00 - INFO - __main__ - ['false']
05/26/2022 22:47:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 22:47:01 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 22:47:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 22:47:01 - INFO - __main__ - Printing 3 examples
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Input ...
05/26/2022 22:47:01 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 22:47:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 22:47:01 - INFO - __main__ - Printing 3 examples
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 22:47:01 - INFO - __main__ - ['false']
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Input ...
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 22:47:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 22:47:01 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 22:47:01 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 22:47:19 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 22:47:19 - INFO - __main__ - task name: wiki_qa
05/26/2022 22:47:19 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 22:47:19 - INFO - __main__ - task name: wiki_qa
05/26/2022 22:47:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 22:47:20 - INFO - __main__ - Starting training!
05/26/2022 22:47:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 22:47:20 - INFO - __main__ - Starting training!
05/26/2022 22:47:23 - INFO - __main__ - Step 10 Global step 10 Train loss 5.60 on epoch=0
05/26/2022 22:47:25 - INFO - __main__ - Step 20 Global step 20 Train loss 1.49 on epoch=1
05/26/2022 22:47:28 - INFO - __main__ - Step 30 Global step 30 Train loss 2.10 on epoch=1
05/26/2022 22:47:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=2
05/26/2022 22:47:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=3
05/26/2022 22:47:43 - INFO - __main__ - Global step 50 Train loss 2.15 Classification-F1 0.3712545436683367 on epoch=3
05/26/2022 22:47:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3712545436683367 on epoch=3, global_step=50
05/26/2022 22:47:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=3
05/26/2022 22:47:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.61 on epoch=4
05/26/2022 22:47:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=4
05/26/2022 22:47:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=5
05/26/2022 22:47:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=6
05/26/2022 22:48:36 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.3333333333333333 on epoch=6
05/26/2022 22:48:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=6
05/26/2022 22:48:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
05/26/2022 22:48:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=8
05/26/2022 22:48:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
05/26/2022 22:48:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=9
05/26/2022 22:49:41 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.3401530406766009 on epoch=9
05/26/2022 22:49:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/26/2022 22:49:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/26/2022 22:49:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
05/26/2022 22:49:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=11
05/26/2022 22:49:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=12
05/26/2022 22:49:59 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.36318407960199 on epoch=12
05/26/2022 22:50:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
05/26/2022 22:50:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/26/2022 22:50:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
05/26/2022 22:50:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/26/2022 22:50:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=15
05/26/2022 22:50:16 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5767662294720599 on epoch=15
05/26/2022 22:50:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3712545436683367 -> 0.5767662294720599 on epoch=15, global_step=250
05/26/2022 22:50:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=16
05/26/2022 22:50:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/26/2022 22:50:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=17
05/26/2022 22:50:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
05/26/2022 22:50:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
05/26/2022 22:50:33 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=18
05/26/2022 22:50:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=19
05/26/2022 22:50:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
05/26/2022 22:50:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=20
05/26/2022 22:50:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/26/2022 22:50:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=21
05/26/2022 22:50:50 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.4886459209419681 on epoch=21
05/26/2022 22:50:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/26/2022 22:50:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/26/2022 22:50:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=23
05/26/2022 22:50:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=24
05/26/2022 22:51:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
05/26/2022 22:51:07 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/26/2022 22:51:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/26/2022 22:51:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
05/26/2022 22:51:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
05/26/2022 22:51:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/26/2022 22:51:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=28
05/26/2022 22:51:23 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
05/26/2022 22:51:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=28
05/26/2022 22:51:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=29
05/26/2022 22:51:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=29
05/26/2022 22:51:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=30
05/26/2022 22:51:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
05/26/2022 22:51:40 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.4613654501922658 on epoch=31
05/26/2022 22:51:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/26/2022 22:51:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
05/26/2022 22:51:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/26/2022 22:51:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
05/26/2022 22:51:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
05/26/2022 22:51:57 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
05/26/2022 22:52:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/26/2022 22:52:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/26/2022 22:52:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/26/2022 22:52:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/26/2022 22:52:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=37
05/26/2022 22:52:13 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.43222439660795825 on epoch=37
05/26/2022 22:52:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
05/26/2022 22:52:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/26/2022 22:52:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
05/26/2022 22:52:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=39
05/26/2022 22:52:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=40
05/26/2022 22:52:30 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.5755755755755756 on epoch=40
05/26/2022 22:52:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=41
05/26/2022 22:52:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=41
05/26/2022 22:52:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/26/2022 22:52:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/26/2022 22:52:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/26/2022 22:52:46 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
05/26/2022 22:52:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
05/26/2022 22:52:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
05/26/2022 22:52:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/26/2022 22:52:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/26/2022 22:52:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=46
05/26/2022 22:53:02 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.4573099415204678 on epoch=46
05/26/2022 22:53:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
05/26/2022 22:53:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/26/2022 22:53:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/26/2022 22:53:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
05/26/2022 22:53:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/26/2022 22:53:18 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
05/26/2022 22:53:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=50
05/26/2022 22:53:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
05/26/2022 22:53:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
05/26/2022 22:53:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
05/26/2022 22:53:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=53
05/26/2022 22:53:34 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=53
05/26/2022 22:53:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=53
05/26/2022 22:53:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
05/26/2022 22:53:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
05/26/2022 22:53:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
05/26/2022 22:53:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=56
05/26/2022 22:53:50 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.5119639389242254 on epoch=56
05/26/2022 22:53:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/26/2022 22:53:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/26/2022 22:53:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/26/2022 22:53:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
05/26/2022 22:54:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/26/2022 22:54:06 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=59
05/26/2022 22:54:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
05/26/2022 22:54:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=60
05/26/2022 22:54:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/26/2022 22:54:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/26/2022 22:54:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/26/2022 22:54:22 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.6143392977925353 on epoch=62
05/26/2022 22:54:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5767662294720599 -> 0.6143392977925353 on epoch=62, global_step=1000
05/26/2022 22:54:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=63
05/26/2022 22:54:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/26/2022 22:54:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=64
05/26/2022 22:54:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
05/26/2022 22:54:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=65
05/26/2022 22:54:38 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.5263405772724215 on epoch=65
05/26/2022 22:54:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
05/26/2022 22:54:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=66
05/26/2022 22:54:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
05/26/2022 22:54:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/26/2022 22:54:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
05/26/2022 22:54:54 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=68
05/26/2022 22:54:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
05/26/2022 22:54:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=69
05/26/2022 22:55:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/26/2022 22:55:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/26/2022 22:55:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=71
05/26/2022 22:55:11 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.35693779904306216 on epoch=71
05/26/2022 22:55:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/26/2022 22:55:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=73
05/26/2022 22:55:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
05/26/2022 22:55:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=74
05/26/2022 22:55:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=74
05/26/2022 22:55:27 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/26/2022 22:55:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=75
05/26/2022 22:55:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
05/26/2022 22:55:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
05/26/2022 22:55:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=77
05/26/2022 22:55:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=78
05/26/2022 22:55:43 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.5298918588043257 on epoch=78
05/26/2022 22:55:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=78
05/26/2022 22:55:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=79
05/26/2022 22:55:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/26/2022 22:55:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=80
05/26/2022 22:55:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/26/2022 22:55:59 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=81
05/26/2022 22:56:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
05/26/2022 22:56:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=82
05/26/2022 22:56:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=83
05/26/2022 22:56:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
05/26/2022 22:56:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=84
05/26/2022 22:56:16 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.5513344794778923 on epoch=84
05/26/2022 22:56:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
05/26/2022 22:56:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
05/26/2022 22:56:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.22 on epoch=86
05/26/2022 22:56:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.64 on epoch=86
05/26/2022 22:56:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.98 on epoch=87
05/26/2022 22:56:32 - INFO - __main__ - Global step 1400 Train loss 1.11 Classification-F1 0.3813144709696433 on epoch=87
05/26/2022 22:56:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.54 on epoch=88
05/26/2022 22:56:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=88
05/26/2022 22:56:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.09 on epoch=89
05/26/2022 22:56:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=89
05/26/2022 22:56:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=90
05/26/2022 22:56:48 - INFO - __main__ - Global step 1450 Train loss 0.58 Classification-F1 0.4297594297594297 on epoch=90
05/26/2022 22:56:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=91
05/26/2022 22:56:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
05/26/2022 22:56:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
05/26/2022 22:56:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/26/2022 22:57:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=93
05/26/2022 22:57:04 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.553628466060765 on epoch=93
05/26/2022 22:57:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=94
05/26/2022 22:57:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
05/26/2022 22:57:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/26/2022 22:57:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=96
05/26/2022 22:57:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=96
05/26/2022 22:57:20 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.5306646260780844 on epoch=96
05/26/2022 22:57:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=97
05/26/2022 22:57:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
05/26/2022 22:57:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=98
05/26/2022 22:57:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=99
05/26/2022 22:57:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
05/26/2022 22:57:36 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.503499451893077 on epoch=99
05/26/2022 22:57:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=100
05/26/2022 22:57:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
05/26/2022 22:57:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
05/26/2022 22:57:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=102
05/26/2022 22:57:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=103
05/26/2022 22:57:52 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.4468452895419188 on epoch=103
05/26/2022 22:57:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
05/26/2022 22:57:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=104
05/26/2022 22:58:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=104
05/26/2022 22:58:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=105
05/26/2022 22:58:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=106
05/26/2022 22:58:08 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.4782235571765716 on epoch=106
05/26/2022 22:58:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=106
05/26/2022 22:58:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=107
05/26/2022 22:58:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=108
05/26/2022 22:58:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=108
05/26/2022 22:58:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=109
05/26/2022 22:58:24 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.5521765457848349 on epoch=109
05/26/2022 22:58:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=109
05/26/2022 22:58:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=110
05/26/2022 22:58:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=111
05/26/2022 22:58:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
05/26/2022 22:58:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=112
05/26/2022 22:58:40 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.5670995670995671 on epoch=112
05/26/2022 22:58:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=113
05/26/2022 22:58:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=113
05/26/2022 22:58:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=114
05/26/2022 22:58:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=114
05/26/2022 22:58:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=115
05/26/2022 22:58:57 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.5848444641014301 on epoch=115
05/26/2022 22:58:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=116
05/26/2022 22:59:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=116
05/26/2022 22:59:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=117
05/26/2022 22:59:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=118
05/26/2022 22:59:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=118
05/26/2022 22:59:13 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.5473032714412025 on epoch=118
05/26/2022 22:59:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
05/26/2022 22:59:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=119
05/26/2022 22:59:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=120
05/26/2022 22:59:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=121
05/26/2022 22:59:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=121
05/26/2022 22:59:29 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.5673382820784729 on epoch=121
05/26/2022 22:59:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=122
05/26/2022 22:59:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=123
05/26/2022 22:59:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
05/26/2022 22:59:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
05/26/2022 22:59:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=124
05/26/2022 22:59:45 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.5272229822161423 on epoch=124
05/26/2022 22:59:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=125
05/26/2022 22:59:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=126
05/26/2022 22:59:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=126
05/26/2022 22:59:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=127
05/26/2022 22:59:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=128
05/26/2022 23:00:01 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.5830086234429894 on epoch=128
05/26/2022 23:00:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=128
05/26/2022 23:00:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=129
05/26/2022 23:00:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=129
05/26/2022 23:00:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=130
05/26/2022 23:00:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.33 on epoch=131
05/26/2022 23:00:16 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.5731938542007191 on epoch=131
05/26/2022 23:00:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=131
05/26/2022 23:00:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=132
05/26/2022 23:00:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=133
05/26/2022 23:00:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=133
05/26/2022 23:00:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
05/26/2022 23:00:32 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.5784035133040558 on epoch=134
05/26/2022 23:00:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.35 on epoch=134
05/26/2022 23:00:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=135
05/26/2022 23:00:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
05/26/2022 23:00:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
05/26/2022 23:00:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=137
05/26/2022 23:00:48 - INFO - __main__ - Global step 2200 Train loss 0.36 Classification-F1 0.5951417004048583 on epoch=137
05/26/2022 23:00:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=138
05/26/2022 23:00:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=138
05/26/2022 23:00:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=139
05/26/2022 23:00:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=139
05/26/2022 23:01:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=140
05/26/2022 23:01:04 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.5577128753599342 on epoch=140
05/26/2022 23:01:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.35 on epoch=141
05/26/2022 23:01:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=141
05/26/2022 23:01:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
05/26/2022 23:01:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.35 on epoch=143
05/26/2022 23:01:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=143
05/26/2022 23:01:20 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.5702585841458244 on epoch=143
05/26/2022 23:01:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.41 on epoch=144
05/26/2022 23:01:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=144
05/26/2022 23:01:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=145
05/26/2022 23:01:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
05/26/2022 23:01:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=146
05/26/2022 23:01:36 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.5764574835782267 on epoch=146
05/26/2022 23:01:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=147
05/26/2022 23:01:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=148
05/26/2022 23:01:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
05/26/2022 23:01:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=149
05/26/2022 23:01:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=149
05/26/2022 23:01:52 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.6118571693792932 on epoch=149
05/26/2022 23:01:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=150
05/26/2022 23:01:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.34 on epoch=151
05/26/2022 23:01:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.37 on epoch=151
05/26/2022 23:02:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.34 on epoch=152
05/26/2022 23:02:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=153
05/26/2022 23:02:08 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.6154548974061169 on epoch=153
05/26/2022 23:02:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6143392977925353 -> 0.6154548974061169 on epoch=153, global_step=2450
05/26/2022 23:02:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
05/26/2022 23:02:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=154
05/26/2022 23:02:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.39 on epoch=154
05/26/2022 23:02:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=155
05/26/2022 23:02:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=156
05/26/2022 23:02:24 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.6457215717373388 on epoch=156
05/26/2022 23:02:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6154548974061169 -> 0.6457215717373388 on epoch=156, global_step=2500
05/26/2022 23:02:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=156
05/26/2022 23:02:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.38 on epoch=157
05/26/2022 23:02:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=158
05/26/2022 23:02:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=158
05/26/2022 23:02:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=159
05/26/2022 23:02:40 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.6168132942326491 on epoch=159
05/26/2022 23:02:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=159
05/26/2022 23:02:44 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=160
05/26/2022 23:02:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=161
05/26/2022 23:02:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=161
05/26/2022 23:02:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=162
05/26/2022 23:02:55 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.615686274509804 on epoch=162
05/26/2022 23:02:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=163
05/26/2022 23:03:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.37 on epoch=163
05/26/2022 23:03:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=164
05/26/2022 23:03:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=164
05/26/2022 23:03:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.41 on epoch=165
05/26/2022 23:03:11 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.6367132066834515 on epoch=165
05/26/2022 23:03:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.36 on epoch=166
05/26/2022 23:03:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.37 on epoch=166
05/26/2022 23:03:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.36 on epoch=167
05/26/2022 23:03:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=168
05/26/2022 23:03:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=168
05/26/2022 23:03:27 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.5717186154920656 on epoch=168
05/26/2022 23:03:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=169
05/26/2022 23:03:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=169
05/26/2022 23:03:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=170
05/26/2022 23:03:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.35 on epoch=171
05/26/2022 23:03:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.37 on epoch=171
05/26/2022 23:03:43 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.5524770763242958 on epoch=171
05/26/2022 23:03:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=172
05/26/2022 23:03:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=173
05/26/2022 23:03:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=173
05/26/2022 23:03:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.40 on epoch=174
05/26/2022 23:03:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.33 on epoch=174
05/26/2022 23:03:59 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.590464876851192 on epoch=174
05/26/2022 23:04:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=175
05/26/2022 23:04:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=176
05/26/2022 23:04:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.37 on epoch=176
05/26/2022 23:04:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=177
05/26/2022 23:04:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.38 on epoch=178
05/26/2022 23:04:15 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.5620442805653828 on epoch=178
05/26/2022 23:04:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=178
05/26/2022 23:04:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.41 on epoch=179
05/26/2022 23:04:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=179
05/26/2022 23:04:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.35 on epoch=180
05/26/2022 23:04:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=181
05/26/2022 23:04:31 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.6399103485586212 on epoch=181
05/26/2022 23:04:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=181
05/26/2022 23:04:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=182
05/26/2022 23:04:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.38 on epoch=183
05/26/2022 23:04:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=183
05/26/2022 23:04:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=184
05/26/2022 23:04:47 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.6377358490566036 on epoch=184
05/26/2022 23:04:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=184
05/26/2022 23:04:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=185
05/26/2022 23:04:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.38 on epoch=186
05/26/2022 23:04:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=186
05/26/2022 23:04:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.36 on epoch=187
05/26/2022 23:05:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:05:00 - INFO - __main__ - Printing 3 examples
05/26/2022 23:05:00 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:05:00 - INFO - __main__ - ['false']
05/26/2022 23:05:00 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:05:00 - INFO - __main__ - ['false']
05/26/2022 23:05:00 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:05:00 - INFO - __main__ - ['false']
05/26/2022 23:05:00 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:05:00 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:05:01 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:05:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:05:01 - INFO - __main__ - Printing 3 examples
05/26/2022 23:05:01 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:05:01 - INFO - __main__ - ['false']
05/26/2022 23:05:01 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:05:01 - INFO - __main__ - ['false']
05/26/2022 23:05:01 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:05:01 - INFO - __main__ - ['false']
05/26/2022 23:05:01 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:05:01 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:05:01 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:05:03 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.6398336187912894 on epoch=187
05/26/2022 23:05:03 - INFO - __main__ - save last model!
05/26/2022 23:05:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 23:05:03 - INFO - __main__ - Start tokenizing ... 2733 instances
05/26/2022 23:05:03 - INFO - __main__ - Printing 3 examples
05/26/2022 23:05:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/26/2022 23:05:03 - INFO - __main__ - ['false']
05/26/2022 23:05:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/26/2022 23:05:03 - INFO - __main__ - ['false']
05/26/2022 23:05:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/26/2022 23:05:03 - INFO - __main__ - ['false']
05/26/2022 23:05:03 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:05:04 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:05:07 - INFO - __main__ - Loaded 2733 examples from test data
05/26/2022 23:05:15 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:05:15 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:05:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:05:16 - INFO - __main__ - Starting training!
05/26/2022 23:05:46 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.5_8_predictions.txt
05/26/2022 23:05:46 - INFO - __main__ - Classification-F1 on test data: 0.4287
05/26/2022 23:05:46 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.5, bsz=8, dev_performance=0.6457215717373388, test_performance=0.42870980541589654
05/26/2022 23:05:46 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.4, bsz=8 ...
05/26/2022 23:05:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:05:47 - INFO - __main__ - Printing 3 examples
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:05:47 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:05:47 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:05:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:05:47 - INFO - __main__ - Printing 3 examples
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:05:47 - INFO - __main__ - ['false']
05/26/2022 23:05:47 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:05:48 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:05:48 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:06:06 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:06:06 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:06:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:06:07 - INFO - __main__ - Starting training!
05/26/2022 23:06:10 - INFO - __main__ - Step 10 Global step 10 Train loss 6.02 on epoch=0
05/26/2022 23:06:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.60 on epoch=1
05/26/2022 23:06:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=1
05/26/2022 23:06:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=2
05/26/2022 23:06:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/26/2022 23:06:24 - INFO - __main__ - Global step 50 Train loss 1.86 Classification-F1 0.3333333333333333 on epoch=3
05/26/2022 23:06:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/26/2022 23:06:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/26/2022 23:06:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
05/26/2022 23:06:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=4
05/26/2022 23:06:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
05/26/2022 23:06:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=6
05/26/2022 23:06:40 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.350463149416029 on epoch=6
05/26/2022 23:06:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.350463149416029 on epoch=6, global_step=100
05/26/2022 23:06:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=6
05/26/2022 23:06:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/26/2022 23:06:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=8
05/26/2022 23:06:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=8
05/26/2022 23:06:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/26/2022 23:06:57 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=9
05/26/2022 23:06:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
05/26/2022 23:07:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/26/2022 23:07:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/26/2022 23:07:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
05/26/2022 23:07:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=12
05/26/2022 23:07:14 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3732922688146569 on epoch=12
05/26/2022 23:07:14 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.3732922688146569 on epoch=12, global_step=200
05/26/2022 23:07:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/26/2022 23:07:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=13
05/26/2022 23:07:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/26/2022 23:07:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/26/2022 23:07:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/26/2022 23:07:31 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=15
05/26/2022 23:07:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=16
05/26/2022 23:07:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/26/2022 23:07:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
05/26/2022 23:07:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=18
05/26/2022 23:07:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/26/2022 23:07:47 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
05/26/2022 23:07:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/26/2022 23:07:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/26/2022 23:07:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
05/26/2022 23:07:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/26/2022 23:07:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/26/2022 23:08:05 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=21
05/26/2022 23:08:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/26/2022 23:08:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
05/26/2022 23:08:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=23
05/26/2022 23:08:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=24
05/26/2022 23:08:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/26/2022 23:08:22 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/26/2022 23:08:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=25
05/26/2022 23:08:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=26
05/26/2022 23:08:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
05/26/2022 23:08:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/26/2022 23:08:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=28
05/26/2022 23:08:38 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.40030721966205834 on epoch=28
05/26/2022 23:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3732922688146569 -> 0.40030721966205834 on epoch=28, global_step=450
05/26/2022 23:08:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/26/2022 23:08:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
05/26/2022 23:08:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/26/2022 23:08:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
05/26/2022 23:08:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
05/26/2022 23:08:54 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.5211861524858872 on epoch=31
05/26/2022 23:08:54 - INFO - __main__ - Saving model with best Classification-F1: 0.40030721966205834 -> 0.5211861524858872 on epoch=31, global_step=500
05/26/2022 23:08:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/26/2022 23:08:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/26/2022 23:09:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=33
05/26/2022 23:09:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/26/2022 23:09:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=34
05/26/2022 23:09:11 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=34
05/26/2022 23:09:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/26/2022 23:09:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/26/2022 23:09:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/26/2022 23:09:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/26/2022 23:09:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
05/26/2022 23:09:27 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.35501021683496337 on epoch=37
05/26/2022 23:09:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/26/2022 23:09:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/26/2022 23:09:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
05/26/2022 23:09:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/26/2022 23:09:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
05/26/2022 23:09:43 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.627262556132849 on epoch=40
05/26/2022 23:09:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5211861524858872 -> 0.627262556132849 on epoch=40, global_step=650
05/26/2022 23:09:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/26/2022 23:09:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/26/2022 23:09:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
05/26/2022 23:09:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/26/2022 23:09:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/26/2022 23:10:00 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.35885876860812244 on epoch=43
05/26/2022 23:10:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=44
05/26/2022 23:10:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/26/2022 23:10:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
05/26/2022 23:10:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/26/2022 23:10:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/26/2022 23:10:16 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.632453567937439 on epoch=46
05/26/2022 23:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.627262556132849 -> 0.632453567937439 on epoch=46, global_step=750
05/26/2022 23:10:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/26/2022 23:10:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
05/26/2022 23:10:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
05/26/2022 23:10:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
05/26/2022 23:10:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/26/2022 23:10:33 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=49
05/26/2022 23:10:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
05/26/2022 23:10:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
05/26/2022 23:10:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/26/2022 23:10:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
05/26/2022 23:10:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
05/26/2022 23:10:50 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=53
05/26/2022 23:10:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/26/2022 23:10:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
05/26/2022 23:10:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=54
05/26/2022 23:11:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
05/26/2022 23:11:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
05/26/2022 23:11:06 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.4880996620127055 on epoch=56
05/26/2022 23:11:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/26/2022 23:11:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=57
05/26/2022 23:11:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/26/2022 23:11:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=58
05/26/2022 23:11:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
05/26/2022 23:11:23 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
05/26/2022 23:11:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
05/26/2022 23:11:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=60
05/26/2022 23:11:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=61
05/26/2022 23:11:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/26/2022 23:11:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
05/26/2022 23:11:41 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.6240822320117474 on epoch=62
05/26/2022 23:11:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
05/26/2022 23:11:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/26/2022 23:11:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
05/26/2022 23:11:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
05/26/2022 23:11:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/26/2022 23:11:57 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.6289855072463768 on epoch=65
05/26/2022 23:12:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
05/26/2022 23:12:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=66
05/26/2022 23:12:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
05/26/2022 23:12:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=68
05/26/2022 23:12:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=68
05/26/2022 23:12:14 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=68
05/26/2022 23:12:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=69
05/26/2022 23:12:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
05/26/2022 23:12:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
05/26/2022 23:12:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/26/2022 23:12:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/26/2022 23:12:30 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.5318670625933196 on epoch=71
05/26/2022 23:12:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/26/2022 23:12:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
05/26/2022 23:12:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
05/26/2022 23:12:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/26/2022 23:12:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/26/2022 23:12:47 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/26/2022 23:12:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/26/2022 23:12:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
05/26/2022 23:12:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/26/2022 23:12:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=77
05/26/2022 23:12:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=78
05/26/2022 23:13:03 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.36516753625488524 on epoch=78
05/26/2022 23:13:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=78
05/26/2022 23:13:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=79
05/26/2022 23:13:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
05/26/2022 23:13:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
05/26/2022 23:13:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
05/26/2022 23:13:21 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.350463149416029 on epoch=81
05/26/2022 23:13:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
05/26/2022 23:13:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
05/26/2022 23:13:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
05/26/2022 23:13:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
05/26/2022 23:13:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
05/26/2022 23:13:38 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.3671451355661882 on epoch=84
05/26/2022 23:13:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
05/26/2022 23:13:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
05/26/2022 23:13:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
05/26/2022 23:13:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=86
05/26/2022 23:13:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
05/26/2022 23:13:55 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.6763490595127968 on epoch=87
05/26/2022 23:13:55 - INFO - __main__ - Saving model with best Classification-F1: 0.632453567937439 -> 0.6763490595127968 on epoch=87, global_step=1400
05/26/2022 23:13:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=88
05/26/2022 23:14:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=88
05/26/2022 23:14:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
05/26/2022 23:14:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=89
05/26/2022 23:14:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
05/26/2022 23:14:11 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.5451204995945969 on epoch=90
05/26/2022 23:14:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/26/2022 23:14:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
05/26/2022 23:14:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=92
05/26/2022 23:14:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/26/2022 23:14:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
05/26/2022 23:14:28 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.5544852075166558 on epoch=93
05/26/2022 23:14:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=94
05/26/2022 23:14:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
05/26/2022 23:14:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=95
05/26/2022 23:14:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=96
05/26/2022 23:14:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
05/26/2022 23:14:45 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.597322623828648 on epoch=96
05/26/2022 23:14:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=97
05/26/2022 23:14:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=98
05/26/2022 23:14:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
05/26/2022 23:14:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=99
05/26/2022 23:14:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=99
05/26/2022 23:15:02 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.5243403939056113 on epoch=99
05/26/2022 23:15:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
05/26/2022 23:15:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=101
05/26/2022 23:15:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
05/26/2022 23:15:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=102
05/26/2022 23:15:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=103
05/26/2022 23:15:19 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.6788850174216028 on epoch=103
05/26/2022 23:15:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6763490595127968 -> 0.6788850174216028 on epoch=103, global_step=1650
05/26/2022 23:15:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=103
05/26/2022 23:15:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
05/26/2022 23:15:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=104
05/26/2022 23:15:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=105
05/26/2022 23:15:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=106
05/26/2022 23:15:35 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.6868845261243179 on epoch=106
05/26/2022 23:15:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6788850174216028 -> 0.6868845261243179 on epoch=106, global_step=1700
05/26/2022 23:15:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
05/26/2022 23:15:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=107
05/26/2022 23:15:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
05/26/2022 23:15:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=108
05/26/2022 23:15:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=109
05/26/2022 23:15:52 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.6583333333333333 on epoch=109
05/26/2022 23:15:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=109
05/26/2022 23:15:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=110
05/26/2022 23:15:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=111
05/26/2022 23:16:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
05/26/2022 23:16:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=112
05/26/2022 23:16:08 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.7100704009794918 on epoch=112
05/26/2022 23:16:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6868845261243179 -> 0.7100704009794918 on epoch=112, global_step=1800
05/26/2022 23:16:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=113
05/26/2022 23:16:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=113
05/26/2022 23:16:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=114
05/26/2022 23:16:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=114
05/26/2022 23:16:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=115
05/26/2022 23:16:26 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.551443790299972 on epoch=115
05/26/2022 23:16:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=116
05/26/2022 23:16:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=116
05/26/2022 23:16:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=117
05/26/2022 23:16:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=118
05/26/2022 23:16:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=118
05/26/2022 23:16:43 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.5747508305647842 on epoch=118
05/26/2022 23:16:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=119
05/26/2022 23:16:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
05/26/2022 23:16:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=120
05/26/2022 23:16:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=121
05/26/2022 23:16:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=121
05/26/2022 23:17:00 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.6806417483853648 on epoch=121
05/26/2022 23:17:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=122
05/26/2022 23:17:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=123
05/26/2022 23:17:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
05/26/2022 23:17:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=124
05/26/2022 23:17:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=124
05/26/2022 23:17:17 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.6027759305676336 on epoch=124
05/26/2022 23:17:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=125
05/26/2022 23:17:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=126
05/26/2022 23:17:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=126
05/26/2022 23:17:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=127
05/26/2022 23:17:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=128
05/26/2022 23:17:35 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.645359192558876 on epoch=128
05/26/2022 23:17:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.26 on epoch=128
05/26/2022 23:17:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=129
05/26/2022 23:17:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=129
05/26/2022 23:17:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=130
05/26/2022 23:17:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=131
05/26/2022 23:17:51 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.7032228147461164 on epoch=131
05/26/2022 23:17:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=131
05/26/2022 23:17:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=132
05/26/2022 23:17:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=133
05/26/2022 23:18:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=133
05/26/2022 23:18:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=134
05/26/2022 23:18:08 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.7129118979952377 on epoch=134
05/26/2022 23:18:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7100704009794918 -> 0.7129118979952377 on epoch=134, global_step=2150
05/26/2022 23:18:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=134
05/26/2022 23:18:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=135
05/26/2022 23:18:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=136
05/26/2022 23:18:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=136
05/26/2022 23:18:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=137
05/26/2022 23:18:26 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.7062738078847124 on epoch=137
05/26/2022 23:18:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=138
05/26/2022 23:18:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=138
05/26/2022 23:18:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=139
05/26/2022 23:18:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=139
05/26/2022 23:18:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
05/26/2022 23:18:44 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.7130759651307597 on epoch=140
05/26/2022 23:18:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7129118979952377 -> 0.7130759651307597 on epoch=140, global_step=2250
05/26/2022 23:18:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
05/26/2022 23:18:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=141
05/26/2022 23:18:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=142
05/26/2022 23:18:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
05/26/2022 23:18:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=143
05/26/2022 23:19:01 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.5453722404067136 on epoch=143
05/26/2022 23:19:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=144
05/26/2022 23:19:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/26/2022 23:19:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=145
05/26/2022 23:19:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/26/2022 23:19:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=146
05/26/2022 23:19:19 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.6976901639086143 on epoch=146
05/26/2022 23:19:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=147
05/26/2022 23:19:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=148
05/26/2022 23:19:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=148
05/26/2022 23:19:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=149
05/26/2022 23:19:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=149
05/26/2022 23:19:36 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.7538724495246234 on epoch=149
05/26/2022 23:19:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7130759651307597 -> 0.7538724495246234 on epoch=149, global_step=2400
05/26/2022 23:19:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/26/2022 23:19:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=151
05/26/2022 23:19:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=151
05/26/2022 23:19:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=152
05/26/2022 23:19:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=153
05/26/2022 23:19:53 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.7199858267473926 on epoch=153
05/26/2022 23:19:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
05/26/2022 23:19:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/26/2022 23:20:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=154
05/26/2022 23:20:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=155
05/26/2022 23:20:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=156
05/26/2022 23:20:11 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.7095013187756855 on epoch=156
05/26/2022 23:20:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/26/2022 23:20:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.24 on epoch=157
05/26/2022 23:20:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
05/26/2022 23:20:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
05/26/2022 23:20:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
05/26/2022 23:20:28 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7262952101661779 on epoch=159
05/26/2022 23:20:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/26/2022 23:20:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=160
05/26/2022 23:20:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/26/2022 23:20:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/26/2022 23:20:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.24 on epoch=162
05/26/2022 23:20:45 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.7183201907207042 on epoch=162
05/26/2022 23:20:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/26/2022 23:20:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=163
05/26/2022 23:20:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
05/26/2022 23:20:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=164
05/26/2022 23:20:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
05/26/2022 23:21:03 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.738085421534045 on epoch=165
05/26/2022 23:21:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=166
05/26/2022 23:21:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/26/2022 23:21:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=167
05/26/2022 23:21:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/26/2022 23:21:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/26/2022 23:21:20 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.637650389242746 on epoch=168
05/26/2022 23:21:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/26/2022 23:21:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=169
05/26/2022 23:21:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=170
05/26/2022 23:21:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/26/2022 23:21:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
05/26/2022 23:21:38 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.7147349300117538 on epoch=171
05/26/2022 23:21:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
05/26/2022 23:21:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/26/2022 23:21:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.25 on epoch=173
05/26/2022 23:21:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/26/2022 23:21:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=174
05/26/2022 23:21:56 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.7282754164936084 on epoch=174
05/26/2022 23:21:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/26/2022 23:22:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/26/2022 23:22:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=176
05/26/2022 23:22:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=177
05/26/2022 23:22:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=178
05/26/2022 23:22:14 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.6355201484538722 on epoch=178
05/26/2022 23:22:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
05/26/2022 23:22:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/26/2022 23:22:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=179
05/26/2022 23:22:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/26/2022 23:22:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
05/26/2022 23:22:31 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.6804593396991825 on epoch=181
05/26/2022 23:22:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=181
05/26/2022 23:22:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
05/26/2022 23:22:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=183
05/26/2022 23:22:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=183
05/26/2022 23:22:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
05/26/2022 23:22:49 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.7176470588235293 on epoch=184
05/26/2022 23:22:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=184
05/26/2022 23:22:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/26/2022 23:22:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=186
05/26/2022 23:22:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
05/26/2022 23:23:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=187
05/26/2022 23:23:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:23:02 - INFO - __main__ - Printing 3 examples
05/26/2022 23:23:02 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:23:02 - INFO - __main__ - ['false']
05/26/2022 23:23:02 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:23:02 - INFO - __main__ - ['false']
05/26/2022 23:23:02 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:23:02 - INFO - __main__ - ['false']
05/26/2022 23:23:02 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:23:03 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:23:03 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:23:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:23:03 - INFO - __main__ - Printing 3 examples
05/26/2022 23:23:03 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:23:03 - INFO - __main__ - ['false']
05/26/2022 23:23:03 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:23:03 - INFO - __main__ - ['false']
05/26/2022 23:23:03 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:23:03 - INFO - __main__ - ['false']
05/26/2022 23:23:03 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:23:03 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:23:03 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:23:06 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.6663683326267527 on epoch=187
05/26/2022 23:23:06 - INFO - __main__ - save last model!
05/26/2022 23:23:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 23:23:06 - INFO - __main__ - Start tokenizing ... 2733 instances
05/26/2022 23:23:06 - INFO - __main__ - Printing 3 examples
05/26/2022 23:23:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/26/2022 23:23:06 - INFO - __main__ - ['false']
05/26/2022 23:23:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/26/2022 23:23:06 - INFO - __main__ - ['false']
05/26/2022 23:23:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/26/2022 23:23:06 - INFO - __main__ - ['false']
05/26/2022 23:23:06 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:23:08 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:23:10 - INFO - __main__ - Loaded 2733 examples from test data
05/26/2022 23:23:18 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:23:18 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:23:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:23:18 - INFO - __main__ - Starting training!
05/26/2022 23:24:03 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.4_8_predictions.txt
05/26/2022 23:24:03 - INFO - __main__ - Classification-F1 on test data: 0.4136
05/26/2022 23:24:03 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.4, bsz=8, dev_performance=0.7538724495246234, test_performance=0.4135828741326716
05/26/2022 23:24:03 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.3, bsz=8 ...
05/26/2022 23:24:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:24:04 - INFO - __main__ - Printing 3 examples
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:24:04 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:24:04 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:24:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:24:04 - INFO - __main__ - Printing 3 examples
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:24:04 - INFO - __main__ - ['false']
05/26/2022 23:24:04 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:24:05 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:24:05 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:24:23 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:24:23 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:24:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:24:24 - INFO - __main__ - Starting training!
05/26/2022 23:24:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.71 on epoch=0
05/26/2022 23:24:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.07 on epoch=1
05/26/2022 23:24:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=1
05/26/2022 23:24:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=2
05/26/2022 23:24:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=3
05/26/2022 23:24:41 - INFO - __main__ - Global step 50 Train loss 2.11 Classification-F1 0.3333333333333333 on epoch=3
05/26/2022 23:24:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/26/2022 23:24:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=3
05/26/2022 23:24:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
05/26/2022 23:24:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=4
05/26/2022 23:24:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=5
05/26/2022 23:24:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/26/2022 23:24:56 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.4885946573751452 on epoch=6
05/26/2022 23:24:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4885946573751452 on epoch=6, global_step=100
05/26/2022 23:24:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
05/26/2022 23:25:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
05/26/2022 23:25:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
05/26/2022 23:25:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=8
05/26/2022 23:25:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=9
05/26/2022 23:25:13 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=9
05/26/2022 23:25:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
05/26/2022 23:25:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=10
05/26/2022 23:25:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/26/2022 23:25:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/26/2022 23:25:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
05/26/2022 23:25:29 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.36304897101085887 on epoch=12
05/26/2022 23:25:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/26/2022 23:25:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/26/2022 23:25:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
05/26/2022 23:25:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/26/2022 23:25:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
05/26/2022 23:25:46 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.36318407960199 on epoch=15
05/26/2022 23:25:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
05/26/2022 23:25:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/26/2022 23:25:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=17
05/26/2022 23:25:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=18
05/26/2022 23:25:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
05/26/2022 23:26:01 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.350463149416029 on epoch=18
05/26/2022 23:26:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=19
05/26/2022 23:26:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
05/26/2022 23:26:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=20
05/26/2022 23:26:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
05/26/2022 23:26:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=21
05/26/2022 23:26:17 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.39405460814462767 on epoch=21
05/26/2022 23:26:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=22
05/26/2022 23:26:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=23
05/26/2022 23:26:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=23
05/26/2022 23:26:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=24
05/26/2022 23:26:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/26/2022 23:26:33 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/26/2022 23:26:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
05/26/2022 23:26:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
05/26/2022 23:26:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=26
05/26/2022 23:26:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
05/26/2022 23:26:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
05/26/2022 23:26:49 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.4083606042117329 on epoch=28
05/26/2022 23:26:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/26/2022 23:26:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
05/26/2022 23:26:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/26/2022 23:26:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
05/26/2022 23:27:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/26/2022 23:27:04 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.583394743306313 on epoch=31
05/26/2022 23:27:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4885946573751452 -> 0.583394743306313 on epoch=31, global_step=500
05/26/2022 23:27:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/26/2022 23:27:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
05/26/2022 23:27:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/26/2022 23:27:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
05/26/2022 23:27:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=34
05/26/2022 23:27:20 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.33159268929503916 on epoch=34
05/26/2022 23:27:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/26/2022 23:27:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/26/2022 23:27:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/26/2022 23:27:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
05/26/2022 23:27:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/26/2022 23:27:36 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.4342541436464089 on epoch=37
05/26/2022 23:27:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
05/26/2022 23:27:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=38
05/26/2022 23:27:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
05/26/2022 23:27:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
05/26/2022 23:27:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
05/26/2022 23:27:51 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.4297594297594297 on epoch=40
05/26/2022 23:27:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=41
05/26/2022 23:27:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/26/2022 23:27:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
05/26/2022 23:28:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
05/26/2022 23:28:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=43
05/26/2022 23:28:07 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.35885876860812244 on epoch=43
05/26/2022 23:28:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=44
05/26/2022 23:28:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
05/26/2022 23:28:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=45
05/26/2022 23:28:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=46
05/26/2022 23:28:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/26/2022 23:28:23 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.544213649851632 on epoch=46
05/26/2022 23:28:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
05/26/2022 23:28:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=48
05/26/2022 23:28:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/26/2022 23:28:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
05/26/2022 23:28:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
05/26/2022 23:28:39 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
05/26/2022 23:28:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
05/26/2022 23:28:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/26/2022 23:28:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/26/2022 23:28:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=52
05/26/2022 23:28:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=53
05/26/2022 23:28:54 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.35693779904306216 on epoch=53
05/26/2022 23:28:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=53
05/26/2022 23:28:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=54
05/26/2022 23:29:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
05/26/2022 23:29:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=55
05/26/2022 23:29:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
05/26/2022 23:29:10 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.6009536035209977 on epoch=56
05/26/2022 23:29:10 - INFO - __main__ - Saving model with best Classification-F1: 0.583394743306313 -> 0.6009536035209977 on epoch=56, global_step=900
05/26/2022 23:29:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/26/2022 23:29:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
05/26/2022 23:29:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=58
05/26/2022 23:29:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
05/26/2022 23:29:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/26/2022 23:29:27 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=59
05/26/2022 23:29:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/26/2022 23:29:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
05/26/2022 23:29:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/26/2022 23:29:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
05/26/2022 23:29:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=62
05/26/2022 23:29:43 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.4226819494921256 on epoch=62
05/26/2022 23:29:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=63
05/26/2022 23:29:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
05/26/2022 23:29:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
05/26/2022 23:29:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
05/26/2022 23:29:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=65
05/26/2022 23:29:58 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.5895158803576935 on epoch=65
05/26/2022 23:30:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=66
05/26/2022 23:30:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
05/26/2022 23:30:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
05/26/2022 23:30:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
05/26/2022 23:30:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
05/26/2022 23:30:14 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.4025498100179374 on epoch=68
05/26/2022 23:30:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=69
05/26/2022 23:30:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
05/26/2022 23:30:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/26/2022 23:30:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/26/2022 23:30:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/26/2022 23:30:30 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.36119461636703015 on epoch=71
05/26/2022 23:30:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
05/26/2022 23:30:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=73
05/26/2022 23:30:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/26/2022 23:30:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/26/2022 23:30:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=74
05/26/2022 23:30:46 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/26/2022 23:30:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=75
05/26/2022 23:30:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
05/26/2022 23:30:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/26/2022 23:30:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=77
05/26/2022 23:30:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=78
05/26/2022 23:31:02 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.5591249339267141 on epoch=78
05/26/2022 23:31:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=78
05/26/2022 23:31:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
05/26/2022 23:31:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
05/26/2022 23:31:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=80
05/26/2022 23:31:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=81
05/26/2022 23:31:18 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=81
05/26/2022 23:31:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
05/26/2022 23:31:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
05/26/2022 23:31:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
05/26/2022 23:31:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=83
05/26/2022 23:31:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
05/26/2022 23:31:34 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.6476476476476476 on epoch=84
05/26/2022 23:31:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6009536035209977 -> 0.6476476476476476 on epoch=84, global_step=1350
05/26/2022 23:31:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=84
05/26/2022 23:31:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=85
05/26/2022 23:31:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=86
05/26/2022 23:31:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
05/26/2022 23:31:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
05/26/2022 23:31:50 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.599835458658988 on epoch=87
05/26/2022 23:31:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=88
05/26/2022 23:31:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=88
05/26/2022 23:31:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=89
05/26/2022 23:31:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=89
05/26/2022 23:32:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=90
05/26/2022 23:32:05 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.6605612087573234 on epoch=90
05/26/2022 23:32:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6476476476476476 -> 0.6605612087573234 on epoch=90, global_step=1450
05/26/2022 23:32:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
05/26/2022 23:32:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=91
05/26/2022 23:32:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=92
05/26/2022 23:32:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/26/2022 23:32:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
05/26/2022 23:32:21 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.34195559333697656 on epoch=93
05/26/2022 23:32:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
05/26/2022 23:32:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
05/26/2022 23:32:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/26/2022 23:32:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
05/26/2022 23:32:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=96
05/26/2022 23:32:37 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.5009574413028102 on epoch=96
05/26/2022 23:32:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
05/26/2022 23:32:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
05/26/2022 23:32:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=98
05/26/2022 23:32:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=99
05/26/2022 23:32:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=99
05/26/2022 23:32:54 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=99
05/26/2022 23:32:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
05/26/2022 23:32:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
05/26/2022 23:33:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
05/26/2022 23:33:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=102
05/26/2022 23:33:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
05/26/2022 23:33:11 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.509090909090909 on epoch=103
05/26/2022 23:33:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=103
05/26/2022 23:33:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
05/26/2022 23:33:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=104
05/26/2022 23:33:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=105
05/26/2022 23:33:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=106
05/26/2022 23:33:28 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.6222099243681978 on epoch=106
05/26/2022 23:33:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=106
05/26/2022 23:33:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=107
05/26/2022 23:33:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
05/26/2022 23:33:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=108
05/26/2022 23:33:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=109
05/26/2022 23:33:45 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.6356210161519896 on epoch=109
05/26/2022 23:33:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
05/26/2022 23:33:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=110
05/26/2022 23:33:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=111
05/26/2022 23:33:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=111
05/26/2022 23:33:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
05/26/2022 23:34:02 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.6436122202334511 on epoch=112
05/26/2022 23:34:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
05/26/2022 23:34:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=113
05/26/2022 23:34:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=114
05/26/2022 23:34:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=114
05/26/2022 23:34:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=115
05/26/2022 23:34:18 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.6751815332874722 on epoch=115
05/26/2022 23:34:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6605612087573234 -> 0.6751815332874722 on epoch=115, global_step=1850
05/26/2022 23:34:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=116
05/26/2022 23:34:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=116
05/26/2022 23:34:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=117
05/26/2022 23:34:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=118
05/26/2022 23:34:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=118
05/26/2022 23:34:35 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.5355320050869012 on epoch=118
05/26/2022 23:34:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
05/26/2022 23:34:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=119
05/26/2022 23:34:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=120
05/26/2022 23:34:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=121
05/26/2022 23:34:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=121
05/26/2022 23:34:52 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.6209889818754188 on epoch=121
05/26/2022 23:34:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=122
05/26/2022 23:34:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=123
05/26/2022 23:34:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=123
05/26/2022 23:35:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=124
05/26/2022 23:35:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=124
05/26/2022 23:35:08 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.6583290153406504 on epoch=124
05/26/2022 23:35:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=125
05/26/2022 23:35:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=126
05/26/2022 23:35:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=126
05/26/2022 23:35:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=127
05/26/2022 23:35:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
05/26/2022 23:35:25 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.5304540420819491 on epoch=128
05/26/2022 23:35:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=128
05/26/2022 23:35:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.39 on epoch=129
05/26/2022 23:35:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=129
05/26/2022 23:35:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=130
05/26/2022 23:35:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=131
05/26/2022 23:35:42 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.7022344658708295 on epoch=131
05/26/2022 23:35:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6751815332874722 -> 0.7022344658708295 on epoch=131, global_step=2100
05/26/2022 23:35:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=131
05/26/2022 23:35:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=132
05/26/2022 23:35:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=133
05/26/2022 23:35:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=133
05/26/2022 23:35:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=134
05/26/2022 23:35:59 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.6263921960211999 on epoch=134
05/26/2022 23:36:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=134
05/26/2022 23:36:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
05/26/2022 23:36:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/26/2022 23:36:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=136
05/26/2022 23:36:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=137
05/26/2022 23:36:16 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.6507936507936507 on epoch=137
05/26/2022 23:36:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=138
05/26/2022 23:36:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=138
05/26/2022 23:36:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=139
05/26/2022 23:36:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=139
05/26/2022 23:36:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=140
05/26/2022 23:36:33 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.6471699574138599 on epoch=140
05/26/2022 23:36:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.35 on epoch=141
05/26/2022 23:36:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=141
05/26/2022 23:36:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.33 on epoch=142
05/26/2022 23:36:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=143
05/26/2022 23:36:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=143
05/26/2022 23:36:50 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.6349206349206349 on epoch=143
05/26/2022 23:36:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=144
05/26/2022 23:36:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/26/2022 23:36:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=145
05/26/2022 23:36:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/26/2022 23:37:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=146
05/26/2022 23:37:07 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.6502896980545481 on epoch=146
05/26/2022 23:37:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=147
05/26/2022 23:37:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=148
05/26/2022 23:37:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/26/2022 23:37:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=149
05/26/2022 23:37:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
05/26/2022 23:37:24 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.6673545822823512 on epoch=149
05/26/2022 23:37:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=150
05/26/2022 23:37:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=151
05/26/2022 23:37:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=151
05/26/2022 23:37:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=152
05/26/2022 23:37:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.24 on epoch=153
05/26/2022 23:37:40 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.5919942288657373 on epoch=153
05/26/2022 23:37:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.26 on epoch=153
05/26/2022 23:37:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=154
05/26/2022 23:37:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=154
05/26/2022 23:37:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=155
05/26/2022 23:37:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=156
05/26/2022 23:37:57 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.6711524345485687 on epoch=156
05/26/2022 23:38:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/26/2022 23:38:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=157
05/26/2022 23:38:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=158
05/26/2022 23:38:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=158
05/26/2022 23:38:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.26 on epoch=159
05/26/2022 23:38:14 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.6549019607843137 on epoch=159
05/26/2022 23:38:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/26/2022 23:38:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=160
05/26/2022 23:38:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=161
05/26/2022 23:38:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/26/2022 23:38:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.27 on epoch=162
05/26/2022 23:38:31 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.6489963180354639 on epoch=162
05/26/2022 23:38:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.27 on epoch=163
05/26/2022 23:38:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=163
05/26/2022 23:38:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=164
05/26/2022 23:38:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=164
05/26/2022 23:38:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=165
05/26/2022 23:38:48 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.5932314446246334 on epoch=165
05/26/2022 23:38:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/26/2022 23:38:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=166
05/26/2022 23:38:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.25 on epoch=167
05/26/2022 23:38:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
05/26/2022 23:39:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/26/2022 23:39:06 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.6586509724585039 on epoch=168
05/26/2022 23:39:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=169
05/26/2022 23:39:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
05/26/2022 23:39:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=170
05/26/2022 23:39:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
05/26/2022 23:39:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
05/26/2022 23:39:22 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.6868118424272083 on epoch=171
05/26/2022 23:39:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
05/26/2022 23:39:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=173
05/26/2022 23:39:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/26/2022 23:39:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=174
05/26/2022 23:39:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=174
05/26/2022 23:39:39 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.6768472906403942 on epoch=174
05/26/2022 23:39:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=175
05/26/2022 23:39:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=176
05/26/2022 23:39:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=176
05/26/2022 23:39:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=177
05/26/2022 23:39:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=178
05/26/2022 23:39:56 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.6483016722724905 on epoch=178
05/26/2022 23:39:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
05/26/2022 23:40:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=179
05/26/2022 23:40:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=179
05/26/2022 23:40:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=180
05/26/2022 23:40:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/26/2022 23:40:13 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.6457215717373388 on epoch=181
05/26/2022 23:40:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=181
05/26/2022 23:40:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=182
05/26/2022 23:40:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
05/26/2022 23:40:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=183
05/26/2022 23:40:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=184
05/26/2022 23:40:30 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.6536828847320055 on epoch=184
05/26/2022 23:40:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=184
05/26/2022 23:40:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=185
05/26/2022 23:40:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=186
05/26/2022 23:40:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=186
05/26/2022 23:40:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=187
05/26/2022 23:40:43 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:40:43 - INFO - __main__ - Printing 3 examples
05/26/2022 23:40:43 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:40:43 - INFO - __main__ - ['false']
05/26/2022 23:40:43 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:40:43 - INFO - __main__ - ['false']
05/26/2022 23:40:43 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:40:43 - INFO - __main__ - ['false']
05/26/2022 23:40:43 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:40:43 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:40:44 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:40:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:40:44 - INFO - __main__ - Printing 3 examples
05/26/2022 23:40:44 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:40:44 - INFO - __main__ - ['false']
05/26/2022 23:40:44 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:40:44 - INFO - __main__ - ['false']
05/26/2022 23:40:44 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:40:44 - INFO - __main__ - ['false']
05/26/2022 23:40:44 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:40:44 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:40:44 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:40:47 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.6080895979373137 on epoch=187
05/26/2022 23:40:47 - INFO - __main__ - save last model!
05/26/2022 23:40:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 23:40:47 - INFO - __main__ - Start tokenizing ... 2733 instances
05/26/2022 23:40:47 - INFO - __main__ - Printing 3 examples
05/26/2022 23:40:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/26/2022 23:40:47 - INFO - __main__ - ['false']
05/26/2022 23:40:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/26/2022 23:40:47 - INFO - __main__ - ['false']
05/26/2022 23:40:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/26/2022 23:40:47 - INFO - __main__ - ['false']
05/26/2022 23:40:47 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:40:48 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:40:51 - INFO - __main__ - Loaded 2733 examples from test data
05/26/2022 23:41:03 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:41:03 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:41:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:41:03 - INFO - __main__ - Starting training!
05/26/2022 23:41:46 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.3_8_predictions.txt
05/26/2022 23:41:46 - INFO - __main__ - Classification-F1 on test data: 0.3762
05/26/2022 23:41:46 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.3, bsz=8, dev_performance=0.7022344658708295, test_performance=0.376224920702252
05/26/2022 23:41:46 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.2, bsz=8 ...
05/26/2022 23:41:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:41:47 - INFO - __main__ - Printing 3 examples
05/26/2022 23:41:47 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/26/2022 23:41:47 - INFO - __main__ - ['false']
05/26/2022 23:41:47 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/26/2022 23:41:47 - INFO - __main__ - ['false']
05/26/2022 23:41:47 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/26/2022 23:41:47 - INFO - __main__ - ['false']
05/26/2022 23:41:47 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:41:47 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:41:48 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:41:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:41:48 - INFO - __main__ - Printing 3 examples
05/26/2022 23:41:48 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/26/2022 23:41:48 - INFO - __main__ - ['false']
05/26/2022 23:41:48 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/26/2022 23:41:48 - INFO - __main__ - ['false']
05/26/2022 23:41:48 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/26/2022 23:41:48 - INFO - __main__ - ['false']
05/26/2022 23:41:48 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:41:48 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:41:48 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:42:03 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:42:03 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:42:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:42:03 - INFO - __main__ - Starting training!
05/26/2022 23:42:06 - INFO - __main__ - Step 10 Global step 10 Train loss 7.59 on epoch=0
05/26/2022 23:42:09 - INFO - __main__ - Step 20 Global step 20 Train loss 3.92 on epoch=1
05/26/2022 23:42:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.61 on epoch=1
05/26/2022 23:42:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.82 on epoch=2
05/26/2022 23:42:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=3
05/26/2022 23:42:36 - INFO - __main__ - Global step 50 Train loss 2.91 Classification-F1 0.3333333333333333 on epoch=3
05/26/2022 23:42:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/26/2022 23:42:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/26/2022 23:42:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=4
05/26/2022 23:42:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/26/2022 23:42:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=5
05/26/2022 23:42:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=6
05/26/2022 23:42:57 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.38904444235647845 on epoch=6
05/26/2022 23:42:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.38904444235647845 on epoch=6, global_step=100
05/26/2022 23:42:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=6
05/26/2022 23:43:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
05/26/2022 23:43:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=8
05/26/2022 23:43:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/26/2022 23:43:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
05/26/2022 23:43:15 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.34195559333697656 on epoch=9
05/26/2022 23:43:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/26/2022 23:43:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=10
05/26/2022 23:43:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/26/2022 23:43:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
05/26/2022 23:43:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
05/26/2022 23:43:35 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=12
05/26/2022 23:43:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=13
05/26/2022 23:43:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=13
05/26/2022 23:43:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/26/2022 23:43:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=14
05/26/2022 23:43:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=15
05/26/2022 23:43:51 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.37922403003754696 on epoch=15
05/26/2022 23:43:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=16
05/26/2022 23:43:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
05/26/2022 23:43:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
05/26/2022 23:44:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=18
05/26/2022 23:44:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
05/26/2022 23:44:08 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
05/26/2022 23:44:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/26/2022 23:44:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=19
05/26/2022 23:44:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
05/26/2022 23:44:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
05/26/2022 23:44:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/26/2022 23:44:25 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.5668358714043994 on epoch=21
05/26/2022 23:44:25 - INFO - __main__ - Saving model with best Classification-F1: 0.38904444235647845 -> 0.5668358714043994 on epoch=21, global_step=350
05/26/2022 23:44:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/26/2022 23:44:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
05/26/2022 23:44:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/26/2022 23:44:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=24
05/26/2022 23:44:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/26/2022 23:44:42 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
05/26/2022 23:44:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/26/2022 23:44:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
05/26/2022 23:44:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
05/26/2022 23:44:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=27
05/26/2022 23:44:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=28
05/26/2022 23:44:59 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.34195559333697656 on epoch=28
05/26/2022 23:45:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
05/26/2022 23:45:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/26/2022 23:45:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/26/2022 23:45:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/26/2022 23:45:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
05/26/2022 23:45:16 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.3486005089058525 on epoch=31
05/26/2022 23:45:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/26/2022 23:45:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/26/2022 23:45:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/26/2022 23:45:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/26/2022 23:45:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=34
05/26/2022 23:45:34 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.3970608272506082 on epoch=34
05/26/2022 23:45:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/26/2022 23:45:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/26/2022 23:45:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/26/2022 23:45:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/26/2022 23:45:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=37
05/26/2022 23:45:51 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=37
05/26/2022 23:45:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/26/2022 23:45:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/26/2022 23:45:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=39
05/26/2022 23:46:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/26/2022 23:46:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/26/2022 23:46:08 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.5696558469344092 on epoch=40
05/26/2022 23:46:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5668358714043994 -> 0.5696558469344092 on epoch=40, global_step=650
05/26/2022 23:46:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/26/2022 23:46:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/26/2022 23:46:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=42
05/26/2022 23:46:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/26/2022 23:46:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
05/26/2022 23:46:25 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=43
05/26/2022 23:46:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/26/2022 23:46:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
05/26/2022 23:46:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
05/26/2022 23:46:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/26/2022 23:46:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
05/26/2022 23:46:42 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.49371960019242067 on epoch=46
05/26/2022 23:46:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/26/2022 23:46:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=48
05/26/2022 23:46:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/26/2022 23:46:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
05/26/2022 23:46:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/26/2022 23:46:58 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/26/2022 23:47:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=50
05/26/2022 23:47:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
05/26/2022 23:47:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
05/26/2022 23:47:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
05/26/2022 23:47:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
05/26/2022 23:47:15 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=53
05/26/2022 23:47:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=53
05/26/2022 23:47:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=54
05/26/2022 23:47:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
05/26/2022 23:47:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
05/26/2022 23:47:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=56
05/26/2022 23:47:31 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.5640491958372753 on epoch=56
05/26/2022 23:47:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/26/2022 23:47:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/26/2022 23:47:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
05/26/2022 23:47:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
05/26/2022 23:47:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=59
05/26/2022 23:47:48 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.5550017690402572 on epoch=59
05/26/2022 23:47:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/26/2022 23:47:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
05/26/2022 23:47:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/26/2022 23:47:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
05/26/2022 23:48:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=62
05/26/2022 23:48:05 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.5858945325137497 on epoch=62
05/26/2022 23:48:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5696558469344092 -> 0.5858945325137497 on epoch=62, global_step=1000
05/26/2022 23:48:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=63
05/26/2022 23:48:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
05/26/2022 23:48:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=64
05/26/2022 23:48:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
05/26/2022 23:48:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
05/26/2022 23:48:21 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.518467852257182 on epoch=65
05/26/2022 23:48:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=66
05/26/2022 23:48:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
05/26/2022 23:48:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
05/26/2022 23:48:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/26/2022 23:48:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/26/2022 23:48:38 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.4837222702391241 on epoch=68
05/26/2022 23:48:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/26/2022 23:48:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/26/2022 23:48:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
05/26/2022 23:48:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=71
05/26/2022 23:48:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=71
05/26/2022 23:48:55 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.5743842364532019 on epoch=71
05/26/2022 23:48:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
05/26/2022 23:49:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
05/26/2022 23:49:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/26/2022 23:49:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
05/26/2022 23:49:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/26/2022 23:49:11 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.41763682590659706 on epoch=74
05/26/2022 23:49:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=75
05/26/2022 23:49:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/26/2022 23:49:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/26/2022 23:49:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
05/26/2022 23:49:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
05/26/2022 23:49:28 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=78
05/26/2022 23:49:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/26/2022 23:49:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=79
05/26/2022 23:49:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/26/2022 23:49:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
05/26/2022 23:49:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/26/2022 23:49:44 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.6300805509438603 on epoch=81
05/26/2022 23:49:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5858945325137497 -> 0.6300805509438603 on epoch=81, global_step=1300
05/26/2022 23:49:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
05/26/2022 23:49:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
05/26/2022 23:49:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
05/26/2022 23:49:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
05/26/2022 23:49:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
05/26/2022 23:50:01 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.494320987654321 on epoch=84
05/26/2022 23:50:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
05/26/2022 23:50:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=85
05/26/2022 23:50:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
05/26/2022 23:50:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
05/26/2022 23:50:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/26/2022 23:50:18 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.6399103485586212 on epoch=87
05/26/2022 23:50:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6300805509438603 -> 0.6399103485586212 on epoch=87, global_step=1400
05/26/2022 23:50:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=88
05/26/2022 23:50:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=88
05/26/2022 23:50:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=89
05/26/2022 23:50:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
05/26/2022 23:50:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
05/26/2022 23:50:35 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.6014943960149439 on epoch=90
05/26/2022 23:50:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/26/2022 23:50:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=91
05/26/2022 23:50:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
05/26/2022 23:50:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=93
05/26/2022 23:50:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/26/2022 23:50:51 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.5119639389242254 on epoch=93
05/26/2022 23:50:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
05/26/2022 23:50:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
05/26/2022 23:50:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=95
05/26/2022 23:51:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
05/26/2022 23:51:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=96
05/26/2022 23:51:08 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.6392156862745098 on epoch=96
05/26/2022 23:51:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
05/26/2022 23:51:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=98
05/26/2022 23:51:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
05/26/2022 23:51:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=99
05/26/2022 23:51:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=99
05/26/2022 23:51:26 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.5701369155257039 on epoch=99
05/26/2022 23:51:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/26/2022 23:51:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
05/26/2022 23:51:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=101
05/26/2022 23:51:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
05/26/2022 23:51:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=103
05/26/2022 23:51:42 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.6090126119884745 on epoch=103
05/26/2022 23:51:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=103
05/26/2022 23:51:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
05/26/2022 23:51:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=104
05/26/2022 23:51:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=105
05/26/2022 23:51:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=106
05/26/2022 23:52:00 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.5485885302920004 on epoch=106
05/26/2022 23:52:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
05/26/2022 23:52:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=107
05/26/2022 23:52:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=108
05/26/2022 23:52:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.28 on epoch=108
05/26/2022 23:52:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=109
05/26/2022 23:52:17 - INFO - __main__ - Global step 1750 Train loss 0.31 Classification-F1 0.5647572892667038 on epoch=109
05/26/2022 23:52:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=109
05/26/2022 23:52:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=110
05/26/2022 23:52:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=111
05/26/2022 23:52:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=111
05/26/2022 23:52:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=112
05/26/2022 23:52:35 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.608203244566881 on epoch=112
05/26/2022 23:52:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=113
05/26/2022 23:52:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=113
05/26/2022 23:52:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=114
05/26/2022 23:52:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=114
05/26/2022 23:52:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=115
05/26/2022 23:52:52 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.4863404846426918 on epoch=115
05/26/2022 23:52:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=116
05/26/2022 23:52:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=116
05/26/2022 23:52:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=117
05/26/2022 23:53:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=118
05/26/2022 23:53:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=118
05/26/2022 23:53:09 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.48622986761109704 on epoch=118
05/26/2022 23:53:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=119
05/26/2022 23:53:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
05/26/2022 23:53:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=120
05/26/2022 23:53:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=121
05/26/2022 23:53:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=121
05/26/2022 23:53:26 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.5812581212260185 on epoch=121
05/26/2022 23:53:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=122
05/26/2022 23:53:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=123
05/26/2022 23:53:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=123
05/26/2022 23:53:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=124
05/26/2022 23:53:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=124
05/26/2022 23:53:44 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.5208523299899218 on epoch=124
05/26/2022 23:53:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
05/26/2022 23:53:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=126
05/26/2022 23:53:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/26/2022 23:53:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=127
05/26/2022 23:53:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=128
05/26/2022 23:54:01 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.5917315831442066 on epoch=128
05/26/2022 23:54:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=128
05/26/2022 23:54:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=129
05/26/2022 23:54:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=129
05/26/2022 23:54:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/26/2022 23:54:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=131
05/26/2022 23:54:19 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.5215686274509803 on epoch=131
05/26/2022 23:54:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=131
05/26/2022 23:54:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/26/2022 23:54:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=133
05/26/2022 23:54:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=133
05/26/2022 23:54:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=134
05/26/2022 23:54:36 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.4740190880169671 on epoch=134
05/26/2022 23:54:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=134
05/26/2022 23:54:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.22 on epoch=135
05/26/2022 23:54:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/26/2022 23:54:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
05/26/2022 23:54:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=137
05/26/2022 23:54:54 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.5492957746478874 on epoch=137
05/26/2022 23:54:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=138
05/26/2022 23:54:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
05/26/2022 23:55:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=139
05/26/2022 23:55:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=139
05/26/2022 23:55:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/26/2022 23:55:12 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.515625 on epoch=140
05/26/2022 23:55:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=141
05/26/2022 23:55:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=141
05/26/2022 23:55:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/26/2022 23:55:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=143
05/26/2022 23:55:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
05/26/2022 23:55:29 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.5484869113187698 on epoch=143
05/26/2022 23:55:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/26/2022 23:55:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=144
05/26/2022 23:55:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/26/2022 23:55:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
05/26/2022 23:55:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=146
05/26/2022 23:55:47 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.6015381798205457 on epoch=146
05/26/2022 23:55:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.27 on epoch=147
05/26/2022 23:55:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.26 on epoch=148
05/26/2022 23:55:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/26/2022 23:55:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/26/2022 23:55:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/26/2022 23:56:05 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.5607697658242522 on epoch=149
05/26/2022 23:56:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
05/26/2022 23:56:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
05/26/2022 23:56:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/26/2022 23:56:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/26/2022 23:56:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/26/2022 23:56:23 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.5273365377279318 on epoch=153
05/26/2022 23:56:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=153
05/26/2022 23:56:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/26/2022 23:56:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=154
05/26/2022 23:56:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
05/26/2022 23:56:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
05/26/2022 23:56:41 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.5585331237505151 on epoch=156
05/26/2022 23:56:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
05/26/2022 23:56:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/26/2022 23:56:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=158
05/26/2022 23:56:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
05/26/2022 23:56:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/26/2022 23:56:58 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.5289041182816169 on epoch=159
05/26/2022 23:57:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
05/26/2022 23:57:03 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=160
05/26/2022 23:57:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=161
05/26/2022 23:57:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=161
05/26/2022 23:57:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
05/26/2022 23:57:16 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.5372549019607842 on epoch=162
05/26/2022 23:57:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=163
05/26/2022 23:57:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/26/2022 23:57:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=164
05/26/2022 23:57:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
05/26/2022 23:57:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/26/2022 23:57:34 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.559813313682142 on epoch=165
05/26/2022 23:57:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/26/2022 23:57:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
05/26/2022 23:57:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=167
05/26/2022 23:57:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=168
05/26/2022 23:57:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/26/2022 23:57:51 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.5676737902235323 on epoch=168
05/26/2022 23:57:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/26/2022 23:57:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=169
05/26/2022 23:57:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
05/26/2022 23:58:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=171
05/26/2022 23:58:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
05/26/2022 23:58:09 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.559813313682142 on epoch=171
05/26/2022 23:58:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=172
05/26/2022 23:58:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/26/2022 23:58:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/26/2022 23:58:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/26/2022 23:58:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
05/26/2022 23:58:27 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.5352395772547494 on epoch=174
05/26/2022 23:58:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=175
05/26/2022 23:58:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=176
05/26/2022 23:58:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/26/2022 23:58:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
05/26/2022 23:58:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/26/2022 23:58:45 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.5880270357257805 on epoch=178
05/26/2022 23:58:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
05/26/2022 23:58:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=179
05/26/2022 23:58:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/26/2022 23:58:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
05/26/2022 23:58:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
05/26/2022 23:59:02 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.5698924731182795 on epoch=181
05/26/2022 23:59:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=181
05/26/2022 23:59:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=182
05/26/2022 23:59:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=183
05/26/2022 23:59:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
05/26/2022 23:59:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=184
05/26/2022 23:59:20 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.4947797300738477 on epoch=184
05/26/2022 23:59:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=184
05/26/2022 23:59:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/26/2022 23:59:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=186
05/26/2022 23:59:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
05/26/2022 23:59:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=187
05/26/2022 23:59:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:59:34 - INFO - __main__ - Printing 3 examples
05/26/2022 23:59:34 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/26/2022 23:59:34 - INFO - __main__ - ['false']
05/26/2022 23:59:34 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/26/2022 23:59:34 - INFO - __main__ - ['false']
05/26/2022 23:59:34 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/26/2022 23:59:34 - INFO - __main__ - ['false']
05/26/2022 23:59:34 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:59:34 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:59:35 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 23:59:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 23:59:35 - INFO - __main__ - Printing 3 examples
05/26/2022 23:59:35 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/26/2022 23:59:35 - INFO - __main__ - ['false']
05/26/2022 23:59:35 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/26/2022 23:59:35 - INFO - __main__ - ['false']
05/26/2022 23:59:35 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/26/2022 23:59:35 - INFO - __main__ - ['false']
05/26/2022 23:59:35 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:59:35 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:59:35 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 23:59:38 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.5356330320359097 on epoch=187
05/26/2022 23:59:38 - INFO - __main__ - save last model!
05/26/2022 23:59:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 23:59:38 - INFO - __main__ - Start tokenizing ... 2733 instances
05/26/2022 23:59:38 - INFO - __main__ - Printing 3 examples
05/26/2022 23:59:38 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/26/2022 23:59:38 - INFO - __main__ - ['false']
05/26/2022 23:59:38 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/26/2022 23:59:38 - INFO - __main__ - ['false']
05/26/2022 23:59:38 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/26/2022 23:59:38 - INFO - __main__ - ['false']
05/26/2022 23:59:38 - INFO - __main__ - Tokenizing Input ...
05/26/2022 23:59:40 - INFO - __main__ - Tokenizing Output ...
05/26/2022 23:59:42 - INFO - __main__ - Loaded 2733 examples from test data
05/26/2022 23:59:54 - INFO - __main__ - try to initialize prompt embeddings
05/26/2022 23:59:54 - INFO - __main__ - task name: wiki_qa
05/26/2022 23:59:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 23:59:54 - INFO - __main__ - Starting training!
05/27/2022 00:00:36 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.2_8_predictions.txt
05/27/2022 00:00:36 - INFO - __main__ - Classification-F1 on test data: 0.4533
05/27/2022 00:00:36 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.2, bsz=8, dev_performance=0.6399103485586212, test_performance=0.4532536165995545
05/27/2022 00:00:36 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.5, bsz=8 ...
05/27/2022 00:00:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:00:37 - INFO - __main__ - Printing 3 examples
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:00:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:00:37 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:00:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:00:37 - INFO - __main__ - Printing 3 examples
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:00:37 - INFO - __main__ - ['false']
05/27/2022 00:00:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:00:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:00:38 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:00:52 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:00:52 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:00:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:00:53 - INFO - __main__ - Starting training!
05/27/2022 00:00:56 - INFO - __main__ - Step 10 Global step 10 Train loss 5.24 on epoch=0
05/27/2022 00:00:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.89 on epoch=1
05/27/2022 00:01:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=1
05/27/2022 00:01:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=2
05/27/2022 00:01:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=3
05/27/2022 00:01:11 - INFO - __main__ - Global step 50 Train loss 1.56 Classification-F1 0.350463149416029 on epoch=3
05/27/2022 00:01:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.350463149416029 on epoch=3, global_step=50
05/27/2022 00:01:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=3
05/27/2022 00:01:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=4
05/27/2022 00:01:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=4
05/27/2022 00:01:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
05/27/2022 00:01:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
05/27/2022 00:01:28 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 00:01:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
05/27/2022 00:01:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=7
05/27/2022 00:01:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.78 on epoch=8
05/27/2022 00:01:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=8
05/27/2022 00:01:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
05/27/2022 00:01:45 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 00:01:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/27/2022 00:01:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
05/27/2022 00:01:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=11
05/27/2022 00:01:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
05/27/2022 00:01:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
05/27/2022 00:02:02 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.4145983208425832 on epoch=12
05/27/2022 00:02:02 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.4145983208425832 on epoch=12, global_step=200
05/27/2022 00:02:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/27/2022 00:02:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
05/27/2022 00:02:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=14
05/27/2022 00:02:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=14
05/27/2022 00:02:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/27/2022 00:02:19 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.43160127253446456 on epoch=15
05/27/2022 00:02:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4145983208425832 -> 0.43160127253446456 on epoch=15, global_step=250
05/27/2022 00:02:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/27/2022 00:02:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
05/27/2022 00:02:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
05/27/2022 00:02:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
05/27/2022 00:02:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/27/2022 00:02:36 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=18
05/27/2022 00:02:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 00:02:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/27/2022 00:02:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/27/2022 00:02:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
05/27/2022 00:02:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
05/27/2022 00:02:54 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3591989987484355 on epoch=21
05/27/2022 00:02:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/27/2022 00:02:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/27/2022 00:03:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=23
05/27/2022 00:03:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=24
05/27/2022 00:03:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=24
05/27/2022 00:03:11 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 00:03:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=25
05/27/2022 00:03:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=26
05/27/2022 00:03:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/27/2022 00:03:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 00:03:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
05/27/2022 00:03:29 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.36318407960199 on epoch=28
05/27/2022 00:03:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/27/2022 00:03:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=29
05/27/2022 00:03:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=29
05/27/2022 00:03:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 00:03:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
05/27/2022 00:03:46 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=31
05/27/2022 00:03:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
05/27/2022 00:03:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/27/2022 00:03:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/27/2022 00:03:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/27/2022 00:03:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=34
05/27/2022 00:04:03 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
05/27/2022 00:04:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/27/2022 00:04:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/27/2022 00:04:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/27/2022 00:04:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
05/27/2022 00:04:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/27/2022 00:04:21 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.50778245742538 on epoch=37
05/27/2022 00:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.43160127253446456 -> 0.50778245742538 on epoch=37, global_step=600
05/27/2022 00:04:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/27/2022 00:04:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/27/2022 00:04:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
05/27/2022 00:04:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
05/27/2022 00:04:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
05/27/2022 00:04:37 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.4781408768738631 on epoch=40
05/27/2022 00:04:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
05/27/2022 00:04:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/27/2022 00:04:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=42
05/27/2022 00:04:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
05/27/2022 00:04:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/27/2022 00:04:54 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 00:04:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/27/2022 00:05:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/27/2022 00:05:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
05/27/2022 00:05:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/27/2022 00:05:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/27/2022 00:05:12 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=46
05/27/2022 00:05:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/27/2022 00:05:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/27/2022 00:05:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
05/27/2022 00:05:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
05/27/2022 00:05:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/27/2022 00:05:29 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 00:05:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/27/2022 00:05:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=51
05/27/2022 00:05:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
05/27/2022 00:05:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
05/27/2022 00:05:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
05/27/2022 00:05:46 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=53
05/27/2022 00:05:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=53
05/27/2022 00:05:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
05/27/2022 00:05:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=54
05/27/2022 00:05:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
05/27/2022 00:05:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/27/2022 00:06:03 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.4842006978229888 on epoch=56
05/27/2022 00:06:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/27/2022 00:06:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=57
05/27/2022 00:06:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/27/2022 00:06:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
05/27/2022 00:06:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/27/2022 00:06:21 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=59
05/27/2022 00:06:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
05/27/2022 00:06:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
05/27/2022 00:06:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=61
05/27/2022 00:06:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/27/2022 00:06:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=62
05/27/2022 00:06:38 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.5313735443875275 on epoch=62
05/27/2022 00:06:38 - INFO - __main__ - Saving model with best Classification-F1: 0.50778245742538 -> 0.5313735443875275 on epoch=62, global_step=1000
05/27/2022 00:06:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=63
05/27/2022 00:06:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/27/2022 00:06:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 00:06:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
05/27/2022 00:06:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
05/27/2022 00:06:55 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.45548192029813495 on epoch=65
05/27/2022 00:06:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
05/27/2022 00:07:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=66
05/27/2022 00:07:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/27/2022 00:07:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/27/2022 00:07:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/27/2022 00:07:12 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=68
05/27/2022 00:07:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/27/2022 00:07:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=69
05/27/2022 00:07:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/27/2022 00:07:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 00:07:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/27/2022 00:07:30 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.4655355249204667 on epoch=71
05/27/2022 00:07:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/27/2022 00:07:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
05/27/2022 00:07:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
05/27/2022 00:07:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
05/27/2022 00:07:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
05/27/2022 00:07:48 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 00:07:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 00:07:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/27/2022 00:07:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/27/2022 00:07:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
05/27/2022 00:08:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
05/27/2022 00:08:05 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3671451355661882 on epoch=78
05/27/2022 00:08:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
05/27/2022 00:08:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=79
05/27/2022 00:08:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
05/27/2022 00:08:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
05/27/2022 00:08:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/27/2022 00:08:22 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.46980910944498855 on epoch=81
05/27/2022 00:08:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=81
05/27/2022 00:08:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
05/27/2022 00:08:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=83
05/27/2022 00:08:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
05/27/2022 00:08:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
05/27/2022 00:08:40 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.45718194254445965 on epoch=84
05/27/2022 00:08:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
05/27/2022 00:08:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
05/27/2022 00:08:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=86
05/27/2022 00:08:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
05/27/2022 00:08:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
05/27/2022 00:08:57 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.5102552844508562 on epoch=87
05/27/2022 00:09:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
05/27/2022 00:09:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
05/27/2022 00:09:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
05/27/2022 00:09:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
05/27/2022 00:09:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
05/27/2022 00:09:14 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.5398724940471618 on epoch=90
05/27/2022 00:09:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5313735443875275 -> 0.5398724940471618 on epoch=90, global_step=1450
05/27/2022 00:09:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
05/27/2022 00:09:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
05/27/2022 00:09:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
05/27/2022 00:09:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
05/27/2022 00:09:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=93
05/27/2022 00:09:32 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=93
05/27/2022 00:09:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=94
05/27/2022 00:09:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=94
05/27/2022 00:09:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/27/2022 00:09:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=96
05/27/2022 00:09:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.79 on epoch=96
05/27/2022 00:09:51 - INFO - __main__ - Global step 1550 Train loss 0.65 Classification-F1 0.5228331780055917 on epoch=96
05/27/2022 00:09:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.41 on epoch=97
05/27/2022 00:09:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=98
05/27/2022 00:09:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=98
05/27/2022 00:10:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=99
05/27/2022 00:10:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=99
05/27/2022 00:10:08 - INFO - __main__ - Global step 1600 Train loss 0.60 Classification-F1 0.4324536937710076 on epoch=99
05/27/2022 00:10:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=100
05/27/2022 00:10:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=101
05/27/2022 00:10:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=101
05/27/2022 00:10:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=102
05/27/2022 00:10:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=103
05/27/2022 00:10:26 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.40030721966205834 on epoch=103
05/27/2022 00:10:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=103
05/27/2022 00:10:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=104
05/27/2022 00:10:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=104
05/27/2022 00:10:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=105
05/27/2022 00:10:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=106
05/27/2022 00:10:43 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.38279939051439815 on epoch=106
05/27/2022 00:10:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=106
05/27/2022 00:10:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=107
05/27/2022 00:10:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=108
05/27/2022 00:10:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=108
05/27/2022 00:10:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
05/27/2022 00:11:00 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.41296252132597094 on epoch=109
05/27/2022 00:11:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=109
05/27/2022 00:11:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=110
05/27/2022 00:11:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=111
05/27/2022 00:11:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=111
05/27/2022 00:11:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=112
05/27/2022 00:11:17 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.39267460026616785 on epoch=112
05/27/2022 00:11:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
05/27/2022 00:11:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=113
05/27/2022 00:11:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=114
05/27/2022 00:11:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=114
05/27/2022 00:11:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=115
05/27/2022 00:11:34 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.40030721966205834 on epoch=115
05/27/2022 00:11:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=116
05/27/2022 00:11:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=116
05/27/2022 00:11:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=117
05/27/2022 00:11:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=118
05/27/2022 00:11:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=118
05/27/2022 00:11:52 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.4944691508661518 on epoch=118
05/27/2022 00:11:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=119
05/27/2022 00:11:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=119
05/27/2022 00:12:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=120
05/27/2022 00:12:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=121
05/27/2022 00:12:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=121
05/27/2022 00:12:09 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.4679241987599337 on epoch=121
05/27/2022 00:12:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=122
05/27/2022 00:12:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=123
05/27/2022 00:12:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=123
05/27/2022 00:12:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=124
05/27/2022 00:12:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=124
05/27/2022 00:12:27 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.4178268345443452 on epoch=124
05/27/2022 00:12:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=125
05/27/2022 00:12:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=126
05/27/2022 00:12:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=126
05/27/2022 00:12:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
05/27/2022 00:12:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=128
05/27/2022 00:12:44 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.4834421364985163 on epoch=128
05/27/2022 00:12:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=128
05/27/2022 00:12:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=129
05/27/2022 00:12:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=129
05/27/2022 00:12:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=130
05/27/2022 00:12:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=131
05/27/2022 00:13:02 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.4961502731810238 on epoch=131
05/27/2022 00:13:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
05/27/2022 00:13:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=132
05/27/2022 00:13:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=133
05/27/2022 00:13:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=133
05/27/2022 00:13:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
05/27/2022 00:13:19 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.48622986761109704 on epoch=134
05/27/2022 00:13:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=134
05/27/2022 00:13:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=135
05/27/2022 00:13:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=136
05/27/2022 00:13:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=136
05/27/2022 00:13:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.39 on epoch=137
05/27/2022 00:13:36 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.38279939051439815 on epoch=137
05/27/2022 00:13:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=138
05/27/2022 00:13:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=138
05/27/2022 00:13:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=139
05/27/2022 00:13:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=139
05/27/2022 00:13:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=140
05/27/2022 00:13:54 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.48891908668444983 on epoch=140
05/27/2022 00:13:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.37 on epoch=141
05/27/2022 00:13:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=141
05/27/2022 00:14:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=142
05/27/2022 00:14:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=143
05/27/2022 00:14:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=143
05/27/2022 00:14:11 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.48032949150267584 on epoch=143
05/27/2022 00:14:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=144
05/27/2022 00:14:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=144
05/27/2022 00:14:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=145
05/27/2022 00:14:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=146
05/27/2022 00:14:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=146
05/27/2022 00:14:28 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.4785162726798996 on epoch=146
05/27/2022 00:14:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=147
05/27/2022 00:14:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=148
05/27/2022 00:14:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=148
05/27/2022 00:14:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=149
05/27/2022 00:14:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.34 on epoch=149
05/27/2022 00:14:46 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.45739720220432384 on epoch=149
05/27/2022 00:14:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=150
05/27/2022 00:14:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=151
05/27/2022 00:14:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=151
05/27/2022 00:14:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=152
05/27/2022 00:14:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=153
05/27/2022 00:15:03 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.4655355249204667 on epoch=153
05/27/2022 00:15:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
05/27/2022 00:15:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.34 on epoch=154
05/27/2022 00:15:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=154
05/27/2022 00:15:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=155
05/27/2022 00:15:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.42 on epoch=156
05/27/2022 00:15:21 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.47989953768136806 on epoch=156
05/27/2022 00:15:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
05/27/2022 00:15:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=157
05/27/2022 00:15:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=158
05/27/2022 00:15:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=158
05/27/2022 00:15:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.38 on epoch=159
05/27/2022 00:15:38 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.46895191457525676 on epoch=159
05/27/2022 00:15:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=159
05/27/2022 00:15:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=160
05/27/2022 00:15:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=161
05/27/2022 00:15:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=161
05/27/2022 00:15:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.38 on epoch=162
05/27/2022 00:15:55 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.46281389748882007 on epoch=162
05/27/2022 00:15:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.38 on epoch=163
05/27/2022 00:16:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.40 on epoch=163
05/27/2022 00:16:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=164
05/27/2022 00:16:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=164
05/27/2022 00:16:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=165
05/27/2022 00:16:12 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.3383422492035824 on epoch=165
05/27/2022 00:16:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=166
05/27/2022 00:16:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=166
05/27/2022 00:16:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.38 on epoch=167
05/27/2022 00:16:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=168
05/27/2022 00:16:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=168
05/27/2022 00:16:29 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.45003929787791463 on epoch=168
05/27/2022 00:16:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.36 on epoch=169
05/27/2022 00:16:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=169
05/27/2022 00:16:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.41 on epoch=170
05/27/2022 00:16:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=171
05/27/2022 00:16:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=171
05/27/2022 00:16:45 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.46173254835996635 on epoch=171
05/27/2022 00:16:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=172
05/27/2022 00:16:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=173
05/27/2022 00:16:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=173
05/27/2022 00:16:56 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=174
05/27/2022 00:16:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=174
05/27/2022 00:17:02 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.35501021683496337 on epoch=174
05/27/2022 00:17:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.33 on epoch=175
05/27/2022 00:17:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=176
05/27/2022 00:17:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=176
05/27/2022 00:17:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=177
05/27/2022 00:17:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.41 on epoch=178
05/27/2022 00:17:19 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.46980910944498855 on epoch=178
05/27/2022 00:17:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.36 on epoch=178
05/27/2022 00:17:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=179
05/27/2022 00:17:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=179
05/27/2022 00:17:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=180
05/27/2022 00:17:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.34 on epoch=181
05/27/2022 00:17:36 - INFO - __main__ - Global step 2900 Train loss 0.38 Classification-F1 0.4376260443676175 on epoch=181
05/27/2022 00:17:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=181
05/27/2022 00:17:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=182
05/27/2022 00:17:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.38 on epoch=183
05/27/2022 00:17:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.38 on epoch=183
05/27/2022 00:17:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=184
05/27/2022 00:17:52 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=184
05/27/2022 00:17:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=184
05/27/2022 00:17:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.37 on epoch=185
05/27/2022 00:18:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.38 on epoch=186
05/27/2022 00:18:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.38 on epoch=186
05/27/2022 00:18:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=187
05/27/2022 00:18:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:18:07 - INFO - __main__ - Printing 3 examples
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:18:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:18:07 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:18:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:18:07 - INFO - __main__ - Printing 3 examples
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:18:07 - INFO - __main__ - ['false']
05/27/2022 00:18:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:18:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:18:07 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:18:09 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.3784863604213263 on epoch=187
05/27/2022 00:18:09 - INFO - __main__ - save last model!
05/27/2022 00:18:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 00:18:09 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 00:18:09 - INFO - __main__ - Printing 3 examples
05/27/2022 00:18:09 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 00:18:09 - INFO - __main__ - ['false']
05/27/2022 00:18:09 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 00:18:09 - INFO - __main__ - ['false']
05/27/2022 00:18:09 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 00:18:09 - INFO - __main__ - ['false']
05/27/2022 00:18:09 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:18:10 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:18:13 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 00:18:22 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:18:22 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:18:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:18:23 - INFO - __main__ - Starting training!
05/27/2022 00:18:52 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.5_8_predictions.txt
05/27/2022 00:18:52 - INFO - __main__ - Classification-F1 on test data: 0.5284
05/27/2022 00:18:53 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.5, bsz=8, dev_performance=0.5398724940471618, test_performance=0.5283533927868802
05/27/2022 00:18:53 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.4, bsz=8 ...
05/27/2022 00:18:53 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:18:53 - INFO - __main__ - Printing 3 examples
05/27/2022 00:18:53 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:18:53 - INFO - __main__ - ['false']
05/27/2022 00:18:53 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:18:53 - INFO - __main__ - ['false']
05/27/2022 00:18:53 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:18:53 - INFO - __main__ - ['false']
05/27/2022 00:18:53 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:18:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:18:54 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:18:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:18:54 - INFO - __main__ - Printing 3 examples
05/27/2022 00:18:54 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:18:54 - INFO - __main__ - ['false']
05/27/2022 00:18:54 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:18:54 - INFO - __main__ - ['false']
05/27/2022 00:18:54 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:18:54 - INFO - __main__ - ['false']
05/27/2022 00:18:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:18:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:18:54 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:19:09 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:19:09 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:19:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:19:09 - INFO - __main__ - Starting training!
05/27/2022 00:19:12 - INFO - __main__ - Step 10 Global step 10 Train loss 6.75 on epoch=0
05/27/2022 00:19:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.47 on epoch=1
05/27/2022 00:19:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=1
05/27/2022 00:19:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=2
05/27/2022 00:19:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=3
05/27/2022 00:19:28 - INFO - __main__ - Global step 50 Train loss 1.95 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 00:19:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 00:19:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=3
05/27/2022 00:19:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=4
05/27/2022 00:19:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=4
05/27/2022 00:19:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/27/2022 00:19:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/27/2022 00:19:46 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 00:19:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/27/2022 00:19:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=7
05/27/2022 00:19:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/27/2022 00:19:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/27/2022 00:19:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
05/27/2022 00:20:04 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.34195559333697656 on epoch=9
05/27/2022 00:20:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=9, global_step=150
05/27/2022 00:20:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
05/27/2022 00:20:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=10
05/27/2022 00:20:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
05/27/2022 00:20:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=11
05/27/2022 00:20:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
05/27/2022 00:20:20 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.5100365941418751 on epoch=12
05/27/2022 00:20:20 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.5100365941418751 on epoch=12, global_step=200
05/27/2022 00:20:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/27/2022 00:20:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/27/2022 00:20:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/27/2022 00:20:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/27/2022 00:20:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=15
05/27/2022 00:20:38 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=15
05/27/2022 00:20:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
05/27/2022 00:20:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=16
05/27/2022 00:20:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/27/2022 00:20:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
05/27/2022 00:20:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=18
05/27/2022 00:20:54 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 00:20:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 00:21:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/27/2022 00:21:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/27/2022 00:21:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 00:21:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
05/27/2022 00:21:11 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=21
05/27/2022 00:21:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=22
05/27/2022 00:21:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/27/2022 00:21:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/27/2022 00:21:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=24
05/27/2022 00:21:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/27/2022 00:21:28 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 00:21:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/27/2022 00:21:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=26
05/27/2022 00:21:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=26
05/27/2022 00:21:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=27
05/27/2022 00:21:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/27/2022 00:21:45 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.350463149416029 on epoch=28
05/27/2022 00:21:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/27/2022 00:21:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/27/2022 00:21:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/27/2022 00:21:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=30
05/27/2022 00:21:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
05/27/2022 00:22:02 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=31
05/27/2022 00:22:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
05/27/2022 00:22:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=32
05/27/2022 00:22:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=33
05/27/2022 00:22:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
05/27/2022 00:22:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
05/27/2022 00:22:19 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=34
05/27/2022 00:22:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/27/2022 00:22:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
05/27/2022 00:22:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=36
05/27/2022 00:22:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/27/2022 00:22:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/27/2022 00:22:35 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.48331756543677074 on epoch=37
05/27/2022 00:22:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
05/27/2022 00:22:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/27/2022 00:22:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=39
05/27/2022 00:22:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
05/27/2022 00:22:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=40
05/27/2022 00:22:52 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.36318407960199 on epoch=40
05/27/2022 00:22:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
05/27/2022 00:22:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/27/2022 00:23:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/27/2022 00:23:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/27/2022 00:23:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
05/27/2022 00:23:09 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 00:23:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
05/27/2022 00:23:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
05/27/2022 00:23:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/27/2022 00:23:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
05/27/2022 00:23:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/27/2022 00:23:27 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.49787512258908145 on epoch=46
05/27/2022 00:23:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
05/27/2022 00:23:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=48
05/27/2022 00:23:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
05/27/2022 00:23:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
05/27/2022 00:23:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
05/27/2022 00:23:44 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 00:23:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/27/2022 00:23:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
05/27/2022 00:23:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/27/2022 00:23:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/27/2022 00:23:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=53
05/27/2022 00:24:01 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=53
05/27/2022 00:24:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=53
05/27/2022 00:24:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
05/27/2022 00:24:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
05/27/2022 00:24:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=55
05/27/2022 00:24:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=56
05/27/2022 00:24:18 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.48550832959622636 on epoch=56
05/27/2022 00:24:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
05/27/2022 00:24:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/27/2022 00:24:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
05/27/2022 00:24:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=58
05/27/2022 00:24:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
05/27/2022 00:24:35 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 00:24:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=59
05/27/2022 00:24:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/27/2022 00:24:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 00:24:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/27/2022 00:24:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/27/2022 00:24:52 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.42995169082125606 on epoch=62
05/27/2022 00:24:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=63
05/27/2022 00:24:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/27/2022 00:25:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 00:25:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
05/27/2022 00:25:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/27/2022 00:25:11 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.4734825179742297 on epoch=65
05/27/2022 00:25:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
05/27/2022 00:25:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
05/27/2022 00:25:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
05/27/2022 00:25:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/27/2022 00:25:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=68
05/27/2022 00:25:28 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=68
05/27/2022 00:25:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=69
05/27/2022 00:25:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
05/27/2022 00:25:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
05/27/2022 00:25:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/27/2022 00:25:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=71
05/27/2022 00:25:45 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=71
05/27/2022 00:25:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
05/27/2022 00:25:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
05/27/2022 00:25:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/27/2022 00:25:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
05/27/2022 00:25:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
05/27/2022 00:26:02 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 00:26:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 00:26:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
05/27/2022 00:26:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
05/27/2022 00:26:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/27/2022 00:26:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
05/27/2022 00:26:18 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.3712545436683367 on epoch=78
05/27/2022 00:26:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=78
05/27/2022 00:26:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=79
05/27/2022 00:26:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=79
05/27/2022 00:26:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
05/27/2022 00:26:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=81
05/27/2022 00:26:35 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.5287817938420348 on epoch=81
05/27/2022 00:26:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5100365941418751 -> 0.5287817938420348 on epoch=81, global_step=1300
05/27/2022 00:26:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=81
05/27/2022 00:26:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=82
05/27/2022 00:26:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=83
05/27/2022 00:26:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=83
05/27/2022 00:26:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/27/2022 00:26:52 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3671451355661882 on epoch=84
05/27/2022 00:26:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
05/27/2022 00:26:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=85
05/27/2022 00:26:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=86
05/27/2022 00:27:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
05/27/2022 00:27:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
05/27/2022 00:27:09 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.5323076923076923 on epoch=87
05/27/2022 00:27:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5287817938420348 -> 0.5323076923076923 on epoch=87, global_step=1400
05/27/2022 00:27:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=88
05/27/2022 00:27:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
05/27/2022 00:27:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=89
05/27/2022 00:27:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=89
05/27/2022 00:27:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
05/27/2022 00:27:27 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.49220246238030096 on epoch=90
05/27/2022 00:27:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/27/2022 00:27:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
05/27/2022 00:27:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
05/27/2022 00:27:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=93
05/27/2022 00:27:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=93
05/27/2022 00:27:45 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=93
05/27/2022 00:27:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
05/27/2022 00:27:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=94
05/27/2022 00:27:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=95
05/27/2022 00:27:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
05/27/2022 00:27:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=96
05/27/2022 00:28:02 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.627262556132849 on epoch=96
05/27/2022 00:28:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5323076923076923 -> 0.627262556132849 on epoch=96, global_step=1550
05/27/2022 00:28:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
05/27/2022 00:28:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=98
05/27/2022 00:28:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=98
05/27/2022 00:28:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
05/27/2022 00:28:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=99
05/27/2022 00:28:20 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=99
05/27/2022 00:28:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=100
05/27/2022 00:28:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
05/27/2022 00:28:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/27/2022 00:28:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=102
05/27/2022 00:28:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=103
05/27/2022 00:28:37 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.37532485567480706 on epoch=103
05/27/2022 00:28:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
05/27/2022 00:28:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=104
05/27/2022 00:28:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=104
05/27/2022 00:28:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=105
05/27/2022 00:28:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=106
05/27/2022 00:28:54 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.5524770763242958 on epoch=106
05/27/2022 00:28:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
05/27/2022 00:28:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=107
05/27/2022 00:29:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
05/27/2022 00:29:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=108
05/27/2022 00:29:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=109
05/27/2022 00:29:15 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.6170742076175781 on epoch=109
05/27/2022 00:29:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=109
05/27/2022 00:29:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=110
05/27/2022 00:29:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=111
05/27/2022 00:29:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=111
05/27/2022 00:29:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=112
05/27/2022 00:29:32 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.6286285827721533 on epoch=112
05/27/2022 00:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.627262556132849 -> 0.6286285827721533 on epoch=112, global_step=1800
05/27/2022 00:29:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.34 on epoch=113
05/27/2022 00:29:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=113
05/27/2022 00:29:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=114
05/27/2022 00:29:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=114
05/27/2022 00:29:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=115
05/27/2022 00:29:50 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.6027579146070292 on epoch=115
05/27/2022 00:29:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=116
05/27/2022 00:29:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=116
05/27/2022 00:29:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
05/27/2022 00:30:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=118
05/27/2022 00:30:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=118
05/27/2022 00:30:08 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.35501021683496337 on epoch=118
05/27/2022 00:30:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
05/27/2022 00:30:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=119
05/27/2022 00:30:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=120
05/27/2022 00:30:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=121
05/27/2022 00:30:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=121
05/27/2022 00:30:25 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.7219392047975277 on epoch=121
05/27/2022 00:30:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6286285827721533 -> 0.7219392047975277 on epoch=121, global_step=1950
05/27/2022 00:30:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=122
05/27/2022 00:30:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=123
05/27/2022 00:30:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=123
05/27/2022 00:30:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=124
05/27/2022 00:30:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
05/27/2022 00:30:43 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.7320197044334975 on epoch=124
05/27/2022 00:30:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7219392047975277 -> 0.7320197044334975 on epoch=124, global_step=2000
05/27/2022 00:30:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=125
05/27/2022 00:30:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=126
05/27/2022 00:30:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/27/2022 00:30:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=127
05/27/2022 00:30:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
05/27/2022 00:31:02 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.5596835225318197 on epoch=128
05/27/2022 00:31:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=128
05/27/2022 00:31:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=129
05/27/2022 00:31:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=129
05/27/2022 00:31:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=130
05/27/2022 00:31:15 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
05/27/2022 00:31:20 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.7457795431976166 on epoch=131
05/27/2022 00:31:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7320197044334975 -> 0.7457795431976166 on epoch=131, global_step=2100
05/27/2022 00:31:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=131
05/27/2022 00:31:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/27/2022 00:31:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=133
05/27/2022 00:31:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=133
05/27/2022 00:31:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/27/2022 00:31:39 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.7295401999663151 on epoch=134
05/27/2022 00:31:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=134
05/27/2022 00:31:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=135
05/27/2022 00:31:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/27/2022 00:31:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/27/2022 00:31:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=137
05/27/2022 00:31:57 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.6449375866851594 on epoch=137
05/27/2022 00:32:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/27/2022 00:32:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=138
05/27/2022 00:32:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/27/2022 00:32:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=139
05/27/2022 00:32:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=140
05/27/2022 00:32:15 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.5142314990512334 on epoch=140
05/27/2022 00:32:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=141
05/27/2022 00:32:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=141
05/27/2022 00:32:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=142
05/27/2022 00:32:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=143
05/27/2022 00:32:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=143
05/27/2022 00:32:33 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.4487674487674488 on epoch=143
05/27/2022 00:32:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
05/27/2022 00:32:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/27/2022 00:32:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.24 on epoch=145
05/27/2022 00:32:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=146
05/27/2022 00:32:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=146
05/27/2022 00:32:51 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.7075956596230726 on epoch=146
05/27/2022 00:32:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=147
05/27/2022 00:32:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=148
05/27/2022 00:32:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/27/2022 00:33:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/27/2022 00:33:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=149
05/27/2022 00:33:10 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.7539024948500801 on epoch=149
05/27/2022 00:33:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7457795431976166 -> 0.7539024948500801 on epoch=149, global_step=2400
05/27/2022 00:33:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/27/2022 00:33:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/27/2022 00:33:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/27/2022 00:33:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.26 on epoch=152
05/27/2022 00:33:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/27/2022 00:33:27 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.5634855063547032 on epoch=153
05/27/2022 00:33:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=153
05/27/2022 00:33:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=154
05/27/2022 00:33:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=154
05/27/2022 00:33:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=155
05/27/2022 00:33:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=156
05/27/2022 00:33:46 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.7073369352439121 on epoch=156
05/27/2022 00:33:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=156
05/27/2022 00:33:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/27/2022 00:33:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/27/2022 00:33:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/27/2022 00:33:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
05/27/2022 00:34:04 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.745219028953775 on epoch=159
05/27/2022 00:34:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/27/2022 00:34:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
05/27/2022 00:34:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/27/2022 00:34:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/27/2022 00:34:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
05/27/2022 00:34:22 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.7004048582995952 on epoch=162
05/27/2022 00:34:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=163
05/27/2022 00:34:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/27/2022 00:34:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
05/27/2022 00:34:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/27/2022 00:34:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=165
05/27/2022 00:34:40 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.5544852075166558 on epoch=165
05/27/2022 00:34:42 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=166
05/27/2022 00:34:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=166
05/27/2022 00:34:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/27/2022 00:34:50 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=168
05/27/2022 00:34:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=168
05/27/2022 00:34:57 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.5049787656998284 on epoch=168
05/27/2022 00:34:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
05/27/2022 00:35:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/27/2022 00:35:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/27/2022 00:35:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/27/2022 00:35:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/27/2022 00:35:15 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.6588070752776636 on epoch=171
05/27/2022 00:35:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
05/27/2022 00:35:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/27/2022 00:35:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=173
05/27/2022 00:35:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
05/27/2022 00:35:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=174
05/27/2022 00:35:34 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.7487578973195117 on epoch=174
05/27/2022 00:35:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=175
05/27/2022 00:35:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
05/27/2022 00:35:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=176
05/27/2022 00:35:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=177
05/27/2022 00:35:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=178
05/27/2022 00:35:53 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.6905970510031424 on epoch=178
05/27/2022 00:35:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=178
05/27/2022 00:35:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=179
05/27/2022 00:36:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=179
05/27/2022 00:36:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/27/2022 00:36:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/27/2022 00:36:11 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.6334332334332334 on epoch=181
05/27/2022 00:36:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
05/27/2022 00:36:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/27/2022 00:36:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=183
05/27/2022 00:36:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=183
05/27/2022 00:36:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=184
05/27/2022 00:36:29 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.7160665469030696 on epoch=184
05/27/2022 00:36:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
05/27/2022 00:36:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/27/2022 00:36:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=186
05/27/2022 00:36:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=186
05/27/2022 00:36:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=187
05/27/2022 00:36:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:36:44 - INFO - __main__ - Printing 3 examples
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:36:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:36:44 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:36:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:36:44 - INFO - __main__ - Printing 3 examples
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:36:44 - INFO - __main__ - ['false']
05/27/2022 00:36:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:36:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:36:45 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:36:48 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.6342093855860773 on epoch=187
05/27/2022 00:36:48 - INFO - __main__ - save last model!
05/27/2022 00:36:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 00:36:48 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 00:36:48 - INFO - __main__ - Printing 3 examples
05/27/2022 00:36:48 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 00:36:48 - INFO - __main__ - ['false']
05/27/2022 00:36:48 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 00:36:48 - INFO - __main__ - ['false']
05/27/2022 00:36:48 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 00:36:48 - INFO - __main__ - ['false']
05/27/2022 00:36:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:36:49 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:36:52 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 00:37:03 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:37:03 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:37:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:37:04 - INFO - __main__ - Starting training!
05/27/2022 00:37:46 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.4_8_predictions.txt
05/27/2022 00:37:46 - INFO - __main__ - Classification-F1 on test data: 0.4143
05/27/2022 00:37:47 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.4, bsz=8, dev_performance=0.7539024948500801, test_performance=0.4143394829599543
05/27/2022 00:37:47 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.3, bsz=8 ...
05/27/2022 00:37:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:37:48 - INFO - __main__ - Printing 3 examples
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:37:48 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:37:48 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:37:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:37:48 - INFO - __main__ - Printing 3 examples
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:37:48 - INFO - __main__ - ['false']
05/27/2022 00:37:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:37:48 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:37:48 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:38:03 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:38:03 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:38:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:38:04 - INFO - __main__ - Starting training!
05/27/2022 00:38:06 - INFO - __main__ - Step 10 Global step 10 Train loss 6.67 on epoch=0
05/27/2022 00:38:09 - INFO - __main__ - Step 20 Global step 20 Train loss 2.19 on epoch=1
05/27/2022 00:38:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=1
05/27/2022 00:38:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=2
05/27/2022 00:38:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=3
05/27/2022 00:38:21 - INFO - __main__ - Global step 50 Train loss 2.17 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 00:38:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 00:38:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=3
05/27/2022 00:38:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=4
05/27/2022 00:38:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
05/27/2022 00:38:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=5
05/27/2022 00:38:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
05/27/2022 00:38:39 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 00:38:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/27/2022 00:38:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
05/27/2022 00:38:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
05/27/2022 00:38:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/27/2022 00:38:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=9
05/27/2022 00:38:58 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.34195559333697656 on epoch=9
05/27/2022 00:38:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=9, global_step=150
05/27/2022 00:39:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
05/27/2022 00:39:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
05/27/2022 00:39:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/27/2022 00:39:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=11
05/27/2022 00:39:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
05/27/2022 00:39:15 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.5123809523809524 on epoch=12
05/27/2022 00:39:15 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.5123809523809524 on epoch=12, global_step=200
05/27/2022 00:39:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
05/27/2022 00:39:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=13
05/27/2022 00:39:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
05/27/2022 00:39:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=14
05/27/2022 00:39:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=15
05/27/2022 00:39:31 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5568713095526976 on epoch=15
05/27/2022 00:39:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5123809523809524 -> 0.5568713095526976 on epoch=15, global_step=250
05/27/2022 00:39:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=16
05/27/2022 00:39:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/27/2022 00:39:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/27/2022 00:39:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=18
05/27/2022 00:39:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
05/27/2022 00:39:49 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.34195559333697656 on epoch=18
05/27/2022 00:39:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/27/2022 00:39:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=19
05/27/2022 00:39:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/27/2022 00:39:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 00:40:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=21
05/27/2022 00:40:06 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.48544687380130414 on epoch=21
05/27/2022 00:40:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=22
05/27/2022 00:40:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
05/27/2022 00:40:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/27/2022 00:40:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/27/2022 00:40:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=24
05/27/2022 00:40:24 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 00:40:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/27/2022 00:40:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
05/27/2022 00:40:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/27/2022 00:40:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
05/27/2022 00:40:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/27/2022 00:40:41 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.35693779904306216 on epoch=28
05/27/2022 00:40:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/27/2022 00:40:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=29
05/27/2022 00:40:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/27/2022 00:40:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 00:40:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
05/27/2022 00:40:58 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=31
05/27/2022 00:41:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=31
05/27/2022 00:41:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/27/2022 00:41:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=33
05/27/2022 00:41:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 00:41:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/27/2022 00:41:15 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
05/27/2022 00:41:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/27/2022 00:41:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
05/27/2022 00:41:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/27/2022 00:41:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/27/2022 00:41:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/27/2022 00:41:32 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.413139502376293 on epoch=37
05/27/2022 00:41:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/27/2022 00:41:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/27/2022 00:41:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
05/27/2022 00:41:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
05/27/2022 00:41:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
05/27/2022 00:41:50 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5469736120898911 on epoch=40
05/27/2022 00:41:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
05/27/2022 00:41:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/27/2022 00:41:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/27/2022 00:42:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/27/2022 00:42:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/27/2022 00:42:07 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=43
05/27/2022 00:42:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/27/2022 00:42:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/27/2022 00:42:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
05/27/2022 00:42:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/27/2022 00:42:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
05/27/2022 00:42:24 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=46
05/27/2022 00:42:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=47
05/27/2022 00:42:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=48
05/27/2022 00:42:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/27/2022 00:42:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
05/27/2022 00:42:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/27/2022 00:42:43 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 00:42:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/27/2022 00:42:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
05/27/2022 00:42:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/27/2022 00:42:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=52
05/27/2022 00:42:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
05/27/2022 00:43:00 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=53
05/27/2022 00:43:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
05/27/2022 00:43:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
05/27/2022 00:43:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
05/27/2022 00:43:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
05/27/2022 00:43:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/27/2022 00:43:17 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.4858090996419526 on epoch=56
05/27/2022 00:43:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/27/2022 00:43:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
05/27/2022 00:43:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=58
05/27/2022 00:43:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
05/27/2022 00:43:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=59
05/27/2022 00:43:35 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 00:43:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=59
05/27/2022 00:43:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
05/27/2022 00:43:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 00:43:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
05/27/2022 00:43:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/27/2022 00:43:52 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=62
05/27/2022 00:43:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=63
05/27/2022 00:43:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/27/2022 00:43:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
05/27/2022 00:44:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
05/27/2022 00:44:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
05/27/2022 00:44:08 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5352395772547494 on epoch=65
05/27/2022 00:44:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
05/27/2022 00:44:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/27/2022 00:44:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/27/2022 00:44:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/27/2022 00:44:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/27/2022 00:44:25 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=68
05/27/2022 00:44:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
05/27/2022 00:44:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=69
05/27/2022 00:44:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
05/27/2022 00:44:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 00:44:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=71
05/27/2022 00:44:44 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=71
05/27/2022 00:44:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/27/2022 00:44:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
05/27/2022 00:44:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
05/27/2022 00:44:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/27/2022 00:44:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/27/2022 00:45:02 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 00:45:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=75
05/27/2022 00:45:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/27/2022 00:45:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
05/27/2022 00:45:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=77
05/27/2022 00:45:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=78
05/27/2022 00:45:20 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.4560645347162201 on epoch=78
05/27/2022 00:45:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=78
05/27/2022 00:45:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=79
05/27/2022 00:45:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/27/2022 00:45:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=80
05/27/2022 00:45:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=81
05/27/2022 00:45:36 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.6191390315304506 on epoch=81
05/27/2022 00:45:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5568713095526976 -> 0.6191390315304506 on epoch=81, global_step=1300
05/27/2022 00:45:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
05/27/2022 00:45:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
05/27/2022 00:45:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=83
05/27/2022 00:45:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
05/27/2022 00:45:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=84
05/27/2022 00:45:54 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.565365025466893 on epoch=84
05/27/2022 00:45:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=84
05/27/2022 00:45:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=85
05/27/2022 00:46:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
05/27/2022 00:46:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/27/2022 00:46:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=87
05/27/2022 00:46:11 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.6775668237160051 on epoch=87
05/27/2022 00:46:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6191390315304506 -> 0.6775668237160051 on epoch=87, global_step=1400
05/27/2022 00:46:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=88
05/27/2022 00:46:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=88
05/27/2022 00:46:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
05/27/2022 00:46:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=89
05/27/2022 00:46:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
05/27/2022 00:46:29 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.6218376327533393 on epoch=90
05/27/2022 00:46:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
05/27/2022 00:46:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
05/27/2022 00:46:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
05/27/2022 00:46:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/27/2022 00:46:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=93
05/27/2022 00:46:46 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.4392634392634393 on epoch=93
05/27/2022 00:46:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
05/27/2022 00:46:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=94
05/27/2022 00:46:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=95
05/27/2022 00:46:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
05/27/2022 00:46:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=96
05/27/2022 00:47:04 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.680034564167451 on epoch=96
05/27/2022 00:47:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6775668237160051 -> 0.680034564167451 on epoch=96, global_step=1550
05/27/2022 00:47:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=97
05/27/2022 00:47:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=98
05/27/2022 00:47:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=98
05/27/2022 00:47:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
05/27/2022 00:47:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=99
05/27/2022 00:47:22 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.4467725265432678 on epoch=99
05/27/2022 00:47:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/27/2022 00:47:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=101
05/27/2022 00:47:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=101
05/27/2022 00:47:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
05/27/2022 00:47:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=103
05/27/2022 00:47:40 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.5801874163319946 on epoch=103
05/27/2022 00:47:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=103
05/27/2022 00:47:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=104
05/27/2022 00:47:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=104
05/27/2022 00:47:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
05/27/2022 00:47:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=106
05/27/2022 00:47:58 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.7087868167004857 on epoch=106
05/27/2022 00:47:58 - INFO - __main__ - Saving model with best Classification-F1: 0.680034564167451 -> 0.7087868167004857 on epoch=106, global_step=1700
05/27/2022 00:48:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=106
05/27/2022 00:48:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=107
05/27/2022 00:48:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=108
05/27/2022 00:48:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=108
05/27/2022 00:48:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=109
05/27/2022 00:48:16 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.6989912742125131 on epoch=109
05/27/2022 00:48:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
05/27/2022 00:48:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=110
05/27/2022 00:48:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=111
05/27/2022 00:48:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=111
05/27/2022 00:48:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=112
05/27/2022 00:48:34 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.6825396825396826 on epoch=112
05/27/2022 00:48:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=113
05/27/2022 00:48:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
05/27/2022 00:48:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
05/27/2022 00:48:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=114
05/27/2022 00:48:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=115
05/27/2022 00:48:52 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.4971598449859319 on epoch=115
05/27/2022 00:48:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=116
05/27/2022 00:48:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=116
05/27/2022 00:49:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=117
05/27/2022 00:49:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=118
05/27/2022 00:49:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=118
05/27/2022 00:49:10 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.5478040163948482 on epoch=118
05/27/2022 00:49:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/27/2022 00:49:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=119
05/27/2022 00:49:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
05/27/2022 00:49:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=121
05/27/2022 00:49:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=121
05/27/2022 00:49:28 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.6947425474254743 on epoch=121
05/27/2022 00:49:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=122
05/27/2022 00:49:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
05/27/2022 00:49:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=123
05/27/2022 00:49:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/27/2022 00:49:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=124
05/27/2022 00:49:46 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.70267131242741 on epoch=124
05/27/2022 00:49:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=125
05/27/2022 00:49:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=126
05/27/2022 00:49:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=126
05/27/2022 00:49:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=127
05/27/2022 00:49:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=128
05/27/2022 00:50:04 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.5927889713679746 on epoch=128
05/27/2022 00:50:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=128
05/27/2022 00:50:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/27/2022 00:50:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/27/2022 00:50:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.24 on epoch=130
05/27/2022 00:50:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
05/27/2022 00:50:22 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.7183201907207042 on epoch=131
05/27/2022 00:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7087868167004857 -> 0.7183201907207042 on epoch=131, global_step=2100
05/27/2022 00:50:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=131
05/27/2022 00:50:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=132
05/27/2022 00:50:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/27/2022 00:50:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/27/2022 00:50:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
05/27/2022 00:50:40 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.7144908715911695 on epoch=134
05/27/2022 00:50:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
05/27/2022 00:50:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=135
05/27/2022 00:50:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/27/2022 00:50:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=136
05/27/2022 00:50:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=137
05/27/2022 00:50:58 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.6716716716716716 on epoch=137
05/27/2022 00:51:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=138
05/27/2022 00:51:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/27/2022 00:51:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.22 on epoch=139
05/27/2022 00:51:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=139
05/27/2022 00:51:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=140
05/27/2022 00:51:16 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.6410925791636451 on epoch=140
05/27/2022 00:51:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=141
05/27/2022 00:51:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
05/27/2022 00:51:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=142
05/27/2022 00:51:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/27/2022 00:51:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
05/27/2022 00:51:34 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.6716716716716716 on epoch=143
05/27/2022 00:51:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
05/27/2022 00:51:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/27/2022 00:51:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=145
05/27/2022 00:51:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=146
05/27/2022 00:51:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/27/2022 00:51:52 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.6599190283400811 on epoch=146
05/27/2022 00:51:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=147
05/27/2022 00:51:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
05/27/2022 00:52:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/27/2022 00:52:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=149
05/27/2022 00:52:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/27/2022 00:52:10 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7264122137404581 on epoch=149
05/27/2022 00:52:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7183201907207042 -> 0.7264122137404581 on epoch=149, global_step=2400
05/27/2022 00:52:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/27/2022 00:52:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=151
05/27/2022 00:52:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/27/2022 00:52:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/27/2022 00:52:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/27/2022 00:52:28 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.6725567293571462 on epoch=153
05/27/2022 00:52:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=153
05/27/2022 00:52:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/27/2022 00:52:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=154
05/27/2022 00:52:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=155
05/27/2022 00:52:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.20 on epoch=156
05/27/2022 00:52:46 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.664771638454168 on epoch=156
05/27/2022 00:52:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/27/2022 00:52:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
05/27/2022 00:52:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/27/2022 00:52:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
05/27/2022 00:52:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
05/27/2022 00:53:04 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.7116360886941226 on epoch=159
05/27/2022 00:53:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/27/2022 00:53:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/27/2022 00:53:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/27/2022 00:53:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
05/27/2022 00:53:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=162
05/27/2022 00:53:22 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.6437246963562753 on epoch=162
05/27/2022 00:53:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=163
05/27/2022 00:53:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
05/27/2022 00:53:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=164
05/27/2022 00:53:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/27/2022 00:53:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/27/2022 00:53:41 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.6066688460281137 on epoch=165
05/27/2022 00:53:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=166
05/27/2022 00:53:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/27/2022 00:53:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
05/27/2022 00:53:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/27/2022 00:53:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=168
05/27/2022 00:54:00 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.6166666666666667 on epoch=168
05/27/2022 00:54:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
05/27/2022 00:54:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=169
05/27/2022 00:54:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.23 on epoch=170
05/27/2022 00:54:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/27/2022 00:54:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/27/2022 00:54:18 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.6891656288916563 on epoch=171
05/27/2022 00:54:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/27/2022 00:54:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=173
05/27/2022 00:54:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/27/2022 00:54:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=174
05/27/2022 00:54:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=174
05/27/2022 00:54:36 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7382772564278629 on epoch=174
05/27/2022 00:54:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7264122137404581 -> 0.7382772564278629 on epoch=174, global_step=2800
05/27/2022 00:54:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
05/27/2022 00:54:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
05/27/2022 00:54:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/27/2022 00:54:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=177
05/27/2022 00:54:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/27/2022 00:54:54 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.6259893717790228 on epoch=178
05/27/2022 00:54:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=178
05/27/2022 00:55:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
05/27/2022 00:55:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
05/27/2022 00:55:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
05/27/2022 00:55:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
05/27/2022 00:55:13 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.6967711169566354 on epoch=181
05/27/2022 00:55:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
05/27/2022 00:55:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/27/2022 00:55:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=183
05/27/2022 00:55:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=183
05/27/2022 00:55:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
05/27/2022 00:55:31 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.7343587865470305 on epoch=184
05/27/2022 00:55:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=184
05/27/2022 00:55:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
05/27/2022 00:55:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=186
05/27/2022 00:55:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/27/2022 00:55:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
05/27/2022 00:55:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:55:45 - INFO - __main__ - Printing 3 examples
05/27/2022 00:55:45 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:55:45 - INFO - __main__ - ['false']
05/27/2022 00:55:45 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:55:45 - INFO - __main__ - ['false']
05/27/2022 00:55:45 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:55:45 - INFO - __main__ - ['false']
05/27/2022 00:55:45 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:55:45 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:55:46 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:55:46 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:55:46 - INFO - __main__ - Printing 3 examples
05/27/2022 00:55:46 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:55:46 - INFO - __main__ - ['false']
05/27/2022 00:55:46 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:55:46 - INFO - __main__ - ['false']
05/27/2022 00:55:46 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:55:46 - INFO - __main__ - ['false']
05/27/2022 00:55:46 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:55:46 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:55:46 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:55:49 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.6166666666666667 on epoch=187
05/27/2022 00:55:49 - INFO - __main__ - save last model!
05/27/2022 00:55:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 00:55:49 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 00:55:49 - INFO - __main__ - Printing 3 examples
05/27/2022 00:55:49 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 00:55:49 - INFO - __main__ - ['false']
05/27/2022 00:55:49 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 00:55:49 - INFO - __main__ - ['false']
05/27/2022 00:55:49 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 00:55:49 - INFO - __main__ - ['false']
05/27/2022 00:55:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:55:51 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:55:53 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 00:56:05 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:56:05 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:56:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:56:05 - INFO - __main__ - Starting training!
05/27/2022 00:56:50 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.3_8_predictions.txt
05/27/2022 00:56:50 - INFO - __main__ - Classification-F1 on test data: 0.3902
05/27/2022 00:56:51 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.3, bsz=8, dev_performance=0.7382772564278629, test_performance=0.39023185889292483
05/27/2022 00:56:51 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.2, bsz=8 ...
05/27/2022 00:56:52 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:56:52 - INFO - __main__ - Printing 3 examples
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:56:52 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:56:52 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 00:56:52 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 00:56:52 - INFO - __main__ - Printing 3 examples
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/27/2022 00:56:52 - INFO - __main__ - ['false']
05/27/2022 00:56:52 - INFO - __main__ - Tokenizing Input ...
05/27/2022 00:56:52 - INFO - __main__ - Tokenizing Output ...
05/27/2022 00:56:53 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 00:57:07 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 00:57:07 - INFO - __main__ - task name: wiki_qa
05/27/2022 00:57:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 00:57:08 - INFO - __main__ - Starting training!
05/27/2022 00:57:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.27 on epoch=0
05/27/2022 00:57:13 - INFO - __main__ - Step 20 Global step 20 Train loss 3.66 on epoch=1
05/27/2022 00:57:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=1
05/27/2022 00:57:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.77 on epoch=2
05/27/2022 00:57:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=3
05/27/2022 00:57:26 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.3651088894055646 on epoch=3
05/27/2022 00:57:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3651088894055646 on epoch=3, global_step=50
05/27/2022 00:57:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=3
05/27/2022 00:57:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=4
05/27/2022 00:57:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=4
05/27/2022 00:57:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=5
05/27/2022 00:57:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
05/27/2022 00:57:43 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 00:57:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
05/27/2022 00:57:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=7
05/27/2022 00:57:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=8
05/27/2022 00:57:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=8
05/27/2022 00:57:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
05/27/2022 00:58:01 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 00:58:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=9
05/27/2022 00:58:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
05/27/2022 00:58:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=11
05/27/2022 00:58:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
05/27/2022 00:58:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
05/27/2022 00:58:19 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.3591989987484355 on epoch=12
05/27/2022 00:58:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/27/2022 00:58:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/27/2022 00:58:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=14
05/27/2022 00:58:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/27/2022 00:58:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=15
05/27/2022 00:58:36 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.34918648310387984 on epoch=15
05/27/2022 00:58:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
05/27/2022 00:58:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/27/2022 00:58:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
05/27/2022 00:58:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=18
05/27/2022 00:58:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
05/27/2022 00:58:53 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 00:58:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/27/2022 00:58:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
05/27/2022 00:59:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/27/2022 00:59:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 00:59:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=21
05/27/2022 00:59:10 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.5355320050869012 on epoch=21
05/27/2022 00:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3651088894055646 -> 0.5355320050869012 on epoch=21, global_step=350
05/27/2022 00:59:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/27/2022 00:59:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/27/2022 00:59:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=23
05/27/2022 00:59:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/27/2022 00:59:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/27/2022 00:59:27 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 00:59:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/27/2022 00:59:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
05/27/2022 00:59:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=26
05/27/2022 00:59:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=27
05/27/2022 00:59:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/27/2022 00:59:44 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.36516753625488524 on epoch=28
05/27/2022 00:59:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/27/2022 00:59:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
05/27/2022 00:59:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/27/2022 00:59:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/27/2022 00:59:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
05/27/2022 01:00:01 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.41763682590659706 on epoch=31
05/27/2022 01:00:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=31
05/27/2022 01:00:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/27/2022 01:00:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
05/27/2022 01:00:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 01:00:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/27/2022 01:00:20 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.4417668437237909 on epoch=34
05/27/2022 01:00:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/27/2022 01:00:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
05/27/2022 01:00:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/27/2022 01:00:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/27/2022 01:00:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/27/2022 01:00:38 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.35885876860812244 on epoch=37
05/27/2022 01:00:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=38
05/27/2022 01:00:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/27/2022 01:00:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
05/27/2022 01:00:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
05/27/2022 01:00:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
05/27/2022 01:00:55 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5870445344129555 on epoch=40
05/27/2022 01:00:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5355320050869012 -> 0.5870445344129555 on epoch=40, global_step=650
05/27/2022 01:00:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/27/2022 01:01:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/27/2022 01:01:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
05/27/2022 01:01:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/27/2022 01:01:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/27/2022 01:01:12 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=43
05/27/2022 01:01:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
05/27/2022 01:01:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=44
05/27/2022 01:01:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
05/27/2022 01:01:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/27/2022 01:01:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/27/2022 01:01:30 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=46
05/27/2022 01:01:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=47
05/27/2022 01:01:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
05/27/2022 01:01:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
05/27/2022 01:01:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
05/27/2022 01:01:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
05/27/2022 01:01:47 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 01:01:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/27/2022 01:01:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
05/27/2022 01:01:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
05/27/2022 01:01:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
05/27/2022 01:02:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
05/27/2022 01:02:05 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3671451355661882 on epoch=53
05/27/2022 01:02:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 01:02:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
05/27/2022 01:02:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=54
05/27/2022 01:02:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
05/27/2022 01:02:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
05/27/2022 01:02:23 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.5378277037974293 on epoch=56
05/27/2022 01:02:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/27/2022 01:02:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
05/27/2022 01:02:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
05/27/2022 01:02:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
05/27/2022 01:02:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
05/27/2022 01:02:40 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.350463149416029 on epoch=59
05/27/2022 01:02:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
05/27/2022 01:02:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=60
05/27/2022 01:02:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 01:02:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
05/27/2022 01:02:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/27/2022 01:02:57 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.5731179341257822 on epoch=62
05/27/2022 01:03:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
05/27/2022 01:03:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=63
05/27/2022 01:03:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
05/27/2022 01:03:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
05/27/2022 01:03:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/27/2022 01:03:15 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.53125 on epoch=65
05/27/2022 01:03:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
05/27/2022 01:03:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=66
05/27/2022 01:03:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=67
05/27/2022 01:03:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/27/2022 01:03:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
05/27/2022 01:03:32 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=68
05/27/2022 01:03:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/27/2022 01:03:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=69
05/27/2022 01:03:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/27/2022 01:03:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 01:03:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/27/2022 01:03:50 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.48344733242134064 on epoch=71
05/27/2022 01:03:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/27/2022 01:03:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
05/27/2022 01:03:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/27/2022 01:04:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/27/2022 01:04:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/27/2022 01:04:08 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 01:04:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 01:04:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
05/27/2022 01:04:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=76
05/27/2022 01:04:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/27/2022 01:04:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
05/27/2022 01:04:25 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.3980615931639617 on epoch=78
05/27/2022 01:04:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/27/2022 01:04:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
05/27/2022 01:04:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=79
05/27/2022 01:04:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=80
05/27/2022 01:04:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=81
05/27/2022 01:04:43 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.5666666666666667 on epoch=81
05/27/2022 01:04:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=81
05/27/2022 01:04:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
05/27/2022 01:04:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=83
05/27/2022 01:04:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
05/27/2022 01:04:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
05/27/2022 01:05:00 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.5988021164677952 on epoch=84
05/27/2022 01:05:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5870445344129555 -> 0.5988021164677952 on epoch=84, global_step=1350
05/27/2022 01:05:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
05/27/2022 01:05:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=85
05/27/2022 01:05:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
05/27/2022 01:05:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=86
05/27/2022 01:05:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
05/27/2022 01:05:18 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.6044150110375275 on epoch=87
05/27/2022 01:05:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5988021164677952 -> 0.6044150110375275 on epoch=87, global_step=1400
05/27/2022 01:05:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
05/27/2022 01:05:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
05/27/2022 01:05:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
05/27/2022 01:05:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=89
05/27/2022 01:05:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
05/27/2022 01:05:36 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.531350114416476 on epoch=90
05/27/2022 01:05:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/27/2022 01:05:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=91
05/27/2022 01:05:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=92
05/27/2022 01:05:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
05/27/2022 01:05:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=93
05/27/2022 01:05:53 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.35885876860812244 on epoch=93
05/27/2022 01:05:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=94
05/27/2022 01:05:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
05/27/2022 01:06:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
05/27/2022 01:06:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=96
05/27/2022 01:06:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=96
05/27/2022 01:06:11 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.597948717948718 on epoch=96
05/27/2022 01:06:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=97
05/27/2022 01:06:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=98
05/27/2022 01:06:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=98
05/27/2022 01:06:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=99
05/27/2022 01:06:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=99
05/27/2022 01:06:29 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.5821260967149561 on epoch=99
05/27/2022 01:06:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
05/27/2022 01:06:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
05/27/2022 01:06:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/27/2022 01:06:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=102
05/27/2022 01:06:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=103
05/27/2022 01:06:46 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.43605148651847 on epoch=103
05/27/2022 01:06:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=103
05/27/2022 01:06:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=104
05/27/2022 01:06:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=104
05/27/2022 01:06:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=105
05/27/2022 01:06:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=106
05/27/2022 01:07:04 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.5888312330121077 on epoch=106
05/27/2022 01:07:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=106
05/27/2022 01:07:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=107
05/27/2022 01:07:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=108
05/27/2022 01:07:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=108
05/27/2022 01:07:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=109
05/27/2022 01:07:21 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.6288552810291941 on epoch=109
05/27/2022 01:07:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6044150110375275 -> 0.6288552810291941 on epoch=109, global_step=1750
05/27/2022 01:07:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=109
05/27/2022 01:07:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=110
05/27/2022 01:07:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=111
05/27/2022 01:07:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
05/27/2022 01:07:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=112
05/27/2022 01:07:39 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.5905413033933388 on epoch=112
05/27/2022 01:07:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=113
05/27/2022 01:07:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=113
05/27/2022 01:07:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=114
05/27/2022 01:07:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=114
05/27/2022 01:07:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=115
05/27/2022 01:07:56 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.4385964912280702 on epoch=115
05/27/2022 01:07:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=116
05/27/2022 01:08:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=116
05/27/2022 01:08:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
05/27/2022 01:08:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=118
05/27/2022 01:08:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=118
05/27/2022 01:08:12 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.42266947144995926 on epoch=118
05/27/2022 01:08:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
05/27/2022 01:08:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=119
05/27/2022 01:08:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=120
05/27/2022 01:08:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=121
05/27/2022 01:08:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=121
05/27/2022 01:08:30 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.6438737292669877 on epoch=121
05/27/2022 01:08:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6288552810291941 -> 0.6438737292669877 on epoch=121, global_step=1950
05/27/2022 01:08:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=122
05/27/2022 01:08:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=123
05/27/2022 01:08:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=123
05/27/2022 01:08:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=124
05/27/2022 01:08:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=124
05/27/2022 01:08:48 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.6156156156156156 on epoch=124
05/27/2022 01:08:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=125
05/27/2022 01:08:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=126
05/27/2022 01:08:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/27/2022 01:08:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=127
05/27/2022 01:09:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=128
05/27/2022 01:09:06 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.4153026265702322 on epoch=128
05/27/2022 01:09:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=128
05/27/2022 01:09:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=129
05/27/2022 01:09:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=129
05/27/2022 01:09:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=130
05/27/2022 01:09:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=131
05/27/2022 01:09:24 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.6320664324496965 on epoch=131
05/27/2022 01:09:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=131
05/27/2022 01:09:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=132
05/27/2022 01:09:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=133
05/27/2022 01:09:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=133
05/27/2022 01:09:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=134
05/27/2022 01:09:42 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.6855809383443872 on epoch=134
05/27/2022 01:09:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6438737292669877 -> 0.6855809383443872 on epoch=134, global_step=2150
05/27/2022 01:09:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=134
05/27/2022 01:09:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
05/27/2022 01:09:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=136
05/27/2022 01:09:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=136
05/27/2022 01:09:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=137
05/27/2022 01:09:59 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.6930455635491606 on epoch=137
05/27/2022 01:09:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6855809383443872 -> 0.6930455635491606 on epoch=137, global_step=2200
05/27/2022 01:10:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=138
05/27/2022 01:10:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.31 on epoch=138
05/27/2022 01:10:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=139
05/27/2022 01:10:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=139
05/27/2022 01:10:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=140
05/27/2022 01:10:17 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.4957260323484232 on epoch=140
05/27/2022 01:10:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
05/27/2022 01:10:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.31 on epoch=141
05/27/2022 01:10:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.29 on epoch=142
05/27/2022 01:10:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
05/27/2022 01:10:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=143
05/27/2022 01:10:34 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.43468822516655437 on epoch=143
05/27/2022 01:10:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=144
05/27/2022 01:10:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/27/2022 01:10:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=145
05/27/2022 01:10:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=146
05/27/2022 01:10:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=146
05/27/2022 01:10:52 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.6589854695227451 on epoch=146
05/27/2022 01:10:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=147
05/27/2022 01:10:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/27/2022 01:10:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/27/2022 01:11:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=149
05/27/2022 01:11:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.31 on epoch=149
05/27/2022 01:11:09 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.634257623473385 on epoch=149
05/27/2022 01:11:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=150
05/27/2022 01:11:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=151
05/27/2022 01:11:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.25 on epoch=151
05/27/2022 01:11:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.31 on epoch=152
05/27/2022 01:11:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.22 on epoch=153
05/27/2022 01:11:27 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.5647146840780793 on epoch=153
05/27/2022 01:11:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=153
05/27/2022 01:11:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=154
05/27/2022 01:11:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=154
05/27/2022 01:11:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=155
05/27/2022 01:11:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=156
05/27/2022 01:11:45 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.6906084109718972 on epoch=156
05/27/2022 01:11:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=156
05/27/2022 01:11:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=157
05/27/2022 01:11:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=158
05/27/2022 01:11:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=158
05/27/2022 01:11:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.26 on epoch=159
05/27/2022 01:12:02 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.6464691917389482 on epoch=159
05/27/2022 01:12:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.29 on epoch=159
05/27/2022 01:12:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=160
05/27/2022 01:12:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=161
05/27/2022 01:12:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=161
05/27/2022 01:12:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.26 on epoch=162
05/27/2022 01:12:20 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.6507392601053367 on epoch=162
05/27/2022 01:12:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/27/2022 01:12:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
05/27/2022 01:12:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
05/27/2022 01:12:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=164
05/27/2022 01:12:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.31 on epoch=165
05/27/2022 01:12:37 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.5725777890209968 on epoch=165
05/27/2022 01:12:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=166
05/27/2022 01:12:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=166
05/27/2022 01:12:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=167
05/27/2022 01:12:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.24 on epoch=168
05/27/2022 01:12:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/27/2022 01:12:55 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.4745030250648228 on epoch=168
05/27/2022 01:12:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=169
05/27/2022 01:13:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=169
05/27/2022 01:13:03 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=170
05/27/2022 01:13:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
05/27/2022 01:13:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=171
05/27/2022 01:13:12 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.656493790176621 on epoch=171
05/27/2022 01:13:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.24 on epoch=172
05/27/2022 01:13:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=173
05/27/2022 01:13:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=173
05/27/2022 01:13:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.26 on epoch=174
05/27/2022 01:13:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.30 on epoch=174
05/27/2022 01:13:30 - INFO - __main__ - Global step 2800 Train loss 0.24 Classification-F1 0.6619995087202162 on epoch=174
05/27/2022 01:13:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.30 on epoch=175
05/27/2022 01:13:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=176
05/27/2022 01:13:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=176
05/27/2022 01:13:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
05/27/2022 01:13:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=178
05/27/2022 01:13:48 - INFO - __main__ - Global step 2850 Train loss 0.23 Classification-F1 0.5094837133264191 on epoch=178
05/27/2022 01:13:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=178
05/27/2022 01:13:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/27/2022 01:13:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/27/2022 01:13:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=180
05/27/2022 01:14:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=181
05/27/2022 01:14:05 - INFO - __main__ - Global step 2900 Train loss 0.21 Classification-F1 0.6788559242300697 on epoch=181
05/27/2022 01:14:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=181
05/27/2022 01:14:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.21 on epoch=182
05/27/2022 01:14:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=183
05/27/2022 01:14:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=183
05/27/2022 01:14:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=184
05/27/2022 01:14:23 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.670956252419667 on epoch=184
05/27/2022 01:14:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
05/27/2022 01:14:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.25 on epoch=185
05/27/2022 01:14:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/27/2022 01:14:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=186
05/27/2022 01:14:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=187
05/27/2022 01:14:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:14:37 - INFO - __main__ - Printing 3 examples
05/27/2022 01:14:37 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:14:37 - INFO - __main__ - ['false']
05/27/2022 01:14:37 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:14:37 - INFO - __main__ - ['false']
05/27/2022 01:14:37 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:14:37 - INFO - __main__ - ['false']
05/27/2022 01:14:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:14:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:14:38 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:14:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:14:38 - INFO - __main__ - Printing 3 examples
05/27/2022 01:14:38 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:14:38 - INFO - __main__ - ['false']
05/27/2022 01:14:38 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:14:38 - INFO - __main__ - ['false']
05/27/2022 01:14:38 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:14:38 - INFO - __main__ - ['false']
05/27/2022 01:14:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:14:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:14:38 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:14:41 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.6775668237160051 on epoch=187
05/27/2022 01:14:41 - INFO - __main__ - save last model!
05/27/2022 01:14:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 01:14:41 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 01:14:41 - INFO - __main__ - Printing 3 examples
05/27/2022 01:14:41 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 01:14:41 - INFO - __main__ - ['false']
05/27/2022 01:14:41 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 01:14:41 - INFO - __main__ - ['false']
05/27/2022 01:14:41 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 01:14:41 - INFO - __main__ - ['false']
05/27/2022 01:14:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:14:42 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:14:45 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 01:14:57 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:14:57 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:14:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:14:58 - INFO - __main__ - Starting training!
05/27/2022 01:15:35 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.2_8_predictions.txt
05/27/2022 01:15:35 - INFO - __main__ - Classification-F1 on test data: 0.4089
05/27/2022 01:15:35 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.2, bsz=8, dev_performance=0.6930455635491606, test_performance=0.40885787233702975
05/27/2022 01:15:35 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.5, bsz=8 ...
05/27/2022 01:15:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:15:36 - INFO - __main__ - Printing 3 examples
05/27/2022 01:15:36 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:15:36 - INFO - __main__ - ['false']
05/27/2022 01:15:36 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:15:36 - INFO - __main__ - ['false']
05/27/2022 01:15:36 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:15:36 - INFO - __main__ - ['false']
05/27/2022 01:15:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:15:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:15:37 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:15:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:15:37 - INFO - __main__ - Printing 3 examples
05/27/2022 01:15:37 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:15:37 - INFO - __main__ - ['false']
05/27/2022 01:15:37 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:15:37 - INFO - __main__ - ['false']
05/27/2022 01:15:37 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:15:37 - INFO - __main__ - ['false']
05/27/2022 01:15:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:15:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:15:37 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:15:56 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:15:56 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:15:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:15:57 - INFO - __main__ - Starting training!
05/27/2022 01:15:59 - INFO - __main__ - Step 10 Global step 10 Train loss 5.26 on epoch=0
05/27/2022 01:16:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.91 on epoch=1
05/27/2022 01:16:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=1
05/27/2022 01:16:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=2
05/27/2022 01:16:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/27/2022 01:16:13 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 01:16:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 01:16:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=3
05/27/2022 01:16:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=4
05/27/2022 01:16:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=4
05/27/2022 01:16:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=5
05/27/2022 01:16:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/27/2022 01:16:29 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 01:16:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
05/27/2022 01:16:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=7
05/27/2022 01:16:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/27/2022 01:16:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/27/2022 01:16:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/27/2022 01:16:46 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3383422492035824 on epoch=9
05/27/2022 01:16:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3383422492035824 on epoch=9, global_step=150
05/27/2022 01:16:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=9
05/27/2022 01:16:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
05/27/2022 01:16:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
05/27/2022 01:16:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
05/27/2022 01:16:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=12
05/27/2022 01:17:02 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=12
05/27/2022 01:17:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
05/27/2022 01:17:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=13
05/27/2022 01:17:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=14
05/27/2022 01:17:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=14
05/27/2022 01:17:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/27/2022 01:17:19 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=15
05/27/2022 01:17:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/27/2022 01:17:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/27/2022 01:17:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/27/2022 01:17:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=18
05/27/2022 01:17:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
05/27/2022 01:17:36 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 01:17:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 01:17:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/27/2022 01:17:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
05/27/2022 01:17:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 01:17:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
05/27/2022 01:17:53 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.350463149416029 on epoch=21
05/27/2022 01:17:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3383422492035824 -> 0.350463149416029 on epoch=21, global_step=350
05/27/2022 01:17:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=22
05/27/2022 01:17:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
05/27/2022 01:18:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/27/2022 01:18:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=24
05/27/2022 01:18:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/27/2022 01:18:09 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 01:18:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/27/2022 01:18:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=26
05/27/2022 01:18:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
05/27/2022 01:18:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=27
05/27/2022 01:18:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=28
05/27/2022 01:18:26 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 01:18:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/27/2022 01:18:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
05/27/2022 01:18:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/27/2022 01:18:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/27/2022 01:18:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
05/27/2022 01:18:43 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.35113468906572354 on epoch=31
05/27/2022 01:18:43 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.35113468906572354 on epoch=31, global_step=500
05/27/2022 01:18:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
05/27/2022 01:18:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/27/2022 01:18:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=33
05/27/2022 01:18:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 01:18:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/27/2022 01:18:59 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3823105733218093 on epoch=34
05/27/2022 01:18:59 - INFO - __main__ - Saving model with best Classification-F1: 0.35113468906572354 -> 0.3823105733218093 on epoch=34, global_step=550
05/27/2022 01:19:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/27/2022 01:19:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
05/27/2022 01:19:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/27/2022 01:19:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/27/2022 01:19:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=37
05/27/2022 01:19:16 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.3871086556169429 on epoch=37
05/27/2022 01:19:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3823105733218093 -> 0.3871086556169429 on epoch=37, global_step=600
05/27/2022 01:19:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/27/2022 01:19:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/27/2022 01:19:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/27/2022 01:19:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/27/2022 01:19:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/27/2022 01:19:33 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.4300061585105879 on epoch=40
05/27/2022 01:19:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3871086556169429 -> 0.4300061585105879 on epoch=40, global_step=650
05/27/2022 01:19:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/27/2022 01:19:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/27/2022 01:19:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/27/2022 01:19:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=43
05/27/2022 01:19:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/27/2022 01:19:50 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 01:19:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=44
05/27/2022 01:19:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/27/2022 01:19:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
05/27/2022 01:20:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=46
05/27/2022 01:20:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
05/27/2022 01:20:07 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3917433917433918 on epoch=46
05/27/2022 01:20:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/27/2022 01:20:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
05/27/2022 01:20:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/27/2022 01:20:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/27/2022 01:20:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
05/27/2022 01:20:23 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 01:20:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/27/2022 01:20:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=51
05/27/2022 01:20:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/27/2022 01:20:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=52
05/27/2022 01:20:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/27/2022 01:20:40 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 01:20:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
05/27/2022 01:20:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
05/27/2022 01:20:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
05/27/2022 01:20:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
05/27/2022 01:20:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=56
05/27/2022 01:20:57 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.4037987130608795 on epoch=56
05/27/2022 01:20:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/27/2022 01:21:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=57
05/27/2022 01:21:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
05/27/2022 01:21:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
05/27/2022 01:21:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
05/27/2022 01:21:14 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.4819427148194272 on epoch=59
05/27/2022 01:21:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4300061585105879 -> 0.4819427148194272 on epoch=59, global_step=950
05/27/2022 01:21:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
05/27/2022 01:21:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=60
05/27/2022 01:21:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
05/27/2022 01:21:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/27/2022 01:21:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
05/27/2022 01:21:31 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3712545436683367 on epoch=62
05/27/2022 01:21:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
05/27/2022 01:21:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
05/27/2022 01:21:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/27/2022 01:21:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
05/27/2022 01:21:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/27/2022 01:21:48 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.3845121610287951 on epoch=65
05/27/2022 01:21:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=66
05/27/2022 01:21:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=66
05/27/2022 01:21:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
05/27/2022 01:21:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/27/2022 01:22:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/27/2022 01:22:05 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.34673046251993617 on epoch=68
05/27/2022 01:22:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/27/2022 01:22:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/27/2022 01:22:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
05/27/2022 01:22:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/27/2022 01:22:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/27/2022 01:22:24 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.4300061585105879 on epoch=71
05/27/2022 01:22:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/27/2022 01:22:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=73
05/27/2022 01:22:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
05/27/2022 01:22:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
05/27/2022 01:22:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
05/27/2022 01:22:41 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.4059435586495995 on epoch=74
05/27/2022 01:22:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
05/27/2022 01:22:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=76
05/27/2022 01:22:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=76
05/27/2022 01:22:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
05/27/2022 01:22:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
05/27/2022 01:22:57 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3750290630086026 on epoch=78
05/27/2022 01:23:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/27/2022 01:23:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=79
05/27/2022 01:23:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
05/27/2022 01:23:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
05/27/2022 01:23:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=81
05/27/2022 01:23:15 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.548791515318712 on epoch=81
05/27/2022 01:23:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4819427148194272 -> 0.548791515318712 on epoch=81, global_step=1300
05/27/2022 01:23:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
05/27/2022 01:23:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=82
05/27/2022 01:23:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
05/27/2022 01:23:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
05/27/2022 01:23:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
05/27/2022 01:23:32 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.5207817754933689 on epoch=84
05/27/2022 01:23:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=84
05/27/2022 01:23:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=85
05/27/2022 01:23:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=86
05/27/2022 01:23:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
05/27/2022 01:23:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/27/2022 01:23:50 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.5674514819903043 on epoch=87
05/27/2022 01:23:50 - INFO - __main__ - Saving model with best Classification-F1: 0.548791515318712 -> 0.5674514819903043 on epoch=87, global_step=1400
05/27/2022 01:23:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=88
05/27/2022 01:23:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=88
05/27/2022 01:23:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=89
05/27/2022 01:24:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
05/27/2022 01:24:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=90
05/27/2022 01:24:07 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.5466259541984734 on epoch=90
05/27/2022 01:24:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=91
05/27/2022 01:24:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=91
05/27/2022 01:24:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=92
05/27/2022 01:24:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=93
05/27/2022 01:24:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=93
05/27/2022 01:24:25 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.5418875927889714 on epoch=93
05/27/2022 01:24:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=94
05/27/2022 01:24:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=94
05/27/2022 01:24:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
05/27/2022 01:24:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=96
05/27/2022 01:24:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=96
05/27/2022 01:24:42 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.5815384615384616 on epoch=96
05/27/2022 01:24:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5674514819903043 -> 0.5815384615384616 on epoch=96, global_step=1550
05/27/2022 01:24:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=97
05/27/2022 01:24:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=98
05/27/2022 01:24:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=98
05/27/2022 01:24:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=99
05/27/2022 01:24:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=99
05/27/2022 01:25:00 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.5897435897435898 on epoch=99
05/27/2022 01:25:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5815384615384616 -> 0.5897435897435898 on epoch=99, global_step=1600
05/27/2022 01:25:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=100
05/27/2022 01:25:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
05/27/2022 01:25:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=101
05/27/2022 01:25:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=102
05/27/2022 01:25:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
05/27/2022 01:25:17 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.6210879682612345 on epoch=103
05/27/2022 01:25:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5897435897435898 -> 0.6210879682612345 on epoch=103, global_step=1650
05/27/2022 01:25:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=103
05/27/2022 01:25:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
05/27/2022 01:25:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/27/2022 01:25:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=105
05/27/2022 01:25:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
05/27/2022 01:25:35 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.6047389742413819 on epoch=106
05/27/2022 01:25:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
05/27/2022 01:25:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=107
05/27/2022 01:25:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=108
05/27/2022 01:25:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=108
05/27/2022 01:25:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=109
05/27/2022 01:25:53 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.605911330049261 on epoch=109
05/27/2022 01:25:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/27/2022 01:25:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=110
05/27/2022 01:26:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/27/2022 01:26:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/27/2022 01:26:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/27/2022 01:26:11 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.6326106870229008 on epoch=112
05/27/2022 01:26:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6210879682612345 -> 0.6326106870229008 on epoch=112, global_step=1800
05/27/2022 01:26:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=113
05/27/2022 01:26:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
05/27/2022 01:26:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
05/27/2022 01:26:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/27/2022 01:26:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=115
05/27/2022 01:26:29 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.625333148464821 on epoch=115
05/27/2022 01:26:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=116
05/27/2022 01:26:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
05/27/2022 01:26:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=117
05/27/2022 01:26:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/27/2022 01:26:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
05/27/2022 01:26:46 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.52754370081482 on epoch=118
05/27/2022 01:26:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=119
05/27/2022 01:26:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
05/27/2022 01:26:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=120
05/27/2022 01:26:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
05/27/2022 01:26:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=121
05/27/2022 01:27:05 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.6678420417945078 on epoch=121
05/27/2022 01:27:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6326106870229008 -> 0.6678420417945078 on epoch=121, global_step=1950
05/27/2022 01:27:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=122
05/27/2022 01:27:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=123
05/27/2022 01:27:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=123
05/27/2022 01:27:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/27/2022 01:27:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
05/27/2022 01:27:23 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.6165512076390189 on epoch=124
05/27/2022 01:27:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/27/2022 01:27:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
05/27/2022 01:27:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=126
05/27/2022 01:27:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=127
05/27/2022 01:27:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=128
05/27/2022 01:27:41 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.6020052033758487 on epoch=128
05/27/2022 01:27:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=128
05/27/2022 01:27:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=129
05/27/2022 01:27:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/27/2022 01:27:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
05/27/2022 01:27:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=131
05/27/2022 01:27:59 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.5512965420086163 on epoch=131
05/27/2022 01:28:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=131
05/27/2022 01:28:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/27/2022 01:28:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/27/2022 01:28:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/27/2022 01:28:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=134
05/27/2022 01:28:17 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.5615794881831973 on epoch=134
05/27/2022 01:28:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=134
05/27/2022 01:28:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/27/2022 01:28:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
05/27/2022 01:28:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/27/2022 01:28:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/27/2022 01:28:36 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.5661084417906734 on epoch=137
05/27/2022 01:28:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
05/27/2022 01:28:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
05/27/2022 01:28:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/27/2022 01:28:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/27/2022 01:28:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=140
05/27/2022 01:28:54 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.5979536937693397 on epoch=140
05/27/2022 01:28:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=141
05/27/2022 01:28:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=141
05/27/2022 01:29:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=142
05/27/2022 01:29:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/27/2022 01:29:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/27/2022 01:29:13 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.6541390321788259 on epoch=143
05/27/2022 01:29:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=144
05/27/2022 01:29:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=144
05/27/2022 01:29:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/27/2022 01:29:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/27/2022 01:29:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
05/27/2022 01:29:31 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.6305576025546549 on epoch=146
05/27/2022 01:29:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=147
05/27/2022 01:29:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
05/27/2022 01:29:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=148
05/27/2022 01:29:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/27/2022 01:29:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=149
05/27/2022 01:29:49 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.6364469283980025 on epoch=149
05/27/2022 01:29:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=150
05/27/2022 01:29:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/27/2022 01:29:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=151
05/27/2022 01:29:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
05/27/2022 01:30:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/27/2022 01:30:07 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.6289681612043574 on epoch=153
05/27/2022 01:30:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=153
05/27/2022 01:30:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
05/27/2022 01:30:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=154
05/27/2022 01:30:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
05/27/2022 01:30:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=156
05/27/2022 01:30:25 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.5630099444052933 on epoch=156
05/27/2022 01:30:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/27/2022 01:30:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
05/27/2022 01:30:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/27/2022 01:30:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
05/27/2022 01:30:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/27/2022 01:30:43 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6351096602142627 on epoch=159
05/27/2022 01:30:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
05/27/2022 01:30:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/27/2022 01:30:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=161
05/27/2022 01:30:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=161
05/27/2022 01:30:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
05/27/2022 01:31:02 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.6055345911949686 on epoch=162
05/27/2022 01:31:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=163
05/27/2022 01:31:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
05/27/2022 01:31:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
05/27/2022 01:31:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=164
05/27/2022 01:31:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
05/27/2022 01:31:20 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.635467226041555 on epoch=165
05/27/2022 01:31:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
05/27/2022 01:31:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=166
05/27/2022 01:31:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/27/2022 01:31:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/27/2022 01:31:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
05/27/2022 01:31:38 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6280871670702179 on epoch=168
05/27/2022 01:31:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=169
05/27/2022 01:31:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/27/2022 01:31:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/27/2022 01:31:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/27/2022 01:31:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=171
05/27/2022 01:31:56 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.6559139784946237 on epoch=171
05/27/2022 01:31:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/27/2022 01:32:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/27/2022 01:32:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/27/2022 01:32:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
05/27/2022 01:32:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=174
05/27/2022 01:32:15 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.634257623473385 on epoch=174
05/27/2022 01:32:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/27/2022 01:32:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/27/2022 01:32:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/27/2022 01:32:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/27/2022 01:32:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/27/2022 01:32:33 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6236236236236236 on epoch=178
05/27/2022 01:32:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/27/2022 01:32:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/27/2022 01:32:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/27/2022 01:32:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/27/2022 01:32:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
05/27/2022 01:32:52 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.6398336187912894 on epoch=181
05/27/2022 01:32:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=181
05/27/2022 01:32:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/27/2022 01:33:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/27/2022 01:33:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/27/2022 01:33:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/27/2022 01:33:10 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6091603053435114 on epoch=184
05/27/2022 01:33:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/27/2022 01:33:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=185
05/27/2022 01:33:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/27/2022 01:33:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/27/2022 01:33:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/27/2022 01:33:24 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:33:24 - INFO - __main__ - Printing 3 examples
05/27/2022 01:33:24 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:33:24 - INFO - __main__ - ['false']
05/27/2022 01:33:24 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:33:24 - INFO - __main__ - ['false']
05/27/2022 01:33:24 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:33:24 - INFO - __main__ - ['false']
05/27/2022 01:33:24 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:33:24 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:33:25 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:33:25 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:33:25 - INFO - __main__ - Printing 3 examples
05/27/2022 01:33:25 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:33:25 - INFO - __main__ - ['false']
05/27/2022 01:33:25 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:33:25 - INFO - __main__ - ['false']
05/27/2022 01:33:25 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:33:25 - INFO - __main__ - ['false']
05/27/2022 01:33:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:33:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:33:25 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:33:29 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.6106613411168291 on epoch=187
05/27/2022 01:33:29 - INFO - __main__ - save last model!
05/27/2022 01:33:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 01:33:29 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 01:33:29 - INFO - __main__ - Printing 3 examples
05/27/2022 01:33:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 01:33:29 - INFO - __main__ - ['false']
05/27/2022 01:33:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 01:33:29 - INFO - __main__ - ['false']
05/27/2022 01:33:29 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 01:33:29 - INFO - __main__ - ['false']
05/27/2022 01:33:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:33:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:33:33 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 01:33:44 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:33:44 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:33:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:33:45 - INFO - __main__ - Starting training!
05/27/2022 01:34:36 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.5_8_predictions.txt
05/27/2022 01:34:36 - INFO - __main__ - Classification-F1 on test data: 0.4755
05/27/2022 01:34:37 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.5, bsz=8, dev_performance=0.6678420417945078, test_performance=0.47545248055182476
05/27/2022 01:34:37 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.4, bsz=8 ...
05/27/2022 01:34:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:34:38 - INFO - __main__ - Printing 3 examples
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:34:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:34:38 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:34:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:34:38 - INFO - __main__ - Printing 3 examples
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:34:38 - INFO - __main__ - ['false']
05/27/2022 01:34:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:34:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:34:39 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:34:53 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:34:53 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:34:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:34:54 - INFO - __main__ - Starting training!
05/27/2022 01:34:57 - INFO - __main__ - Step 10 Global step 10 Train loss 5.99 on epoch=0
05/27/2022 01:34:59 - INFO - __main__ - Step 20 Global step 20 Train loss 1.30 on epoch=1
05/27/2022 01:35:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=1
05/27/2022 01:35:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
05/27/2022 01:35:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=3
05/27/2022 01:35:24 - INFO - __main__ - Global step 50 Train loss 1.78 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 01:35:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 01:35:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=3
05/27/2022 01:35:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
05/27/2022 01:35:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=4
05/27/2022 01:35:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
05/27/2022 01:35:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
05/27/2022 01:35:40 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.43686064577660866 on epoch=6
05/27/2022 01:35:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43686064577660866 on epoch=6, global_step=100
05/27/2022 01:35:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/27/2022 01:35:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=7
05/27/2022 01:35:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/27/2022 01:35:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=8
05/27/2022 01:35:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
05/27/2022 01:35:56 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3401530406766009 on epoch=9
05/27/2022 01:35:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
05/27/2022 01:36:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
05/27/2022 01:36:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=11
05/27/2022 01:36:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=11
05/27/2022 01:36:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
05/27/2022 01:36:12 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.405327045672867 on epoch=12
05/27/2022 01:36:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/27/2022 01:36:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=13
05/27/2022 01:36:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=14
05/27/2022 01:36:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
05/27/2022 01:36:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/27/2022 01:36:28 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=15
05/27/2022 01:36:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=16
05/27/2022 01:36:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
05/27/2022 01:36:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/27/2022 01:36:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/27/2022 01:36:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/27/2022 01:36:44 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 01:36:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=19
05/27/2022 01:36:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/27/2022 01:36:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=20
05/27/2022 01:36:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=21
05/27/2022 01:36:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
05/27/2022 01:37:00 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.40402116402116395 on epoch=21
05/27/2022 01:37:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/27/2022 01:37:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=23
05/27/2022 01:37:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/27/2022 01:37:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/27/2022 01:37:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/27/2022 01:37:17 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 01:37:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/27/2022 01:37:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=26
05/27/2022 01:37:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/27/2022 01:37:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
05/27/2022 01:37:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
05/27/2022 01:37:33 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 01:37:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/27/2022 01:37:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
05/27/2022 01:37:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=29
05/27/2022 01:37:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=30
05/27/2022 01:37:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
05/27/2022 01:37:50 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=31
05/27/2022 01:37:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
05/27/2022 01:37:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/27/2022 01:37:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
05/27/2022 01:38:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
05/27/2022 01:38:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/27/2022 01:38:06 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.37996191321389616 on epoch=34
05/27/2022 01:38:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/27/2022 01:38:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/27/2022 01:38:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=36
05/27/2022 01:38:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/27/2022 01:38:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
05/27/2022 01:38:22 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.41307155773701887 on epoch=37
05/27/2022 01:38:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/27/2022 01:38:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/27/2022 01:38:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
05/27/2022 01:38:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=39
05/27/2022 01:38:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
05/27/2022 01:38:38 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.4079058031959629 on epoch=40
05/27/2022 01:38:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
05/27/2022 01:38:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
05/27/2022 01:38:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
05/27/2022 01:38:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
05/27/2022 01:38:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
05/27/2022 01:38:54 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 01:38:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
05/27/2022 01:38:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/27/2022 01:39:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
05/27/2022 01:39:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
05/27/2022 01:39:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=46
05/27/2022 01:39:10 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.3849492366116407 on epoch=46
05/27/2022 01:39:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
05/27/2022 01:39:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
05/27/2022 01:39:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/27/2022 01:39:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=49
05/27/2022 01:39:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=49
05/27/2022 01:39:26 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 01:39:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
05/27/2022 01:39:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/27/2022 01:39:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=51
05/27/2022 01:39:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
05/27/2022 01:39:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
05/27/2022 01:39:42 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 01:39:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 01:39:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=54
05/27/2022 01:39:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=54
05/27/2022 01:39:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
05/27/2022 01:39:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
05/27/2022 01:39:59 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.3822393822393822 on epoch=56
05/27/2022 01:40:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/27/2022 01:40:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
05/27/2022 01:40:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/27/2022 01:40:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
05/27/2022 01:40:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
05/27/2022 01:40:17 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3589111276786909 on epoch=59
05/27/2022 01:40:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
05/27/2022 01:40:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=60
05/27/2022 01:40:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 01:40:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
05/27/2022 01:40:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
05/27/2022 01:40:33 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.4297594297594297 on epoch=62
05/27/2022 01:40:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
05/27/2022 01:40:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
05/27/2022 01:40:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/27/2022 01:40:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=64
05/27/2022 01:40:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
05/27/2022 01:40:50 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.5186409907281333 on epoch=65
05/27/2022 01:40:50 - INFO - __main__ - Saving model with best Classification-F1: 0.43686064577660866 -> 0.5186409907281333 on epoch=65, global_step=1050
05/27/2022 01:40:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
05/27/2022 01:40:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
05/27/2022 01:40:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
05/27/2022 01:41:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/27/2022 01:41:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=68
05/27/2022 01:41:07 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.350463149416029 on epoch=68
05/27/2022 01:41:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/27/2022 01:41:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=69
05/27/2022 01:41:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
05/27/2022 01:41:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=71
05/27/2022 01:41:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/27/2022 01:41:25 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.4885114885114885 on epoch=71
05/27/2022 01:41:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/27/2022 01:41:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=73
05/27/2022 01:41:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
05/27/2022 01:41:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=74
05/27/2022 01:41:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=74
05/27/2022 01:41:43 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.5388277681271312 on epoch=74
05/27/2022 01:41:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5186409907281333 -> 0.5388277681271312 on epoch=74, global_step=1200
05/27/2022 01:41:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
05/27/2022 01:41:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
05/27/2022 01:41:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=76
05/27/2022 01:41:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=77
05/27/2022 01:41:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
05/27/2022 01:42:01 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.5524750046003803 on epoch=78
05/27/2022 01:42:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5388277681271312 -> 0.5524750046003803 on epoch=78, global_step=1250
05/27/2022 01:42:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=78
05/27/2022 01:42:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=79
05/27/2022 01:42:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=79
05/27/2022 01:42:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=80
05/27/2022 01:42:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=81
05/27/2022 01:42:19 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.5674514819903043 on epoch=81
05/27/2022 01:42:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5524750046003803 -> 0.5674514819903043 on epoch=81, global_step=1300
05/27/2022 01:42:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=81
05/27/2022 01:42:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=82
05/27/2022 01:42:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=83
05/27/2022 01:42:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
05/27/2022 01:42:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
05/27/2022 01:42:38 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.5665024630541872 on epoch=84
05/27/2022 01:42:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=84
05/27/2022 01:42:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=85
05/27/2022 01:42:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=86
05/27/2022 01:42:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=86
05/27/2022 01:42:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=87
05/27/2022 01:42:56 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.40030721966205834 on epoch=87
05/27/2022 01:42:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/27/2022 01:43:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=88
05/27/2022 01:43:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=89
05/27/2022 01:43:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
05/27/2022 01:43:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=90
05/27/2022 01:43:14 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.6016699788941782 on epoch=90
05/27/2022 01:43:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5674514819903043 -> 0.6016699788941782 on epoch=90, global_step=1450
05/27/2022 01:43:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=91
05/27/2022 01:43:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/27/2022 01:43:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/27/2022 01:43:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=93
05/27/2022 01:43:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
05/27/2022 01:43:31 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.55 on epoch=93
05/27/2022 01:43:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/27/2022 01:43:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/27/2022 01:43:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=95
05/27/2022 01:43:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=96
05/27/2022 01:43:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=96
05/27/2022 01:43:48 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.5895158803576935 on epoch=96
05/27/2022 01:43:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=97
05/27/2022 01:43:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/27/2022 01:43:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=98
05/27/2022 01:43:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=99
05/27/2022 01:44:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/27/2022 01:44:06 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.5599535423925668 on epoch=99
05/27/2022 01:44:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=100
05/27/2022 01:44:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=101
05/27/2022 01:44:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=101
05/27/2022 01:44:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
05/27/2022 01:44:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/27/2022 01:44:24 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.5888312330121077 on epoch=103
05/27/2022 01:44:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
05/27/2022 01:44:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=104
05/27/2022 01:44:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=104
05/27/2022 01:44:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
05/27/2022 01:44:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=106
05/27/2022 01:44:42 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.5801955219173762 on epoch=106
05/27/2022 01:44:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/27/2022 01:44:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
05/27/2022 01:44:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=108
05/27/2022 01:44:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/27/2022 01:44:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=109
05/27/2022 01:44:59 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.5935242839352428 on epoch=109
05/27/2022 01:45:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/27/2022 01:45:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=110
05/27/2022 01:45:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=111
05/27/2022 01:45:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=111
05/27/2022 01:45:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/27/2022 01:45:16 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.6067466290129875 on epoch=112
05/27/2022 01:45:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6016699788941782 -> 0.6067466290129875 on epoch=112, global_step=1800
05/27/2022 01:45:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=113
05/27/2022 01:45:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=113
05/27/2022 01:45:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=114
05/27/2022 01:45:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/27/2022 01:45:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
05/27/2022 01:45:34 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.6231368459792677 on epoch=115
05/27/2022 01:45:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6067466290129875 -> 0.6231368459792677 on epoch=115, global_step=1850
05/27/2022 01:45:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/27/2022 01:45:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=116
05/27/2022 01:45:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/27/2022 01:45:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=118
05/27/2022 01:45:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
05/27/2022 01:45:52 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.5790462767206954 on epoch=118
05/27/2022 01:45:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
05/27/2022 01:45:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=119
05/27/2022 01:45:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=120
05/27/2022 01:46:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=121
05/27/2022 01:46:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=121
05/27/2022 01:46:10 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.6216748768472906 on epoch=121
05/27/2022 01:46:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/27/2022 01:46:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/27/2022 01:46:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=123
05/27/2022 01:46:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/27/2022 01:46:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=124
05/27/2022 01:46:27 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.5894339622641509 on epoch=124
05/27/2022 01:46:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=125
05/27/2022 01:46:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/27/2022 01:46:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=126
05/27/2022 01:46:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=127
05/27/2022 01:46:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.15 on epoch=128
05/27/2022 01:46:45 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.5613934894346089 on epoch=128
05/27/2022 01:46:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=128
05/27/2022 01:46:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=129
05/27/2022 01:46:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
05/27/2022 01:46:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=130
05/27/2022 01:46:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/27/2022 01:47:04 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.625333148464821 on epoch=131
05/27/2022 01:47:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6231368459792677 -> 0.625333148464821 on epoch=131, global_step=2100
05/27/2022 01:47:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=131
05/27/2022 01:47:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=132
05/27/2022 01:47:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/27/2022 01:47:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
05/27/2022 01:47:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=134
05/27/2022 01:47:22 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.5728926670374195 on epoch=134
05/27/2022 01:47:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=134
05/27/2022 01:47:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/27/2022 01:47:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=136
05/27/2022 01:47:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=136
05/27/2022 01:47:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
05/27/2022 01:47:40 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.5668358714043994 on epoch=137
05/27/2022 01:47:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/27/2022 01:47:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=138
05/27/2022 01:47:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/27/2022 01:47:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=139
05/27/2022 01:47:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=140
05/27/2022 01:47:58 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5901477832512315 on epoch=140
05/27/2022 01:48:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=141
05/27/2022 01:48:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/27/2022 01:48:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=142
05/27/2022 01:48:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=143
05/27/2022 01:48:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=143
05/27/2022 01:48:16 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.6050306232175418 on epoch=143
05/27/2022 01:48:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/27/2022 01:48:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/27/2022 01:48:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=145
05/27/2022 01:48:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=146
05/27/2022 01:48:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
05/27/2022 01:48:34 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.5810930852307261 on epoch=146
05/27/2022 01:48:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
05/27/2022 01:48:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
05/27/2022 01:48:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/27/2022 01:48:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=149
05/27/2022 01:48:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/27/2022 01:48:52 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.6210879682612345 on epoch=149
05/27/2022 01:48:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/27/2022 01:48:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
05/27/2022 01:48:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=151
05/27/2022 01:49:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/27/2022 01:49:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/27/2022 01:49:10 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.5686838240976891 on epoch=153
05/27/2022 01:49:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
05/27/2022 01:49:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=154
05/27/2022 01:49:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
05/27/2022 01:49:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/27/2022 01:49:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
05/27/2022 01:49:28 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.5780068093235353 on epoch=156
05/27/2022 01:49:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=156
05/27/2022 01:49:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/27/2022 01:49:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/27/2022 01:49:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/27/2022 01:49:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
05/27/2022 01:49:46 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.5629104281586551 on epoch=159
05/27/2022 01:49:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/27/2022 01:49:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=160
05/27/2022 01:49:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=161
05/27/2022 01:49:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
05/27/2022 01:49:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/27/2022 01:50:04 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.5535402860132543 on epoch=162
05/27/2022 01:50:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/27/2022 01:50:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=163
05/27/2022 01:50:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/27/2022 01:50:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
05/27/2022 01:50:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=165
05/27/2022 01:50:23 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.6313124274099884 on epoch=165
05/27/2022 01:50:23 - INFO - __main__ - Saving model with best Classification-F1: 0.625333148464821 -> 0.6313124274099884 on epoch=165, global_step=2650
05/27/2022 01:50:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/27/2022 01:50:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=166
05/27/2022 01:50:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=167
05/27/2022 01:50:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/27/2022 01:50:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/27/2022 01:50:41 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.640625 on epoch=168
05/27/2022 01:50:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6313124274099884 -> 0.640625 on epoch=168, global_step=2700
05/27/2022 01:50:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=169
05/27/2022 01:50:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/27/2022 01:50:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/27/2022 01:50:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=171
05/27/2022 01:50:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/27/2022 01:50:58 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.5790462767206954 on epoch=171
05/27/2022 01:51:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=172
05/27/2022 01:51:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/27/2022 01:51:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/27/2022 01:51:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=174
05/27/2022 01:51:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=174
05/27/2022 01:51:16 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.5845119406445629 on epoch=174
05/27/2022 01:51:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
05/27/2022 01:51:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/27/2022 01:51:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/27/2022 01:51:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=177
05/27/2022 01:51:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/27/2022 01:51:35 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.6244269209609389 on epoch=178
05/27/2022 01:51:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
05/27/2022 01:51:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
05/27/2022 01:51:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/27/2022 01:51:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
05/27/2022 01:51:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=181
05/27/2022 01:51:52 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.590857603922603 on epoch=181
05/27/2022 01:51:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/27/2022 01:51:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
05/27/2022 01:52:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/27/2022 01:52:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
05/27/2022 01:52:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/27/2022 01:52:10 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.5882686849574267 on epoch=184
05/27/2022 01:52:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/27/2022 01:52:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=185
05/27/2022 01:52:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/27/2022 01:52:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=186
05/27/2022 01:52:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/27/2022 01:52:24 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:52:24 - INFO - __main__ - Printing 3 examples
05/27/2022 01:52:24 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:52:24 - INFO - __main__ - ['false']
05/27/2022 01:52:24 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:52:24 - INFO - __main__ - ['false']
05/27/2022 01:52:24 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:52:24 - INFO - __main__ - ['false']
05/27/2022 01:52:24 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:52:24 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:52:25 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:52:25 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:52:25 - INFO - __main__ - Printing 3 examples
05/27/2022 01:52:25 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:52:25 - INFO - __main__ - ['false']
05/27/2022 01:52:25 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:52:25 - INFO - __main__ - ['false']
05/27/2022 01:52:25 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:52:25 - INFO - __main__ - ['false']
05/27/2022 01:52:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:52:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:52:25 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:52:28 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6064686712168728 on epoch=187
05/27/2022 01:52:28 - INFO - __main__ - save last model!
05/27/2022 01:52:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 01:52:28 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 01:52:28 - INFO - __main__ - Printing 3 examples
05/27/2022 01:52:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 01:52:28 - INFO - __main__ - ['false']
05/27/2022 01:52:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 01:52:28 - INFO - __main__ - ['false']
05/27/2022 01:52:28 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 01:52:28 - INFO - __main__ - ['false']
05/27/2022 01:52:28 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:52:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:52:32 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 01:52:43 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:52:43 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:52:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:52:44 - INFO - __main__ - Starting training!
05/27/2022 01:53:26 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.4_8_predictions.txt
05/27/2022 01:53:26 - INFO - __main__ - Classification-F1 on test data: 0.4432
05/27/2022 01:53:26 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.4, bsz=8, dev_performance=0.640625, test_performance=0.4432425923653994
05/27/2022 01:53:26 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.3, bsz=8 ...
05/27/2022 01:53:27 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:53:27 - INFO - __main__ - Printing 3 examples
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:53:27 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:53:27 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 01:53:27 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 01:53:27 - INFO - __main__ - Printing 3 examples
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 01:53:27 - INFO - __main__ - ['false']
05/27/2022 01:53:27 - INFO - __main__ - Tokenizing Input ...
05/27/2022 01:53:28 - INFO - __main__ - Tokenizing Output ...
05/27/2022 01:53:28 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 01:53:47 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 01:53:47 - INFO - __main__ - task name: wiki_qa
05/27/2022 01:53:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 01:53:48 - INFO - __main__ - Starting training!
05/27/2022 01:53:50 - INFO - __main__ - Step 10 Global step 10 Train loss 6.67 on epoch=0
05/27/2022 01:53:53 - INFO - __main__ - Step 20 Global step 20 Train loss 1.86 on epoch=1
05/27/2022 01:53:56 - INFO - __main__ - Step 30 Global step 30 Train loss 0.73 on epoch=1
05/27/2022 01:53:58 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=2
05/27/2022 01:54:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/27/2022 01:54:05 - INFO - __main__ - Global step 50 Train loss 2.06 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 01:54:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 01:54:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/27/2022 01:54:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=4
05/27/2022 01:54:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=4
05/27/2022 01:54:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/27/2022 01:54:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=6
05/27/2022 01:54:22 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 01:54:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=6
05/27/2022 01:54:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=7
05/27/2022 01:54:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
05/27/2022 01:54:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/27/2022 01:54:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=9
05/27/2022 01:54:38 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 01:54:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/27/2022 01:54:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
05/27/2022 01:54:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/27/2022 01:54:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
05/27/2022 01:54:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=12
05/27/2022 01:54:54 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=12
05/27/2022 01:54:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
05/27/2022 01:54:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
05/27/2022 01:55:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/27/2022 01:55:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=14
05/27/2022 01:55:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=15
05/27/2022 01:55:10 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.37778422694623814 on epoch=15
05/27/2022 01:55:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37778422694623814 on epoch=15, global_step=250
05/27/2022 01:55:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=16
05/27/2022 01:55:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=16
05/27/2022 01:55:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
05/27/2022 01:55:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
05/27/2022 01:55:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/27/2022 01:55:26 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 01:55:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
05/27/2022 01:55:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=19
05/27/2022 01:55:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=20
05/27/2022 01:55:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 01:55:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=21
05/27/2022 01:55:42 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.44904108278731647 on epoch=21
05/27/2022 01:55:42 - INFO - __main__ - Saving model with best Classification-F1: 0.37778422694623814 -> 0.44904108278731647 on epoch=21, global_step=350
05/27/2022 01:55:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/27/2022 01:55:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
05/27/2022 01:55:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/27/2022 01:55:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=24
05/27/2022 01:55:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
05/27/2022 01:56:00 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 01:56:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
05/27/2022 01:56:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=26
05/27/2022 01:56:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
05/27/2022 01:56:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
05/27/2022 01:56:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
05/27/2022 01:56:16 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.46825396825396826 on epoch=28
05/27/2022 01:56:16 - INFO - __main__ - Saving model with best Classification-F1: 0.44904108278731647 -> 0.46825396825396826 on epoch=28, global_step=450
05/27/2022 01:56:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
05/27/2022 01:56:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
05/27/2022 01:56:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/27/2022 01:56:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 01:56:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
05/27/2022 01:56:32 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.37922403003754696 on epoch=31
05/27/2022 01:56:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
05/27/2022 01:56:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/27/2022 01:56:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=33
05/27/2022 01:56:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 01:56:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=34
05/27/2022 01:56:48 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3987656343874597 on epoch=34
05/27/2022 01:56:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/27/2022 01:56:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
05/27/2022 01:56:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/27/2022 01:56:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=36
05/27/2022 01:57:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/27/2022 01:57:04 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.39405460814462767 on epoch=37
05/27/2022 01:57:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=38
05/27/2022 01:57:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/27/2022 01:57:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
05/27/2022 01:57:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
05/27/2022 01:57:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/27/2022 01:57:20 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.391964389522813 on epoch=40
05/27/2022 01:57:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/27/2022 01:57:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/27/2022 01:57:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/27/2022 01:57:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/27/2022 01:57:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/27/2022 01:57:37 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 01:57:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
05/27/2022 01:57:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
05/27/2022 01:57:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/27/2022 01:57:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
05/27/2022 01:57:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/27/2022 01:57:53 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.4729460207167213 on epoch=46
05/27/2022 01:57:53 - INFO - __main__ - Saving model with best Classification-F1: 0.46825396825396826 -> 0.4729460207167213 on epoch=46, global_step=750
05/27/2022 01:57:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=47
05/27/2022 01:57:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
05/27/2022 01:58:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/27/2022 01:58:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
05/27/2022 01:58:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=49
05/27/2022 01:58:11 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 01:58:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/27/2022 01:58:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
05/27/2022 01:58:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/27/2022 01:58:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/27/2022 01:58:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
05/27/2022 01:58:27 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3712545436683367 on epoch=53
05/27/2022 01:58:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 01:58:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
05/27/2022 01:58:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
05/27/2022 01:58:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
05/27/2022 01:58:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/27/2022 01:58:43 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.534341090133601 on epoch=56
05/27/2022 01:58:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4729460207167213 -> 0.534341090133601 on epoch=56, global_step=900
05/27/2022 01:58:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/27/2022 01:58:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
05/27/2022 01:58:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/27/2022 01:58:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
05/27/2022 01:58:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
05/27/2022 01:59:00 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 01:59:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/27/2022 01:59:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=60
05/27/2022 01:59:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/27/2022 01:59:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
05/27/2022 01:59:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
05/27/2022 01:59:16 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.5307917888563051 on epoch=62
05/27/2022 01:59:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/27/2022 01:59:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/27/2022 01:59:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 01:59:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
05/27/2022 01:59:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
05/27/2022 01:59:32 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5226107226107226 on epoch=65
05/27/2022 01:59:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
05/27/2022 01:59:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=66
05/27/2022 01:59:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
05/27/2022 01:59:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=68
05/27/2022 01:59:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=68
05/27/2022 01:59:48 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 01:59:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=69
05/27/2022 01:59:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/27/2022 01:59:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
05/27/2022 01:59:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/27/2022 02:00:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=71
05/27/2022 02:00:05 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.5793650793650793 on epoch=71
05/27/2022 02:00:05 - INFO - __main__ - Saving model with best Classification-F1: 0.534341090133601 -> 0.5793650793650793 on epoch=71, global_step=1150
05/27/2022 02:00:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=72
05/27/2022 02:00:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=73
05/27/2022 02:00:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/27/2022 02:00:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
05/27/2022 02:00:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
05/27/2022 02:00:22 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.34195559333697656 on epoch=74
05/27/2022 02:00:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=75
05/27/2022 02:00:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/27/2022 02:00:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
05/27/2022 02:00:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
05/27/2022 02:00:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=78
05/27/2022 02:00:38 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3712545436683367 on epoch=78
05/27/2022 02:00:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=78
05/27/2022 02:00:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
05/27/2022 02:00:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
05/27/2022 02:00:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
05/27/2022 02:00:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
05/27/2022 02:00:55 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.6404274809160306 on epoch=81
05/27/2022 02:00:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5793650793650793 -> 0.6404274809160306 on epoch=81, global_step=1300
05/27/2022 02:00:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
05/27/2022 02:01:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
05/27/2022 02:01:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=83
05/27/2022 02:01:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=83
05/27/2022 02:01:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/27/2022 02:01:12 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.6580304442373408 on epoch=84
05/27/2022 02:01:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6404274809160306 -> 0.6580304442373408 on epoch=84, global_step=1350
05/27/2022 02:01:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=84
05/27/2022 02:01:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
05/27/2022 02:01:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
05/27/2022 02:01:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
05/27/2022 02:01:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/27/2022 02:01:28 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.5373493975903615 on epoch=87
05/27/2022 02:01:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=88
05/27/2022 02:01:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=88
05/27/2022 02:01:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=89
05/27/2022 02:01:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=89
05/27/2022 02:01:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=90
05/27/2022 02:01:46 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.609464508094645 on epoch=90
05/27/2022 02:01:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/27/2022 02:01:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=91
05/27/2022 02:01:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=92
05/27/2022 02:01:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=93
05/27/2022 02:01:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
05/27/2022 02:02:02 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.33159268929503916 on epoch=93
05/27/2022 02:02:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
05/27/2022 02:02:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=94
05/27/2022 02:02:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=95
05/27/2022 02:02:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
05/27/2022 02:02:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=96
05/27/2022 02:02:19 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.5008008914269796 on epoch=96
05/27/2022 02:02:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
05/27/2022 02:02:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
05/27/2022 02:02:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=98
05/27/2022 02:02:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=99
05/27/2022 02:02:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
05/27/2022 02:02:36 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.6732254047322541 on epoch=99
05/27/2022 02:02:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6580304442373408 -> 0.6732254047322541 on epoch=99, global_step=1600
05/27/2022 02:02:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=100
05/27/2022 02:02:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=101
05/27/2022 02:02:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/27/2022 02:02:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
05/27/2022 02:02:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
05/27/2022 02:02:52 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.5466734125980268 on epoch=103
05/27/2022 02:02:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=103
05/27/2022 02:02:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=104
05/27/2022 02:03:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=104
05/27/2022 02:03:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=105
05/27/2022 02:03:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
05/27/2022 02:03:10 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.7070267795834286 on epoch=106
05/27/2022 02:03:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6732254047322541 -> 0.7070267795834286 on epoch=106, global_step=1700
05/27/2022 02:03:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=106
05/27/2022 02:03:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=107
05/27/2022 02:03:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=108
05/27/2022 02:03:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=108
05/27/2022 02:03:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=109
05/27/2022 02:03:27 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.6044150110375275 on epoch=109
05/27/2022 02:03:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
05/27/2022 02:03:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=110
05/27/2022 02:03:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=111
05/27/2022 02:03:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=111
05/27/2022 02:03:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=112
05/27/2022 02:03:44 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.6440834179489181 on epoch=112
05/27/2022 02:03:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=113
05/27/2022 02:03:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=113
05/27/2022 02:03:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=114
05/27/2022 02:03:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=114
05/27/2022 02:03:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=115
05/27/2022 02:04:01 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.6875 on epoch=115
05/27/2022 02:04:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=116
05/27/2022 02:04:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=116
05/27/2022 02:04:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=117
05/27/2022 02:04:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=118
05/27/2022 02:04:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=118
05/27/2022 02:04:18 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.43675929520956974 on epoch=118
05/27/2022 02:04:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=119
05/27/2022 02:04:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
05/27/2022 02:04:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=120
05/27/2022 02:04:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=121
05/27/2022 02:04:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
05/27/2022 02:04:35 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.6154548974061169 on epoch=121
05/27/2022 02:04:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=122
05/27/2022 02:04:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=123
05/27/2022 02:04:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
05/27/2022 02:04:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
05/27/2022 02:04:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
05/27/2022 02:04:52 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.669433683822173 on epoch=124
05/27/2022 02:04:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=125
05/27/2022 02:04:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=126
05/27/2022 02:05:00 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.27 on epoch=126
05/27/2022 02:05:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=127
05/27/2022 02:05:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
05/27/2022 02:05:10 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.5396603396603397 on epoch=128
05/27/2022 02:05:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/27/2022 02:05:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=129
05/27/2022 02:05:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=129
05/27/2022 02:05:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=130
05/27/2022 02:05:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=131
05/27/2022 02:05:27 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.6796092796092795 on epoch=131
05/27/2022 02:05:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=131
05/27/2022 02:05:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.33 on epoch=132
05/27/2022 02:05:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=133
05/27/2022 02:05:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=133
05/27/2022 02:05:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=134
05/27/2022 02:05:44 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.6572851805728519 on epoch=134
05/27/2022 02:05:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=134
05/27/2022 02:05:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=135
05/27/2022 02:05:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/27/2022 02:05:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
05/27/2022 02:05:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.27 on epoch=137
05/27/2022 02:06:01 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.6441808747220164 on epoch=137
05/27/2022 02:06:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/27/2022 02:06:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/27/2022 02:06:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.34 on epoch=139
05/27/2022 02:06:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=139
05/27/2022 02:06:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
05/27/2022 02:06:19 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.6893156156386819 on epoch=140
05/27/2022 02:06:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=141
05/27/2022 02:06:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=141
05/27/2022 02:06:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=142
05/27/2022 02:06:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/27/2022 02:06:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
05/27/2022 02:06:36 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.5394638022880098 on epoch=143
05/27/2022 02:06:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/27/2022 02:06:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/27/2022 02:06:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=145
05/27/2022 02:06:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=146
05/27/2022 02:06:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=146
05/27/2022 02:06:53 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.6572851805728519 on epoch=146
05/27/2022 02:06:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=147
05/27/2022 02:06:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=148
05/27/2022 02:07:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/27/2022 02:07:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=149
05/27/2022 02:07:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=149
05/27/2022 02:07:11 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.6587301587301586 on epoch=149
05/27/2022 02:07:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=150
05/27/2022 02:07:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=151
05/27/2022 02:07:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=151
05/27/2022 02:07:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=152
05/27/2022 02:07:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=153
05/27/2022 02:07:28 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.6225641025641027 on epoch=153
05/27/2022 02:07:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/27/2022 02:07:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=154
05/27/2022 02:07:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/27/2022 02:07:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=155
05/27/2022 02:07:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=156
05/27/2022 02:07:45 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.6757763027389945 on epoch=156
05/27/2022 02:07:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/27/2022 02:07:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
05/27/2022 02:07:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
05/27/2022 02:07:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=158
05/27/2022 02:07:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=159
05/27/2022 02:08:03 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.6756575231640487 on epoch=159
05/27/2022 02:08:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/27/2022 02:08:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=160
05/27/2022 02:08:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=161
05/27/2022 02:08:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/27/2022 02:08:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=162
05/27/2022 02:08:20 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.7070267795834286 on epoch=162
05/27/2022 02:08:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=163
05/27/2022 02:08:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
05/27/2022 02:08:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=164
05/27/2022 02:08:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=164
05/27/2022 02:08:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=165
05/27/2022 02:08:38 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.6900393887841587 on epoch=165
05/27/2022 02:08:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=166
05/27/2022 02:08:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/27/2022 02:08:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=167
05/27/2022 02:08:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=168
05/27/2022 02:08:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/27/2022 02:08:56 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.6806417483853648 on epoch=168
05/27/2022 02:08:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=169
05/27/2022 02:09:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=169
05/27/2022 02:09:03 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.23 on epoch=170
05/27/2022 02:09:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=171
05/27/2022 02:09:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
05/27/2022 02:09:14 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.6791979949874687 on epoch=171
05/27/2022 02:09:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
05/27/2022 02:09:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/27/2022 02:09:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=173
05/27/2022 02:09:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=174
05/27/2022 02:09:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/27/2022 02:09:32 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.6946415463665281 on epoch=174
05/27/2022 02:09:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=175
05/27/2022 02:09:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=176
05/27/2022 02:09:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/27/2022 02:09:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
05/27/2022 02:09:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/27/2022 02:09:49 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.6744076499128551 on epoch=178
05/27/2022 02:09:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=178
05/27/2022 02:09:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/27/2022 02:09:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=179
05/27/2022 02:09:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=180
05/27/2022 02:10:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=181
05/27/2022 02:10:07 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.6986623863028356 on epoch=181
05/27/2022 02:10:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
05/27/2022 02:10:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/27/2022 02:10:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=183
05/27/2022 02:10:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/27/2022 02:10:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=184
05/27/2022 02:10:24 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.6633227306092488 on epoch=184
05/27/2022 02:10:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=184
05/27/2022 02:10:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/27/2022 02:10:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=186
05/27/2022 02:10:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
05/27/2022 02:10:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=187
05/27/2022 02:10:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:10:38 - INFO - __main__ - Printing 3 examples
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:10:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:10:38 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:10:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:10:38 - INFO - __main__ - Printing 3 examples
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 02:10:38 - INFO - __main__ - ['false']
05/27/2022 02:10:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:10:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:10:39 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:10:43 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.6888950420723923 on epoch=187
05/27/2022 02:10:43 - INFO - __main__ - save last model!
05/27/2022 02:10:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 02:10:43 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 02:10:43 - INFO - __main__ - Printing 3 examples
05/27/2022 02:10:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 02:10:43 - INFO - __main__ - ['false']
05/27/2022 02:10:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 02:10:43 - INFO - __main__ - ['false']
05/27/2022 02:10:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 02:10:43 - INFO - __main__ - ['false']
05/27/2022 02:10:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:10:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:10:47 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 02:10:57 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:10:57 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:10:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:10:58 - INFO - __main__ - Starting training!
05/27/2022 02:11:48 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.3_8_predictions.txt
05/27/2022 02:11:48 - INFO - __main__ - Classification-F1 on test data: 0.4579
05/27/2022 02:11:48 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.3, bsz=8, dev_performance=0.7070267795834286, test_performance=0.4579403160712362
05/27/2022 02:11:48 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.2, bsz=8 ...
05/27/2022 02:11:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:11:49 - INFO - __main__ - Printing 3 examples
05/27/2022 02:11:49 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/27/2022 02:11:49 - INFO - __main__ - ['false']
05/27/2022 02:11:49 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/27/2022 02:11:49 - INFO - __main__ - ['false']
05/27/2022 02:11:49 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/27/2022 02:11:49 - INFO - __main__ - ['false']
05/27/2022 02:11:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:11:50 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:11:50 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:11:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:11:50 - INFO - __main__ - Printing 3 examples
05/27/2022 02:11:50 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/27/2022 02:11:50 - INFO - __main__ - ['false']
05/27/2022 02:11:50 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/27/2022 02:11:50 - INFO - __main__ - ['false']
05/27/2022 02:11:50 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/27/2022 02:11:50 - INFO - __main__ - ['false']
05/27/2022 02:11:50 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:11:50 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:11:50 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:12:05 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:12:05 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:12:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:12:06 - INFO - __main__ - Starting training!
05/27/2022 02:12:09 - INFO - __main__ - Step 10 Global step 10 Train loss 7.06 on epoch=0
05/27/2022 02:12:11 - INFO - __main__ - Step 20 Global step 20 Train loss 2.91 on epoch=1
05/27/2022 02:12:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=1
05/27/2022 02:12:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.78 on epoch=2
05/27/2022 02:12:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=3
05/27/2022 02:12:28 - INFO - __main__ - Global step 50 Train loss 2.52 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 02:12:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 02:12:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/27/2022 02:12:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=4
05/27/2022 02:12:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=4
05/27/2022 02:12:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=5
05/27/2022 02:12:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/27/2022 02:12:55 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.39405460814462767 on epoch=6
05/27/2022 02:12:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39405460814462767 on epoch=6, global_step=100
05/27/2022 02:12:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
05/27/2022 02:13:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=7
05/27/2022 02:13:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
05/27/2022 02:13:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
05/27/2022 02:13:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/27/2022 02:13:13 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3776795580110497 on epoch=9
05/27/2022 02:13:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
05/27/2022 02:13:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=10
05/27/2022 02:13:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/27/2022 02:13:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/27/2022 02:13:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
05/27/2022 02:13:30 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.45040790038643197 on epoch=12
05/27/2022 02:13:30 - INFO - __main__ - Saving model with best Classification-F1: 0.39405460814462767 -> 0.45040790038643197 on epoch=12, global_step=200
05/27/2022 02:13:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=13
05/27/2022 02:13:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/27/2022 02:13:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=14
05/27/2022 02:13:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=14
05/27/2022 02:13:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=15
05/27/2022 02:13:46 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.34195559333697656 on epoch=15
05/27/2022 02:13:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=16
05/27/2022 02:13:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=16
05/27/2022 02:13:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/27/2022 02:13:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/27/2022 02:13:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=18
05/27/2022 02:14:02 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=18
05/27/2022 02:14:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 02:14:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=19
05/27/2022 02:14:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=20
05/27/2022 02:14:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 02:14:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=21
05/27/2022 02:14:19 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.38064516129032255 on epoch=21
05/27/2022 02:14:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/27/2022 02:14:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=23
05/27/2022 02:14:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/27/2022 02:14:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/27/2022 02:14:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/27/2022 02:14:36 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 02:14:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
05/27/2022 02:14:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=26
05/27/2022 02:14:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
05/27/2022 02:14:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 02:14:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/27/2022 02:14:52 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 02:14:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/27/2022 02:14:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/27/2022 02:15:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=29
05/27/2022 02:15:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 02:15:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
05/27/2022 02:15:08 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.42542338298306465 on epoch=31
05/27/2022 02:15:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
05/27/2022 02:15:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
05/27/2022 02:15:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/27/2022 02:15:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 02:15:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
05/27/2022 02:15:25 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.34485289741504155 on epoch=34
05/27/2022 02:15:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/27/2022 02:15:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=35
05/27/2022 02:15:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/27/2022 02:15:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=36
05/27/2022 02:15:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
05/27/2022 02:15:41 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.38854874520621674 on epoch=37
05/27/2022 02:15:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/27/2022 02:15:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=38
05/27/2022 02:15:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
05/27/2022 02:15:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
05/27/2022 02:15:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/27/2022 02:15:57 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.34673046251993617 on epoch=40
05/27/2022 02:15:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
05/27/2022 02:16:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/27/2022 02:16:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=42
05/27/2022 02:16:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
05/27/2022 02:16:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/27/2022 02:16:13 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 02:16:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/27/2022 02:16:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
05/27/2022 02:16:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=45
05/27/2022 02:16:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/27/2022 02:16:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=46
05/27/2022 02:16:29 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.44119308207781904 on epoch=46
05/27/2022 02:16:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
05/27/2022 02:16:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
05/27/2022 02:16:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/27/2022 02:16:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
05/27/2022 02:16:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=49
05/27/2022 02:16:45 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 02:16:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/27/2022 02:16:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
05/27/2022 02:16:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/27/2022 02:16:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
05/27/2022 02:16:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/27/2022 02:17:01 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 02:17:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/27/2022 02:17:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
05/27/2022 02:17:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
05/27/2022 02:17:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
05/27/2022 02:17:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
05/27/2022 02:17:17 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.41426949611387653 on epoch=56
05/27/2022 02:17:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/27/2022 02:17:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/27/2022 02:17:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
05/27/2022 02:17:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
05/27/2022 02:17:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/27/2022 02:17:33 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3486005089058525 on epoch=59
05/27/2022 02:17:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/27/2022 02:17:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
05/27/2022 02:17:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 02:17:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=61
05/27/2022 02:17:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/27/2022 02:17:49 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.48055313997305915 on epoch=62
05/27/2022 02:17:49 - INFO - __main__ - Saving model with best Classification-F1: 0.45040790038643197 -> 0.48055313997305915 on epoch=62, global_step=1000
05/27/2022 02:17:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/27/2022 02:17:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/27/2022 02:17:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 02:17:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
05/27/2022 02:18:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
05/27/2022 02:18:05 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.48624913705909734 on epoch=65
05/27/2022 02:18:05 - INFO - __main__ - Saving model with best Classification-F1: 0.48055313997305915 -> 0.48624913705909734 on epoch=65, global_step=1050
05/27/2022 02:18:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
05/27/2022 02:18:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=66
05/27/2022 02:18:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
05/27/2022 02:18:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/27/2022 02:18:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
05/27/2022 02:18:21 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 02:18:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/27/2022 02:18:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=69
05/27/2022 02:18:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
05/27/2022 02:18:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=71
05/27/2022 02:18:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
05/27/2022 02:18:37 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.48511665325824616 on epoch=71
05/27/2022 02:18:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/27/2022 02:18:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
05/27/2022 02:18:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/27/2022 02:18:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
05/27/2022 02:18:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
05/27/2022 02:18:54 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.36318407960199 on epoch=74
05/27/2022 02:18:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 02:18:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/27/2022 02:19:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/27/2022 02:19:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
05/27/2022 02:19:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
05/27/2022 02:19:10 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=78
05/27/2022 02:19:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=78
05/27/2022 02:19:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
05/27/2022 02:19:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/27/2022 02:19:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=80
05/27/2022 02:19:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=81
05/27/2022 02:19:26 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.5734311702208974 on epoch=81
05/27/2022 02:19:26 - INFO - __main__ - Saving model with best Classification-F1: 0.48624913705909734 -> 0.5734311702208974 on epoch=81, global_step=1300
05/27/2022 02:19:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
05/27/2022 02:19:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
05/27/2022 02:19:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=83
05/27/2022 02:19:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=83
05/27/2022 02:19:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/27/2022 02:19:42 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.5740697547853401 on epoch=84
05/27/2022 02:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5734311702208974 -> 0.5740697547853401 on epoch=84, global_step=1350
05/27/2022 02:19:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
05/27/2022 02:19:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=85
05/27/2022 02:19:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=86
05/27/2022 02:19:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/27/2022 02:19:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=87
05/27/2022 02:19:59 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.5479028697571744 on epoch=87
05/27/2022 02:20:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=88
05/27/2022 02:20:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=88
05/27/2022 02:20:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
05/27/2022 02:20:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=89
05/27/2022 02:20:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=90
05/27/2022 02:20:15 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.5699303550749457 on epoch=90
05/27/2022 02:20:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
05/27/2022 02:20:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=91
05/27/2022 02:20:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
05/27/2022 02:20:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
05/27/2022 02:20:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=93
05/27/2022 02:20:31 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.350463149416029 on epoch=93
05/27/2022 02:20:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=94
05/27/2022 02:20:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=94
05/27/2022 02:20:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/27/2022 02:20:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=96
05/27/2022 02:20:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=96
05/27/2022 02:20:48 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.557777268210655 on epoch=96
05/27/2022 02:20:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=97
05/27/2022 02:20:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
05/27/2022 02:20:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=98
05/27/2022 02:20:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=99
05/27/2022 02:21:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=99
05/27/2022 02:21:05 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.5387387387387387 on epoch=99
05/27/2022 02:21:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/27/2022 02:21:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=101
05/27/2022 02:21:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
05/27/2022 02:21:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=102
05/27/2022 02:21:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
05/27/2022 02:21:23 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.5133482105959171 on epoch=103
05/27/2022 02:21:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=103
05/27/2022 02:21:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=104
05/27/2022 02:21:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=104
05/27/2022 02:21:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=105
05/27/2022 02:21:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=106
05/27/2022 02:21:40 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.5681181563534505 on epoch=106
05/27/2022 02:21:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=106
05/27/2022 02:21:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=107
05/27/2022 02:21:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=108
05/27/2022 02:21:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=108
05/27/2022 02:21:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=109
05/27/2022 02:21:57 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.5962701535729049 on epoch=109
05/27/2022 02:21:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5740697547853401 -> 0.5962701535729049 on epoch=109, global_step=1750
05/27/2022 02:21:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=109
05/27/2022 02:22:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=110
05/27/2022 02:22:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=111
05/27/2022 02:22:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
05/27/2022 02:22:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=112
05/27/2022 02:22:14 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.5790462767206954 on epoch=112
05/27/2022 02:22:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=113
05/27/2022 02:22:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=113
05/27/2022 02:22:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=114
05/27/2022 02:22:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/27/2022 02:22:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=115
05/27/2022 02:22:31 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.5472022481196792 on epoch=115
05/27/2022 02:22:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=116
05/27/2022 02:22:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=116
05/27/2022 02:22:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
05/27/2022 02:22:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/27/2022 02:22:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/27/2022 02:22:48 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.5950826090360974 on epoch=118
05/27/2022 02:22:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=119
05/27/2022 02:22:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
05/27/2022 02:22:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=120
05/27/2022 02:22:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=121
05/27/2022 02:23:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=121
05/27/2022 02:23:05 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.5696558469344092 on epoch=121
05/27/2022 02:23:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=122
05/27/2022 02:23:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/27/2022 02:23:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
05/27/2022 02:23:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=124
05/27/2022 02:23:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=124
05/27/2022 02:23:23 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.5895368546429063 on epoch=124
05/27/2022 02:23:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=125
05/27/2022 02:23:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=126
05/27/2022 02:23:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=126
05/27/2022 02:23:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=127
05/27/2022 02:23:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
05/27/2022 02:23:40 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.611948982560365 on epoch=128
05/27/2022 02:23:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5962701535729049 -> 0.611948982560365 on epoch=128, global_step=2050
05/27/2022 02:23:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=128
05/27/2022 02:23:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=129
05/27/2022 02:23:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=129
05/27/2022 02:23:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=130
05/27/2022 02:23:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=131
05/27/2022 02:23:57 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.608203244566881 on epoch=131
05/27/2022 02:23:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=131
05/27/2022 02:24:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/27/2022 02:24:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/27/2022 02:24:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
05/27/2022 02:24:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=134
05/27/2022 02:24:14 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.5546442994718856 on epoch=134
05/27/2022 02:24:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=134
05/27/2022 02:24:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/27/2022 02:24:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/27/2022 02:24:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=136
05/27/2022 02:24:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=137
05/27/2022 02:24:31 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.6089931573802541 on epoch=137
05/27/2022 02:24:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=138
05/27/2022 02:24:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.31 on epoch=138
05/27/2022 02:24:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=139
05/27/2022 02:24:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=139
05/27/2022 02:24:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
05/27/2022 02:24:49 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.583394743306313 on epoch=140
05/27/2022 02:24:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.31 on epoch=141
05/27/2022 02:24:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/27/2022 02:24:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=142
05/27/2022 02:24:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=143
05/27/2022 02:25:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/27/2022 02:25:06 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.5521765457848349 on epoch=143
05/27/2022 02:25:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=144
05/27/2022 02:25:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=144
05/27/2022 02:25:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=145
05/27/2022 02:25:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=146
05/27/2022 02:25:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/27/2022 02:25:23 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.605414561936301 on epoch=146
05/27/2022 02:25:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=147
05/27/2022 02:25:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
05/27/2022 02:25:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/27/2022 02:25:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=149
05/27/2022 02:25:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
05/27/2022 02:25:40 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.5700763358778627 on epoch=149
05/27/2022 02:25:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=150
05/27/2022 02:25:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=151
05/27/2022 02:25:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=151
05/27/2022 02:25:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.22 on epoch=152
05/27/2022 02:25:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/27/2022 02:25:57 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.6009510163593708 on epoch=153
05/27/2022 02:25:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
05/27/2022 02:26:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/27/2022 02:26:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/27/2022 02:26:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.28 on epoch=155
05/27/2022 02:26:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.30 on epoch=156
05/27/2022 02:26:14 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.5873015873015872 on epoch=156
05/27/2022 02:26:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
05/27/2022 02:26:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
05/27/2022 02:26:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
05/27/2022 02:26:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=158
05/27/2022 02:26:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=159
05/27/2022 02:26:32 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.5696139476961395 on epoch=159
05/27/2022 02:26:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=159
05/27/2022 02:26:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/27/2022 02:26:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/27/2022 02:26:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
05/27/2022 02:26:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=162
05/27/2022 02:26:49 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.6111392405063292 on epoch=162
05/27/2022 02:26:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=163
05/27/2022 02:26:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
05/27/2022 02:26:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=164
05/27/2022 02:26:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=164
05/27/2022 02:27:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=165
05/27/2022 02:27:06 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.5485885302920004 on epoch=165
05/27/2022 02:27:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=166
05/27/2022 02:27:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=166
05/27/2022 02:27:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.27 on epoch=167
05/27/2022 02:27:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
05/27/2022 02:27:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=168
05/27/2022 02:27:24 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.6003400571914368 on epoch=168
05/27/2022 02:27:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/27/2022 02:27:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/27/2022 02:27:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
05/27/2022 02:27:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=171
05/27/2022 02:27:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=171
05/27/2022 02:27:41 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.5845119406445629 on epoch=171
05/27/2022 02:27:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=172
05/27/2022 02:27:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=173
05/27/2022 02:27:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/27/2022 02:27:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=174
05/27/2022 02:27:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=174
05/27/2022 02:27:58 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.5801720403490316 on epoch=174
05/27/2022 02:28:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=175
05/27/2022 02:28:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=176
05/27/2022 02:28:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=176
05/27/2022 02:28:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=177
05/27/2022 02:28:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/27/2022 02:28:15 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.5960146048641624 on epoch=178
05/27/2022 02:28:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=178
05/27/2022 02:28:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=179
05/27/2022 02:28:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
05/27/2022 02:28:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=180
05/27/2022 02:28:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=181
05/27/2022 02:28:32 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.5721002889947668 on epoch=181
05/27/2022 02:28:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=181
05/27/2022 02:28:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/27/2022 02:28:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.21 on epoch=183
05/27/2022 02:28:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=183
05/27/2022 02:28:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=184
05/27/2022 02:28:50 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.5760289517266761 on epoch=184
05/27/2022 02:28:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=184
05/27/2022 02:28:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/27/2022 02:28:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=186
05/27/2022 02:29:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=186
05/27/2022 02:29:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=187
05/27/2022 02:29:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:29:03 - INFO - __main__ - Printing 3 examples
05/27/2022 02:29:03 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 02:29:03 - INFO - __main__ - ['false']
05/27/2022 02:29:03 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 02:29:03 - INFO - __main__ - ['false']
05/27/2022 02:29:03 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 02:29:03 - INFO - __main__ - ['false']
05/27/2022 02:29:03 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:29:03 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:29:04 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:29:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:29:04 - INFO - __main__ - Printing 3 examples
05/27/2022 02:29:04 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 02:29:04 - INFO - __main__ - ['false']
05/27/2022 02:29:04 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 02:29:04 - INFO - __main__ - ['false']
05/27/2022 02:29:04 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 02:29:04 - INFO - __main__ - ['false']
05/27/2022 02:29:04 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:29:04 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:29:04 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:29:07 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.5873015873015872 on epoch=187
05/27/2022 02:29:07 - INFO - __main__ - save last model!
05/27/2022 02:29:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 02:29:07 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 02:29:07 - INFO - __main__ - Printing 3 examples
05/27/2022 02:29:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 02:29:07 - INFO - __main__ - ['false']
05/27/2022 02:29:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 02:29:07 - INFO - __main__ - ['false']
05/27/2022 02:29:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 02:29:07 - INFO - __main__ - ['false']
05/27/2022 02:29:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:29:08 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:29:11 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 02:29:23 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:29:23 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:29:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:29:24 - INFO - __main__ - Starting training!
05/27/2022 02:30:10 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.2_8_predictions.txt
05/27/2022 02:30:10 - INFO - __main__ - Classification-F1 on test data: 0.4934
05/27/2022 02:30:11 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.2, bsz=8, dev_performance=0.611948982560365, test_performance=0.4933592387357249
05/27/2022 02:30:11 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.5, bsz=8 ...
05/27/2022 02:30:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:30:12 - INFO - __main__ - Printing 3 examples
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:30:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:30:12 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:30:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:30:12 - INFO - __main__ - Printing 3 examples
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 02:30:12 - INFO - __main__ - ['false']
05/27/2022 02:30:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:30:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:30:13 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:30:31 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:30:31 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:30:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:30:32 - INFO - __main__ - Starting training!
05/27/2022 02:30:35 - INFO - __main__ - Step 10 Global step 10 Train loss 5.04 on epoch=0
05/27/2022 02:30:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.88 on epoch=1
05/27/2022 02:30:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=1
05/27/2022 02:30:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=2
05/27/2022 02:30:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/27/2022 02:30:49 - INFO - __main__ - Global step 50 Train loss 1.50 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 02:30:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 02:30:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/27/2022 02:30:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=4
05/27/2022 02:30:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/27/2022 02:30:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
05/27/2022 02:31:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=6
05/27/2022 02:31:05 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3931034482758621 on epoch=6
05/27/2022 02:31:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3931034482758621 on epoch=6, global_step=100
05/27/2022 02:31:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/27/2022 02:31:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=7
05/27/2022 02:31:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
05/27/2022 02:31:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=8
05/27/2022 02:31:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/27/2022 02:31:21 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 02:31:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=9
05/27/2022 02:31:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
05/27/2022 02:31:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
05/27/2022 02:31:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/27/2022 02:31:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
05/27/2022 02:31:38 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3771988921326446 on epoch=12
05/27/2022 02:31:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=13
05/27/2022 02:31:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=13
05/27/2022 02:31:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
05/27/2022 02:31:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/27/2022 02:31:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
05/27/2022 02:31:54 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.4049917757276693 on epoch=15
05/27/2022 02:31:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3931034482758621 -> 0.4049917757276693 on epoch=15, global_step=250
05/27/2022 02:31:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
05/27/2022 02:31:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
05/27/2022 02:32:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
05/27/2022 02:32:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
05/27/2022 02:32:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
05/27/2022 02:32:10 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 02:32:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=19
05/27/2022 02:32:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=19
05/27/2022 02:32:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=20
05/27/2022 02:32:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 02:32:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
05/27/2022 02:32:28 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=21
05/27/2022 02:32:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/27/2022 02:32:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=23
05/27/2022 02:32:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/27/2022 02:32:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
05/27/2022 02:32:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=24
05/27/2022 02:32:45 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 02:32:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
05/27/2022 02:32:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
05/27/2022 02:32:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=26
05/27/2022 02:32:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
05/27/2022 02:32:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
05/27/2022 02:33:02 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 02:33:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/27/2022 02:33:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
05/27/2022 02:33:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/27/2022 02:33:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
05/27/2022 02:33:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
05/27/2022 02:33:19 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.35518871580252653 on epoch=31
05/27/2022 02:33:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/27/2022 02:33:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/27/2022 02:33:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/27/2022 02:33:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/27/2022 02:33:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/27/2022 02:33:36 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3982955055171795 on epoch=34
05/27/2022 02:33:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/27/2022 02:33:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/27/2022 02:33:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=36
05/27/2022 02:33:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/27/2022 02:33:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/27/2022 02:33:53 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.4919083969465649 on epoch=37
05/27/2022 02:33:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4049917757276693 -> 0.4919083969465649 on epoch=37, global_step=600
05/27/2022 02:33:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=38
05/27/2022 02:33:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/27/2022 02:34:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
05/27/2022 02:34:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=39
05/27/2022 02:34:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
05/27/2022 02:34:11 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.44205889172269885 on epoch=40
05/27/2022 02:34:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
05/27/2022 02:34:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/27/2022 02:34:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
05/27/2022 02:34:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
05/27/2022 02:34:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/27/2022 02:34:27 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 02:34:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
05/27/2022 02:34:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=44
05/27/2022 02:34:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=45
05/27/2022 02:34:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
05/27/2022 02:34:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/27/2022 02:34:44 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.3987656343874597 on epoch=46
05/27/2022 02:34:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
05/27/2022 02:34:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/27/2022 02:34:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/27/2022 02:34:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/27/2022 02:34:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=49
05/27/2022 02:35:02 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 02:35:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/27/2022 02:35:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=51
05/27/2022 02:35:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
05/27/2022 02:35:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
05/27/2022 02:35:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/27/2022 02:35:18 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.33469557799511973 on epoch=53
05/27/2022 02:35:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
05/27/2022 02:35:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=54
05/27/2022 02:35:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=54
05/27/2022 02:35:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=55
05/27/2022 02:35:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
05/27/2022 02:35:35 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.34918648310387984 on epoch=56
05/27/2022 02:35:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/27/2022 02:35:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
05/27/2022 02:35:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=58
05/27/2022 02:35:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
05/27/2022 02:35:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=59
05/27/2022 02:35:53 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3682504604051565 on epoch=59
05/27/2022 02:35:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
05/27/2022 02:35:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=60
05/27/2022 02:36:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
05/27/2022 02:36:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/27/2022 02:36:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/27/2022 02:36:10 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.4599989710346246 on epoch=62
05/27/2022 02:36:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
05/27/2022 02:36:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/27/2022 02:36:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/27/2022 02:36:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
05/27/2022 02:36:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
05/27/2022 02:36:27 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.5339544418437439 on epoch=65
05/27/2022 02:36:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4919083969465649 -> 0.5339544418437439 on epoch=65, global_step=1050
05/27/2022 02:36:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
05/27/2022 02:36:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=66
05/27/2022 02:36:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=67
05/27/2022 02:36:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=68
05/27/2022 02:36:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=68
05/27/2022 02:36:44 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.34485289741504155 on epoch=68
05/27/2022 02:36:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=69
05/27/2022 02:36:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
05/27/2022 02:36:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/27/2022 02:36:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/27/2022 02:36:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
05/27/2022 02:37:01 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.5618314077877622 on epoch=71
05/27/2022 02:37:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5339544418437439 -> 0.5618314077877622 on epoch=71, global_step=1150
05/27/2022 02:37:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=72
05/27/2022 02:37:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
05/27/2022 02:37:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
05/27/2022 02:37:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
05/27/2022 02:37:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=74
05/27/2022 02:37:18 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.4740435169457251 on epoch=74
05/27/2022 02:37:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=75
05/27/2022 02:37:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=76
05/27/2022 02:37:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=76
05/27/2022 02:37:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
05/27/2022 02:37:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=78
05/27/2022 02:37:36 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.5695158322228568 on epoch=78
05/27/2022 02:37:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5618314077877622 -> 0.5695158322228568 on epoch=78, global_step=1250
05/27/2022 02:37:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.30 on epoch=78
05/27/2022 02:37:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=79
05/27/2022 02:37:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=79
05/27/2022 02:37:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
05/27/2022 02:37:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
05/27/2022 02:37:53 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.5599535423925668 on epoch=81
05/27/2022 02:37:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=81
05/27/2022 02:37:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=82
05/27/2022 02:38:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=83
05/27/2022 02:38:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=83
05/27/2022 02:38:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
05/27/2022 02:38:11 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.6091603053435114 on epoch=84
05/27/2022 02:38:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5695158322228568 -> 0.6091603053435114 on epoch=84, global_step=1350
05/27/2022 02:38:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=84
05/27/2022 02:38:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=85
05/27/2022 02:38:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
05/27/2022 02:38:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/27/2022 02:38:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=87
05/27/2022 02:38:28 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.5635334234060349 on epoch=87
05/27/2022 02:38:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=88
05/27/2022 02:38:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=88
05/27/2022 02:38:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=89
05/27/2022 02:38:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=89
05/27/2022 02:38:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=90
05/27/2022 02:38:45 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.5684492751296418 on epoch=90
05/27/2022 02:38:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=91
05/27/2022 02:38:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=91
05/27/2022 02:38:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=92
05/27/2022 02:38:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=93
05/27/2022 02:38:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=93
05/27/2022 02:39:03 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.45871509894280293 on epoch=93
05/27/2022 02:39:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
05/27/2022 02:39:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
05/27/2022 02:39:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=95
05/27/2022 02:39:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=96
05/27/2022 02:39:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=96
05/27/2022 02:39:20 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.6087780426676447 on epoch=96
05/27/2022 02:39:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=97
05/27/2022 02:39:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=98
05/27/2022 02:39:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=98
05/27/2022 02:39:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=99
05/27/2022 02:39:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=99
05/27/2022 02:39:37 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.6262193227710469 on epoch=99
05/27/2022 02:39:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6091603053435114 -> 0.6262193227710469 on epoch=99, global_step=1600
05/27/2022 02:39:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=100
05/27/2022 02:39:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=101
05/27/2022 02:39:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
05/27/2022 02:39:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=102
05/27/2022 02:39:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
05/27/2022 02:39:54 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.5036940005239716 on epoch=103
05/27/2022 02:39:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=103
05/27/2022 02:40:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=104
05/27/2022 02:40:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=104
05/27/2022 02:40:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
05/27/2022 02:40:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
05/27/2022 02:40:12 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.6357795217770434 on epoch=106
05/27/2022 02:40:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6262193227710469 -> 0.6357795217770434 on epoch=106, global_step=1700
05/27/2022 02:40:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=106
05/27/2022 02:40:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=107
05/27/2022 02:40:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=108
05/27/2022 02:40:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
05/27/2022 02:40:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=109
05/27/2022 02:40:29 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.6005741746239781 on epoch=109
05/27/2022 02:40:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=109
05/27/2022 02:40:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=110
05/27/2022 02:40:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/27/2022 02:40:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=111
05/27/2022 02:40:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=112
05/27/2022 02:40:46 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.6218376327533393 on epoch=112
05/27/2022 02:40:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=113
05/27/2022 02:40:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=113
05/27/2022 02:40:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/27/2022 02:40:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/27/2022 02:40:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=115
05/27/2022 02:41:03 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.5933346300979058 on epoch=115
05/27/2022 02:41:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=116
05/27/2022 02:41:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/27/2022 02:41:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
05/27/2022 02:41:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/27/2022 02:41:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/27/2022 02:41:21 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.6200890416308678 on epoch=118
05/27/2022 02:41:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/27/2022 02:41:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=119
05/27/2022 02:41:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=120
05/27/2022 02:41:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=121
05/27/2022 02:41:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=121
05/27/2022 02:41:39 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.6689655172413793 on epoch=121
05/27/2022 02:41:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6357795217770434 -> 0.6689655172413793 on epoch=121, global_step=1950
05/27/2022 02:41:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/27/2022 02:41:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=123
05/27/2022 02:41:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/27/2022 02:41:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/27/2022 02:41:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=124
05/27/2022 02:41:56 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.6768432371779887 on epoch=124
05/27/2022 02:41:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6689655172413793 -> 0.6768432371779887 on epoch=124, global_step=2000
05/27/2022 02:41:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
05/27/2022 02:42:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=126
05/27/2022 02:42:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
05/27/2022 02:42:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=127
05/27/2022 02:42:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/27/2022 02:42:13 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.6636525233789319 on epoch=128
05/27/2022 02:42:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=128
05/27/2022 02:42:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
05/27/2022 02:42:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/27/2022 02:42:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/27/2022 02:42:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/27/2022 02:42:31 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.6796875 on epoch=131
05/27/2022 02:42:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6768432371779887 -> 0.6796875 on epoch=131, global_step=2100
05/27/2022 02:42:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=131
05/27/2022 02:42:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=132
05/27/2022 02:42:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=133
05/27/2022 02:42:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/27/2022 02:42:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=134
05/27/2022 02:42:48 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.6493150684931506 on epoch=134
05/27/2022 02:42:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
05/27/2022 02:42:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=135
05/27/2022 02:42:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/27/2022 02:42:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/27/2022 02:43:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/27/2022 02:43:05 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.6934894497053381 on epoch=137
05/27/2022 02:43:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6796875 -> 0.6934894497053381 on epoch=137, global_step=2200
05/27/2022 02:43:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
05/27/2022 02:43:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/27/2022 02:43:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
05/27/2022 02:43:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=139
05/27/2022 02:43:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
05/27/2022 02:43:23 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.6471699574138599 on epoch=140
05/27/2022 02:43:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=141
05/27/2022 02:43:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/27/2022 02:43:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=142
05/27/2022 02:43:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/27/2022 02:43:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=143
05/27/2022 02:43:40 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.6758092414133927 on epoch=143
05/27/2022 02:43:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=144
05/27/2022 02:43:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=144
05/27/2022 02:43:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=145
05/27/2022 02:43:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/27/2022 02:43:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=146
05/27/2022 02:43:57 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.46515798620740656 on epoch=146
05/27/2022 02:43:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
05/27/2022 02:44:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
05/27/2022 02:44:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
05/27/2022 02:44:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/27/2022 02:44:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
05/27/2022 02:44:14 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.6296855345911949 on epoch=149
05/27/2022 02:44:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/27/2022 02:44:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=151
05/27/2022 02:44:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/27/2022 02:44:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/27/2022 02:44:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=153
05/27/2022 02:44:32 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.661644197726672 on epoch=153
05/27/2022 02:44:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/27/2022 02:44:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=154
05/27/2022 02:44:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=154
05/27/2022 02:44:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
05/27/2022 02:44:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=156
05/27/2022 02:44:49 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7014507627000046 on epoch=156
05/27/2022 02:44:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6934894497053381 -> 0.7014507627000046 on epoch=156, global_step=2500
05/27/2022 02:44:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
05/27/2022 02:44:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/27/2022 02:44:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=158
05/27/2022 02:44:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
05/27/2022 02:45:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
05/27/2022 02:45:07 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.6731428923039057 on epoch=159
05/27/2022 02:45:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
05/27/2022 02:45:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/27/2022 02:45:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=161
05/27/2022 02:45:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=161
05/27/2022 02:45:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
05/27/2022 02:45:24 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.6684551341350601 on epoch=162
05/27/2022 02:45:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
05/27/2022 02:45:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/27/2022 02:45:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/27/2022 02:45:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
05/27/2022 02:45:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
05/27/2022 02:45:41 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.7132642884541619 on epoch=165
05/27/2022 02:45:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7014507627000046 -> 0.7132642884541619 on epoch=165, global_step=2650
05/27/2022 02:45:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=166
05/27/2022 02:45:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=166
05/27/2022 02:45:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/27/2022 02:45:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/27/2022 02:45:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/27/2022 02:45:59 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.5576358481386414 on epoch=168
05/27/2022 02:46:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=169
05/27/2022 02:46:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/27/2022 02:46:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/27/2022 02:46:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/27/2022 02:46:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/27/2022 02:46:16 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.6031400488158849 on epoch=171
05/27/2022 02:46:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
05/27/2022 02:46:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=173
05/27/2022 02:46:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/27/2022 02:46:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/27/2022 02:46:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/27/2022 02:46:33 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.5965327029156817 on epoch=174
05/27/2022 02:46:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/27/2022 02:46:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/27/2022 02:46:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
05/27/2022 02:46:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/27/2022 02:46:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=178
05/27/2022 02:46:51 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.6263722317405926 on epoch=178
05/27/2022 02:46:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/27/2022 02:46:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/27/2022 02:46:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/27/2022 02:47:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
05/27/2022 02:47:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/27/2022 02:47:08 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.6911753477789484 on epoch=181
05/27/2022 02:47:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=181
05/27/2022 02:47:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
05/27/2022 02:47:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=183
05/27/2022 02:47:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
05/27/2022 02:47:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
05/27/2022 02:47:25 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.6833570021530991 on epoch=184
05/27/2022 02:47:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=184
05/27/2022 02:47:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/27/2022 02:47:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=186
05/27/2022 02:47:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/27/2022 02:47:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/27/2022 02:47:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:47:38 - INFO - __main__ - Printing 3 examples
05/27/2022 02:47:38 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 02:47:38 - INFO - __main__ - ['false']
05/27/2022 02:47:38 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 02:47:38 - INFO - __main__ - ['false']
05/27/2022 02:47:38 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 02:47:38 - INFO - __main__ - ['false']
05/27/2022 02:47:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:47:39 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:47:39 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:47:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:47:39 - INFO - __main__ - Printing 3 examples
05/27/2022 02:47:39 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 02:47:39 - INFO - __main__ - ['false']
05/27/2022 02:47:39 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 02:47:39 - INFO - __main__ - ['false']
05/27/2022 02:47:39 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 02:47:39 - INFO - __main__ - ['false']
05/27/2022 02:47:39 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:47:39 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:47:39 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:47:42 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.6480938416422288 on epoch=187
05/27/2022 02:47:42 - INFO - __main__ - save last model!
05/27/2022 02:47:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 02:47:42 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 02:47:42 - INFO - __main__ - Printing 3 examples
05/27/2022 02:47:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 02:47:42 - INFO - __main__ - ['false']
05/27/2022 02:47:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 02:47:42 - INFO - __main__ - ['false']
05/27/2022 02:47:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 02:47:42 - INFO - __main__ - ['false']
05/27/2022 02:47:42 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:47:43 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:47:46 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 02:47:54 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:47:54 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:47:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:47:55 - INFO - __main__ - Starting training!
05/27/2022 02:48:42 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.5_8_predictions.txt
05/27/2022 02:48:42 - INFO - __main__ - Classification-F1 on test data: 0.4894
05/27/2022 02:48:43 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.5, bsz=8, dev_performance=0.7132642884541619, test_performance=0.489449626786197
05/27/2022 02:48:43 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.4, bsz=8 ...
05/27/2022 02:48:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:48:44 - INFO - __main__ - Printing 3 examples
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:48:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:48:44 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 02:48:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 02:48:44 - INFO - __main__ - Printing 3 examples
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 02:48:44 - INFO - __main__ - ['false']
05/27/2022 02:48:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 02:48:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 02:48:45 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 02:49:03 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 02:49:03 - INFO - __main__ - task name: wiki_qa
05/27/2022 02:49:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 02:49:04 - INFO - __main__ - Starting training!
05/27/2022 02:49:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.16 on epoch=0
05/27/2022 02:49:09 - INFO - __main__ - Step 20 Global step 20 Train loss 1.52 on epoch=1
05/27/2022 02:49:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=1
05/27/2022 02:49:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=2
05/27/2022 02:49:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=3
05/27/2022 02:49:21 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 02:49:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 02:49:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/27/2022 02:49:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=4
05/27/2022 02:49:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=4
05/27/2022 02:49:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=5
05/27/2022 02:49:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
05/27/2022 02:49:38 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 02:49:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/27/2022 02:49:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
05/27/2022 02:49:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/27/2022 02:49:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=8
05/27/2022 02:49:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
05/27/2022 02:49:54 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.4291300097751711 on epoch=9
05/27/2022 02:49:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4291300097751711 on epoch=9, global_step=150
05/27/2022 02:49:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=9
05/27/2022 02:49:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
05/27/2022 02:50:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/27/2022 02:50:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/27/2022 02:50:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
05/27/2022 02:50:10 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.41606406212084446 on epoch=12
05/27/2022 02:50:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/27/2022 02:50:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
05/27/2022 02:50:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=14
05/27/2022 02:50:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/27/2022 02:50:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
05/27/2022 02:50:26 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3637775706741224 on epoch=15
05/27/2022 02:50:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
05/27/2022 02:50:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/27/2022 02:50:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/27/2022 02:50:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
05/27/2022 02:50:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
05/27/2022 02:50:42 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 02:50:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=19
05/27/2022 02:50:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=19
05/27/2022 02:50:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/27/2022 02:50:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 02:50:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
05/27/2022 02:50:58 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=21
05/27/2022 02:51:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=22
05/27/2022 02:51:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=23
05/27/2022 02:51:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=23
05/27/2022 02:51:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
05/27/2022 02:51:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=24
05/27/2022 02:51:14 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 02:51:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/27/2022 02:51:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
05/27/2022 02:51:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
05/27/2022 02:51:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 02:51:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
05/27/2022 02:51:30 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3383422492035824 on epoch=28
05/27/2022 02:51:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/27/2022 02:51:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/27/2022 02:51:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=29
05/27/2022 02:51:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
05/27/2022 02:51:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
05/27/2022 02:51:47 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=31
05/27/2022 02:51:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/27/2022 02:51:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/27/2022 02:51:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/27/2022 02:51:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/27/2022 02:52:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=34
05/27/2022 02:52:04 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.4121377802077638 on epoch=34
05/27/2022 02:52:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/27/2022 02:52:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/27/2022 02:52:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/27/2022 02:52:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/27/2022 02:52:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/27/2022 02:52:20 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.4832395400048935 on epoch=37
05/27/2022 02:52:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4291300097751711 -> 0.4832395400048935 on epoch=37, global_step=600
05/27/2022 02:52:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/27/2022 02:52:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/27/2022 02:52:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
05/27/2022 02:52:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=39
05/27/2022 02:52:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/27/2022 02:52:37 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.47022485791944646 on epoch=40
05/27/2022 02:52:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
05/27/2022 02:52:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=41
05/27/2022 02:52:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=42
05/27/2022 02:52:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/27/2022 02:52:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/27/2022 02:52:53 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 02:52:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/27/2022 02:52:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
05/27/2022 02:53:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/27/2022 02:53:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/27/2022 02:53:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/27/2022 02:53:10 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.4931496000633563 on epoch=46
05/27/2022 02:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4832395400048935 -> 0.4931496000633563 on epoch=46, global_step=750
05/27/2022 02:53:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
05/27/2022 02:53:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=48
05/27/2022 02:53:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
05/27/2022 02:53:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/27/2022 02:53:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
05/27/2022 02:53:27 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.34673046251993617 on epoch=49
05/27/2022 02:53:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/27/2022 02:53:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/27/2022 02:53:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=51
05/27/2022 02:53:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=52
05/27/2022 02:53:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/27/2022 02:53:43 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.38279939051439815 on epoch=53
05/27/2022 02:53:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
05/27/2022 02:53:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=54
05/27/2022 02:53:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=54
05/27/2022 02:53:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=55
05/27/2022 02:53:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/27/2022 02:53:59 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.504537089916873 on epoch=56
05/27/2022 02:53:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4931496000633563 -> 0.504537089916873 on epoch=56, global_step=900
05/27/2022 02:54:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=56
05/27/2022 02:54:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=57
05/27/2022 02:54:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
05/27/2022 02:54:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=58
05/27/2022 02:54:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/27/2022 02:54:15 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.477376171352075 on epoch=59
05/27/2022 02:54:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/27/2022 02:54:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
05/27/2022 02:54:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/27/2022 02:54:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=61
05/27/2022 02:54:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/27/2022 02:54:31 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.584313725490196 on epoch=62
05/27/2022 02:54:31 - INFO - __main__ - Saving model with best Classification-F1: 0.504537089916873 -> 0.584313725490196 on epoch=62, global_step=1000
05/27/2022 02:54:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/27/2022 02:54:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=63
05/27/2022 02:54:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/27/2022 02:54:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=64
05/27/2022 02:54:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
05/27/2022 02:54:46 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.5476190476190477 on epoch=65
05/27/2022 02:54:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=66
05/27/2022 02:54:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=66
05/27/2022 02:54:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
05/27/2022 02:54:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=68
05/27/2022 02:54:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=68
05/27/2022 02:55:02 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.3948694102146787 on epoch=68
05/27/2022 02:55:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
05/27/2022 02:55:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
05/27/2022 02:55:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=70
05/27/2022 02:55:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=71
05/27/2022 02:55:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=71
05/27/2022 02:55:19 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.6111392405063292 on epoch=71
05/27/2022 02:55:19 - INFO - __main__ - Saving model with best Classification-F1: 0.584313725490196 -> 0.6111392405063292 on epoch=71, global_step=1150
05/27/2022 02:55:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/27/2022 02:55:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=73
05/27/2022 02:55:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
05/27/2022 02:55:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/27/2022 02:55:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=74
05/27/2022 02:55:35 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.46895191457525676 on epoch=74
05/27/2022 02:55:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=75
05/27/2022 02:55:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
05/27/2022 02:55:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=76
05/27/2022 02:55:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
05/27/2022 02:55:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=78
05/27/2022 02:55:51 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.4417668437237909 on epoch=78
05/27/2022 02:55:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=78
05/27/2022 02:55:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=79
05/27/2022 02:55:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=79
05/27/2022 02:56:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=80
05/27/2022 02:56:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=81
05/27/2022 02:56:08 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.5810930852307261 on epoch=81
05/27/2022 02:56:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=81
05/27/2022 02:56:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=82
05/27/2022 02:56:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=83
05/27/2022 02:56:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=83
05/27/2022 02:56:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=84
05/27/2022 02:56:24 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.6152855302705025 on epoch=84
05/27/2022 02:56:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6111392405063292 -> 0.6152855302705025 on epoch=84, global_step=1350
05/27/2022 02:56:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=84
05/27/2022 02:56:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=85
05/27/2022 02:56:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=86
05/27/2022 02:56:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
05/27/2022 02:56:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/27/2022 02:56:41 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.5893361851653808 on epoch=87
05/27/2022 02:56:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=88
05/27/2022 02:56:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
05/27/2022 02:56:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=89
05/27/2022 02:56:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=89
05/27/2022 02:56:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=90
05/27/2022 02:56:58 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.6284470246734397 on epoch=90
05/27/2022 02:56:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6152855302705025 -> 0.6284470246734397 on epoch=90, global_step=1450
05/27/2022 02:57:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=91
05/27/2022 02:57:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=91
05/27/2022 02:57:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=92
05/27/2022 02:57:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
05/27/2022 02:57:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.27 on epoch=93
05/27/2022 02:57:14 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.5065881693299691 on epoch=93
05/27/2022 02:57:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
05/27/2022 02:57:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=94
05/27/2022 02:57:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
05/27/2022 02:57:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=96
05/27/2022 02:57:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=96
05/27/2022 02:57:30 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.6563334207724474 on epoch=96
05/27/2022 02:57:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6284470246734397 -> 0.6563334207724474 on epoch=96, global_step=1550
05/27/2022 02:57:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=97
05/27/2022 02:57:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
05/27/2022 02:57:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=98
05/27/2022 02:57:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=99
05/27/2022 02:57:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=99
05/27/2022 02:57:46 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.6185267685690146 on epoch=99
05/27/2022 02:57:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=100
05/27/2022 02:57:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=101
05/27/2022 02:57:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=101
05/27/2022 02:57:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=102
05/27/2022 02:57:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=103
05/27/2022 02:58:02 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.6814501881864967 on epoch=103
05/27/2022 02:58:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6563334207724474 -> 0.6814501881864967 on epoch=103, global_step=1650
05/27/2022 02:58:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/27/2022 02:58:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=104
05/27/2022 02:58:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=104
05/27/2022 02:58:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=105
05/27/2022 02:58:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=106
05/27/2022 02:58:18 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.6181324289101386 on epoch=106
05/27/2022 02:58:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=106
05/27/2022 02:58:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/27/2022 02:58:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
05/27/2022 02:58:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/27/2022 02:58:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=109
05/27/2022 02:58:34 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.6514635806671205 on epoch=109
05/27/2022 02:58:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=109
05/27/2022 02:58:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=110
05/27/2022 02:58:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
05/27/2022 02:58:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=111
05/27/2022 02:58:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=112
05/27/2022 02:58:51 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.597322623828648 on epoch=112
05/27/2022 02:58:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/27/2022 02:58:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=113
05/27/2022 02:58:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/27/2022 02:59:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/27/2022 02:59:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=115
05/27/2022 02:59:08 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.6080895979373137 on epoch=115
05/27/2022 02:59:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=116
05/27/2022 02:59:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/27/2022 02:59:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/27/2022 02:59:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=118
05/27/2022 02:59:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
05/27/2022 02:59:24 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.6507936507936507 on epoch=118
05/27/2022 02:59:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=119
05/27/2022 02:59:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/27/2022 02:59:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=120
05/27/2022 02:59:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=121
05/27/2022 02:59:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/27/2022 02:59:41 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.5512279788249588 on epoch=121
05/27/2022 02:59:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=122
05/27/2022 02:59:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=123
05/27/2022 02:59:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=123
05/27/2022 02:59:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/27/2022 02:59:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
05/27/2022 02:59:57 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.6703027293745247 on epoch=124
05/27/2022 02:59:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
05/27/2022 03:00:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=126
05/27/2022 03:00:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
05/27/2022 03:00:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=127
05/27/2022 03:00:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=128
05/27/2022 03:00:13 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.637650389242746 on epoch=128
05/27/2022 03:00:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
05/27/2022 03:00:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=129
05/27/2022 03:00:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/27/2022 03:00:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/27/2022 03:00:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/27/2022 03:00:29 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.6441808747220164 on epoch=131
05/27/2022 03:00:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=131
05/27/2022 03:00:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
05/27/2022 03:00:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=133
05/27/2022 03:00:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/27/2022 03:00:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/27/2022 03:00:45 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.6735847607343113 on epoch=134
05/27/2022 03:00:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/27/2022 03:00:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
05/27/2022 03:00:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/27/2022 03:00:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=136
05/27/2022 03:00:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/27/2022 03:01:01 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.6000000000000001 on epoch=137
05/27/2022 03:01:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=138
05/27/2022 03:01:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
05/27/2022 03:01:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=139
05/27/2022 03:01:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/27/2022 03:01:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
05/27/2022 03:01:18 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.6679021497405486 on epoch=140
05/27/2022 03:01:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=141
05/27/2022 03:01:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
05/27/2022 03:01:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=142
05/27/2022 03:01:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/27/2022 03:01:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=143
05/27/2022 03:01:35 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.620450523697929 on epoch=143
05/27/2022 03:01:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
05/27/2022 03:01:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=144
05/27/2022 03:01:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/27/2022 03:01:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
05/27/2022 03:01:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/27/2022 03:01:52 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.6593848629246859 on epoch=146
05/27/2022 03:01:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=147
05/27/2022 03:01:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
05/27/2022 03:01:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/27/2022 03:02:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=149
05/27/2022 03:02:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
05/27/2022 03:02:08 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.6673545822823512 on epoch=149
05/27/2022 03:02:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/27/2022 03:02:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/27/2022 03:02:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/27/2022 03:02:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/27/2022 03:02:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=153
05/27/2022 03:02:25 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.6508038683770903 on epoch=153
05/27/2022 03:02:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
05/27/2022 03:02:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
05/27/2022 03:02:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=154
05/27/2022 03:02:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/27/2022 03:02:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=156
05/27/2022 03:02:42 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6083333333333334 on epoch=156
05/27/2022 03:02:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/27/2022 03:02:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
05/27/2022 03:02:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/27/2022 03:02:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/27/2022 03:02:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/27/2022 03:02:59 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6705882352941177 on epoch=159
05/27/2022 03:03:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=159
05/27/2022 03:03:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
05/27/2022 03:03:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/27/2022 03:03:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
05/27/2022 03:03:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/27/2022 03:03:16 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.6005741746239781 on epoch=162
05/27/2022 03:03:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/27/2022 03:03:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/27/2022 03:03:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=164
05/27/2022 03:03:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/27/2022 03:03:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=165
05/27/2022 03:03:33 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.6100931116240363 on epoch=165
05/27/2022 03:03:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=166
05/27/2022 03:03:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=166
05/27/2022 03:03:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/27/2022 03:03:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=168
05/27/2022 03:03:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
05/27/2022 03:03:50 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.661644197726672 on epoch=168
05/27/2022 03:03:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/27/2022 03:03:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/27/2022 03:03:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/27/2022 03:04:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=171
05/27/2022 03:04:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/27/2022 03:04:07 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.5294038283160469 on epoch=171
05/27/2022 03:04:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
05/27/2022 03:04:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/27/2022 03:04:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/27/2022 03:04:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/27/2022 03:04:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/27/2022 03:04:24 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.6421248835041938 on epoch=174
05/27/2022 03:04:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/27/2022 03:04:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/27/2022 03:04:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/27/2022 03:04:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
05/27/2022 03:04:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/27/2022 03:04:40 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.6240466151878346 on epoch=178
05/27/2022 03:04:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=178
05/27/2022 03:04:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/27/2022 03:04:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/27/2022 03:04:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/27/2022 03:04:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
05/27/2022 03:04:57 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.6601510643167773 on epoch=181
05/27/2022 03:04:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/27/2022 03:05:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/27/2022 03:05:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/27/2022 03:05:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/27/2022 03:05:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
05/27/2022 03:05:14 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.6520836196497015 on epoch=184
05/27/2022 03:05:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=184
05/27/2022 03:05:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=185
05/27/2022 03:05:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=186
05/27/2022 03:05:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
05/27/2022 03:05:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/27/2022 03:05:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:05:28 - INFO - __main__ - Printing 3 examples
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:05:28 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:05:28 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:05:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:05:28 - INFO - __main__ - Printing 3 examples
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 03:05:28 - INFO - __main__ - ['false']
05/27/2022 03:05:28 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:05:28 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:05:28 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:05:30 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.615686274509804 on epoch=187
05/27/2022 03:05:30 - INFO - __main__ - save last model!
05/27/2022 03:05:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 03:05:30 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 03:05:30 - INFO - __main__ - Printing 3 examples
05/27/2022 03:05:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 03:05:30 - INFO - __main__ - ['false']
05/27/2022 03:05:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 03:05:30 - INFO - __main__ - ['false']
05/27/2022 03:05:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 03:05:30 - INFO - __main__ - ['false']
05/27/2022 03:05:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:05:31 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:05:34 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 03:05:43 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:05:43 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:05:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:05:43 - INFO - __main__ - Starting training!
05/27/2022 03:06:15 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.4_8_predictions.txt
05/27/2022 03:06:15 - INFO - __main__ - Classification-F1 on test data: 0.4977
05/27/2022 03:06:16 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.4, bsz=8, dev_performance=0.6814501881864967, test_performance=0.4976971439489751
05/27/2022 03:06:16 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.3, bsz=8 ...
05/27/2022 03:06:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:06:17 - INFO - __main__ - Printing 3 examples
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:06:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:06:17 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:06:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:06:17 - INFO - __main__ - Printing 3 examples
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 03:06:17 - INFO - __main__ - ['false']
05/27/2022 03:06:17 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:06:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:06:18 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:06:32 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:06:32 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:06:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:06:33 - INFO - __main__ - Starting training!
05/27/2022 03:06:36 - INFO - __main__ - Step 10 Global step 10 Train loss 6.45 on epoch=0
05/27/2022 03:06:39 - INFO - __main__ - Step 20 Global step 20 Train loss 1.81 on epoch=1
05/27/2022 03:06:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.79 on epoch=1
05/27/2022 03:06:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=2
05/27/2022 03:06:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=3
05/27/2022 03:06:54 - INFO - __main__ - Global step 50 Train loss 2.03 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 03:06:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 03:06:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
05/27/2022 03:06:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/27/2022 03:07:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
05/27/2022 03:07:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=5
05/27/2022 03:07:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/27/2022 03:07:11 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 03:07:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/27/2022 03:07:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/27/2022 03:07:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
05/27/2022 03:07:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
05/27/2022 03:07:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/27/2022 03:07:27 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 03:07:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=9
05/27/2022 03:07:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=10
05/27/2022 03:07:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
05/27/2022 03:07:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
05/27/2022 03:07:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
05/27/2022 03:07:43 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=12
05/27/2022 03:07:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=12, global_step=200
05/27/2022 03:07:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/27/2022 03:07:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/27/2022 03:07:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/27/2022 03:07:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=14
05/27/2022 03:07:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=15
05/27/2022 03:07:59 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.36098310291858676 on epoch=15
05/27/2022 03:07:59 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.36098310291858676 on epoch=15, global_step=250
05/27/2022 03:08:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/27/2022 03:08:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/27/2022 03:08:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/27/2022 03:08:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=18
05/27/2022 03:08:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/27/2022 03:08:15 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 03:08:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 03:08:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/27/2022 03:08:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
05/27/2022 03:08:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
05/27/2022 03:08:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/27/2022 03:08:34 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.39889437387912063 on epoch=21
05/27/2022 03:08:34 - INFO - __main__ - Saving model with best Classification-F1: 0.36098310291858676 -> 0.39889437387912063 on epoch=21, global_step=350
05/27/2022 03:08:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
05/27/2022 03:08:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/27/2022 03:08:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=23
05/27/2022 03:08:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/27/2022 03:08:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/27/2022 03:08:52 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 03:08:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/27/2022 03:08:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
05/27/2022 03:08:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
05/27/2022 03:09:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
05/27/2022 03:09:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
05/27/2022 03:09:09 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 03:09:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=28
05/27/2022 03:09:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
05/27/2022 03:09:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
05/27/2022 03:09:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
05/27/2022 03:09:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
05/27/2022 03:09:26 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.39394596548296745 on epoch=31
05/27/2022 03:09:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/27/2022 03:09:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
05/27/2022 03:09:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=33
05/27/2022 03:09:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
05/27/2022 03:09:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
05/27/2022 03:09:44 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.4181818181818182 on epoch=34
05/27/2022 03:09:44 - INFO - __main__ - Saving model with best Classification-F1: 0.39889437387912063 -> 0.4181818181818182 on epoch=34, global_step=550
05/27/2022 03:09:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/27/2022 03:09:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
05/27/2022 03:09:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/27/2022 03:09:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/27/2022 03:09:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/27/2022 03:10:00 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.40575110142593207 on epoch=37
05/27/2022 03:10:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=38
05/27/2022 03:10:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=38
05/27/2022 03:10:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=39
05/27/2022 03:10:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/27/2022 03:10:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
05/27/2022 03:10:18 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.4608058608058607 on epoch=40
05/27/2022 03:10:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.4608058608058607 on epoch=40, global_step=650
05/27/2022 03:10:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
05/27/2022 03:10:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=41
05/27/2022 03:10:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
05/27/2022 03:10:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=43
05/27/2022 03:10:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/27/2022 03:10:35 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 03:10:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=44
05/27/2022 03:10:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=44
05/27/2022 03:10:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/27/2022 03:10:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/27/2022 03:10:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/27/2022 03:10:52 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.4461680092059839 on epoch=46
05/27/2022 03:10:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
05/27/2022 03:10:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=48
05/27/2022 03:11:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/27/2022 03:11:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/27/2022 03:11:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/27/2022 03:11:09 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3816425120772947 on epoch=49
05/27/2022 03:11:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/27/2022 03:11:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=51
05/27/2022 03:11:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/27/2022 03:11:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=52
05/27/2022 03:11:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/27/2022 03:11:25 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=53
05/27/2022 03:11:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
05/27/2022 03:11:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
05/27/2022 03:11:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=54
05/27/2022 03:11:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
05/27/2022 03:11:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
05/27/2022 03:11:43 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.4930379355158116 on epoch=56
05/27/2022 03:11:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4608058608058607 -> 0.4930379355158116 on epoch=56, global_step=900
05/27/2022 03:11:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/27/2022 03:11:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
05/27/2022 03:11:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
05/27/2022 03:11:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=58
05/27/2022 03:11:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/27/2022 03:11:59 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.460545937884977 on epoch=59
05/27/2022 03:12:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.31 on epoch=59
05/27/2022 03:12:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
05/27/2022 03:12:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/27/2022 03:12:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/27/2022 03:12:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/27/2022 03:12:15 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.4819427148194272 on epoch=62
05/27/2022 03:12:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
05/27/2022 03:12:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/27/2022 03:12:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 03:12:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
05/27/2022 03:12:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=65
05/27/2022 03:12:31 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.44270730259251934 on epoch=65
05/27/2022 03:12:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
05/27/2022 03:12:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/27/2022 03:12:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
05/27/2022 03:12:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=68
05/27/2022 03:12:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=68
05/27/2022 03:12:47 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 03:12:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=69
05/27/2022 03:12:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/27/2022 03:12:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
05/27/2022 03:12:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=71
05/27/2022 03:12:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/27/2022 03:13:03 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.43722943722943725 on epoch=71
05/27/2022 03:13:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
05/27/2022 03:13:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=73
05/27/2022 03:13:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/27/2022 03:13:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
05/27/2022 03:13:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
05/27/2022 03:13:19 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.38064516129032255 on epoch=74
05/27/2022 03:13:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 03:13:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=76
05/27/2022 03:13:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=76
05/27/2022 03:13:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
05/27/2022 03:13:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
05/27/2022 03:13:35 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.4049917757276693 on epoch=78
05/27/2022 03:13:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/27/2022 03:13:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
05/27/2022 03:13:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=79
05/27/2022 03:13:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=80
05/27/2022 03:13:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/27/2022 03:13:52 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.5774802860810564 on epoch=81
05/27/2022 03:13:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4930379355158116 -> 0.5774802860810564 on epoch=81, global_step=1300
05/27/2022 03:13:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
05/27/2022 03:13:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=82
05/27/2022 03:13:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=83
05/27/2022 03:14:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
05/27/2022 03:14:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/27/2022 03:14:10 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.39405460814462767 on epoch=84
05/27/2022 03:14:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=84
05/27/2022 03:14:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=85
05/27/2022 03:14:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=86
05/27/2022 03:14:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/27/2022 03:14:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=87
05/27/2022 03:14:27 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.5291135327305047 on epoch=87
05/27/2022 03:14:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
05/27/2022 03:14:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
05/27/2022 03:14:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=89
05/27/2022 03:14:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
05/27/2022 03:14:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=90
05/27/2022 03:14:44 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.5797161488300728 on epoch=90
05/27/2022 03:14:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5774802860810564 -> 0.5797161488300728 on epoch=90, global_step=1450
05/27/2022 03:14:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
05/27/2022 03:14:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
05/27/2022 03:14:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
05/27/2022 03:14:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
05/27/2022 03:14:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
05/27/2022 03:15:01 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.38279939051439815 on epoch=93
05/27/2022 03:15:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
05/27/2022 03:15:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=94
05/27/2022 03:15:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/27/2022 03:15:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
05/27/2022 03:15:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=96
05/27/2022 03:15:18 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.5943821434614734 on epoch=96
05/27/2022 03:15:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5797161488300728 -> 0.5943821434614734 on epoch=96, global_step=1550
05/27/2022 03:15:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
05/27/2022 03:15:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=98
05/27/2022 03:15:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=98
05/27/2022 03:15:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=99
05/27/2022 03:15:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=99
05/27/2022 03:15:35 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.48055313997305915 on epoch=99
05/27/2022 03:15:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/27/2022 03:15:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=101
05/27/2022 03:15:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/27/2022 03:15:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=102
05/27/2022 03:15:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=103
05/27/2022 03:15:52 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.5102579325813048 on epoch=103
05/27/2022 03:15:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=103
05/27/2022 03:15:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.34 on epoch=104
05/27/2022 03:16:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=104
05/27/2022 03:16:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=105
05/27/2022 03:16:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=106
05/27/2022 03:16:11 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.5852299905874365 on epoch=106
05/27/2022 03:16:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=106
05/27/2022 03:16:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=107
05/27/2022 03:16:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=108
05/27/2022 03:16:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=108
05/27/2022 03:16:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=109
05/27/2022 03:16:28 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.6320039148519696 on epoch=109
05/27/2022 03:16:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5943821434614734 -> 0.6320039148519696 on epoch=109, global_step=1750
05/27/2022 03:16:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=109
05/27/2022 03:16:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=110
05/27/2022 03:16:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=111
05/27/2022 03:16:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
05/27/2022 03:16:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=112
05/27/2022 03:16:45 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.5921568627450979 on epoch=112
05/27/2022 03:16:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=113
05/27/2022 03:16:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=113
05/27/2022 03:16:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=114
05/27/2022 03:16:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=114
05/27/2022 03:16:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=115
05/27/2022 03:17:02 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.6210915818686402 on epoch=115
05/27/2022 03:17:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=116
05/27/2022 03:17:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=116
05/27/2022 03:17:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=117
05/27/2022 03:17:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=118
05/27/2022 03:17:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=118
05/27/2022 03:17:19 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.55 on epoch=118
05/27/2022 03:17:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=119
05/27/2022 03:17:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=119
05/27/2022 03:17:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=120
05/27/2022 03:17:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=121
05/27/2022 03:17:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=121
05/27/2022 03:17:37 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.6168132942326491 on epoch=121
05/27/2022 03:17:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=122
05/27/2022 03:17:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=123
05/27/2022 03:17:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=123
05/27/2022 03:17:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=124
05/27/2022 03:17:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
05/27/2022 03:17:54 - INFO - __main__ - Global step 2000 Train loss 0.31 Classification-F1 0.5437892337084784 on epoch=124
05/27/2022 03:17:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=125
05/27/2022 03:17:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=126
05/27/2022 03:18:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=126
05/27/2022 03:18:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=127
05/27/2022 03:18:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=128
05/27/2022 03:18:11 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.5796387520525451 on epoch=128
05/27/2022 03:18:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=128
05/27/2022 03:18:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=129
05/27/2022 03:18:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.28 on epoch=129
05/27/2022 03:18:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=130
05/27/2022 03:18:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.33 on epoch=131
05/27/2022 03:18:28 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.5666666666666667 on epoch=131
05/27/2022 03:18:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=131
05/27/2022 03:18:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/27/2022 03:18:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=133
05/27/2022 03:18:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=133
05/27/2022 03:18:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.29 on epoch=134
05/27/2022 03:18:45 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.6276278115478251 on epoch=134
05/27/2022 03:18:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=134
05/27/2022 03:18:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.28 on epoch=135
05/27/2022 03:18:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/27/2022 03:18:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.32 on epoch=136
05/27/2022 03:18:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.29 on epoch=137
05/27/2022 03:19:02 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.6191371075092005 on epoch=137
05/27/2022 03:19:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=138
05/27/2022 03:19:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=138
05/27/2022 03:19:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=139
05/27/2022 03:19:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.33 on epoch=139
05/27/2022 03:19:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.31 on epoch=140
05/27/2022 03:19:19 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.6205613371176285 on epoch=140
05/27/2022 03:19:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.27 on epoch=141
05/27/2022 03:19:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=141
05/27/2022 03:19:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=142
05/27/2022 03:19:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=143
05/27/2022 03:19:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=143
05/27/2022 03:19:36 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.5577128753599342 on epoch=143
05/27/2022 03:19:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=144
05/27/2022 03:19:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.30 on epoch=144
05/27/2022 03:19:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.30 on epoch=145
05/27/2022 03:19:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=146
05/27/2022 03:19:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.33 on epoch=146
05/27/2022 03:19:53 - INFO - __main__ - Global step 2350 Train loss 0.30 Classification-F1 0.6118571693792932 on epoch=146
05/27/2022 03:19:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=147
05/27/2022 03:19:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=148
05/27/2022 03:20:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=148
05/27/2022 03:20:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=149
05/27/2022 03:20:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.30 on epoch=149
05/27/2022 03:20:11 - INFO - __main__ - Global step 2400 Train loss 0.29 Classification-F1 0.6392156862745098 on epoch=149
05/27/2022 03:20:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6320039148519696 -> 0.6392156862745098 on epoch=149, global_step=2400
05/27/2022 03:20:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.27 on epoch=150
05/27/2022 03:20:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=151
05/27/2022 03:20:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=151
05/27/2022 03:20:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=152
05/27/2022 03:20:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=153
05/27/2022 03:20:28 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.5028417695583232 on epoch=153
05/27/2022 03:20:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=153
05/27/2022 03:20:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=154
05/27/2022 03:20:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.28 on epoch=154
05/27/2022 03:20:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=155
05/27/2022 03:20:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=156
05/27/2022 03:20:46 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.6249771104193371 on epoch=156
05/27/2022 03:20:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.23 on epoch=156
05/27/2022 03:20:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=157
05/27/2022 03:20:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.24 on epoch=158
05/27/2022 03:20:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=158
05/27/2022 03:20:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=159
05/27/2022 03:21:04 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.6422503807684457 on epoch=159
05/27/2022 03:21:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6392156862745098 -> 0.6422503807684457 on epoch=159, global_step=2550
05/27/2022 03:21:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=159
05/27/2022 03:21:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=160
05/27/2022 03:21:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/27/2022 03:21:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.27 on epoch=161
05/27/2022 03:21:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=162
05/27/2022 03:21:20 - INFO - __main__ - Global step 2600 Train loss 0.26 Classification-F1 0.5995995995995995 on epoch=162
05/27/2022 03:21:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.24 on epoch=163
05/27/2022 03:21:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=163
05/27/2022 03:21:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=164
05/27/2022 03:21:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=164
05/27/2022 03:21:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=165
05/27/2022 03:21:37 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.6384180790960452 on epoch=165
05/27/2022 03:21:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=166
05/27/2022 03:21:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=166
05/27/2022 03:21:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=167
05/27/2022 03:21:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.25 on epoch=168
05/27/2022 03:21:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/27/2022 03:21:54 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.5215789014059907 on epoch=168
05/27/2022 03:21:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=169
05/27/2022 03:21:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=169
05/27/2022 03:22:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=170
05/27/2022 03:22:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=171
05/27/2022 03:22:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
05/27/2022 03:22:11 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.6150375939849624 on epoch=171
05/27/2022 03:22:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=172
05/27/2022 03:22:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=173
05/27/2022 03:22:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.22 on epoch=173
05/27/2022 03:22:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=174
05/27/2022 03:22:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=174
05/27/2022 03:22:28 - INFO - __main__ - Global step 2800 Train loss 0.24 Classification-F1 0.6100931116240363 on epoch=174
05/27/2022 03:22:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=175
05/27/2022 03:22:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=176
05/27/2022 03:22:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/27/2022 03:22:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=177
05/27/2022 03:22:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=178
05/27/2022 03:22:45 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.6295566502463054 on epoch=178
05/27/2022 03:22:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=178
05/27/2022 03:22:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/27/2022 03:22:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
05/27/2022 03:22:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=180
05/27/2022 03:22:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=181
05/27/2022 03:23:03 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.6103137620379 on epoch=181
05/27/2022 03:23:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=181
05/27/2022 03:23:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
05/27/2022 03:23:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=183
05/27/2022 03:23:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/27/2022 03:23:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
05/27/2022 03:23:20 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.6241742109126498 on epoch=184
05/27/2022 03:23:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=184
05/27/2022 03:23:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=185
05/27/2022 03:23:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/27/2022 03:23:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=186
05/27/2022 03:23:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.21 on epoch=187
05/27/2022 03:23:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:23:33 - INFO - __main__ - Printing 3 examples
05/27/2022 03:23:33 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 03:23:33 - INFO - __main__ - ['false']
05/27/2022 03:23:33 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 03:23:33 - INFO - __main__ - ['false']
05/27/2022 03:23:33 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 03:23:33 - INFO - __main__ - ['false']
05/27/2022 03:23:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:23:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:23:34 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:23:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:23:34 - INFO - __main__ - Printing 3 examples
05/27/2022 03:23:34 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 03:23:34 - INFO - __main__ - ['false']
05/27/2022 03:23:34 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 03:23:34 - INFO - __main__ - ['false']
05/27/2022 03:23:34 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 03:23:34 - INFO - __main__ - ['false']
05/27/2022 03:23:34 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:23:34 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:23:34 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:23:37 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.6238751147842057 on epoch=187
05/27/2022 03:23:37 - INFO - __main__ - save last model!
05/27/2022 03:23:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 03:23:37 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 03:23:37 - INFO - __main__ - Printing 3 examples
05/27/2022 03:23:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 03:23:37 - INFO - __main__ - ['false']
05/27/2022 03:23:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 03:23:37 - INFO - __main__ - ['false']
05/27/2022 03:23:37 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 03:23:37 - INFO - __main__ - ['false']
05/27/2022 03:23:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:23:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:23:41 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 03:23:49 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:23:49 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:23:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:23:49 - INFO - __main__ - Starting training!
05/27/2022 03:24:41 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.3_8_predictions.txt
05/27/2022 03:24:41 - INFO - __main__ - Classification-F1 on test data: 0.5106
05/27/2022 03:24:42 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.3, bsz=8, dev_performance=0.6422503807684457, test_performance=0.5105793867261758
05/27/2022 03:24:42 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.2, bsz=8 ...
05/27/2022 03:24:43 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:24:43 - INFO - __main__ - Printing 3 examples
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:24:43 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:24:43 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:24:43 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:24:43 - INFO - __main__ - Printing 3 examples
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/27/2022 03:24:43 - INFO - __main__ - ['false']
05/27/2022 03:24:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:24:43 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:24:43 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:25:02 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:25:02 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:25:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:25:03 - INFO - __main__ - Starting training!
05/27/2022 03:25:06 - INFO - __main__ - Step 10 Global step 10 Train loss 6.97 on epoch=0
05/27/2022 03:25:08 - INFO - __main__ - Step 20 Global step 20 Train loss 3.18 on epoch=1
05/27/2022 03:25:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.81 on epoch=1
05/27/2022 03:25:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=2
05/27/2022 03:25:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.72 on epoch=3
05/27/2022 03:25:34 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.1662303664921466 on epoch=3
05/27/2022 03:25:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1662303664921466 on epoch=3, global_step=50
05/27/2022 03:25:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=3
05/27/2022 03:25:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=4
05/27/2022 03:25:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=4
05/27/2022 03:25:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=5
05/27/2022 03:25:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=6
05/27/2022 03:25:57 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.33101483446311036 on epoch=6
05/27/2022 03:25:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1662303664921466 -> 0.33101483446311036 on epoch=6, global_step=100
05/27/2022 03:25:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=6
05/27/2022 03:26:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/27/2022 03:26:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=8
05/27/2022 03:26:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=8
05/27/2022 03:26:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/27/2022 03:26:13 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 03:26:13 - INFO - __main__ - Saving model with best Classification-F1: 0.33101483446311036 -> 0.3333333333333333 on epoch=9, global_step=150
05/27/2022 03:26:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/27/2022 03:26:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=10
05/27/2022 03:26:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=11
05/27/2022 03:26:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/27/2022 03:26:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=12
05/27/2022 03:26:30 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.4014801272698507 on epoch=12
05/27/2022 03:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4014801272698507 on epoch=12, global_step=200
05/27/2022 03:26:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/27/2022 03:26:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=13
05/27/2022 03:26:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=14
05/27/2022 03:26:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
05/27/2022 03:26:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/27/2022 03:26:46 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.337950186583355 on epoch=15
05/27/2022 03:26:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=16
05/27/2022 03:26:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/27/2022 03:26:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=17
05/27/2022 03:26:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/27/2022 03:26:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/27/2022 03:27:02 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 03:27:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/27/2022 03:27:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/27/2022 03:27:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=20
05/27/2022 03:27:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 03:27:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/27/2022 03:27:19 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.34485289741504155 on epoch=21
05/27/2022 03:27:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
05/27/2022 03:27:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=23
05/27/2022 03:27:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/27/2022 03:27:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=24
05/27/2022 03:27:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/27/2022 03:27:35 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 03:27:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/27/2022 03:27:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
05/27/2022 03:27:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/27/2022 03:27:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=27
05/27/2022 03:27:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=28
05/27/2022 03:27:51 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 03:27:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
05/27/2022 03:27:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/27/2022 03:27:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=29
05/27/2022 03:28:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 03:28:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
05/27/2022 03:28:07 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.4081338153831831 on epoch=31
05/27/2022 03:28:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4014801272698507 -> 0.4081338153831831 on epoch=31, global_step=500
05/27/2022 03:28:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
05/27/2022 03:28:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/27/2022 03:28:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
05/27/2022 03:28:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/27/2022 03:28:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/27/2022 03:28:23 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.36318407960199 on epoch=34
05/27/2022 03:28:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/27/2022 03:28:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=35
05/27/2022 03:28:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=36
05/27/2022 03:28:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/27/2022 03:28:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/27/2022 03:28:39 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.41426949611387653 on epoch=37
05/27/2022 03:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4081338153831831 -> 0.41426949611387653 on epoch=37, global_step=600
05/27/2022 03:28:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/27/2022 03:28:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/27/2022 03:28:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/27/2022 03:28:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
05/27/2022 03:28:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
05/27/2022 03:28:56 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.4179293586739565 on epoch=40
05/27/2022 03:28:56 - INFO - __main__ - Saving model with best Classification-F1: 0.41426949611387653 -> 0.4179293586739565 on epoch=40, global_step=650
05/27/2022 03:28:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/27/2022 03:29:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/27/2022 03:29:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/27/2022 03:29:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/27/2022 03:29:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=43
05/27/2022 03:29:12 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 03:29:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=44
05/27/2022 03:29:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/27/2022 03:29:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
05/27/2022 03:29:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
05/27/2022 03:29:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
05/27/2022 03:29:28 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3328595119639896 on epoch=46
05/27/2022 03:29:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/27/2022 03:29:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
05/27/2022 03:29:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
05/27/2022 03:29:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
05/27/2022 03:29:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=49
05/27/2022 03:29:44 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=49
05/27/2022 03:29:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/27/2022 03:29:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
05/27/2022 03:29:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=51
05/27/2022 03:29:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
05/27/2022 03:29:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
05/27/2022 03:30:00 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 03:30:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 03:30:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=54
05/27/2022 03:30:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
05/27/2022 03:30:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
05/27/2022 03:30:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=56
05/27/2022 03:30:16 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.36098310291858676 on epoch=56
05/27/2022 03:30:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/27/2022 03:30:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/27/2022 03:30:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=58
05/27/2022 03:30:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
05/27/2022 03:30:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/27/2022 03:30:33 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 03:30:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=59
05/27/2022 03:30:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
05/27/2022 03:30:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=61
05/27/2022 03:30:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
05/27/2022 03:30:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/27/2022 03:30:49 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.40320462306277904 on epoch=62
05/27/2022 03:30:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
05/27/2022 03:30:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/27/2022 03:30:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=64
05/27/2022 03:30:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
05/27/2022 03:31:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
05/27/2022 03:31:05 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.4060523109278764 on epoch=65
05/27/2022 03:31:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
05/27/2022 03:31:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/27/2022 03:31:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
05/27/2022 03:31:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=68
05/27/2022 03:31:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
05/27/2022 03:31:21 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=68
05/27/2022 03:31:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=69
05/27/2022 03:31:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
05/27/2022 03:31:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/27/2022 03:31:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 03:31:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/27/2022 03:31:37 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.40577272421449234 on epoch=71
05/27/2022 03:31:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
05/27/2022 03:31:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
05/27/2022 03:31:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=73
05/27/2022 03:31:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
05/27/2022 03:31:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/27/2022 03:31:53 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 03:31:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
05/27/2022 03:31:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
05/27/2022 03:32:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/27/2022 03:32:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=77
05/27/2022 03:32:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
05/27/2022 03:32:09 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.3531737612590892 on epoch=78
05/27/2022 03:32:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
05/27/2022 03:32:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
05/27/2022 03:32:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
05/27/2022 03:32:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=80
05/27/2022 03:32:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
05/27/2022 03:32:25 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.4080870974202123 on epoch=81
05/27/2022 03:32:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
05/27/2022 03:32:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=82
05/27/2022 03:32:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=83
05/27/2022 03:32:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
05/27/2022 03:32:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
05/27/2022 03:32:41 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.3784863604213263 on epoch=84
05/27/2022 03:32:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=84
05/27/2022 03:32:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
05/27/2022 03:32:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
05/27/2022 03:32:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
05/27/2022 03:32:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=87
05/27/2022 03:32:58 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.41561819822689394 on epoch=87
05/27/2022 03:33:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
05/27/2022 03:33:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=88
05/27/2022 03:33:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=89
05/27/2022 03:33:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
05/27/2022 03:33:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
05/27/2022 03:33:14 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.38918345705196183 on epoch=90
05/27/2022 03:33:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=91
05/27/2022 03:33:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
05/27/2022 03:33:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=92
05/27/2022 03:33:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=93
05/27/2022 03:33:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/27/2022 03:33:30 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.35588481968129715 on epoch=93
05/27/2022 03:33:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=94
05/27/2022 03:33:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
05/27/2022 03:33:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=95
05/27/2022 03:33:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
05/27/2022 03:33:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
05/27/2022 03:33:46 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.34918648310387984 on epoch=96
05/27/2022 03:33:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=97
05/27/2022 03:33:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=98
05/27/2022 03:33:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=98
05/27/2022 03:33:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
05/27/2022 03:33:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=99
05/27/2022 03:34:02 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3651088894055646 on epoch=99
05/27/2022 03:34:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
05/27/2022 03:34:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=101
05/27/2022 03:34:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=101
05/27/2022 03:34:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
05/27/2022 03:34:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=103
05/27/2022 03:34:18 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.37984496124031003 on epoch=103
05/27/2022 03:34:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
05/27/2022 03:34:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
05/27/2022 03:34:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=104
05/27/2022 03:34:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=105
05/27/2022 03:34:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=106
05/27/2022 03:34:34 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.48808597029506495 on epoch=106
05/27/2022 03:34:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4179293586739565 -> 0.48808597029506495 on epoch=106, global_step=1700
05/27/2022 03:34:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=106
05/27/2022 03:34:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=107
05/27/2022 03:34:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
05/27/2022 03:34:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
05/27/2022 03:34:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=109
05/27/2022 03:34:50 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.42329972108989483 on epoch=109
05/27/2022 03:34:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=109
05/27/2022 03:34:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=110
05/27/2022 03:34:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=111
05/27/2022 03:35:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
05/27/2022 03:35:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
05/27/2022 03:35:06 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.48601468266740233 on epoch=112
05/27/2022 03:35:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
05/27/2022 03:35:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=113
05/27/2022 03:35:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=114
05/27/2022 03:35:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=114
05/27/2022 03:35:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
05/27/2022 03:35:23 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.4947909495617054 on epoch=115
05/27/2022 03:35:23 - INFO - __main__ - Saving model with best Classification-F1: 0.48808597029506495 -> 0.4947909495617054 on epoch=115, global_step=1850
05/27/2022 03:35:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=116
05/27/2022 03:35:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=116
05/27/2022 03:35:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=117
05/27/2022 03:35:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=118
05/27/2022 03:35:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=118
05/27/2022 03:35:39 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.34296770117665637 on epoch=118
05/27/2022 03:35:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=119
05/27/2022 03:35:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=119
05/27/2022 03:35:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=120
05/27/2022 03:35:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=121
05/27/2022 03:35:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=121
05/27/2022 03:35:55 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.46867924528301885 on epoch=121
05/27/2022 03:35:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=122
05/27/2022 03:36:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=123
05/27/2022 03:36:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
05/27/2022 03:36:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=124
05/27/2022 03:36:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=124
05/27/2022 03:36:11 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.4217041655371826 on epoch=124
05/27/2022 03:36:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=125
05/27/2022 03:36:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=126
05/27/2022 03:36:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=126
05/27/2022 03:36:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=127
05/27/2022 03:36:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=128
05/27/2022 03:36:27 - INFO - __main__ - Global step 2050 Train loss 0.36 Classification-F1 0.4998692448505546 on epoch=128
05/27/2022 03:36:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4947909495617054 -> 0.4998692448505546 on epoch=128, global_step=2050
05/27/2022 03:36:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=128
05/27/2022 03:36:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=129
05/27/2022 03:36:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=129
05/27/2022 03:36:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=130
05/27/2022 03:36:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=131
05/27/2022 03:36:43 - INFO - __main__ - Global step 2100 Train loss 0.35 Classification-F1 0.5104563464745209 on epoch=131
05/27/2022 03:36:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4998692448505546 -> 0.5104563464745209 on epoch=131, global_step=2100
05/27/2022 03:36:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=131
05/27/2022 03:36:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=132
05/27/2022 03:36:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=133
05/27/2022 03:36:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=133
05/27/2022 03:36:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
05/27/2022 03:37:00 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.463150301139128 on epoch=134
05/27/2022 03:37:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.32 on epoch=134
05/27/2022 03:37:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.34 on epoch=135
05/27/2022 03:37:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
05/27/2022 03:37:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
05/27/2022 03:37:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=137
05/27/2022 03:37:17 - INFO - __main__ - Global step 2200 Train loss 0.34 Classification-F1 0.5145583557621727 on epoch=137
05/27/2022 03:37:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5104563464745209 -> 0.5145583557621727 on epoch=137, global_step=2200
05/27/2022 03:37:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=138
05/27/2022 03:37:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=138
05/27/2022 03:37:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=139
05/27/2022 03:37:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.33 on epoch=139
05/27/2022 03:37:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=140
05/27/2022 03:37:33 - INFO - __main__ - Global step 2250 Train loss 0.34 Classification-F1 0.5440960123886953 on epoch=140
05/27/2022 03:37:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5145583557621727 -> 0.5440960123886953 on epoch=140, global_step=2250
05/27/2022 03:37:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=141
05/27/2022 03:37:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=141
05/27/2022 03:37:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=142
05/27/2022 03:37:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=143
05/27/2022 03:37:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=143
05/27/2022 03:37:50 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.475475261460616 on epoch=143
05/27/2022 03:37:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=144
05/27/2022 03:37:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=144
05/27/2022 03:37:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=145
05/27/2022 03:38:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.29 on epoch=146
05/27/2022 03:38:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.33 on epoch=146
05/27/2022 03:38:06 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.5566219114959879 on epoch=146
05/27/2022 03:38:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5440960123886953 -> 0.5566219114959879 on epoch=146, global_step=2350
05/27/2022 03:38:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.33 on epoch=147
05/27/2022 03:38:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=148
05/27/2022 03:38:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.32 on epoch=148
05/27/2022 03:38:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=149
05/27/2022 03:38:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.32 on epoch=149
05/27/2022 03:38:23 - INFO - __main__ - Global step 2400 Train loss 0.33 Classification-F1 0.5555555555555556 on epoch=149
05/27/2022 03:38:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=150
05/27/2022 03:38:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.34 on epoch=151
05/27/2022 03:38:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=151
05/27/2022 03:38:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.32 on epoch=152
05/27/2022 03:38:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=153
05/27/2022 03:38:39 - INFO - __main__ - Global step 2450 Train loss 0.30 Classification-F1 0.5229813664596273 on epoch=153
05/27/2022 03:38:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.30 on epoch=153
05/27/2022 03:38:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.29 on epoch=154
05/27/2022 03:38:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=154
05/27/2022 03:38:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=155
05/27/2022 03:38:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.29 on epoch=156
05/27/2022 03:38:55 - INFO - __main__ - Global step 2500 Train loss 0.29 Classification-F1 0.6218376327533393 on epoch=156
05/27/2022 03:38:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5566219114959879 -> 0.6218376327533393 on epoch=156, global_step=2500
05/27/2022 03:38:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.29 on epoch=156
05/27/2022 03:39:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.31 on epoch=157
05/27/2022 03:39:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=158
05/27/2022 03:39:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=158
05/27/2022 03:39:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=159
05/27/2022 03:39:12 - INFO - __main__ - Global step 2550 Train loss 0.30 Classification-F1 0.6249771104193371 on epoch=159
05/27/2022 03:39:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6218376327533393 -> 0.6249771104193371 on epoch=159, global_step=2550
05/27/2022 03:39:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=159
05/27/2022 03:39:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.27 on epoch=160
05/27/2022 03:39:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.30 on epoch=161
05/27/2022 03:39:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.27 on epoch=161
05/27/2022 03:39:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.28 on epoch=162
05/27/2022 03:39:28 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.6092796092796093 on epoch=162
05/27/2022 03:39:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.29 on epoch=163
05/27/2022 03:39:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=163
05/27/2022 03:39:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=164
05/27/2022 03:39:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.29 on epoch=164
05/27/2022 03:39:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.27 on epoch=165
05/27/2022 03:39:45 - INFO - __main__ - Global step 2650 Train loss 0.28 Classification-F1 0.5960146048641624 on epoch=165
05/27/2022 03:39:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=166
05/27/2022 03:39:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=166
05/27/2022 03:39:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=167
05/27/2022 03:39:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.30 on epoch=168
05/27/2022 03:39:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.31 on epoch=168
05/27/2022 03:40:01 - INFO - __main__ - Global step 2700 Train loss 0.28 Classification-F1 0.5460992907801419 on epoch=168
05/27/2022 03:40:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=169
05/27/2022 03:40:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=169
05/27/2022 03:40:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=170
05/27/2022 03:40:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.30 on epoch=171
05/27/2022 03:40:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=171
05/27/2022 03:40:17 - INFO - __main__ - Global step 2750 Train loss 0.28 Classification-F1 0.5859751597657293 on epoch=171
05/27/2022 03:40:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=172
05/27/2022 03:40:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.26 on epoch=173
05/27/2022 03:40:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=173
05/27/2022 03:40:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.26 on epoch=174
05/27/2022 03:40:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=174
05/27/2022 03:40:34 - INFO - __main__ - Global step 2800 Train loss 0.24 Classification-F1 0.6756575231640487 on epoch=174
05/27/2022 03:40:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6249771104193371 -> 0.6756575231640487 on epoch=174, global_step=2800
05/27/2022 03:40:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.26 on epoch=175
05/27/2022 03:40:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.27 on epoch=176
05/27/2022 03:40:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=176
05/27/2022 03:40:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.29 on epoch=177
05/27/2022 03:40:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=178
05/27/2022 03:40:50 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.6479002384008803 on epoch=178
05/27/2022 03:40:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.26 on epoch=178
05/27/2022 03:40:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=179
05/27/2022 03:40:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=179
05/27/2022 03:41:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=180
05/27/2022 03:41:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=181
05/27/2022 03:41:07 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.6635491166941745 on epoch=181
05/27/2022 03:41:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=181
05/27/2022 03:41:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=182
05/27/2022 03:41:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.25 on epoch=183
05/27/2022 03:41:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=183
05/27/2022 03:41:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=184
05/27/2022 03:41:23 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.6517006802721088 on epoch=184
05/27/2022 03:41:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=184
05/27/2022 03:41:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=185
05/27/2022 03:41:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.24 on epoch=186
05/27/2022 03:41:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=186
05/27/2022 03:41:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=187
05/27/2022 03:41:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:41:37 - INFO - __main__ - Printing 3 examples
05/27/2022 03:41:37 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 03:41:37 - INFO - __main__ - ['false']
05/27/2022 03:41:37 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 03:41:37 - INFO - __main__ - ['false']
05/27/2022 03:41:37 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 03:41:37 - INFO - __main__ - ['false']
05/27/2022 03:41:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:41:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:41:38 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:41:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:41:38 - INFO - __main__ - Printing 3 examples
05/27/2022 03:41:38 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 03:41:38 - INFO - __main__ - ['false']
05/27/2022 03:41:38 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 03:41:38 - INFO - __main__ - ['false']
05/27/2022 03:41:38 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 03:41:38 - INFO - __main__ - ['false']
05/27/2022 03:41:38 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:41:38 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:41:38 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:41:40 - INFO - __main__ - Global step 3000 Train loss 0.22 Classification-F1 0.5673382820784729 on epoch=187
05/27/2022 03:41:40 - INFO - __main__ - save last model!
05/27/2022 03:41:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 03:41:40 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 03:41:40 - INFO - __main__ - Printing 3 examples
05/27/2022 03:41:40 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 03:41:40 - INFO - __main__ - ['false']
05/27/2022 03:41:40 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 03:41:40 - INFO - __main__ - ['false']
05/27/2022 03:41:40 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 03:41:40 - INFO - __main__ - ['false']
05/27/2022 03:41:40 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:41:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:41:44 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 03:41:52 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:41:52 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:41:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:41:53 - INFO - __main__ - Starting training!
05/27/2022 03:42:24 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.2_8_predictions.txt
05/27/2022 03:42:24 - INFO - __main__ - Classification-F1 on test data: 0.3370
05/27/2022 03:42:24 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.2, bsz=8, dev_performance=0.6756575231640487, test_performance=0.3369766642772087
05/27/2022 03:42:24 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.5, bsz=8 ...
05/27/2022 03:42:25 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:42:25 - INFO - __main__ - Printing 3 examples
05/27/2022 03:42:25 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 03:42:25 - INFO - __main__ - ['false']
05/27/2022 03:42:25 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 03:42:25 - INFO - __main__ - ['false']
05/27/2022 03:42:25 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 03:42:25 - INFO - __main__ - ['false']
05/27/2022 03:42:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:42:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:42:26 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 03:42:26 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 03:42:26 - INFO - __main__ - Printing 3 examples
05/27/2022 03:42:26 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 03:42:26 - INFO - __main__ - ['false']
05/27/2022 03:42:26 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 03:42:26 - INFO - __main__ - ['false']
05/27/2022 03:42:26 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 03:42:26 - INFO - __main__ - ['false']
05/27/2022 03:42:26 - INFO - __main__ - Tokenizing Input ...
05/27/2022 03:42:26 - INFO - __main__ - Tokenizing Output ...
05/27/2022 03:42:26 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 03:42:44 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 03:42:44 - INFO - __main__ - task name: wiki_qa
05/27/2022 03:42:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 03:42:45 - INFO - __main__ - Starting training!
05/27/2022 03:42:49 - INFO - __main__ - Step 10 Global step 10 Train loss 5.39 on epoch=0
05/27/2022 03:42:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.95 on epoch=1
05/27/2022 03:42:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=1
05/27/2022 03:42:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=2
05/27/2022 03:42:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=3
05/27/2022 03:43:06 - INFO - __main__ - Global step 50 Train loss 1.60 Classification-F1 0.35693779904306216 on epoch=3
05/27/2022 03:43:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.35693779904306216 on epoch=3, global_step=50
05/27/2022 03:43:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=3
05/27/2022 03:43:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
05/27/2022 03:43:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=4
05/27/2022 03:43:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
05/27/2022 03:43:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/27/2022 03:43:23 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.350463149416029 on epoch=6
05/27/2022 03:43:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/27/2022 03:43:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/27/2022 03:43:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/27/2022 03:43:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.97 on epoch=8
05/27/2022 03:43:36 - INFO - __main__ - Step 150 Global step 150 Train loss 2.45 on epoch=9
05/27/2022 03:43:41 - INFO - __main__ - Global step 150 Train loss 0.94 Classification-F1 0.3860677578987438 on epoch=9
05/27/2022 03:43:41 - INFO - __main__ - Saving model with best Classification-F1: 0.35693779904306216 -> 0.3860677578987438 on epoch=9, global_step=150
05/27/2022 03:43:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=9
05/27/2022 03:43:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/27/2022 03:43:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
05/27/2022 03:43:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=11
05/27/2022 03:43:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=12
05/27/2022 03:43:58 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.45342621596770927 on epoch=12
05/27/2022 03:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3860677578987438 -> 0.45342621596770927 on epoch=12, global_step=200
05/27/2022 03:44:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
05/27/2022 03:44:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
05/27/2022 03:44:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/27/2022 03:44:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/27/2022 03:44:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/27/2022 03:44:16 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.42235512098475536 on epoch=15
05/27/2022 03:44:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=16
05/27/2022 03:44:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/27/2022 03:44:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/27/2022 03:44:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
05/27/2022 03:44:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
05/27/2022 03:44:34 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=18
05/27/2022 03:44:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=19
05/27/2022 03:44:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=19
05/27/2022 03:44:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/27/2022 03:44:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 03:44:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/27/2022 03:44:51 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.519825153724454 on epoch=21
05/27/2022 03:44:51 - INFO - __main__ - Saving model with best Classification-F1: 0.45342621596770927 -> 0.519825153724454 on epoch=21, global_step=350
05/27/2022 03:44:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
05/27/2022 03:44:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=23
05/27/2022 03:44:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/27/2022 03:45:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=24
05/27/2022 03:45:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=24
05/27/2022 03:45:09 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.4679241987599337 on epoch=24
05/27/2022 03:45:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/27/2022 03:45:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
05/27/2022 03:45:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=26
05/27/2022 03:45:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 03:45:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/27/2022 03:45:26 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=28
05/27/2022 03:45:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/27/2022 03:45:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/27/2022 03:45:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/27/2022 03:45:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
05/27/2022 03:45:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
05/27/2022 03:45:44 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.5240804004131246 on epoch=31
05/27/2022 03:45:44 - INFO - __main__ - Saving model with best Classification-F1: 0.519825153724454 -> 0.5240804004131246 on epoch=31, global_step=500
05/27/2022 03:45:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
05/27/2022 03:45:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
05/27/2022 03:45:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/27/2022 03:45:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=33
05/27/2022 03:45:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/27/2022 03:46:02 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.5093688128613288 on epoch=34
05/27/2022 03:46:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/27/2022 03:46:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
05/27/2022 03:46:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=36
05/27/2022 03:46:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/27/2022 03:46:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/27/2022 03:46:19 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.350463149416029 on epoch=37
05/27/2022 03:46:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
05/27/2022 03:46:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/27/2022 03:46:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/27/2022 03:46:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
05/27/2022 03:46:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/27/2022 03:46:36 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.4059435586495995 on epoch=40
05/27/2022 03:46:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=41
05/27/2022 03:46:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/27/2022 03:46:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/27/2022 03:46:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
05/27/2022 03:46:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
05/27/2022 03:46:53 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 03:46:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/27/2022 03:46:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=44
05/27/2022 03:47:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/27/2022 03:47:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
05/27/2022 03:47:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/27/2022 03:47:10 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.48891908668444983 on epoch=46
05/27/2022 03:47:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/27/2022 03:47:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/27/2022 03:47:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
05/27/2022 03:47:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
05/27/2022 03:47:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=49
05/27/2022 03:47:27 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 03:47:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/27/2022 03:47:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
05/27/2022 03:47:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
05/27/2022 03:47:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/27/2022 03:47:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
05/27/2022 03:47:44 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 03:47:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=53
05/27/2022 03:47:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
05/27/2022 03:47:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=54
05/27/2022 03:47:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
05/27/2022 03:47:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/27/2022 03:48:01 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.4748717948717949 on epoch=56
05/27/2022 03:48:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/27/2022 03:48:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
05/27/2022 03:48:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
05/27/2022 03:48:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
05/27/2022 03:48:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
05/27/2022 03:48:19 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.4906642179369452 on epoch=59
05/27/2022 03:48:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
05/27/2022 03:48:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=60
05/27/2022 03:48:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/27/2022 03:48:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/27/2022 03:48:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=62
05/27/2022 03:48:36 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.47476049173378554 on epoch=62
05/27/2022 03:48:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
05/27/2022 03:48:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=63
05/27/2022 03:48:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
05/27/2022 03:48:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
05/27/2022 03:48:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=65
05/27/2022 03:48:53 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.4309568988885877 on epoch=65
05/27/2022 03:48:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=66
05/27/2022 03:48:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=66
05/27/2022 03:49:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/27/2022 03:49:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
05/27/2022 03:49:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
05/27/2022 03:49:11 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 03:49:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
05/27/2022 03:49:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=69
05/27/2022 03:49:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
05/27/2022 03:49:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 03:49:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=71
05/27/2022 03:49:28 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.3849492366116407 on epoch=71
05/27/2022 03:49:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/27/2022 03:49:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=73
05/27/2022 03:49:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=73
05/27/2022 03:49:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
05/27/2022 03:49:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=74
05/27/2022 03:49:45 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 03:49:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/27/2022 03:49:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=76
05/27/2022 03:49:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/27/2022 03:49:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
05/27/2022 03:49:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
05/27/2022 03:50:02 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=78
05/27/2022 03:50:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
05/27/2022 03:50:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
05/27/2022 03:50:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
05/27/2022 03:50:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=80
05/27/2022 03:50:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/27/2022 03:50:18 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.350463149416029 on epoch=81
05/27/2022 03:50:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=81
05/27/2022 03:50:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
05/27/2022 03:50:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=83
05/27/2022 03:50:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=83
05/27/2022 03:50:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=84
05/27/2022 03:50:36 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.5145583557621727 on epoch=84
05/27/2022 03:50:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
05/27/2022 03:50:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=85
05/27/2022 03:50:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=86
05/27/2022 03:50:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=86
05/27/2022 03:50:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/27/2022 03:50:52 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.5282384823848238 on epoch=87
05/27/2022 03:50:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5240804004131246 -> 0.5282384823848238 on epoch=87, global_step=1400
05/27/2022 03:50:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
05/27/2022 03:50:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
05/27/2022 03:51:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=89
05/27/2022 03:51:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
05/27/2022 03:51:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=90
05/27/2022 03:51:10 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.4980135386294269 on epoch=90
05/27/2022 03:51:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
05/27/2022 03:51:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
05/27/2022 03:51:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
05/27/2022 03:51:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=93
05/27/2022 03:51:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
05/27/2022 03:51:28 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=93
05/27/2022 03:51:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=94
05/27/2022 03:51:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
05/27/2022 03:51:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=95
05/27/2022 03:51:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
05/27/2022 03:51:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
05/27/2022 03:51:49 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=96
05/27/2022 03:51:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=97
05/27/2022 03:51:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
05/27/2022 03:51:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=98
05/27/2022 03:52:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=99
05/27/2022 03:52:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=99
05/27/2022 03:52:08 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=99
05/27/2022 03:52:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
05/27/2022 03:52:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=101
05/27/2022 03:52:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=101
05/27/2022 03:52:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
05/27/2022 03:52:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=103
05/27/2022 03:52:25 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=103
05/27/2022 03:52:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=103
05/27/2022 03:52:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=104
05/27/2022 03:52:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=104
05/27/2022 03:52:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=105
05/27/2022 03:52:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=106
05/27/2022 03:52:42 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=106
05/27/2022 03:52:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
05/27/2022 03:52:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=107
05/27/2022 03:52:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=108
05/27/2022 03:52:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
05/27/2022 03:52:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=109
05/27/2022 03:52:59 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=109
05/27/2022 03:53:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=109
05/27/2022 03:53:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=110
05/27/2022 03:53:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=111
05/27/2022 03:53:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=111
05/27/2022 03:53:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=112
05/27/2022 03:53:16 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.3771289537712896 on epoch=112
05/27/2022 03:53:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=113
05/27/2022 03:53:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=113
05/27/2022 03:53:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=114
05/27/2022 03:53:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=114
05/27/2022 03:53:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=115
05/27/2022 03:53:33 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.3813144709696433 on epoch=115
05/27/2022 03:53:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=116
05/27/2022 03:53:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=116
05/27/2022 03:53:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=117
05/27/2022 03:53:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=118
05/27/2022 03:53:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=118
05/27/2022 03:53:51 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=118
05/27/2022 03:53:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=119
05/27/2022 03:53:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=119
05/27/2022 03:53:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=120
05/27/2022 03:54:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
05/27/2022 03:54:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=121
05/27/2022 03:54:07 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.5457036114570362 on epoch=121
05/27/2022 03:54:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5282384823848238 -> 0.5457036114570362 on epoch=121, global_step=1950
05/27/2022 03:54:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=122
05/27/2022 03:54:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=123
05/27/2022 03:54:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=123
05/27/2022 03:54:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=124
05/27/2022 03:54:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=124
05/27/2022 03:54:26 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
05/27/2022 03:54:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=125
05/27/2022 03:54:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=126
05/27/2022 03:54:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=126
05/27/2022 03:54:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
05/27/2022 03:54:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=128
05/27/2022 03:54:43 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.36318407960199 on epoch=128
05/27/2022 03:54:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=128
05/27/2022 03:54:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=129
05/27/2022 03:54:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.35 on epoch=129
05/27/2022 03:54:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=130
05/27/2022 03:54:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=131
05/27/2022 03:54:59 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.5364582513265419 on epoch=131
05/27/2022 03:55:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=131
05/27/2022 03:55:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=132
05/27/2022 03:55:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=133
05/27/2022 03:55:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=133
05/27/2022 03:55:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=134
05/27/2022 03:55:17 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.5810930852307261 on epoch=134
05/27/2022 03:55:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5457036114570362 -> 0.5810930852307261 on epoch=134, global_step=2150
05/27/2022 03:55:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
05/27/2022 03:55:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=135
05/27/2022 03:55:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.37 on epoch=136
05/27/2022 03:55:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=136
05/27/2022 03:55:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=137
05/27/2022 03:55:35 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.557073081104255 on epoch=137
05/27/2022 03:55:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.37 on epoch=138
05/27/2022 03:55:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=138
05/27/2022 03:55:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=139
05/27/2022 03:55:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.37 on epoch=139
05/27/2022 03:55:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
05/27/2022 03:55:52 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.503499451893077 on epoch=140
05/27/2022 03:55:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=141
05/27/2022 03:55:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=141
05/27/2022 03:55:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
05/27/2022 03:56:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=143
05/27/2022 03:56:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.35 on epoch=143
05/27/2022 03:56:09 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=143
05/27/2022 03:56:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=144
05/27/2022 03:56:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=144
05/27/2022 03:56:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=145
05/27/2022 03:56:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=146
05/27/2022 03:56:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.37 on epoch=146
05/27/2022 03:56:30 - INFO - __main__ - Global step 2350 Train loss 0.36 Classification-F1 0.5834046336839633 on epoch=146
05/27/2022 03:56:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5810930852307261 -> 0.5834046336839633 on epoch=146, global_step=2350
05/27/2022 03:56:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.36 on epoch=147
05/27/2022 03:56:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=148
05/27/2022 03:56:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=148
05/27/2022 03:56:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=149
05/27/2022 03:56:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=149
05/27/2022 03:56:48 - INFO - __main__ - Global step 2400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=149
05/27/2022 03:56:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.32 on epoch=150
05/27/2022 03:56:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=151
05/27/2022 03:56:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=151
05/27/2022 03:56:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=152
05/27/2022 03:57:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=153
05/27/2022 03:57:06 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.42752983181433807 on epoch=153
05/27/2022 03:57:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=153
05/27/2022 03:57:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=154
05/27/2022 03:57:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=154
05/27/2022 03:57:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=155
05/27/2022 03:57:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=156
05/27/2022 03:57:23 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.6425623321825853 on epoch=156
05/27/2022 03:57:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5834046336839633 -> 0.6425623321825853 on epoch=156, global_step=2500
05/27/2022 03:57:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
05/27/2022 03:57:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=157
05/27/2022 03:57:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=158
05/27/2022 03:57:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=158
05/27/2022 03:57:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=159
05/27/2022 03:57:40 - INFO - __main__ - Global step 2550 Train loss 0.34 Classification-F1 0.4420755396365153 on epoch=159
05/27/2022 03:57:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=159
05/27/2022 03:57:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=160
05/27/2022 03:57:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.33 on epoch=161
05/27/2022 03:57:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=161
05/27/2022 03:57:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=162
05/27/2022 03:57:58 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.6226971260132645 on epoch=162
05/27/2022 03:58:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=163
05/27/2022 03:58:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.38 on epoch=163
05/27/2022 03:58:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.35 on epoch=164
05/27/2022 03:58:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=164
05/27/2022 03:58:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.33 on epoch=165
05/27/2022 03:58:15 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.6182665424044735 on epoch=165
05/27/2022 03:58:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.35 on epoch=166
05/27/2022 03:58:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.42 on epoch=166
05/27/2022 03:58:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=167
05/27/2022 03:58:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=168
05/27/2022 03:58:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.36 on epoch=168
05/27/2022 03:58:32 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=168
05/27/2022 03:58:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.36 on epoch=169
05/27/2022 03:58:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=169
05/27/2022 03:58:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.36 on epoch=170
05/27/2022 03:58:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=171
05/27/2022 03:58:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=171
05/27/2022 03:58:50 - INFO - __main__ - Global step 2750 Train loss 0.36 Classification-F1 0.643306640535285 on epoch=171
05/27/2022 03:58:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6425623321825853 -> 0.643306640535285 on epoch=171, global_step=2750
05/27/2022 03:58:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=172
05/27/2022 03:58:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=173
05/27/2022 03:58:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.33 on epoch=173
05/27/2022 03:59:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.36 on epoch=174
05/27/2022 03:59:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.33 on epoch=174
05/27/2022 03:59:09 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.4070267639902676 on epoch=174
05/27/2022 03:59:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.35 on epoch=175
05/27/2022 03:59:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.32 on epoch=176
05/27/2022 03:59:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=176
05/27/2022 03:59:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=177
05/27/2022 03:59:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.35 on epoch=178
05/27/2022 03:59:26 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.4025498100179374 on epoch=178
05/27/2022 03:59:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=178
05/27/2022 03:59:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=179
05/27/2022 03:59:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.37 on epoch=179
05/27/2022 03:59:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.31 on epoch=180
05/27/2022 03:59:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.33 on epoch=181
05/27/2022 03:59:44 - INFO - __main__ - Global step 2900 Train loss 0.35 Classification-F1 0.4863404846426918 on epoch=181
05/27/2022 03:59:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=181
05/27/2022 03:59:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.34 on epoch=182
05/27/2022 03:59:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=183
05/27/2022 03:59:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=183
05/27/2022 03:59:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=184
05/27/2022 04:00:02 - INFO - __main__ - Global step 2950 Train loss 0.33 Classification-F1 0.5484865337252721 on epoch=184
05/27/2022 04:00:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.33 on epoch=184
05/27/2022 04:00:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=185
05/27/2022 04:00:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.33 on epoch=186
05/27/2022 04:00:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.34 on epoch=186
05/27/2022 04:00:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.35 on epoch=187
05/27/2022 04:00:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:00:16 - INFO - __main__ - Printing 3 examples
05/27/2022 04:00:16 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:00:16 - INFO - __main__ - ['false']
05/27/2022 04:00:16 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:00:16 - INFO - __main__ - ['false']
05/27/2022 04:00:16 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:00:16 - INFO - __main__ - ['false']
05/27/2022 04:00:16 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:00:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:00:17 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:00:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:00:17 - INFO - __main__ - Printing 3 examples
05/27/2022 04:00:17 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:00:17 - INFO - __main__ - ['false']
05/27/2022 04:00:17 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:00:17 - INFO - __main__ - ['false']
05/27/2022 04:00:17 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:00:17 - INFO - __main__ - ['false']
05/27/2022 04:00:17 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:00:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:00:17 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:00:20 - INFO - __main__ - Global step 3000 Train loss 0.34 Classification-F1 0.531350114416476 on epoch=187
05/27/2022 04:00:20 - INFO - __main__ - save last model!
05/27/2022 04:00:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 04:00:20 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 04:00:20 - INFO - __main__ - Printing 3 examples
05/27/2022 04:00:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 04:00:20 - INFO - __main__ - ['false']
05/27/2022 04:00:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 04:00:20 - INFO - __main__ - ['false']
05/27/2022 04:00:20 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 04:00:20 - INFO - __main__ - ['false']
05/27/2022 04:00:20 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:00:21 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:00:24 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 04:00:32 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:00:32 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:00:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:00:32 - INFO - __main__ - Starting training!
05/27/2022 04:01:15 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.5_8_predictions.txt
05/27/2022 04:01:15 - INFO - __main__ - Classification-F1 on test data: 0.3061
05/27/2022 04:01:15 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.5, bsz=8, dev_performance=0.643306640535285, test_performance=0.3060581070510349
05/27/2022 04:01:15 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.4, bsz=8 ...
05/27/2022 04:01:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:01:16 - INFO - __main__ - Printing 3 examples
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:01:16 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:01:16 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:01:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:01:16 - INFO - __main__ - Printing 3 examples
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:01:16 - INFO - __main__ - ['false']
05/27/2022 04:01:16 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:01:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:01:17 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:01:35 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:01:35 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:01:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:01:36 - INFO - __main__ - Starting training!
05/27/2022 04:01:39 - INFO - __main__ - Step 10 Global step 10 Train loss 5.25 on epoch=0
05/27/2022 04:01:42 - INFO - __main__ - Step 20 Global step 20 Train loss 1.45 on epoch=1
05/27/2022 04:01:44 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=1
05/27/2022 04:01:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=2
05/27/2022 04:01:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=3
05/27/2022 04:01:55 - INFO - __main__ - Global step 50 Train loss 1.67 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 04:01:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 04:01:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/27/2022 04:02:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=4
05/27/2022 04:02:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=4
05/27/2022 04:02:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/27/2022 04:02:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/27/2022 04:02:13 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 04:02:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=6
05/27/2022 04:02:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=7
05/27/2022 04:02:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=8
05/27/2022 04:02:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/27/2022 04:02:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/27/2022 04:02:31 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.453125 on epoch=9
05/27/2022 04:02:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.453125 on epoch=9, global_step=150
05/27/2022 04:02:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
05/27/2022 04:02:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
05/27/2022 04:02:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/27/2022 04:02:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
05/27/2022 04:02:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
05/27/2022 04:02:47 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.45899737824739817 on epoch=12
05/27/2022 04:02:47 - INFO - __main__ - Saving model with best Classification-F1: 0.453125 -> 0.45899737824739817 on epoch=12, global_step=200
05/27/2022 04:02:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/27/2022 04:02:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/27/2022 04:02:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=14
05/27/2022 04:02:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
05/27/2022 04:03:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
05/27/2022 04:03:05 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.5048847012149764 on epoch=15
05/27/2022 04:03:05 - INFO - __main__ - Saving model with best Classification-F1: 0.45899737824739817 -> 0.5048847012149764 on epoch=15, global_step=250
05/27/2022 04:03:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
05/27/2022 04:03:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
05/27/2022 04:03:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
05/27/2022 04:03:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/27/2022 04:03:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
05/27/2022 04:03:22 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 04:03:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=19
05/27/2022 04:03:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
05/27/2022 04:03:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=20
05/27/2022 04:03:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/27/2022 04:03:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
05/27/2022 04:03:38 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=21
05/27/2022 04:03:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=22
05/27/2022 04:03:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/27/2022 04:03:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/27/2022 04:03:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/27/2022 04:03:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=24
05/27/2022 04:03:56 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 04:03:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
05/27/2022 04:04:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
05/27/2022 04:04:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
05/27/2022 04:04:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/27/2022 04:04:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/27/2022 04:04:12 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 04:04:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/27/2022 04:04:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
05/27/2022 04:04:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/27/2022 04:04:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/27/2022 04:04:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/27/2022 04:04:29 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.3712545436683367 on epoch=31
05/27/2022 04:04:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=31
05/27/2022 04:04:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/27/2022 04:04:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/27/2022 04:04:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
05/27/2022 04:04:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
05/27/2022 04:04:45 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.35885876860812244 on epoch=34
05/27/2022 04:04:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/27/2022 04:04:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
05/27/2022 04:04:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/27/2022 04:04:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=36
05/27/2022 04:04:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
05/27/2022 04:05:03 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.4527614241659658 on epoch=37
05/27/2022 04:05:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
05/27/2022 04:05:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=38
05/27/2022 04:05:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/27/2022 04:05:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=39
05/27/2022 04:05:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/27/2022 04:05:19 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.4292397660818714 on epoch=40
05/27/2022 04:05:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
05/27/2022 04:05:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/27/2022 04:05:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/27/2022 04:05:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
05/27/2022 04:05:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
05/27/2022 04:05:36 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 04:05:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/27/2022 04:05:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/27/2022 04:05:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/27/2022 04:05:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/27/2022 04:05:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
05/27/2022 04:05:53 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.45207864760937383 on epoch=46
05/27/2022 04:05:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/27/2022 04:05:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/27/2022 04:06:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/27/2022 04:06:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/27/2022 04:06:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/27/2022 04:06:10 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 04:06:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/27/2022 04:06:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/27/2022 04:06:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
05/27/2022 04:06:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
05/27/2022 04:06:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=53
05/27/2022 04:06:27 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 04:06:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 04:06:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
05/27/2022 04:06:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
05/27/2022 04:06:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
05/27/2022 04:06:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
05/27/2022 04:06:44 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.5887833310385975 on epoch=56
05/27/2022 04:06:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5048847012149764 -> 0.5887833310385975 on epoch=56, global_step=900
05/27/2022 04:06:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/27/2022 04:06:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
05/27/2022 04:06:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=58
05/27/2022 04:06:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
05/27/2022 04:06:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/27/2022 04:07:01 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.5384540419380512 on epoch=59
05/27/2022 04:07:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
05/27/2022 04:07:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
05/27/2022 04:07:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
05/27/2022 04:07:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/27/2022 04:07:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/27/2022 04:07:18 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.5698965251935246 on epoch=62
05/27/2022 04:07:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
05/27/2022 04:07:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/27/2022 04:07:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
05/27/2022 04:07:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
05/27/2022 04:07:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/27/2022 04:07:35 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5916686024003097 on epoch=65
05/27/2022 04:07:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5887833310385975 -> 0.5916686024003097 on epoch=65, global_step=1050
05/27/2022 04:07:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
05/27/2022 04:07:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
05/27/2022 04:07:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/27/2022 04:07:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
05/27/2022 04:07:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
05/27/2022 04:07:51 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 04:07:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/27/2022 04:07:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=69
05/27/2022 04:07:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
05/27/2022 04:08:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
05/27/2022 04:08:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/27/2022 04:08:08 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.39047619047619053 on epoch=71
05/27/2022 04:08:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/27/2022 04:08:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=73
05/27/2022 04:08:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/27/2022 04:08:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=74
05/27/2022 04:08:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
05/27/2022 04:08:25 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.5333333333333333 on epoch=74
05/27/2022 04:08:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
05/27/2022 04:08:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=76
05/27/2022 04:08:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
05/27/2022 04:08:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=77
05/27/2022 04:08:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
05/27/2022 04:08:42 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=78
05/27/2022 04:08:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=78
05/27/2022 04:08:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
05/27/2022 04:08:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
05/27/2022 04:08:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
05/27/2022 04:08:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=81
05/27/2022 04:08:59 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.5793650793650793 on epoch=81
05/27/2022 04:09:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=81
05/27/2022 04:09:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
05/27/2022 04:09:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=83
05/27/2022 04:09:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=83
05/27/2022 04:09:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
05/27/2022 04:09:16 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.5421271159374003 on epoch=84
05/27/2022 04:09:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
05/27/2022 04:09:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
05/27/2022 04:09:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
05/27/2022 04:09:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
05/27/2022 04:09:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
05/27/2022 04:09:33 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.48928333930124307 on epoch=87
05/27/2022 04:09:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=88
05/27/2022 04:09:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
05/27/2022 04:09:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
05/27/2022 04:09:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=89
05/27/2022 04:09:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
05/27/2022 04:09:50 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.47765762089369523 on epoch=90
05/27/2022 04:09:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
05/27/2022 04:09:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
05/27/2022 04:09:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=92
05/27/2022 04:10:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=93
05/27/2022 04:10:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
05/27/2022 04:10:07 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.3401530406766009 on epoch=93
05/27/2022 04:10:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=94
05/27/2022 04:10:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=94
05/27/2022 04:10:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=95
05/27/2022 04:10:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
05/27/2022 04:10:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=96
05/27/2022 04:10:24 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.5429059776885863 on epoch=96
05/27/2022 04:10:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=97
05/27/2022 04:10:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=98
05/27/2022 04:10:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=98
05/27/2022 04:10:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=99
05/27/2022 04:10:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
05/27/2022 04:10:40 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.3671451355661882 on epoch=99
05/27/2022 04:10:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=100
05/27/2022 04:10:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
05/27/2022 04:10:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=101
05/27/2022 04:10:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
05/27/2022 04:10:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
05/27/2022 04:10:59 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.23725981620718462 on epoch=103
05/27/2022 04:11:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=103
05/27/2022 04:11:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=104
05/27/2022 04:11:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=104
05/27/2022 04:11:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=105
05/27/2022 04:11:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=106
05/27/2022 04:11:16 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.5398724940471618 on epoch=106
05/27/2022 04:11:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=106
05/27/2022 04:11:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=107
05/27/2022 04:11:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=108
05/27/2022 04:11:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=108
05/27/2022 04:11:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=109
05/27/2022 04:11:34 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.5662407839904748 on epoch=109
05/27/2022 04:11:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=109
05/27/2022 04:11:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=110
05/27/2022 04:11:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=111
05/27/2022 04:11:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=111
05/27/2022 04:11:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
05/27/2022 04:11:51 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.4410968474369051 on epoch=112
05/27/2022 04:11:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=113
05/27/2022 04:11:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=113
05/27/2022 04:11:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=114
05/27/2022 04:12:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=114
05/27/2022 04:12:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=115
05/27/2022 04:12:10 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.37859829501977654 on epoch=115
05/27/2022 04:12:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=116
05/27/2022 04:12:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=116
05/27/2022 04:12:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=117
05/27/2022 04:12:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=118
05/27/2022 04:12:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=118
05/27/2022 04:12:28 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.25291674653376783 on epoch=118
05/27/2022 04:12:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=119
05/27/2022 04:12:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=119
05/27/2022 04:12:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=120
05/27/2022 04:12:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=121
05/27/2022 04:12:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=121
05/27/2022 04:12:46 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.400984467746272 on epoch=121
05/27/2022 04:12:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=122
05/27/2022 04:12:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=123
05/27/2022 04:12:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
05/27/2022 04:12:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=124
05/27/2022 04:12:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=124
05/27/2022 04:13:04 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.3626160904106523 on epoch=124
05/27/2022 04:13:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=125
05/27/2022 04:13:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=126
05/27/2022 04:13:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=126
05/27/2022 04:13:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=127
05/27/2022 04:13:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=128
05/27/2022 04:13:22 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.5640046029919448 on epoch=128
05/27/2022 04:13:25 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.32 on epoch=128
05/27/2022 04:13:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=129
05/27/2022 04:13:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=129
05/27/2022 04:13:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=130
05/27/2022 04:13:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=131
05/27/2022 04:13:40 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.59859804464121 on epoch=131
05/27/2022 04:13:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5916686024003097 -> 0.59859804464121 on epoch=131, global_step=2100
05/27/2022 04:13:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=131
05/27/2022 04:13:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=132
05/27/2022 04:13:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=133
05/27/2022 04:13:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=133
05/27/2022 04:13:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=134
05/27/2022 04:13:59 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.36730867022629027 on epoch=134
05/27/2022 04:14:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
05/27/2022 04:14:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=135
05/27/2022 04:14:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=136
05/27/2022 04:14:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=136
05/27/2022 04:14:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=137
05/27/2022 04:14:18 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.35279232111692843 on epoch=137
05/27/2022 04:14:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=138
05/27/2022 04:14:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
05/27/2022 04:14:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.22 on epoch=139
05/27/2022 04:14:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=139
05/27/2022 04:14:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/27/2022 04:14:37 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.3865303011576649 on epoch=140
05/27/2022 04:14:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
05/27/2022 04:14:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/27/2022 04:14:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=142
05/27/2022 04:14:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=143
05/27/2022 04:14:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
05/27/2022 04:14:56 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.3381537957042138 on epoch=143
05/27/2022 04:14:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=144
05/27/2022 04:15:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
05/27/2022 04:15:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=145
05/27/2022 04:15:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
05/27/2022 04:15:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
05/27/2022 04:15:15 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.39000215470803706 on epoch=146
05/27/2022 04:15:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/27/2022 04:15:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
05/27/2022 04:15:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=148
05/27/2022 04:15:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/27/2022 04:15:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/27/2022 04:15:33 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.5748323219418715 on epoch=149
05/27/2022 04:15:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
05/27/2022 04:15:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=151
05/27/2022 04:15:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=151
05/27/2022 04:15:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=152
05/27/2022 04:15:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/27/2022 04:15:51 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.36313895808051777 on epoch=153
05/27/2022 04:15:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/27/2022 04:15:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=154
05/27/2022 04:15:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/27/2022 04:16:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/27/2022 04:16:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
05/27/2022 04:16:09 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.604109568066635 on epoch=156
05/27/2022 04:16:09 - INFO - __main__ - Saving model with best Classification-F1: 0.59859804464121 -> 0.604109568066635 on epoch=156, global_step=2500
05/27/2022 04:16:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/27/2022 04:16:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
05/27/2022 04:16:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
05/27/2022 04:16:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=158
05/27/2022 04:16:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=159
05/27/2022 04:16:27 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.6054627298390173 on epoch=159
05/27/2022 04:16:27 - INFO - __main__ - Saving model with best Classification-F1: 0.604109568066635 -> 0.6054627298390173 on epoch=159, global_step=2550
05/27/2022 04:16:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/27/2022 04:16:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=160
05/27/2022 04:16:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/27/2022 04:16:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=161
05/27/2022 04:16:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
05/27/2022 04:16:45 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.4105630293971101 on epoch=162
05/27/2022 04:16:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=163
05/27/2022 04:16:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/27/2022 04:16:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/27/2022 04:16:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/27/2022 04:16:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/27/2022 04:17:02 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.6000000000000001 on epoch=165
05/27/2022 04:17:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/27/2022 04:17:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/27/2022 04:17:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/27/2022 04:17:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=168
05/27/2022 04:17:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/27/2022 04:17:20 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.6357795217770434 on epoch=168
05/27/2022 04:17:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6054627298390173 -> 0.6357795217770434 on epoch=168, global_step=2700
05/27/2022 04:17:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/27/2022 04:17:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=169
05/27/2022 04:17:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=170
05/27/2022 04:17:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=171
05/27/2022 04:17:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
05/27/2022 04:17:38 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.6190476190476191 on epoch=171
05/27/2022 04:17:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
05/27/2022 04:17:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=173
05/27/2022 04:17:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/27/2022 04:17:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/27/2022 04:17:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/27/2022 04:17:57 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.6399103485586212 on epoch=174
05/27/2022 04:17:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6357795217770434 -> 0.6399103485586212 on epoch=174, global_step=2800
05/27/2022 04:17:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/27/2022 04:18:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
05/27/2022 04:18:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/27/2022 04:18:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=177
05/27/2022 04:18:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
05/27/2022 04:18:15 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.40820474914533866 on epoch=178
05/27/2022 04:18:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/27/2022 04:18:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=179
05/27/2022 04:18:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/27/2022 04:18:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/27/2022 04:18:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
05/27/2022 04:18:34 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.6233836624080527 on epoch=181
05/27/2022 04:18:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/27/2022 04:18:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/27/2022 04:18:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=183
05/27/2022 04:18:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=183
05/27/2022 04:18:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
05/27/2022 04:18:53 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.642857142857143 on epoch=184
05/27/2022 04:18:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6399103485586212 -> 0.642857142857143 on epoch=184, global_step=2950
05/27/2022 04:18:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=184
05/27/2022 04:18:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/27/2022 04:19:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/27/2022 04:19:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
05/27/2022 04:19:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/27/2022 04:19:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:19:07 - INFO - __main__ - Printing 3 examples
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:19:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:19:07 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:19:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:19:07 - INFO - __main__ - Printing 3 examples
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:19:07 - INFO - __main__ - ['false']
05/27/2022 04:19:07 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:19:07 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:19:08 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:19:12 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.5890722645596647 on epoch=187
05/27/2022 04:19:12 - INFO - __main__ - save last model!
05/27/2022 04:19:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 04:19:12 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 04:19:12 - INFO - __main__ - Printing 3 examples
05/27/2022 04:19:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 04:19:12 - INFO - __main__ - ['false']
05/27/2022 04:19:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 04:19:12 - INFO - __main__ - ['false']
05/27/2022 04:19:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 04:19:12 - INFO - __main__ - ['false']
05/27/2022 04:19:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:19:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:19:16 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 04:19:22 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:19:22 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:19:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:19:23 - INFO - __main__ - Starting training!
05/27/2022 04:20:18 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.4_8_predictions.txt
05/27/2022 04:20:18 - INFO - __main__ - Classification-F1 on test data: 0.2334
05/27/2022 04:20:19 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.4, bsz=8, dev_performance=0.642857142857143, test_performance=0.2334146488042456
05/27/2022 04:20:19 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.3, bsz=8 ...
05/27/2022 04:20:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:20:20 - INFO - __main__ - Printing 3 examples
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:20:20 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:20:20 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:20:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:20:20 - INFO - __main__ - Printing 3 examples
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:20:20 - INFO - __main__ - ['false']
05/27/2022 04:20:20 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:20:20 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:20:21 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:20:36 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:20:36 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:20:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:20:36 - INFO - __main__ - Starting training!
05/27/2022 04:20:39 - INFO - __main__ - Step 10 Global step 10 Train loss 6.60 on epoch=0
05/27/2022 04:20:42 - INFO - __main__ - Step 20 Global step 20 Train loss 2.21 on epoch=1
05/27/2022 04:20:45 - INFO - __main__ - Step 30 Global step 30 Train loss 0.90 on epoch=1
05/27/2022 04:20:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=2
05/27/2022 04:20:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=3
05/27/2022 04:20:55 - INFO - __main__ - Global step 50 Train loss 2.17 Classification-F1 0.2228024369016536 on epoch=3
05/27/2022 04:20:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2228024369016536 on epoch=3, global_step=50
05/27/2022 04:20:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/27/2022 04:21:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=4
05/27/2022 04:21:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
05/27/2022 04:21:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
05/27/2022 04:21:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
05/27/2022 04:21:15 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=6
05/27/2022 04:21:15 - INFO - __main__ - Saving model with best Classification-F1: 0.2228024369016536 -> 0.3333333333333333 on epoch=6, global_step=100
05/27/2022 04:21:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=6
05/27/2022 04:21:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=7
05/27/2022 04:21:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/27/2022 04:21:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=8
05/27/2022 04:21:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
05/27/2022 04:21:33 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 04:21:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/27/2022 04:21:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
05/27/2022 04:21:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
05/27/2022 04:21:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/27/2022 04:21:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
05/27/2022 04:21:51 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.4174085604529648 on epoch=12
05/27/2022 04:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4174085604529648 on epoch=12, global_step=200
05/27/2022 04:21:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/27/2022 04:21:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=13
05/27/2022 04:21:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
05/27/2022 04:22:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=14
05/27/2022 04:22:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
05/27/2022 04:22:08 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5355320050869012 on epoch=15
05/27/2022 04:22:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4174085604529648 -> 0.5355320050869012 on epoch=15, global_step=250
05/27/2022 04:22:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/27/2022 04:22:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/27/2022 04:22:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=17
05/27/2022 04:22:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
05/27/2022 04:22:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/27/2022 04:22:26 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 04:22:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
05/27/2022 04:22:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=19
05/27/2022 04:22:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
05/27/2022 04:22:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=21
05/27/2022 04:22:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/27/2022 04:22:43 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3383422492035824 on epoch=21
05/27/2022 04:22:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/27/2022 04:22:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/27/2022 04:22:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=23
05/27/2022 04:22:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/27/2022 04:22:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/27/2022 04:23:01 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 04:23:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/27/2022 04:23:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=26
05/27/2022 04:23:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
05/27/2022 04:23:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 04:23:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
05/27/2022 04:23:19 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 04:23:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
05/27/2022 04:23:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/27/2022 04:23:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/27/2022 04:23:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
05/27/2022 04:23:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
05/27/2022 04:23:37 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5283714075165806 on epoch=31
05/27/2022 04:23:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
05/27/2022 04:23:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/27/2022 04:23:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/27/2022 04:23:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
05/27/2022 04:23:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/27/2022 04:23:56 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=34
05/27/2022 04:23:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/27/2022 04:24:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/27/2022 04:24:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=36
05/27/2022 04:24:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
05/27/2022 04:24:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
05/27/2022 04:24:13 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.48032949150267584 on epoch=37
05/27/2022 04:24:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
05/27/2022 04:24:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=38
05/27/2022 04:24:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=39
05/27/2022 04:24:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
05/27/2022 04:24:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
05/27/2022 04:24:30 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.5533516988062442 on epoch=40
05/27/2022 04:24:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5355320050869012 -> 0.5533516988062442 on epoch=40, global_step=650
05/27/2022 04:24:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/27/2022 04:24:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/27/2022 04:24:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/27/2022 04:24:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
05/27/2022 04:24:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/27/2022 04:24:48 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 04:24:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/27/2022 04:24:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=44
05/27/2022 04:24:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
05/27/2022 04:24:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
05/27/2022 04:25:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/27/2022 04:25:05 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.5225022717490075 on epoch=46
05/27/2022 04:25:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
05/27/2022 04:25:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/27/2022 04:25:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=48
05/27/2022 04:25:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
05/27/2022 04:25:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=49
05/27/2022 04:25:22 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 04:25:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=50
05/27/2022 04:25:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
05/27/2022 04:25:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/27/2022 04:25:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
05/27/2022 04:25:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
05/27/2022 04:25:39 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 04:25:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
05/27/2022 04:25:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
05/27/2022 04:25:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
05/27/2022 04:25:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=55
05/27/2022 04:25:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=56
05/27/2022 04:25:56 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.5352395772547494 on epoch=56
05/27/2022 04:25:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
05/27/2022 04:26:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
05/27/2022 04:26:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
05/27/2022 04:26:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
05/27/2022 04:26:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
05/27/2022 04:26:14 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 04:26:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
05/27/2022 04:26:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=60
05/27/2022 04:26:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
05/27/2022 04:26:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/27/2022 04:26:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
05/27/2022 04:26:31 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.5561040381160666 on epoch=62
05/27/2022 04:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5533516988062442 -> 0.5561040381160666 on epoch=62, global_step=1000
05/27/2022 04:26:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/27/2022 04:26:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
05/27/2022 04:26:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/27/2022 04:26:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=64
05/27/2022 04:26:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=65
05/27/2022 04:26:48 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.5273492060943696 on epoch=65
05/27/2022 04:26:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
05/27/2022 04:26:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
05/27/2022 04:26:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
05/27/2022 04:26:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=68
05/27/2022 04:27:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/27/2022 04:27:05 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 04:27:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
05/27/2022 04:27:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
05/27/2022 04:27:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
05/27/2022 04:27:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/27/2022 04:27:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/27/2022 04:27:23 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.536231884057971 on epoch=71
05/27/2022 04:27:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=72
05/27/2022 04:27:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=73
05/27/2022 04:27:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
05/27/2022 04:27:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
05/27/2022 04:27:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
05/27/2022 04:27:41 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 04:27:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=75
05/27/2022 04:27:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=76
05/27/2022 04:27:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=76
05/27/2022 04:27:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=77
05/27/2022 04:27:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
05/27/2022 04:27:58 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=78
05/27/2022 04:28:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=78
05/27/2022 04:28:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=79
05/27/2022 04:28:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=79
05/27/2022 04:28:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=80
05/27/2022 04:28:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=81
05/27/2022 04:28:15 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.559813313682142 on epoch=81
05/27/2022 04:28:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5561040381160666 -> 0.559813313682142 on epoch=81, global_step=1300
05/27/2022 04:28:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
05/27/2022 04:28:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
05/27/2022 04:28:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
05/27/2022 04:28:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
05/27/2022 04:28:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/27/2022 04:28:33 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.5151515151515151 on epoch=84
05/27/2022 04:28:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
05/27/2022 04:28:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
05/27/2022 04:28:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=86
05/27/2022 04:28:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
05/27/2022 04:28:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
05/27/2022 04:28:52 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.5397188623733247 on epoch=87
05/27/2022 04:28:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=88
05/27/2022 04:28:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
05/27/2022 04:28:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=89
05/27/2022 04:29:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=89
05/27/2022 04:29:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
05/27/2022 04:29:10 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.5613934894346089 on epoch=90
05/27/2022 04:29:10 - INFO - __main__ - Saving model with best Classification-F1: 0.559813313682142 -> 0.5613934894346089 on epoch=90, global_step=1450
05/27/2022 04:29:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=91
05/27/2022 04:29:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
05/27/2022 04:29:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
05/27/2022 04:29:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
05/27/2022 04:29:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/27/2022 04:29:26 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=93
05/27/2022 04:29:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
05/27/2022 04:29:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
05/27/2022 04:29:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/27/2022 04:29:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
05/27/2022 04:29:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=96
05/27/2022 04:29:44 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.46895191457525676 on epoch=96
05/27/2022 04:29:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=97
05/27/2022 04:29:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=98
05/27/2022 04:29:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
05/27/2022 04:29:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=99
05/27/2022 04:29:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=99
05/27/2022 04:30:02 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=99
05/27/2022 04:30:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=100
05/27/2022 04:30:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=101
05/27/2022 04:30:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=101
05/27/2022 04:30:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
05/27/2022 04:30:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=103
05/27/2022 04:30:20 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3486005089058525 on epoch=103
05/27/2022 04:30:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=103
05/27/2022 04:30:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=104
05/27/2022 04:30:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=104
05/27/2022 04:30:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=105
05/27/2022 04:30:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=106
05/27/2022 04:30:38 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.5616438356164384 on epoch=106
05/27/2022 04:30:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5613934894346089 -> 0.5616438356164384 on epoch=106, global_step=1700
05/27/2022 04:30:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=106
05/27/2022 04:30:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=107
05/27/2022 04:30:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=108
05/27/2022 04:30:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
05/27/2022 04:30:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
05/27/2022 04:30:59 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.45469441258397514 on epoch=109
05/27/2022 04:31:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=109
05/27/2022 04:31:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=110
05/27/2022 04:31:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=111
05/27/2022 04:31:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
05/27/2022 04:31:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=112
05/27/2022 04:31:18 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.4468452895419188 on epoch=112
05/27/2022 04:31:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
05/27/2022 04:31:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=113
05/27/2022 04:31:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=114
05/27/2022 04:31:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=114
05/27/2022 04:31:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=115
05/27/2022 04:31:35 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.42025542025542023 on epoch=115
05/27/2022 04:31:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=116
05/27/2022 04:31:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=116
05/27/2022 04:31:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=117
05/27/2022 04:31:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=118
05/27/2022 04:31:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=118
05/27/2022 04:31:54 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=118
05/27/2022 04:31:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
05/27/2022 04:31:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=119
05/27/2022 04:32:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.34 on epoch=120
05/27/2022 04:32:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=121
05/27/2022 04:32:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=121
05/27/2022 04:32:12 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.5429095614854129 on epoch=121
05/27/2022 04:32:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=122
05/27/2022 04:32:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=123
05/27/2022 04:32:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=123
05/27/2022 04:32:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
05/27/2022 04:32:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=124
05/27/2022 04:32:30 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3671451355661882 on epoch=124
05/27/2022 04:32:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=125
05/27/2022 04:32:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=126
05/27/2022 04:32:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=126
05/27/2022 04:32:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=127
05/27/2022 04:32:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=128
05/27/2022 04:32:49 - INFO - __main__ - Global step 2050 Train loss 0.36 Classification-F1 0.40828713708540826 on epoch=128
05/27/2022 04:32:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.36 on epoch=128
05/27/2022 04:32:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=129
05/27/2022 04:32:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=129
05/27/2022 04:33:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=130
05/27/2022 04:33:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=131
05/27/2022 04:33:08 - INFO - __main__ - Global step 2100 Train loss 0.35 Classification-F1 0.5351114058774298 on epoch=131
05/27/2022 04:33:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=131
05/27/2022 04:33:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=132
05/27/2022 04:33:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.32 on epoch=133
05/27/2022 04:33:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=133
05/27/2022 04:33:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.33 on epoch=134
05/27/2022 04:33:26 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.5658696814605454 on epoch=134
05/27/2022 04:33:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5616438356164384 -> 0.5658696814605454 on epoch=134, global_step=2150
05/27/2022 04:33:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
05/27/2022 04:33:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=135
05/27/2022 04:33:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.37 on epoch=136
05/27/2022 04:33:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
05/27/2022 04:33:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=137
05/27/2022 04:33:45 - INFO - __main__ - Global step 2200 Train loss 0.34 Classification-F1 0.5361672473867596 on epoch=137
05/27/2022 04:33:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=138
05/27/2022 04:33:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=138
05/27/2022 04:33:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=139
05/27/2022 04:33:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.30 on epoch=139
05/27/2022 04:33:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.33 on epoch=140
05/27/2022 04:34:06 - INFO - __main__ - Global step 2250 Train loss 0.34 Classification-F1 0.4947725530492081 on epoch=140
05/27/2022 04:34:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=141
05/27/2022 04:34:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.32 on epoch=141
05/27/2022 04:34:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.33 on epoch=142
05/27/2022 04:34:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.30 on epoch=143
05/27/2022 04:34:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.34 on epoch=143
05/27/2022 04:34:24 - INFO - __main__ - Global step 2300 Train loss 0.32 Classification-F1 0.36119461636703015 on epoch=143
05/27/2022 04:34:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.33 on epoch=144
05/27/2022 04:34:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=144
05/27/2022 04:34:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=145
05/27/2022 04:34:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=146
05/27/2022 04:34:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=146
05/27/2022 04:34:42 - INFO - __main__ - Global step 2350 Train loss 0.33 Classification-F1 0.5640046029919448 on epoch=146
05/27/2022 04:34:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=147
05/27/2022 04:34:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=148
05/27/2022 04:34:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.34 on epoch=148
05/27/2022 04:34:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=149
05/27/2022 04:34:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=149
05/27/2022 04:34:59 - INFO - __main__ - Global step 2400 Train loss 0.33 Classification-F1 0.49874125874125874 on epoch=149
05/27/2022 04:35:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.34 on epoch=150
05/27/2022 04:35:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.32 on epoch=151
05/27/2022 04:35:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=151
05/27/2022 04:35:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
05/27/2022 04:35:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.29 on epoch=153
05/27/2022 04:35:16 - INFO - __main__ - Global step 2450 Train loss 0.32 Classification-F1 0.5146878722297717 on epoch=153
05/27/2022 04:35:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=153
05/27/2022 04:35:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.29 on epoch=154
05/27/2022 04:35:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=154
05/27/2022 04:35:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=155
05/27/2022 04:35:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.32 on epoch=156
05/27/2022 04:35:33 - INFO - __main__ - Global step 2500 Train loss 0.33 Classification-F1 0.5718603759110088 on epoch=156
05/27/2022 04:35:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5658696814605454 -> 0.5718603759110088 on epoch=156, global_step=2500
05/27/2022 04:35:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.26 on epoch=156
05/27/2022 04:35:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=157
05/27/2022 04:35:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=158
05/27/2022 04:35:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.32 on epoch=158
05/27/2022 04:35:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=159
05/27/2022 04:35:52 - INFO - __main__ - Global step 2550 Train loss 0.30 Classification-F1 0.5537068754587717 on epoch=159
05/27/2022 04:35:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=159
05/27/2022 04:35:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=160
05/27/2022 04:35:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.30 on epoch=161
05/27/2022 04:36:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=161
05/27/2022 04:36:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=162
05/27/2022 04:36:09 - INFO - __main__ - Global step 2600 Train loss 0.29 Classification-F1 0.5060728744939271 on epoch=162
05/27/2022 04:36:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=163
05/27/2022 04:36:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.25 on epoch=163
05/27/2022 04:36:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=164
05/27/2022 04:36:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.33 on epoch=164
05/27/2022 04:36:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=165
05/27/2022 04:36:27 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.5189413678209925 on epoch=165
05/27/2022 04:36:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.30 on epoch=166
05/27/2022 04:36:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=166
05/27/2022 04:36:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.25 on epoch=167
05/27/2022 04:36:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.26 on epoch=168
05/27/2022 04:36:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.31 on epoch=168
05/27/2022 04:36:45 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.44810875336290906 on epoch=168
05/27/2022 04:36:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.29 on epoch=169
05/27/2022 04:36:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=169
05/27/2022 04:36:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.32 on epoch=170
05/27/2022 04:36:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=171
05/27/2022 04:36:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=171
05/27/2022 04:37:03 - INFO - __main__ - Global step 2750 Train loss 0.27 Classification-F1 0.5457036114570362 on epoch=171
05/27/2022 04:37:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.31 on epoch=172
05/27/2022 04:37:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=173
05/27/2022 04:37:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=173
05/27/2022 04:37:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.30 on epoch=174
05/27/2022 04:37:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=174
05/27/2022 04:37:21 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.5492957746478874 on epoch=174
05/27/2022 04:37:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=175
05/27/2022 04:37:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=176
05/27/2022 04:37:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.28 on epoch=176
05/27/2022 04:37:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.25 on epoch=177
05/27/2022 04:37:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=178
05/27/2022 04:37:39 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.4527614241659658 on epoch=178
05/27/2022 04:37:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=178
05/27/2022 04:37:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=179
05/27/2022 04:37:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=179
05/27/2022 04:37:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.22 on epoch=180
05/27/2022 04:37:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=181
05/27/2022 04:37:57 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.5580264512552466 on epoch=181
05/27/2022 04:37:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=181
05/27/2022 04:38:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=182
05/27/2022 04:38:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=183
05/27/2022 04:38:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=183
05/27/2022 04:38:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=184
05/27/2022 04:38:15 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.553628466060765 on epoch=184
05/27/2022 04:38:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=184
05/27/2022 04:38:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=185
05/27/2022 04:38:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=186
05/27/2022 04:38:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
05/27/2022 04:38:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=187
05/27/2022 04:38:29 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:38:29 - INFO - __main__ - Printing 3 examples
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:38:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:38:29 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:38:29 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:38:29 - INFO - __main__ - Printing 3 examples
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:38:29 - INFO - __main__ - ['false']
05/27/2022 04:38:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:38:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:38:30 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:38:33 - INFO - __main__ - Global step 3000 Train loss 0.23 Classification-F1 0.5858945325137497 on epoch=187
05/27/2022 04:38:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5718603759110088 -> 0.5858945325137497 on epoch=187, global_step=3000
05/27/2022 04:38:33 - INFO - __main__ - save last model!
05/27/2022 04:38:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 04:38:33 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 04:38:33 - INFO - __main__ - Printing 3 examples
05/27/2022 04:38:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 04:38:33 - INFO - __main__ - ['false']
05/27/2022 04:38:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 04:38:33 - INFO - __main__ - ['false']
05/27/2022 04:38:33 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 04:38:33 - INFO - __main__ - ['false']
05/27/2022 04:38:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:38:34 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:38:37 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 04:38:44 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:38:44 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:38:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:38:45 - INFO - __main__ - Starting training!
05/27/2022 04:39:33 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.3_8_predictions.txt
05/27/2022 04:39:33 - INFO - __main__ - Classification-F1 on test data: 0.4780
05/27/2022 04:39:33 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.3, bsz=8, dev_performance=0.5858945325137497, test_performance=0.47799237319946586
05/27/2022 04:39:33 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.2, bsz=8 ...
05/27/2022 04:39:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:39:34 - INFO - __main__ - Printing 3 examples
05/27/2022 04:39:34 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/27/2022 04:39:34 - INFO - __main__ - ['false']
05/27/2022 04:39:34 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/27/2022 04:39:34 - INFO - __main__ - ['false']
05/27/2022 04:39:34 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/27/2022 04:39:34 - INFO - __main__ - ['false']
05/27/2022 04:39:34 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:39:35 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:39:35 - INFO - __main__ - Loaded 256 examples from train data
05/27/2022 04:39:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/27/2022 04:39:35 - INFO - __main__ - Printing 3 examples
05/27/2022 04:39:35 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/27/2022 04:39:35 - INFO - __main__ - ['false']
05/27/2022 04:39:35 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/27/2022 04:39:35 - INFO - __main__ - ['false']
05/27/2022 04:39:35 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/27/2022 04:39:35 - INFO - __main__ - ['false']
05/27/2022 04:39:35 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:39:35 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:39:35 - INFO - __main__ - Loaded 256 examples from dev data
05/27/2022 04:39:50 - INFO - __main__ - try to initialize prompt embeddings
05/27/2022 04:39:50 - INFO - __main__ - task name: wiki_qa
05/27/2022 04:39:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 04:39:51 - INFO - __main__ - Starting training!
05/27/2022 04:39:54 - INFO - __main__ - Step 10 Global step 10 Train loss 7.87 on epoch=0
05/27/2022 04:39:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=1
05/27/2022 04:39:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.67 on epoch=1
05/27/2022 04:40:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=2
05/27/2022 04:40:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.77 on epoch=3
05/27/2022 04:40:15 - INFO - __main__ - Global step 50 Train loss 3.03 Classification-F1 0.3333333333333333 on epoch=3
05/27/2022 04:40:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/27/2022 04:40:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=3
05/27/2022 04:40:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=4
05/27/2022 04:40:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=4
05/27/2022 04:40:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/27/2022 04:40:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=6
05/27/2022 04:40:44 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.34195559333697656 on epoch=6
05/27/2022 04:40:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=6, global_step=100
05/27/2022 04:40:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=6
05/27/2022 04:40:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=7
05/27/2022 04:40:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/27/2022 04:40:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
05/27/2022 04:40:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/27/2022 04:41:02 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=9
05/27/2022 04:41:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/27/2022 04:41:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
05/27/2022 04:41:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/27/2022 04:41:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=11
05/27/2022 04:41:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=12
05/27/2022 04:41:20 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.4083606042117329 on epoch=12
05/27/2022 04:41:20 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.4083606042117329 on epoch=12, global_step=200
05/27/2022 04:41:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=13
05/27/2022 04:41:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=13
05/27/2022 04:41:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/27/2022 04:41:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=14
05/27/2022 04:41:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=15
05/27/2022 04:41:37 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.35693779904306216 on epoch=15
05/27/2022 04:41:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=16
05/27/2022 04:41:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
05/27/2022 04:41:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/27/2022 04:41:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=18
05/27/2022 04:41:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
05/27/2022 04:41:56 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=18
05/27/2022 04:41:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=19
05/27/2022 04:42:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
05/27/2022 04:42:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
05/27/2022 04:42:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/27/2022 04:42:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
05/27/2022 04:42:13 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.42482504604051563 on epoch=21
05/27/2022 04:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4083606042117329 -> 0.42482504604051563 on epoch=21, global_step=350
05/27/2022 04:42:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
05/27/2022 04:42:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=23
05/27/2022 04:42:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/27/2022 04:42:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/27/2022 04:42:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=24
05/27/2022 04:42:31 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
05/27/2022 04:42:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=25
05/27/2022 04:42:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
05/27/2022 04:42:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/27/2022 04:42:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/27/2022 04:42:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=28
05/27/2022 04:42:48 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
05/27/2022 04:42:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/27/2022 04:42:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=29
05/27/2022 04:42:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/27/2022 04:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
05/27/2022 04:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
05/27/2022 04:43:05 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.38827397679264397 on epoch=31
05/27/2022 04:43:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/27/2022 04:43:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/27/2022 04:43:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/27/2022 04:43:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/27/2022 04:43:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
05/27/2022 04:43:22 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=34
05/27/2022 04:43:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/27/2022 04:43:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/27/2022 04:43:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/27/2022 04:43:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/27/2022 04:43:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/27/2022 04:43:38 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.48024103821852615 on epoch=37
05/27/2022 04:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.42482504604051563 -> 0.48024103821852615 on epoch=37, global_step=600
05/27/2022 04:43:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/27/2022 04:43:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/27/2022 04:43:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
05/27/2022 04:43:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=39
05/27/2022 04:43:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
05/27/2022 04:43:55 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.35307589038932324 on epoch=40
05/27/2022 04:43:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
05/27/2022 04:44:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/27/2022 04:44:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
05/27/2022 04:44:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=43
05/27/2022 04:44:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=43
05/27/2022 04:44:12 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
05/27/2022 04:44:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=44
05/27/2022 04:44:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/27/2022 04:44:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=45
05/27/2022 04:44:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
05/27/2022 04:44:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
05/27/2022 04:44:29 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.5707539188087436 on epoch=46
05/27/2022 04:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.48024103821852615 -> 0.5707539188087436 on epoch=46, global_step=750
05/27/2022 04:44:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/27/2022 04:44:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
05/27/2022 04:44:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
05/27/2022 04:44:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=49
05/27/2022 04:44:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
05/27/2022 04:44:46 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
05/27/2022 04:44:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=50
05/27/2022 04:44:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/27/2022 04:44:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
05/27/2022 04:44:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/27/2022 04:44:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=53
05/27/2022 04:45:03 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
05/27/2022 04:45:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=53
05/27/2022 04:45:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
05/27/2022 04:45:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
05/27/2022 04:45:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=55
05/27/2022 04:45:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
05/27/2022 04:45:20 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3692115143929912 on epoch=56
05/27/2022 04:45:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/27/2022 04:45:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
05/27/2022 04:45:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=58
05/27/2022 04:45:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
05/27/2022 04:45:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
05/27/2022 04:45:36 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=59
05/27/2022 04:45:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=59
05/27/2022 04:45:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=60
05/27/2022 04:45:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
05/27/2022 04:45:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=61
05/27/2022 04:45:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
05/27/2022 04:45:53 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.5135056117725405 on epoch=62
05/27/2022 04:45:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/27/2022 04:45:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/27/2022 04:46:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
05/27/2022 04:46:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
05/27/2022 04:46:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
05/27/2022 04:46:10 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.4632034632034632 on epoch=65
05/27/2022 04:46:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
05/27/2022 04:46:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=66
05/27/2022 04:46:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
05/27/2022 04:46:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
05/27/2022 04:46:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
05/27/2022 04:46:27 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=68
05/27/2022 04:46:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=69
05/27/2022 04:46:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
05/27/2022 04:46:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/27/2022 04:46:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=71
05/27/2022 04:46:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/27/2022 04:46:44 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.5491823899371069 on epoch=71
05/27/2022 04:46:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=72
05/27/2022 04:46:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
05/27/2022 04:46:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/27/2022 04:46:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
05/27/2022 04:46:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
05/27/2022 04:47:00 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
05/27/2022 04:47:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=75
05/27/2022 04:47:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
05/27/2022 04:47:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
05/27/2022 04:47:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/27/2022 04:47:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=78
05/27/2022 04:47:17 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.350463149416029 on epoch=78
05/27/2022 04:47:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
05/27/2022 04:47:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
05/27/2022 04:47:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
05/27/2022 04:47:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=80
05/27/2022 04:47:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=81
05/27/2022 04:47:34 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.541394252116795 on epoch=81
05/27/2022 04:47:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
05/27/2022 04:47:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=82
05/27/2022 04:47:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
05/27/2022 04:47:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
05/27/2022 04:47:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
05/27/2022 04:47:51 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.5348084352619604 on epoch=84
05/27/2022 04:47:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=84
05/27/2022 04:47:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
05/27/2022 04:47:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=86
05/27/2022 04:48:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
05/27/2022 04:48:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=87
05/27/2022 04:48:08 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.49573753552053734 on epoch=87
05/27/2022 04:48:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
05/27/2022 04:48:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=88
05/27/2022 04:48:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=89
05/27/2022 04:48:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=89
05/27/2022 04:48:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=90
05/27/2022 04:48:24 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.5774802860810564 on epoch=90
05/27/2022 04:48:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5707539188087436 -> 0.5774802860810564 on epoch=90, global_step=1450
05/27/2022 04:48:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=91
05/27/2022 04:48:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
05/27/2022 04:48:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=92
05/27/2022 04:48:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=93
05/27/2022 04:48:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
05/27/2022 04:48:41 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=93
05/27/2022 04:48:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
05/27/2022 04:48:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
05/27/2022 04:48:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=95
05/27/2022 04:48:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
05/27/2022 04:48:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=96
05/27/2022 04:48:58 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.4453729495512225 on epoch=96
05/27/2022 04:49:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=97
05/27/2022 04:49:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=98
05/27/2022 04:49:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
05/27/2022 04:49:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
05/27/2022 04:49:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=99
05/27/2022 04:49:16 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.39137439827095 on epoch=99
05/27/2022 04:49:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
05/27/2022 04:49:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=101
05/27/2022 04:49:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/27/2022 04:49:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
05/27/2022 04:49:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
05/27/2022 04:49:33 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.35693779904306216 on epoch=103
05/27/2022 04:49:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=103
05/27/2022 04:49:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
05/27/2022 04:49:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=104
05/27/2022 04:49:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=105
05/27/2022 04:49:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=106
05/27/2022 04:49:50 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.5901477832512315 on epoch=106
05/27/2022 04:49:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5774802860810564 -> 0.5901477832512315 on epoch=106, global_step=1700
05/27/2022 04:49:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
05/27/2022 04:49:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=107
05/27/2022 04:49:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=108
05/27/2022 04:50:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
05/27/2022 04:50:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=109
05/27/2022 04:50:08 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.6014652014652014 on epoch=109
05/27/2022 04:50:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.6014652014652014 on epoch=109, global_step=1750
05/27/2022 04:50:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=109
05/27/2022 04:50:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=110
05/27/2022 04:50:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=111
05/27/2022 04:50:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
05/27/2022 04:50:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
05/27/2022 04:50:25 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.5370705244122965 on epoch=112
05/27/2022 04:50:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=113
05/27/2022 04:50:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=113
05/27/2022 04:50:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=114
05/27/2022 04:50:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=114
05/27/2022 04:50:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
05/27/2022 04:50:42 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.5846954392408936 on epoch=115
05/27/2022 04:50:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=116
05/27/2022 04:50:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=116
05/27/2022 04:50:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=117
05/27/2022 04:50:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=118
05/27/2022 04:50:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=118
05/27/2022 04:50:59 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.350463149416029 on epoch=118
05/27/2022 04:51:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
05/27/2022 04:51:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=119
05/27/2022 04:51:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=120
05/27/2022 04:51:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=121
05/27/2022 04:51:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=121
05/27/2022 04:51:16 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.5240804004131246 on epoch=121
05/27/2022 04:51:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=122
05/27/2022 04:51:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=123
05/27/2022 04:51:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=123
05/27/2022 04:51:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=124
05/27/2022 04:51:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=124
05/27/2022 04:51:32 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.41075141075141075 on epoch=124
05/27/2022 04:51:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=125
05/27/2022 04:51:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=126
05/27/2022 04:51:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=126
05/27/2022 04:51:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=127
05/27/2022 04:51:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=128
05/27/2022 04:51:49 - INFO - __main__ - Global step 2050 Train loss 0.36 Classification-F1 0.3913043478260869 on epoch=128
05/27/2022 04:51:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=128
05/27/2022 04:51:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=129
05/27/2022 04:51:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=129
05/27/2022 04:52:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=130
05/27/2022 04:52:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=131
05/27/2022 04:52:06 - INFO - __main__ - Global step 2100 Train loss 0.35 Classification-F1 0.44014109347442687 on epoch=131
05/27/2022 04:52:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=131
05/27/2022 04:52:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=132
05/27/2022 04:52:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=133
05/27/2022 04:52:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=133
05/27/2022 04:52:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
05/27/2022 04:52:24 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.45705196182396607 on epoch=134
05/27/2022 04:52:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=134
05/27/2022 04:52:29 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=135
05/27/2022 04:52:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.37 on epoch=136
05/27/2022 04:52:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.35 on epoch=136
05/27/2022 04:52:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=137
05/27/2022 04:52:41 - INFO - __main__ - Global step 2200 Train loss 0.36 Classification-F1 0.4572309227288115 on epoch=137
05/27/2022 04:52:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=138
05/27/2022 04:52:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=138
05/27/2022 04:52:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.34 on epoch=139
05/27/2022 04:52:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.37 on epoch=139
05/27/2022 04:52:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
05/27/2022 04:52:58 - INFO - __main__ - Global step 2250 Train loss 0.36 Classification-F1 0.45436254198850756 on epoch=140
05/27/2022 04:53:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=141
05/27/2022 04:53:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=141
05/27/2022 04:53:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
05/27/2022 04:53:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=143
05/27/2022 04:53:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=143
05/27/2022 04:53:15 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.3486005089058525 on epoch=143
05/27/2022 04:53:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.34 on epoch=144
05/27/2022 04:53:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=144
05/27/2022 04:53:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=145
05/27/2022 04:53:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.35 on epoch=146
05/27/2022 04:53:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=146
05/27/2022 04:53:32 - INFO - __main__ - Global step 2350 Train loss 0.35 Classification-F1 0.46281389748882007 on epoch=146
05/27/2022 04:53:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=147
05/27/2022 04:53:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=148
05/27/2022 04:53:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
05/27/2022 04:53:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=149
05/27/2022 04:53:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=149
05/27/2022 04:53:49 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.632453567937439 on epoch=149
05/27/2022 04:53:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6014652014652014 -> 0.632453567937439 on epoch=149, global_step=2400
05/27/2022 04:53:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.36 on epoch=150
05/27/2022 04:53:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.34 on epoch=151
05/27/2022 04:53:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=151
05/27/2022 04:54:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=152
05/27/2022 04:54:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=153
05/27/2022 04:54:07 - INFO - __main__ - Global step 2450 Train loss 0.35 Classification-F1 0.4226819494921256 on epoch=153
05/27/2022 04:54:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=153
05/27/2022 04:54:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.33 on epoch=154
05/27/2022 04:54:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.35 on epoch=154
05/27/2022 04:54:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.33 on epoch=155
05/27/2022 04:54:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.32 on epoch=156
05/27/2022 04:54:25 - INFO - __main__ - Global step 2500 Train loss 0.34 Classification-F1 0.5552982049797336 on epoch=156
05/27/2022 04:54:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=156
05/27/2022 04:54:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=157
05/27/2022 04:54:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=158
05/27/2022 04:54:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=158
05/27/2022 04:54:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=159
05/27/2022 04:54:43 - INFO - __main__ - Global step 2550 Train loss 0.34 Classification-F1 0.5338328901780679 on epoch=159
05/27/2022 04:54:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.35 on epoch=159
05/27/2022 04:54:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=160
05/27/2022 04:54:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.33 on epoch=161
05/27/2022 04:54:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.36 on epoch=161
05/27/2022 04:54:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=162
05/27/2022 04:55:01 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.48550832959622636 on epoch=162
05/27/2022 04:55:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=163
05/27/2022 04:55:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=163
05/27/2022 04:55:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.34 on epoch=164
05/27/2022 04:55:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.34 on epoch=164
05/27/2022 04:55:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=165
05/27/2022 04:55:19 - INFO - __main__ - Global step 2650 Train loss 0.34 Classification-F1 0.4512184664986023 on epoch=165
05/27/2022 04:55:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=166
05/27/2022 04:55:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=166
05/27/2022 04:55:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=167
05/27/2022 04:55:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=168
05/27/2022 04:55:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=168
05/27/2022 04:55:36 - INFO - __main__ - Global step 2700 Train loss 0.34 Classification-F1 0.4719705340699816 on epoch=168
05/27/2022 04:55:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=169
05/27/2022 04:55:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.32 on epoch=169
05/27/2022 04:55:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.31 on epoch=170
05/27/2022 04:55:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=171
05/27/2022 04:55:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=171
05/27/2022 04:55:53 - INFO - __main__ - Global step 2750 Train loss 0.32 Classification-F1 0.5967383246849518 on epoch=171
05/27/2022 04:55:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=172
05/27/2022 04:55:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=173
05/27/2022 04:56:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=173
05/27/2022 04:56:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.32 on epoch=174
05/27/2022 04:56:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=174
05/27/2022 04:56:11 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.5796387520525451 on epoch=174
05/27/2022 04:56:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.33 on epoch=175
05/27/2022 04:56:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.32 on epoch=176
05/27/2022 04:56:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.30 on epoch=176
05/27/2022 04:56:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.33 on epoch=177
05/27/2022 04:56:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.31 on epoch=178
05/27/2022 04:56:29 - INFO - __main__ - Global step 2850 Train loss 0.32 Classification-F1 0.6027579146070292 on epoch=178
05/27/2022 04:56:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=178
05/27/2022 04:56:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.32 on epoch=179
05/27/2022 04:56:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.31 on epoch=179
05/27/2022 04:56:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=180
05/27/2022 04:56:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=181
05/27/2022 04:56:48 - INFO - __main__ - Global step 2900 Train loss 0.32 Classification-F1 0.5899923807746731 on epoch=181
05/27/2022 04:56:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=181
05/27/2022 04:56:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.29 on epoch=182
05/27/2022 04:56:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.26 on epoch=183
05/27/2022 04:56:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.28 on epoch=183
05/27/2022 04:57:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=184
05/27/2022 04:57:06 - INFO - __main__ - Global step 2950 Train loss 0.29 Classification-F1 0.6226971260132645 on epoch=184
05/27/2022 04:57:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=184
05/27/2022 04:57:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.28 on epoch=185
05/27/2022 04:57:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.28 on epoch=186
05/27/2022 04:57:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.31 on epoch=186
05/27/2022 04:57:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=187
05/27/2022 04:57:25 - INFO - __main__ - Global step 3000 Train loss 0.30 Classification-F1 0.5967383246849518 on epoch=187
05/27/2022 04:57:25 - INFO - __main__ - save last model!
05/27/2022 04:57:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 04:57:25 - INFO - __main__ - Start tokenizing ... 2733 instances
05/27/2022 04:57:25 - INFO - __main__ - Printing 3 examples
05/27/2022 04:57:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/27/2022 04:57:25 - INFO - __main__ - ['false']
05/27/2022 04:57:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/27/2022 04:57:25 - INFO - __main__ - ['false']
05/27/2022 04:57:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/27/2022 04:57:25 - INFO - __main__ - ['false']
05/27/2022 04:57:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 04:57:26 - INFO - __main__ - Tokenizing Output ...
05/27/2022 04:57:29 - INFO - __main__ - Loaded 2733 examples from test data
05/27/2022 04:58:25 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.2_8_predictions.txt
05/27/2022 04:58:25 - INFO - __main__ - Classification-F1 on test data: 0.4222
05/27/2022 04:58:25 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.2, bsz=8, dev_performance=0.632453567937439, test_performance=0.42218093328554146
