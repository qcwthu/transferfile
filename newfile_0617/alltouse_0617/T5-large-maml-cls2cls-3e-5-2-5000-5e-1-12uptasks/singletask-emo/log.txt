05/28/2022 09:23:22 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/28/2022 09:23:22 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo
05/28/2022 09:23:22 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/28/2022 09:23:22 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo
05/28/2022 09:23:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/28/2022 09:23:23 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/28/2022 09:23:23 - INFO - __main__ - args.device: cuda:0
05/28/2022 09:23:23 - INFO - __main__ - args.device: cuda:1
05/28/2022 09:23:23 - INFO - __main__ - Using 2 gpus
05/28/2022 09:23:23 - INFO - __main__ - Using 2 gpus
05/28/2022 09:23:23 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/28/2022 09:23:23 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/28/2022 09:23:28 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/28/2022 09:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 09:23:29 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 09:23:29 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:23:29 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 09:23:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:23:29 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 09:23:29 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:23:29 - INFO - __main__ - ['others']
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:23:29 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:23:29 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:23:47 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:23:47 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:23:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:23:48 - INFO - __main__ - Starting training!
05/28/2022 09:23:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:23:52 - INFO - __main__ - Starting training!
05/28/2022 09:23:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.26 on epoch=2
05/28/2022 09:23:58 - INFO - __main__ - Step 20 Global step 20 Train loss 1.60 on epoch=4
05/28/2022 09:24:01 - INFO - __main__ - Step 30 Global step 30 Train loss 1.16 on epoch=7
05/28/2022 09:24:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=9
05/28/2022 09:24:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=12
05/28/2022 09:24:06 - INFO - __main__ - Global step 50 Train loss 1.81 Classification-F1 0.1 on epoch=12
05/28/2022 09:24:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 09:24:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=14
05/28/2022 09:24:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
05/28/2022 09:24:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=19
05/28/2022 09:24:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
05/28/2022 09:24:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/28/2022 09:24:20 - INFO - __main__ - Global step 100 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=24
05/28/2022 09:24:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10126582278481013 on epoch=24, global_step=100
05/28/2022 09:24:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=27
05/28/2022 09:24:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/28/2022 09:24:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
05/28/2022 09:24:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
05/28/2022 09:24:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
05/28/2022 09:24:33 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.1 on epoch=37
05/28/2022 09:24:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=39
05/28/2022 09:24:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=42
05/28/2022 09:24:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=44
05/28/2022 09:24:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=47
05/28/2022 09:24:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=49
05/28/2022 09:24:46 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.375 on epoch=49
05/28/2022 09:24:46 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.375 on epoch=49, global_step=200
05/28/2022 09:24:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
05/28/2022 09:24:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
05/28/2022 09:24:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=57
05/28/2022 09:24:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=59
05/28/2022 09:24:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=62
05/28/2022 09:24:59 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.495014245014245 on epoch=62
05/28/2022 09:24:59 - INFO - __main__ - Saving model with best Classification-F1: 0.375 -> 0.495014245014245 on epoch=62, global_step=250
05/28/2022 09:25:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=64
05/28/2022 09:25:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.72 on epoch=67
05/28/2022 09:25:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=69
05/28/2022 09:25:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.60 on epoch=72
05/28/2022 09:25:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=74
05/28/2022 09:25:12 - INFO - __main__ - Global step 300 Train loss 0.68 Classification-F1 0.25104166666666666 on epoch=74
05/28/2022 09:25:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=77
05/28/2022 09:25:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=79
05/28/2022 09:25:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=82
05/28/2022 09:25:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=84
05/28/2022 09:25:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=87
05/28/2022 09:25:25 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.3671182266009852 on epoch=87
05/28/2022 09:25:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=89
05/28/2022 09:25:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=92
05/28/2022 09:25:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
05/28/2022 09:25:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=97
05/28/2022 09:25:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=99
05/28/2022 09:25:38 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.4137074829931973 on epoch=99
05/28/2022 09:25:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=102
05/28/2022 09:25:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=104
05/28/2022 09:25:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
05/28/2022 09:25:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
05/28/2022 09:25:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=112
05/28/2022 09:25:51 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.44044820122406325 on epoch=112
05/28/2022 09:25:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=114
05/28/2022 09:25:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=117
05/28/2022 09:25:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=119
05/28/2022 09:26:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
05/28/2022 09:26:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=124
05/28/2022 09:26:05 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.46180585265679813 on epoch=124
05/28/2022 09:26:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
05/28/2022 09:26:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=129
05/28/2022 09:26:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=132
05/28/2022 09:26:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=134
05/28/2022 09:26:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
05/28/2022 09:26:18 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.47146612268095645 on epoch=137
05/28/2022 09:26:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
05/28/2022 09:26:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
05/28/2022 09:26:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/28/2022 09:26:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=147
05/28/2022 09:26:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
05/28/2022 09:26:31 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.49370941558441556 on epoch=149
05/28/2022 09:26:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.15 on epoch=152
05/28/2022 09:26:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=154
05/28/2022 09:26:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
05/28/2022 09:26:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
05/28/2022 09:26:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
05/28/2022 09:26:44 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.546050082340405 on epoch=162
05/28/2022 09:26:44 - INFO - __main__ - Saving model with best Classification-F1: 0.495014245014245 -> 0.546050082340405 on epoch=162, global_step=650
05/28/2022 09:26:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=164
05/28/2022 09:26:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/28/2022 09:26:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=169
05/28/2022 09:26:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/28/2022 09:26:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
05/28/2022 09:26:57 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.4377058004052685 on epoch=174
05/28/2022 09:26:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=177
05/28/2022 09:27:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
05/28/2022 09:27:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
05/28/2022 09:27:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=184
05/28/2022 09:27:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
05/28/2022 09:27:10 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.590018315018315 on epoch=187
05/28/2022 09:27:10 - INFO - __main__ - Saving model with best Classification-F1: 0.546050082340405 -> 0.590018315018315 on epoch=187, global_step=750
05/28/2022 09:27:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
05/28/2022 09:27:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
05/28/2022 09:27:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
05/28/2022 09:27:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
05/28/2022 09:27:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=199
05/28/2022 09:27:23 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.507812172571898 on epoch=199
05/28/2022 09:27:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
05/28/2022 09:27:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
05/28/2022 09:27:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=207
05/28/2022 09:27:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=209
05/28/2022 09:27:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
05/28/2022 09:27:36 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.5021432723045627 on epoch=212
05/28/2022 09:27:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
05/28/2022 09:27:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=217
05/28/2022 09:27:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=219
05/28/2022 09:27:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
05/28/2022 09:27:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
05/28/2022 09:27:49 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.5336134453781514 on epoch=224
05/28/2022 09:27:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/28/2022 09:27:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
05/28/2022 09:27:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/28/2022 09:27:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/28/2022 09:28:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
05/28/2022 09:28:02 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.5512477718360071 on epoch=237
05/28/2022 09:28:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/28/2022 09:28:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/28/2022 09:28:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/28/2022 09:28:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
05/28/2022 09:28:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/28/2022 09:28:16 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.47739495798319326 on epoch=249
05/28/2022 09:28:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/28/2022 09:28:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/28/2022 09:28:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
05/28/2022 09:28:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=259
05/28/2022 09:28:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/28/2022 09:28:29 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.5554009568018189 on epoch=262
05/28/2022 09:28:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
05/28/2022 09:28:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/28/2022 09:28:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/28/2022 09:28:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
05/28/2022 09:28:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/28/2022 09:28:42 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.5304691653375864 on epoch=274
05/28/2022 09:28:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/28/2022 09:28:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
05/28/2022 09:28:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/28/2022 09:28:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/28/2022 09:28:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/28/2022 09:28:56 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.47619047619047616 on epoch=287
05/28/2022 09:28:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/28/2022 09:29:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/28/2022 09:29:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
05/28/2022 09:29:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
05/28/2022 09:29:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/28/2022 09:29:09 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5714306035358667 on epoch=299
05/28/2022 09:29:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=302
05/28/2022 09:29:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/28/2022 09:29:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 09:29:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/28/2022 09:29:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/28/2022 09:29:22 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.581398507485464 on epoch=312
05/28/2022 09:29:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/28/2022 09:29:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/28/2022 09:29:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=319
05/28/2022 09:29:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/28/2022 09:29:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/28/2022 09:29:36 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6137022397891962 on epoch=324
05/28/2022 09:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.590018315018315 -> 0.6137022397891962 on epoch=324, global_step=1300
05/28/2022 09:29:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/28/2022 09:29:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/28/2022 09:29:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
05/28/2022 09:29:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/28/2022 09:29:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/28/2022 09:29:49 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5413731306389816 on epoch=337
05/28/2022 09:29:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/28/2022 09:29:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/28/2022 09:29:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/28/2022 09:29:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/28/2022 09:30:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/28/2022 09:30:02 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.585725677830941 on epoch=349
05/28/2022 09:30:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/28/2022 09:30:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/28/2022 09:30:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/28/2022 09:30:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/28/2022 09:30:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/28/2022 09:30:16 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5522938443670151 on epoch=362
05/28/2022 09:30:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/28/2022 09:30:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 09:30:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 09:30:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/28/2022 09:30:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/28/2022 09:30:29 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5037070399113082 on epoch=374
05/28/2022 09:30:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/28/2022 09:30:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=379
05/28/2022 09:30:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/28/2022 09:30:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/28/2022 09:30:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/28/2022 09:30:42 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.551890756302521 on epoch=387
05/28/2022 09:30:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/28/2022 09:30:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/28/2022 09:30:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 09:30:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/28/2022 09:30:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/28/2022 09:30:55 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5846689311538666 on epoch=399
05/28/2022 09:30:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/28/2022 09:31:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/28/2022 09:31:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/28/2022 09:31:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/28/2022 09:31:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
05/28/2022 09:31:09 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.5601146474259661 on epoch=412
05/28/2022 09:31:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
05/28/2022 09:31:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/28/2022 09:31:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/28/2022 09:31:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 09:31:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/28/2022 09:31:22 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6088046295629634 on epoch=424
05/28/2022 09:31:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=427
05/28/2022 09:31:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 09:31:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/28/2022 09:31:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/28/2022 09:31:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/28/2022 09:31:35 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.5854166666666667 on epoch=437
05/28/2022 09:31:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/28/2022 09:31:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 09:31:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/28/2022 09:31:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/28/2022 09:31:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/28/2022 09:31:48 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6163817663817663 on epoch=449
05/28/2022 09:31:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6137022397891962 -> 0.6163817663817663 on epoch=449, global_step=1800
05/28/2022 09:31:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/28/2022 09:31:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 09:31:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/28/2022 09:31:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 09:32:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/28/2022 09:32:02 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5612523828923103 on epoch=462
05/28/2022 09:32:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/28/2022 09:32:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 09:32:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/28/2022 09:32:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/28/2022 09:32:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/28/2022 09:32:15 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5998917748917748 on epoch=474
05/28/2022 09:32:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/28/2022 09:32:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/28/2022 09:32:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 09:32:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 09:32:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 09:32:29 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6087261834820682 on epoch=487
05/28/2022 09:32:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 09:32:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 09:32:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 09:32:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 09:32:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/28/2022 09:32:42 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5819470551378446 on epoch=499
05/28/2022 09:32:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/28/2022 09:32:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 09:32:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/28/2022 09:32:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/28/2022 09:32:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 09:32:55 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5554256738966616 on epoch=512
05/28/2022 09:32:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/28/2022 09:33:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 09:33:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 09:33:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 09:33:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 09:33:09 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.5687403250773994 on epoch=524
05/28/2022 09:33:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 09:33:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/28/2022 09:33:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/28/2022 09:33:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 09:33:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 09:33:22 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.600366568914956 on epoch=537
05/28/2022 09:33:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 09:33:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 09:33:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/28/2022 09:33:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 09:33:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/28/2022 09:33:36 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5535992796862361 on epoch=549
05/28/2022 09:33:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 09:33:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 09:33:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/28/2022 09:33:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=559
05/28/2022 09:33:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 09:33:49 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5478197877391425 on epoch=562
05/28/2022 09:33:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 09:33:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 09:33:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 09:33:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 09:34:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 09:34:02 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5712831075734301 on epoch=574
05/28/2022 09:34:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 09:34:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/28/2022 09:34:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/28/2022 09:34:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 09:34:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 09:34:16 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.5745694522868436 on epoch=587
05/28/2022 09:34:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 09:34:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 09:34:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 09:34:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/28/2022 09:34:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 09:34:29 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6065616637357596 on epoch=599
05/28/2022 09:34:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
05/28/2022 09:34:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/28/2022 09:34:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 09:34:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 09:34:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=612
05/28/2022 09:34:43 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6336999022482893 on epoch=612
05/28/2022 09:34:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6163817663817663 -> 0.6336999022482893 on epoch=612, global_step=2450
05/28/2022 09:34:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 09:34:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 09:34:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 09:34:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 09:34:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 09:34:56 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6468997035573123 on epoch=624
05/28/2022 09:34:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6336999022482893 -> 0.6468997035573123 on epoch=624, global_step=2500
05/28/2022 09:34:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 09:35:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/28/2022 09:35:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 09:35:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 09:35:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/28/2022 09:35:09 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5870648844333055 on epoch=637
05/28/2022 09:35:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/28/2022 09:35:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 09:35:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 09:35:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 09:35:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 09:35:23 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6184901479019125 on epoch=649
05/28/2022 09:35:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=652
05/28/2022 09:35:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 09:35:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 09:35:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 09:35:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 09:35:36 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.568997668997669 on epoch=662
05/28/2022 09:35:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 09:35:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 09:35:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 09:35:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 09:35:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 09:35:50 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6471288515406163 on epoch=674
05/28/2022 09:35:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6468997035573123 -> 0.6471288515406163 on epoch=674, global_step=2700
05/28/2022 09:35:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 09:35:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 09:35:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 09:35:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/28/2022 09:36:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/28/2022 09:36:03 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.5890905924861782 on epoch=687
05/28/2022 09:36:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=689
05/28/2022 09:36:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 09:36:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=694
05/28/2022 09:36:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 09:36:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 09:36:16 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.6102179429765637 on epoch=699
05/28/2022 09:36:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 09:36:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/28/2022 09:36:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/28/2022 09:36:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/28/2022 09:36:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 09:36:30 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6172667440960123 on epoch=712
05/28/2022 09:36:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 09:36:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 09:36:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=719
05/28/2022 09:36:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 09:36:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 09:36:43 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5672786690433749 on epoch=724
05/28/2022 09:36:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 09:36:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/28/2022 09:36:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 09:36:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/28/2022 09:36:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 09:36:56 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6410762679055362 on epoch=737
05/28/2022 09:36:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/28/2022 09:37:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/28/2022 09:37:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/28/2022 09:37:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/28/2022 09:37:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 09:37:10 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5882672882672882 on epoch=749
05/28/2022 09:37:10 - INFO - __main__ - save last model!
05/28/2022 09:37:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 09:37:10 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 09:37:10 - INFO - __main__ - Printing 3 examples
05/28/2022 09:37:10 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:37:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:37:10 - INFO - __main__ - Printing 3 examples
05/28/2022 09:37:10 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:37:10 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:37:10 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:37:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:37:10 - INFO - __main__ - Printing 3 examples
05/28/2022 09:37:10 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:37:10 - INFO - __main__ - ['others']
05/28/2022 09:37:10 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:37:10 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:37:10 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:37:12 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:37:17 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 09:37:25 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:37:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:37:25 - INFO - __main__ - Starting training!
05/28/2022 09:38:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/28/2022 09:38:50 - INFO - __main__ - Classification-F1 on test data: 0.2799
05/28/2022 09:38:50 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.6471288515406163, test_performance=0.2798894158551056
05/28/2022 09:38:50 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/28/2022 09:38:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:38:51 - INFO - __main__ - Printing 3 examples
05/28/2022 09:38:51 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:38:51 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:38:51 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:38:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:38:51 - INFO - __main__ - Printing 3 examples
05/28/2022 09:38:51 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:38:51 - INFO - __main__ - ['others']
05/28/2022 09:38:51 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:38:51 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:38:51 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:39:06 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:39:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:39:07 - INFO - __main__ - Starting training!
05/28/2022 09:39:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.40 on epoch=2
05/28/2022 09:39:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.98 on epoch=4
05/28/2022 09:39:15 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=7
05/28/2022 09:39:18 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
05/28/2022 09:39:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=12
05/28/2022 09:39:21 - INFO - __main__ - Global step 50 Train loss 1.95 Classification-F1 0.10126582278481013 on epoch=12
05/28/2022 09:39:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
05/28/2022 09:39:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.86 on epoch=14
05/28/2022 09:39:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
05/28/2022 09:39:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=19
05/28/2022 09:39:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
05/28/2022 09:39:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.81 on epoch=24
05/28/2022 09:39:34 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.2363154960981048 on epoch=24
05/28/2022 09:39:34 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.2363154960981048 on epoch=24, global_step=100
05/28/2022 09:39:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=27
05/28/2022 09:39:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
05/28/2022 09:39:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=32
05/28/2022 09:39:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=34
05/28/2022 09:39:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.85 on epoch=37
05/28/2022 09:39:47 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.13067758749069247 on epoch=37
05/28/2022 09:39:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.95 on epoch=39
05/28/2022 09:39:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=42
05/28/2022 09:39:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=44
05/28/2022 09:39:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=47
05/28/2022 09:39:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=49
05/28/2022 09:40:00 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.13034188034188032 on epoch=49
05/28/2022 09:40:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=52
05/28/2022 09:40:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=54
05/28/2022 09:40:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=57
05/28/2022 09:40:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=59
05/28/2022 09:40:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=62
05/28/2022 09:40:13 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.17708333333333331 on epoch=62
05/28/2022 09:40:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=64
05/28/2022 09:40:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=67
05/28/2022 09:40:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=69
05/28/2022 09:40:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/28/2022 09:40:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
05/28/2022 09:40:26 - INFO - __main__ - Global step 300 Train loss 0.83 Classification-F1 0.10256410256410256 on epoch=74
05/28/2022 09:40:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=77
05/28/2022 09:40:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.74 on epoch=79
05/28/2022 09:40:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.72 on epoch=82
05/28/2022 09:40:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=84
05/28/2022 09:40:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.76 on epoch=87
05/28/2022 09:40:39 - INFO - __main__ - Global step 350 Train loss 0.76 Classification-F1 0.43504662762958146 on epoch=87
05/28/2022 09:40:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2363154960981048 -> 0.43504662762958146 on epoch=87, global_step=350
05/28/2022 09:40:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=89
05/28/2022 09:40:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.82 on epoch=92
05/28/2022 09:40:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=94
05/28/2022 09:40:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=97
05/28/2022 09:40:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=99
05/28/2022 09:40:53 - INFO - __main__ - Global step 400 Train loss 0.76 Classification-F1 0.3611447811447811 on epoch=99
05/28/2022 09:40:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=102
05/28/2022 09:40:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=104
05/28/2022 09:41:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.73 on epoch=107
05/28/2022 09:41:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=109
05/28/2022 09:41:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.64 on epoch=112
05/28/2022 09:41:06 - INFO - __main__ - Global step 450 Train loss 0.69 Classification-F1 0.36415356151711376 on epoch=112
05/28/2022 09:41:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.64 on epoch=114
05/28/2022 09:41:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.66 on epoch=117
05/28/2022 09:41:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=119
05/28/2022 09:41:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=122
05/28/2022 09:41:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=124
05/28/2022 09:41:19 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.4701159951159951 on epoch=124
05/28/2022 09:41:19 - INFO - __main__ - Saving model with best Classification-F1: 0.43504662762958146 -> 0.4701159951159951 on epoch=124, global_step=500
05/28/2022 09:41:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=127
05/28/2022 09:41:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=129
05/28/2022 09:41:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=132
05/28/2022 09:41:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=134
05/28/2022 09:41:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=137
05/28/2022 09:41:32 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.6113767726670951 on epoch=137
05/28/2022 09:41:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4701159951159951 -> 0.6113767726670951 on epoch=137, global_step=550
05/28/2022 09:41:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=139
05/28/2022 09:41:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=142
05/28/2022 09:41:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=144
05/28/2022 09:41:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=147
05/28/2022 09:41:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=149
05/28/2022 09:41:46 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.46319665776552676 on epoch=149
05/28/2022 09:41:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=152
05/28/2022 09:41:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=154
05/28/2022 09:41:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=157
05/28/2022 09:41:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=159
05/28/2022 09:41:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=162
05/28/2022 09:41:59 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.6411944202266783 on epoch=162
05/28/2022 09:41:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6113767726670951 -> 0.6411944202266783 on epoch=162, global_step=650
05/28/2022 09:42:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=164
05/28/2022 09:42:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=167
05/28/2022 09:42:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
05/28/2022 09:42:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=172
05/28/2022 09:42:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=174
05/28/2022 09:42:12 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.5376558008136955 on epoch=174
05/28/2022 09:42:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=177
05/28/2022 09:42:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=179
05/28/2022 09:42:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=182
05/28/2022 09:42:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=184
05/28/2022 09:42:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=187
05/28/2022 09:42:25 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.5876182391462527 on epoch=187
05/28/2022 09:42:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=189
05/28/2022 09:42:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/28/2022 09:42:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=194
05/28/2022 09:42:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
05/28/2022 09:42:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/28/2022 09:42:39 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.6338349938539173 on epoch=199
05/28/2022 09:42:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=202
05/28/2022 09:42:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=204
05/28/2022 09:42:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=207
05/28/2022 09:42:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=209
05/28/2022 09:42:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
05/28/2022 09:42:52 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.6361466444197256 on epoch=212
05/28/2022 09:42:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=214
05/28/2022 09:42:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=217
05/28/2022 09:42:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
05/28/2022 09:43:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=222
05/28/2022 09:43:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=224
05/28/2022 09:43:05 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.6300840336134454 on epoch=224
05/28/2022 09:43:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=227
05/28/2022 09:43:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
05/28/2022 09:43:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/28/2022 09:43:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=234
05/28/2022 09:43:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
05/28/2022 09:43:19 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.6505268029461578 on epoch=237
05/28/2022 09:43:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6411944202266783 -> 0.6505268029461578 on epoch=237, global_step=950
05/28/2022 09:43:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/28/2022 09:43:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=242
05/28/2022 09:43:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
05/28/2022 09:43:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=247
05/28/2022 09:43:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/28/2022 09:43:32 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.5641632415825965 on epoch=249
05/28/2022 09:43:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/28/2022 09:43:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/28/2022 09:43:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
05/28/2022 09:43:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
05/28/2022 09:43:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/28/2022 09:43:45 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.6291666666666667 on epoch=262
05/28/2022 09:43:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
05/28/2022 09:43:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/28/2022 09:43:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/28/2022 09:43:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
05/28/2022 09:43:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
05/28/2022 09:43:58 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.5625453720508168 on epoch=274
05/28/2022 09:44:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/28/2022 09:44:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/28/2022 09:44:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=282
05/28/2022 09:44:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
05/28/2022 09:44:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/28/2022 09:44:12 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5486111111111112 on epoch=287
05/28/2022 09:44:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/28/2022 09:44:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/28/2022 09:44:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/28/2022 09:44:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=297
05/28/2022 09:44:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/28/2022 09:44:25 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.5092490984210057 on epoch=299
05/28/2022 09:44:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/28/2022 09:44:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/28/2022 09:44:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/28/2022 09:44:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/28/2022 09:44:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/28/2022 09:44:38 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.5891898864809082 on epoch=312
05/28/2022 09:44:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/28/2022 09:44:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/28/2022 09:44:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/28/2022 09:44:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/28/2022 09:44:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/28/2022 09:44:52 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.579513299377476 on epoch=324
05/28/2022 09:44:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
05/28/2022 09:44:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/28/2022 09:44:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/28/2022 09:45:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
05/28/2022 09:45:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/28/2022 09:45:05 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6158459595959596 on epoch=337
05/28/2022 09:45:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/28/2022 09:45:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/28/2022 09:45:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/28/2022 09:45:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/28/2022 09:45:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/28/2022 09:45:18 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6047037821231369 on epoch=349
05/28/2022 09:45:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 09:45:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/28/2022 09:45:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/28/2022 09:45:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/28/2022 09:45:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/28/2022 09:45:31 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6432739557739557 on epoch=362
05/28/2022 09:45:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/28/2022 09:45:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/28/2022 09:45:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
05/28/2022 09:45:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/28/2022 09:45:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/28/2022 09:45:45 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6415701415701416 on epoch=374
05/28/2022 09:45:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/28/2022 09:45:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
05/28/2022 09:45:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/28/2022 09:45:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 09:45:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/28/2022 09:45:58 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.5905528908478092 on epoch=387
05/28/2022 09:46:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/28/2022 09:46:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
05/28/2022 09:46:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/28/2022 09:46:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=397
05/28/2022 09:46:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/28/2022 09:46:11 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.6185628097392804 on epoch=399
05/28/2022 09:46:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 09:46:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/28/2022 09:46:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/28/2022 09:46:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/28/2022 09:46:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 09:46:25 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5925337938205585 on epoch=412
05/28/2022 09:46:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 09:46:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/28/2022 09:46:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/28/2022 09:46:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 09:46:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 09:46:38 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6243803206277851 on epoch=424
05/28/2022 09:46:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/28/2022 09:46:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/28/2022 09:46:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/28/2022 09:46:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/28/2022 09:46:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/28/2022 09:46:51 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.5210794677278945 on epoch=437
05/28/2022 09:46:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
05/28/2022 09:46:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/28/2022 09:46:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/28/2022 09:47:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/28/2022 09:47:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/28/2022 09:47:05 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5116182918493573 on epoch=449
05/28/2022 09:47:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/28/2022 09:47:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
05/28/2022 09:47:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/28/2022 09:47:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/28/2022 09:47:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/28/2022 09:47:18 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6169377247466492 on epoch=462
05/28/2022 09:47:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/28/2022 09:47:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 09:47:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/28/2022 09:47:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/28/2022 09:47:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 09:47:31 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.600355526826115 on epoch=474
05/28/2022 09:47:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/28/2022 09:47:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/28/2022 09:47:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 09:47:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 09:47:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 09:47:45 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5884407989671148 on epoch=487
05/28/2022 09:47:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/28/2022 09:47:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=492
05/28/2022 09:47:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/28/2022 09:47:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 09:47:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
05/28/2022 09:47:58 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.5519914651493599 on epoch=499
05/28/2022 09:48:00 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/28/2022 09:48:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 09:48:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/28/2022 09:48:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 09:48:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 09:48:11 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.5602716352716353 on epoch=512
05/28/2022 09:48:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/28/2022 09:48:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 09:48:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/28/2022 09:48:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
05/28/2022 09:48:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/28/2022 09:48:25 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6071428571428572 on epoch=524
05/28/2022 09:48:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 09:48:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/28/2022 09:48:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 09:48:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 09:48:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 09:48:38 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.5671436656156521 on epoch=537
05/28/2022 09:48:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/28/2022 09:48:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/28/2022 09:48:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/28/2022 09:48:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 09:48:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/28/2022 09:48:51 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5766201935319581 on epoch=549
05/28/2022 09:48:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/28/2022 09:48:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 09:48:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 09:49:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/28/2022 09:49:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 09:49:05 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5793859649122808 on epoch=562
05/28/2022 09:49:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 09:49:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/28/2022 09:49:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
05/28/2022 09:49:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 09:49:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 09:49:18 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6247079070354933 on epoch=574
05/28/2022 09:49:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 09:49:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/28/2022 09:49:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 09:49:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 09:49:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 09:49:32 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6824962208721942 on epoch=587
05/28/2022 09:49:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6505268029461578 -> 0.6824962208721942 on epoch=587, global_step=2350
05/28/2022 09:49:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 09:49:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 09:49:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=594
05/28/2022 09:49:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=597
05/28/2022 09:49:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/28/2022 09:49:45 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.599298128342246 on epoch=599
05/28/2022 09:49:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
05/28/2022 09:49:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/28/2022 09:49:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/28/2022 09:49:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 09:49:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/28/2022 09:49:59 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.5757936507936507 on epoch=612
05/28/2022 09:50:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/28/2022 09:50:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/28/2022 09:50:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/28/2022 09:50:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 09:50:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 09:50:12 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6147186147186148 on epoch=624
05/28/2022 09:50:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/28/2022 09:50:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/28/2022 09:50:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 09:50:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 09:50:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 09:50:25 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6450914024927183 on epoch=637
05/28/2022 09:50:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 09:50:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/28/2022 09:50:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/28/2022 09:50:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 09:50:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/28/2022 09:50:39 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5933470038733197 on epoch=649
05/28/2022 09:50:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/28/2022 09:50:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/28/2022 09:50:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 09:50:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/28/2022 09:50:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 09:50:52 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6456422780435939 on epoch=662
05/28/2022 09:50:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 09:50:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 09:51:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 09:51:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/28/2022 09:51:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 09:51:06 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.644645705172021 on epoch=674
05/28/2022 09:51:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 09:51:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/28/2022 09:51:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=682
05/28/2022 09:51:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 09:51:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/28/2022 09:51:19 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6667242690260593 on epoch=687
05/28/2022 09:51:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/28/2022 09:51:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 09:51:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/28/2022 09:51:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
05/28/2022 09:51:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 09:51:33 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6543018551814319 on epoch=699
05/28/2022 09:51:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 09:51:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 09:51:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 09:51:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 09:51:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 09:51:46 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6592355561105562 on epoch=712
05/28/2022 09:51:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 09:51:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 09:51:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 09:51:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 09:51:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 09:52:00 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6592355561105562 on epoch=724
05/28/2022 09:52:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 09:52:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 09:52:07 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/28/2022 09:52:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 09:52:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 09:52:13 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6317533891547049 on epoch=737
05/28/2022 09:52:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 09:52:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 09:52:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 09:52:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 09:52:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 09:52:27 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6543959137709138 on epoch=749
05/28/2022 09:52:27 - INFO - __main__ - save last model!
05/28/2022 09:52:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 09:52:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 09:52:27 - INFO - __main__ - Printing 3 examples
05/28/2022 09:52:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:52:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:52:27 - INFO - __main__ - Printing 3 examples
05/28/2022 09:52:27 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:52:27 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:52:27 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:52:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:52:27 - INFO - __main__ - Printing 3 examples
05/28/2022 09:52:27 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:52:27 - INFO - __main__ - ['others']
05/28/2022 09:52:27 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:52:27 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:52:27 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:52:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:52:34 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 09:52:42 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:52:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:52:43 - INFO - __main__ - Starting training!
05/28/2022 09:54:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/28/2022 09:54:06 - INFO - __main__ - Classification-F1 on test data: 0.3783
05/28/2022 09:54:06 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.6824962208721942, test_performance=0.3782506799219418
05/28/2022 09:54:06 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/28/2022 09:54:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:54:07 - INFO - __main__ - Printing 3 examples
05/28/2022 09:54:07 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 09:54:07 - INFO - __main__ - ['others']
05/28/2022 09:54:07 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 09:54:07 - INFO - __main__ - ['others']
05/28/2022 09:54:07 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 09:54:07 - INFO - __main__ - ['others']
05/28/2022 09:54:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:54:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:54:08 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 09:54:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 09:54:08 - INFO - __main__ - Printing 3 examples
05/28/2022 09:54:08 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 09:54:08 - INFO - __main__ - ['others']
05/28/2022 09:54:08 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 09:54:08 - INFO - __main__ - ['others']
05/28/2022 09:54:08 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 09:54:08 - INFO - __main__ - ['others']
05/28/2022 09:54:08 - INFO - __main__ - Tokenizing Input ...
05/28/2022 09:54:08 - INFO - __main__ - Tokenizing Output ...
05/28/2022 09:54:08 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 09:54:23 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 09:54:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 09:54:23 - INFO - __main__ - Starting training!
05/28/2022 09:54:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.60 on epoch=2
05/28/2022 09:54:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.76 on epoch=4
05/28/2022 09:54:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.77 on epoch=7
05/28/2022 09:54:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=9
05/28/2022 09:54:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=12
05/28/2022 09:54:37 - INFO - __main__ - Global step 50 Train loss 2.28 Classification-F1 0.10126582278481013 on epoch=12
05/28/2022 09:54:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
05/28/2022 09:54:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
05/28/2022 09:54:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=17
05/28/2022 09:54:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=19
05/28/2022 09:54:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
05/28/2022 09:54:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/28/2022 09:54:50 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.09154929577464789 on epoch=24
05/28/2022 09:54:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=27
05/28/2022 09:54:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
05/28/2022 09:54:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
05/28/2022 09:55:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 09:55:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
05/28/2022 09:55:03 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.18373015873015874 on epoch=37
05/28/2022 09:55:03 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.18373015873015874 on epoch=37, global_step=150
05/28/2022 09:55:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=39
05/28/2022 09:55:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=42
05/28/2022 09:55:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
05/28/2022 09:55:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=47
05/28/2022 09:55:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=49
05/28/2022 09:55:17 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.24263078868342025 on epoch=49
05/28/2022 09:55:17 - INFO - __main__ - Saving model with best Classification-F1: 0.18373015873015874 -> 0.24263078868342025 on epoch=49, global_step=200
05/28/2022 09:55:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.86 on epoch=52
05/28/2022 09:55:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=54
05/28/2022 09:55:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=57
05/28/2022 09:55:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.69 on epoch=59
05/28/2022 09:55:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=62
05/28/2022 09:55:30 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.1537037037037037 on epoch=62
05/28/2022 09:55:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=64
05/28/2022 09:55:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=67
05/28/2022 09:55:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=69
05/28/2022 09:55:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=72
05/28/2022 09:55:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=74
05/28/2022 09:55:43 - INFO - __main__ - Global step 300 Train loss 0.68 Classification-F1 0.18458247066942718 on epoch=74
05/28/2022 09:55:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=77
05/28/2022 09:55:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=79
05/28/2022 09:55:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=82
05/28/2022 09:55:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=84
05/28/2022 09:55:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=87
05/28/2022 09:55:56 - INFO - __main__ - Global step 350 Train loss 0.63 Classification-F1 0.22534138655462183 on epoch=87
05/28/2022 09:55:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=89
05/28/2022 09:56:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=92
05/28/2022 09:56:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=94
05/28/2022 09:56:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
05/28/2022 09:56:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=99
05/28/2022 09:56:09 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.23604826546003016 on epoch=99
05/28/2022 09:56:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.59 on epoch=102
05/28/2022 09:56:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=104
05/28/2022 09:56:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=107
05/28/2022 09:56:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=109
05/28/2022 09:56:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=112
05/28/2022 09:56:22 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.3185624359537403 on epoch=112
05/28/2022 09:56:22 - INFO - __main__ - Saving model with best Classification-F1: 0.24263078868342025 -> 0.3185624359537403 on epoch=112, global_step=450
05/28/2022 09:56:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=114
05/28/2022 09:56:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=117
05/28/2022 09:56:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=119
05/28/2022 09:56:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=122
05/28/2022 09:56:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=124
05/28/2022 09:56:35 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.35581451206451203 on epoch=124
05/28/2022 09:56:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3185624359537403 -> 0.35581451206451203 on epoch=124, global_step=500
05/28/2022 09:56:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=127
05/28/2022 09:56:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=129
05/28/2022 09:56:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=132
05/28/2022 09:56:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=134
05/28/2022 09:56:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=137
05/28/2022 09:56:49 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.36087268414854623 on epoch=137
05/28/2022 09:56:49 - INFO - __main__ - Saving model with best Classification-F1: 0.35581451206451203 -> 0.36087268414854623 on epoch=137, global_step=550
05/28/2022 09:56:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=139
05/28/2022 09:56:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=142
05/28/2022 09:56:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=144
05/28/2022 09:56:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
05/28/2022 09:57:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=149
05/28/2022 09:57:02 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.33664021164021163 on epoch=149
05/28/2022 09:57:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
05/28/2022 09:57:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=154
05/28/2022 09:57:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
05/28/2022 09:57:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=159
05/28/2022 09:57:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=162
05/28/2022 09:57:15 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3578431372549019 on epoch=162
05/28/2022 09:57:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=164
05/28/2022 09:57:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/28/2022 09:57:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
05/28/2022 09:57:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
05/28/2022 09:57:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
05/28/2022 09:57:28 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.3532242615251514 on epoch=174
05/28/2022 09:57:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
05/28/2022 09:57:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/28/2022 09:57:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=182
05/28/2022 09:57:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
05/28/2022 09:57:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=187
05/28/2022 09:57:41 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.3272066375514651 on epoch=187
05/28/2022 09:57:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
05/28/2022 09:57:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=192
05/28/2022 09:57:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=194
05/28/2022 09:57:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
05/28/2022 09:57:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/28/2022 09:57:54 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.412335927960928 on epoch=199
05/28/2022 09:57:54 - INFO - __main__ - Saving model with best Classification-F1: 0.36087268414854623 -> 0.412335927960928 on epoch=199, global_step=800
05/28/2022 09:57:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/28/2022 09:57:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
05/28/2022 09:58:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=207
05/28/2022 09:58:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
05/28/2022 09:58:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=212
05/28/2022 09:58:08 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.3177006053620114 on epoch=212
05/28/2022 09:58:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
05/28/2022 09:58:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
05/28/2022 09:58:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
05/28/2022 09:58:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=222
05/28/2022 09:58:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/28/2022 09:58:21 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.3932483030265288 on epoch=224
05/28/2022 09:58:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=227
05/28/2022 09:58:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
05/28/2022 09:58:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
05/28/2022 09:58:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/28/2022 09:58:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
05/28/2022 09:58:35 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.345701583936878 on epoch=237
05/28/2022 09:58:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/28/2022 09:58:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/28/2022 09:58:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/28/2022 09:58:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/28/2022 09:58:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/28/2022 09:58:48 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.3651971232790198 on epoch=249
05/28/2022 09:58:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/28/2022 09:58:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
05/28/2022 09:58:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/28/2022 09:58:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/28/2022 09:59:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/28/2022 09:59:01 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.3784233944148469 on epoch=262
05/28/2022 09:59:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
05/28/2022 09:59:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/28/2022 09:59:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/28/2022 09:59:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/28/2022 09:59:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/28/2022 09:59:15 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.43159562211981567 on epoch=274
05/28/2022 09:59:15 - INFO - __main__ - Saving model with best Classification-F1: 0.412335927960928 -> 0.43159562211981567 on epoch=274, global_step=1100
05/28/2022 09:59:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/28/2022 09:59:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=279
05/28/2022 09:59:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
05/28/2022 09:59:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/28/2022 09:59:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
05/28/2022 09:59:28 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.3628352490421456 on epoch=287
05/28/2022 09:59:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/28/2022 09:59:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/28/2022 09:59:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/28/2022 09:59:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/28/2022 09:59:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/28/2022 09:59:41 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.38142187808483696 on epoch=299
05/28/2022 09:59:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/28/2022 09:59:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/28/2022 09:59:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/28/2022 09:59:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/28/2022 09:59:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/28/2022 09:59:55 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.4480431730431731 on epoch=312
05/28/2022 09:59:55 - INFO - __main__ - Saving model with best Classification-F1: 0.43159562211981567 -> 0.4480431730431731 on epoch=312, global_step=1250
05/28/2022 09:59:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/28/2022 09:59:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/28/2022 10:00:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/28/2022 10:00:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/28/2022 10:00:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
05/28/2022 10:00:08 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.41679211469534044 on epoch=324
05/28/2022 10:00:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/28/2022 10:00:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/28/2022 10:00:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/28/2022 10:00:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/28/2022 10:00:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/28/2022 10:00:21 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.37320358052065367 on epoch=337
05/28/2022 10:00:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/28/2022 10:00:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/28/2022 10:00:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
05/28/2022 10:00:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/28/2022 10:00:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/28/2022 10:00:34 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.39953488372093027 on epoch=349
05/28/2022 10:00:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/28/2022 10:00:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
05/28/2022 10:00:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/28/2022 10:00:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/28/2022 10:00:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/28/2022 10:00:48 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.42455326958369555 on epoch=362
05/28/2022 10:00:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/28/2022 10:00:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/28/2022 10:00:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 10:00:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/28/2022 10:01:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/28/2022 10:01:01 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.4174019607843137 on epoch=374
05/28/2022 10:01:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/28/2022 10:01:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/28/2022 10:01:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/28/2022 10:01:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=384
05/28/2022 10:01:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/28/2022 10:01:15 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.43300653594771243 on epoch=387
05/28/2022 10:01:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/28/2022 10:01:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/28/2022 10:01:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 10:01:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/28/2022 10:01:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/28/2022 10:01:28 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.4156593406593406 on epoch=399
05/28/2022 10:01:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/28/2022 10:01:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/28/2022 10:01:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/28/2022 10:01:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/28/2022 10:01:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 10:01:41 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.4679089637175376 on epoch=412
05/28/2022 10:01:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4480431730431731 -> 0.4679089637175376 on epoch=412, global_step=1650
05/28/2022 10:01:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/28/2022 10:01:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/28/2022 10:01:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/28/2022 10:01:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
05/28/2022 10:01:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 10:01:55 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.4574074074074074 on epoch=424
05/28/2022 10:01:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/28/2022 10:02:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/28/2022 10:02:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/28/2022 10:02:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=434
05/28/2022 10:02:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/28/2022 10:02:08 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.4942811272212361 on epoch=437
05/28/2022 10:02:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4679089637175376 -> 0.4942811272212361 on epoch=437, global_step=1750
05/28/2022 10:02:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/28/2022 10:02:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/28/2022 10:02:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/28/2022 10:02:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/28/2022 10:02:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/28/2022 10:02:21 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.42793791574279383 on epoch=449
05/28/2022 10:02:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/28/2022 10:02:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 10:02:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
05/28/2022 10:02:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/28/2022 10:02:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/28/2022 10:02:35 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.42668696246282456 on epoch=462
05/28/2022 10:02:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/28/2022 10:02:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 10:02:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/28/2022 10:02:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/28/2022 10:02:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/28/2022 10:02:48 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.4666666666666667 on epoch=474
05/28/2022 10:02:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 10:02:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/28/2022 10:02:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 10:02:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/28/2022 10:03:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 10:03:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.47149175412293853 on epoch=487
05/28/2022 10:03:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/28/2022 10:03:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 10:03:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/28/2022 10:03:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=497
05/28/2022 10:03:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/28/2022 10:03:15 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.4542114695340502 on epoch=499
05/28/2022 10:03:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/28/2022 10:03:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 10:03:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=507
05/28/2022 10:03:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 10:03:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 10:03:28 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.44591537494763306 on epoch=512
05/28/2022 10:03:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/28/2022 10:03:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 10:03:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/28/2022 10:03:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/28/2022 10:03:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 10:03:41 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.5024794190742466 on epoch=524
05/28/2022 10:03:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4942811272212361 -> 0.5024794190742466 on epoch=524, global_step=2100
05/28/2022 10:03:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 10:03:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 10:03:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
05/28/2022 10:03:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 10:03:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
05/28/2022 10:03:55 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.4884099616858238 on epoch=537
05/28/2022 10:03:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/28/2022 10:04:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/28/2022 10:04:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/28/2022 10:04:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 10:04:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 10:04:08 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.4897186147186147 on epoch=549
05/28/2022 10:04:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 10:04:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/28/2022 10:04:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 10:04:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
05/28/2022 10:04:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 10:04:21 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5282851043643264 on epoch=562
05/28/2022 10:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5024794190742466 -> 0.5282851043643264 on epoch=562, global_step=2250
05/28/2022 10:04:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 10:04:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 10:04:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 10:04:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 10:04:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 10:04:34 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5311714429361488 on epoch=574
05/28/2022 10:04:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5282851043643264 -> 0.5311714429361488 on epoch=574, global_step=2300
05/28/2022 10:04:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 10:04:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 10:04:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 10:04:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/28/2022 10:04:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 10:04:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.5303394522144522 on epoch=587
05/28/2022 10:04:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/28/2022 10:04:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 10:04:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/28/2022 10:04:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 10:05:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 10:05:01 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.511719630380888 on epoch=599
05/28/2022 10:05:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/28/2022 10:05:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/28/2022 10:05:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/28/2022 10:05:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 10:05:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 10:05:14 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5019230769230769 on epoch=612
05/28/2022 10:05:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/28/2022 10:05:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 10:05:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/28/2022 10:05:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 10:05:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 10:05:28 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.4622058684558684 on epoch=624
05/28/2022 10:05:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/28/2022 10:05:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 10:05:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 10:05:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 10:05:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/28/2022 10:05:41 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5267730075901329 on epoch=637
05/28/2022 10:05:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 10:05:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 10:05:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 10:05:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 10:05:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 10:05:55 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.5271796218487396 on epoch=649
05/28/2022 10:05:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=652
05/28/2022 10:06:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 10:06:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 10:06:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=659
05/28/2022 10:06:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 10:06:08 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5115634523604163 on epoch=662
05/28/2022 10:06:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 10:06:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 10:06:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/28/2022 10:06:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/28/2022 10:06:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/28/2022 10:06:21 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.47210433244916006 on epoch=674
05/28/2022 10:06:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 10:06:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 10:06:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 10:06:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 10:06:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 10:06:35 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.48333333333333334 on epoch=687
05/28/2022 10:06:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 10:06:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/28/2022 10:06:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 10:06:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 10:06:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 10:06:48 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.4940092165898618 on epoch=699
05/28/2022 10:06:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 10:06:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 10:06:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/28/2022 10:06:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 10:07:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 10:07:02 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.5186151521540516 on epoch=712
05/28/2022 10:07:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 10:07:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/28/2022 10:07:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=719
05/28/2022 10:07:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 10:07:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/28/2022 10:07:15 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5243605455002514 on epoch=724
05/28/2022 10:07:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 10:07:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 10:07:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 10:07:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/28/2022 10:07:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 10:07:28 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5364870085965421 on epoch=737
05/28/2022 10:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5311714429361488 -> 0.5364870085965421 on epoch=737, global_step=2950
05/28/2022 10:07:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 10:07:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 10:07:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 10:07:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 10:07:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 10:07:42 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.45525972485768507 on epoch=749
05/28/2022 10:07:42 - INFO - __main__ - save last model!
05/28/2022 10:07:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 10:07:42 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 10:07:42 - INFO - __main__ - Printing 3 examples
05/28/2022 10:07:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:07:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:07:42 - INFO - __main__ - Printing 3 examples
05/28/2022 10:07:42 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:07:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:07:42 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:07:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:07:42 - INFO - __main__ - Printing 3 examples
05/28/2022 10:07:42 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 10:07:42 - INFO - __main__ - ['others']
05/28/2022 10:07:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:07:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:07:42 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:07:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:07:49 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 10:07:57 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:07:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:07:58 - INFO - __main__ - Starting training!
05/28/2022 10:09:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/28/2022 10:09:14 - INFO - __main__ - Classification-F1 on test data: 0.2746
05/28/2022 10:09:15 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.5364870085965421, test_performance=0.2745774780954119
05/28/2022 10:09:15 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/28/2022 10:09:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:09:16 - INFO - __main__ - Printing 3 examples
05/28/2022 10:09:16 - INFO - __main__ -  [emo] how cause yes am listening
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:09:16 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:09:16 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:09:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:09:16 - INFO - __main__ - Printing 3 examples
05/28/2022 10:09:16 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/28/2022 10:09:16 - INFO - __main__ - ['others']
05/28/2022 10:09:16 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:09:16 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:09:16 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:09:31 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:09:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:09:32 - INFO - __main__ - Starting training!
05/28/2022 10:09:35 - INFO - __main__ - Step 10 Global step 10 Train loss 5.18 on epoch=2
05/28/2022 10:09:37 - INFO - __main__ - Step 20 Global step 20 Train loss 3.63 on epoch=4
05/28/2022 10:09:40 - INFO - __main__ - Step 30 Global step 30 Train loss 2.54 on epoch=7
05/28/2022 10:09:42 - INFO - __main__ - Step 40 Global step 40 Train loss 1.54 on epoch=9
05/28/2022 10:09:44 - INFO - __main__ - Step 50 Global step 50 Train loss 1.33 on epoch=12
05/28/2022 10:09:45 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.28932625681851687 on epoch=12
05/28/2022 10:09:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28932625681851687 on epoch=12, global_step=50
05/28/2022 10:09:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=14
05/28/2022 10:09:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=17
05/28/2022 10:09:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=19
05/28/2022 10:09:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=22
05/28/2022 10:09:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
05/28/2022 10:09:58 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.10526315789473685 on epoch=24
05/28/2022 10:10:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=27
05/28/2022 10:10:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=29
05/28/2022 10:10:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
05/28/2022 10:10:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=34
05/28/2022 10:10:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=37
05/28/2022 10:10:11 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.13740245261984393 on epoch=37
05/28/2022 10:10:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
05/28/2022 10:10:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/28/2022 10:10:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=44
05/28/2022 10:10:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.83 on epoch=47
05/28/2022 10:10:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=49
05/28/2022 10:10:24 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.13157894736842107 on epoch=49
05/28/2022 10:10:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
05/28/2022 10:10:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=54
05/28/2022 10:10:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=57
05/28/2022 10:10:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=59
05/28/2022 10:10:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=62
05/28/2022 10:10:37 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.1 on epoch=62
05/28/2022 10:10:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=64
05/28/2022 10:10:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=67
05/28/2022 10:10:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=69
05/28/2022 10:10:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/28/2022 10:10:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=74
05/28/2022 10:10:50 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.18922705314009663 on epoch=74
05/28/2022 10:10:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=77
05/28/2022 10:10:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.85 on epoch=79
05/28/2022 10:10:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=82
05/28/2022 10:11:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.79 on epoch=84
05/28/2022 10:11:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
05/28/2022 10:11:03 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.31109391124871 on epoch=87
05/28/2022 10:11:03 - INFO - __main__ - Saving model with best Classification-F1: 0.28932625681851687 -> 0.31109391124871 on epoch=87, global_step=350
05/28/2022 10:11:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.82 on epoch=89
05/28/2022 10:11:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.72 on epoch=92
05/28/2022 10:11:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=94
05/28/2022 10:11:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=97
05/28/2022 10:11:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
05/28/2022 10:11:16 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.361749680715198 on epoch=99
05/28/2022 10:11:16 - INFO - __main__ - Saving model with best Classification-F1: 0.31109391124871 -> 0.361749680715198 on epoch=99, global_step=400
05/28/2022 10:11:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
05/28/2022 10:11:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.80 on epoch=104
05/28/2022 10:11:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.68 on epoch=107
05/28/2022 10:11:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=109
05/28/2022 10:11:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.64 on epoch=112
05/28/2022 10:11:29 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.37177419354838714 on epoch=112
05/28/2022 10:11:29 - INFO - __main__ - Saving model with best Classification-F1: 0.361749680715198 -> 0.37177419354838714 on epoch=112, global_step=450
05/28/2022 10:11:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=114
05/28/2022 10:11:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.68 on epoch=117
05/28/2022 10:11:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.72 on epoch=119
05/28/2022 10:11:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=122
05/28/2022 10:11:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=124
05/28/2022 10:11:43 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.32278539874447804 on epoch=124
05/28/2022 10:11:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=127
05/28/2022 10:11:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=129
05/28/2022 10:11:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=132
05/28/2022 10:11:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=134
05/28/2022 10:11:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=137
05/28/2022 10:11:56 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.4054916985951469 on epoch=137
05/28/2022 10:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.37177419354838714 -> 0.4054916985951469 on epoch=137, global_step=550
05/28/2022 10:11:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=139
05/28/2022 10:12:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=142
05/28/2022 10:12:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=144
05/28/2022 10:12:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=147
05/28/2022 10:12:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=149
05/28/2022 10:12:09 - INFO - __main__ - Global step 600 Train loss 0.59 Classification-F1 0.39368044515103334 on epoch=149
05/28/2022 10:12:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=152
05/28/2022 10:12:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=154
05/28/2022 10:12:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=157
05/28/2022 10:12:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=159
05/28/2022 10:12:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=162
05/28/2022 10:12:22 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.4503227377907685 on epoch=162
05/28/2022 10:12:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4054916985951469 -> 0.4503227377907685 on epoch=162, global_step=650
05/28/2022 10:12:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=164
05/28/2022 10:12:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=167
05/28/2022 10:12:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=169
05/28/2022 10:12:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=172
05/28/2022 10:12:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=174
05/28/2022 10:12:35 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.4637509850275808 on epoch=174
05/28/2022 10:12:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4503227377907685 -> 0.4637509850275808 on epoch=174, global_step=700
05/28/2022 10:12:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
05/28/2022 10:12:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=179
05/28/2022 10:12:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=182
05/28/2022 10:12:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=184
05/28/2022 10:12:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=187
05/28/2022 10:12:48 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.45505449491624606 on epoch=187
05/28/2022 10:12:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=189
05/28/2022 10:12:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=192
05/28/2022 10:12:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=194
05/28/2022 10:12:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=197
05/28/2022 10:13:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=199
05/28/2022 10:13:01 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.4712686077775592 on epoch=199
05/28/2022 10:13:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4637509850275808 -> 0.4712686077775592 on epoch=199, global_step=800
05/28/2022 10:13:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=202
05/28/2022 10:13:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=204
05/28/2022 10:13:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=207
05/28/2022 10:13:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=209
05/28/2022 10:13:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
05/28/2022 10:13:14 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.49139492753623193 on epoch=212
05/28/2022 10:13:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4712686077775592 -> 0.49139492753623193 on epoch=212, global_step=850
05/28/2022 10:13:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=214
05/28/2022 10:13:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=217
05/28/2022 10:13:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=219
05/28/2022 10:13:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=222
05/28/2022 10:13:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=224
05/28/2022 10:13:27 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.4804608585858586 on epoch=224
05/28/2022 10:13:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=227
05/28/2022 10:13:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/28/2022 10:13:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
05/28/2022 10:13:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=234
05/28/2022 10:13:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=237
05/28/2022 10:13:40 - INFO - __main__ - Global step 950 Train loss 0.25 Classification-F1 0.4631226053639847 on epoch=237
05/28/2022 10:13:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
05/28/2022 10:13:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=242
05/28/2022 10:13:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=244
05/28/2022 10:13:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
05/28/2022 10:13:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=249
05/28/2022 10:13:53 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.4996539313399778 on epoch=249
05/28/2022 10:13:53 - INFO - __main__ - Saving model with best Classification-F1: 0.49139492753623193 -> 0.4996539313399778 on epoch=249, global_step=1000
05/28/2022 10:13:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
05/28/2022 10:13:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=254
05/28/2022 10:14:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=257
05/28/2022 10:14:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=259
05/28/2022 10:14:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=262
05/28/2022 10:14:06 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.48361589068825905 on epoch=262
05/28/2022 10:14:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=264
05/28/2022 10:14:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
05/28/2022 10:14:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
05/28/2022 10:14:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=272
05/28/2022 10:14:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=274
05/28/2022 10:14:19 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.5373654141946824 on epoch=274
05/28/2022 10:14:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4996539313399778 -> 0.5373654141946824 on epoch=274, global_step=1100
05/28/2022 10:14:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=277
05/28/2022 10:14:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=279
05/28/2022 10:14:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=282
05/28/2022 10:14:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=284
05/28/2022 10:14:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=287
05/28/2022 10:14:32 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.549795227920228 on epoch=287
05/28/2022 10:14:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5373654141946824 -> 0.549795227920228 on epoch=287, global_step=1150
05/28/2022 10:14:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=289
05/28/2022 10:14:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=292
05/28/2022 10:14:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
05/28/2022 10:14:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=297
05/28/2022 10:14:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/28/2022 10:14:46 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.5146553379416282 on epoch=299
05/28/2022 10:14:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
05/28/2022 10:14:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/28/2022 10:14:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/28/2022 10:14:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
05/28/2022 10:14:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=312
05/28/2022 10:14:59 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.4965729698592602 on epoch=312
05/28/2022 10:15:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=314
05/28/2022 10:15:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
05/28/2022 10:15:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/28/2022 10:15:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
05/28/2022 10:15:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
05/28/2022 10:15:12 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.5036931818181818 on epoch=324
05/28/2022 10:15:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=327
05/28/2022 10:15:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=329
05/28/2022 10:15:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/28/2022 10:15:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
05/28/2022 10:15:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/28/2022 10:15:26 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.49142628205128197 on epoch=337
05/28/2022 10:15:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/28/2022 10:15:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/28/2022 10:15:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/28/2022 10:15:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/28/2022 10:15:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/28/2022 10:15:39 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.5184561965811967 on epoch=349
05/28/2022 10:15:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=352
05/28/2022 10:15:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=354
05/28/2022 10:15:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/28/2022 10:15:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=359
05/28/2022 10:15:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
05/28/2022 10:15:52 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.5292211566405114 on epoch=362
05/28/2022 10:15:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/28/2022 10:15:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/28/2022 10:15:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
05/28/2022 10:16:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=372
05/28/2022 10:16:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/28/2022 10:16:05 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.5173924115100585 on epoch=374
05/28/2022 10:16:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/28/2022 10:16:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/28/2022 10:16:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/28/2022 10:16:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/28/2022 10:16:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/28/2022 10:16:18 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.5155150201106083 on epoch=387
05/28/2022 10:16:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/28/2022 10:16:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=392
05/28/2022 10:16:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/28/2022 10:16:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
05/28/2022 10:16:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=399
05/28/2022 10:16:31 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.5542388167388167 on epoch=399
05/28/2022 10:16:31 - INFO - __main__ - Saving model with best Classification-F1: 0.549795227920228 -> 0.5542388167388167 on epoch=399, global_step=1600
05/28/2022 10:16:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/28/2022 10:16:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
05/28/2022 10:16:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
05/28/2022 10:16:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=409
05/28/2022 10:16:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/28/2022 10:16:45 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.49453629032258056 on epoch=412
05/28/2022 10:16:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
05/28/2022 10:16:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
05/28/2022 10:16:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/28/2022 10:16:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/28/2022 10:16:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=424
05/28/2022 10:16:58 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.49166666666666664 on epoch=424
05/28/2022 10:17:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
05/28/2022 10:17:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=429
05/28/2022 10:17:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/28/2022 10:17:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 10:17:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/28/2022 10:17:11 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.5086976600985221 on epoch=437
05/28/2022 10:17:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/28/2022 10:17:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/28/2022 10:17:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/28/2022 10:17:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/28/2022 10:17:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/28/2022 10:17:24 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.5235999968862872 on epoch=449
05/28/2022 10:17:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
05/28/2022 10:17:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=454
05/28/2022 10:17:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/28/2022 10:17:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 10:17:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/28/2022 10:17:38 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.5050444204038815 on epoch=462
05/28/2022 10:17:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/28/2022 10:17:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 10:17:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/28/2022 10:17:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/28/2022 10:17:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 10:17:51 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.5293062200956937 on epoch=474
05/28/2022 10:17:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/28/2022 10:17:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/28/2022 10:17:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 10:18:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/28/2022 10:18:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 10:18:04 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.5253896103896104 on epoch=487
05/28/2022 10:18:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/28/2022 10:18:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=492
05/28/2022 10:18:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 10:18:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/28/2022 10:18:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/28/2022 10:18:18 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.522805023923445 on epoch=499
05/28/2022 10:18:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=502
05/28/2022 10:18:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
05/28/2022 10:18:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/28/2022 10:18:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 10:18:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=512
05/28/2022 10:18:31 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.528601807549176 on epoch=512
05/28/2022 10:18:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 10:18:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 10:18:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 10:18:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/28/2022 10:18:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/28/2022 10:18:45 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.4991516443129346 on epoch=524
05/28/2022 10:18:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 10:18:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=529
05/28/2022 10:18:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
05/28/2022 10:18:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/28/2022 10:18:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/28/2022 10:18:58 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.546868493207166 on epoch=537
05/28/2022 10:19:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 10:19:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/28/2022 10:19:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/28/2022 10:19:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 10:19:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 10:19:11 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.5591397849462365 on epoch=549
05/28/2022 10:19:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5542388167388167 -> 0.5591397849462365 on epoch=549, global_step=2200
05/28/2022 10:19:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
05/28/2022 10:19:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/28/2022 10:19:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/28/2022 10:19:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=559
05/28/2022 10:19:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 10:19:25 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5263068849656285 on epoch=562
05/28/2022 10:19:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 10:19:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/28/2022 10:19:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 10:19:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/28/2022 10:19:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/28/2022 10:19:38 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.565794499618029 on epoch=574
05/28/2022 10:19:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5591397849462365 -> 0.565794499618029 on epoch=574, global_step=2300
05/28/2022 10:19:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/28/2022 10:19:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 10:19:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
05/28/2022 10:19:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/28/2022 10:19:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/28/2022 10:19:52 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5082164634146342 on epoch=587
05/28/2022 10:19:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/28/2022 10:19:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
05/28/2022 10:19:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 10:20:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/28/2022 10:20:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/28/2022 10:20:05 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.516200842725274 on epoch=599
05/28/2022 10:20:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
05/28/2022 10:20:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/28/2022 10:20:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 10:20:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/28/2022 10:20:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/28/2022 10:20:18 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.5294264558970441 on epoch=612
05/28/2022 10:20:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/28/2022 10:20:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 10:20:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/28/2022 10:20:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/28/2022 10:20:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/28/2022 10:20:32 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5165160515672397 on epoch=624
05/28/2022 10:20:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/28/2022 10:20:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 10:20:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/28/2022 10:20:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 10:20:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 10:20:45 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.5544143356643356 on epoch=637
05/28/2022 10:20:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/28/2022 10:20:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/28/2022 10:20:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/28/2022 10:20:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/28/2022 10:20:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/28/2022 10:20:59 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5622298999988598 on epoch=649
05/28/2022 10:21:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 10:21:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 10:21:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 10:21:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/28/2022 10:21:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 10:21:12 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5321105794790005 on epoch=662
05/28/2022 10:21:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
05/28/2022 10:21:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/28/2022 10:21:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/28/2022 10:21:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/28/2022 10:21:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 10:21:25 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5207604895104896 on epoch=674
05/28/2022 10:21:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 10:21:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/28/2022 10:21:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 10:21:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/28/2022 10:21:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/28/2022 10:21:39 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5199494949494949 on epoch=687
05/28/2022 10:21:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/28/2022 10:21:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/28/2022 10:21:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/28/2022 10:21:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/28/2022 10:21:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/28/2022 10:21:52 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.5220151033386327 on epoch=699
05/28/2022 10:21:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/28/2022 10:21:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/28/2022 10:22:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/28/2022 10:22:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/28/2022 10:22:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
05/28/2022 10:22:06 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.531546442687747 on epoch=712
05/28/2022 10:22:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 10:22:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/28/2022 10:22:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/28/2022 10:22:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/28/2022 10:22:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 10:22:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5856643356643356 on epoch=724
05/28/2022 10:22:19 - INFO - __main__ - Saving model with best Classification-F1: 0.565794499618029 -> 0.5856643356643356 on epoch=724, global_step=2900
05/28/2022 10:22:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 10:22:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/28/2022 10:22:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/28/2022 10:22:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=734
05/28/2022 10:22:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/28/2022 10:22:33 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.559671442687747 on epoch=737
05/28/2022 10:22:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/28/2022 10:22:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/28/2022 10:22:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 10:22:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 10:22:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 10:22:46 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5308956246456247 on epoch=749
05/28/2022 10:22:46 - INFO - __main__ - save last model!
05/28/2022 10:22:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:22:46 - INFO - __main__ - Printing 3 examples
05/28/2022 10:22:46 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:22:46 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:22:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 10:22:46 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 10:22:46 - INFO - __main__ - Printing 3 examples
05/28/2022 10:22:46 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:22:46 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:22:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:22:46 - INFO - __main__ - Printing 3 examples
05/28/2022 10:22:46 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:22:46 - INFO - __main__ - ['others']
05/28/2022 10:22:46 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:22:46 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:22:46 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:22:48 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:22:54 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 10:23:05 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:23:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:23:06 - INFO - __main__ - Starting training!
05/28/2022 10:24:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/28/2022 10:24:32 - INFO - __main__ - Classification-F1 on test data: 0.3698
05/28/2022 10:24:32 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.5856643356643356, test_performance=0.36979038194702885
05/28/2022 10:24:32 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/28/2022 10:24:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:24:33 - INFO - __main__ - Printing 3 examples
05/28/2022 10:24:33 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:24:33 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:24:33 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:24:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:24:33 - INFO - __main__ - Printing 3 examples
05/28/2022 10:24:33 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:24:33 - INFO - __main__ - ['others']
05/28/2022 10:24:33 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:24:33 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:24:33 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:24:52 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:24:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:24:52 - INFO - __main__ - Starting training!
05/28/2022 10:24:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.22 on epoch=2
05/28/2022 10:24:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.14 on epoch=4
05/28/2022 10:25:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.35 on epoch=7
05/28/2022 10:25:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=9
05/28/2022 10:25:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=12
05/28/2022 10:25:06 - INFO - __main__ - Global step 50 Train loss 1.93 Classification-F1 0.10126582278481013 on epoch=12
05/28/2022 10:25:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
05/28/2022 10:25:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.09 on epoch=14
05/28/2022 10:25:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=17
05/28/2022 10:25:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=19
05/28/2022 10:25:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=22
05/28/2022 10:25:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=24
05/28/2022 10:25:19 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.19267605633802815 on epoch=24
05/28/2022 10:25:19 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.19267605633802815 on epoch=24, global_step=100
05/28/2022 10:25:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=27
05/28/2022 10:25:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=29
05/28/2022 10:25:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=32
05/28/2022 10:25:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=34
05/28/2022 10:25:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.90 on epoch=37
05/28/2022 10:25:32 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.1 on epoch=37
05/28/2022 10:25:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.82 on epoch=39
05/28/2022 10:25:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=42
05/28/2022 10:25:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=44
05/28/2022 10:25:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/28/2022 10:25:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=49
05/28/2022 10:25:45 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.1 on epoch=49
05/28/2022 10:25:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=52
05/28/2022 10:25:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
05/28/2022 10:25:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=57
05/28/2022 10:25:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=59
05/28/2022 10:25:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=62
05/28/2022 10:25:57 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.10126582278481013 on epoch=62
05/28/2022 10:26:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.89 on epoch=64
05/28/2022 10:26:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=67
05/28/2022 10:26:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=69
05/28/2022 10:26:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=72
05/28/2022 10:26:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=74
05/28/2022 10:26:10 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.23529411764705882 on epoch=74
05/28/2022 10:26:10 - INFO - __main__ - Saving model with best Classification-F1: 0.19267605633802815 -> 0.23529411764705882 on epoch=74, global_step=300
05/28/2022 10:26:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=77
05/28/2022 10:26:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=79
05/28/2022 10:26:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=82
05/28/2022 10:26:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=84
05/28/2022 10:26:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.69 on epoch=87
05/28/2022 10:26:23 - INFO - __main__ - Global step 350 Train loss 0.70 Classification-F1 0.36830769230769234 on epoch=87
05/28/2022 10:26:23 - INFO - __main__ - Saving model with best Classification-F1: 0.23529411764705882 -> 0.36830769230769234 on epoch=87, global_step=350
05/28/2022 10:26:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=89
05/28/2022 10:26:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=92
05/28/2022 10:26:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=94
05/28/2022 10:26:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=97
05/28/2022 10:26:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=99
05/28/2022 10:26:36 - INFO - __main__ - Global step 400 Train loss 0.59 Classification-F1 0.343739837398374 on epoch=99
05/28/2022 10:26:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.57 on epoch=102
05/28/2022 10:26:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=104
05/28/2022 10:26:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=107
05/28/2022 10:26:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=109
05/28/2022 10:26:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=112
05/28/2022 10:26:49 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.4825174825174825 on epoch=112
05/28/2022 10:26:49 - INFO - __main__ - Saving model with best Classification-F1: 0.36830769230769234 -> 0.4825174825174825 on epoch=112, global_step=450
05/28/2022 10:26:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=114
05/28/2022 10:26:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=117
05/28/2022 10:26:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=119
05/28/2022 10:26:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=122
05/28/2022 10:27:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
05/28/2022 10:27:02 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.6000763941940412 on epoch=124
05/28/2022 10:27:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4825174825174825 -> 0.6000763941940412 on epoch=124, global_step=500
05/28/2022 10:27:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=127
05/28/2022 10:27:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=129
05/28/2022 10:27:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=132
05/28/2022 10:27:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
05/28/2022 10:27:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=137
05/28/2022 10:27:14 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.5826803694450753 on epoch=137
05/28/2022 10:27:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=139
05/28/2022 10:27:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=142
05/28/2022 10:27:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
05/28/2022 10:27:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=147
05/28/2022 10:27:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/28/2022 10:27:27 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.49334253508166553 on epoch=149
05/28/2022 10:27:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=152
05/28/2022 10:27:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=154
05/28/2022 10:27:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=157
05/28/2022 10:27:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=159
05/28/2022 10:27:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
05/28/2022 10:27:40 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.5866013071895425 on epoch=162
05/28/2022 10:27:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=164
05/28/2022 10:27:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=167
05/28/2022 10:27:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
05/28/2022 10:27:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=172
05/28/2022 10:27:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=174
05/28/2022 10:27:53 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.5145891690009337 on epoch=174
05/28/2022 10:27:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=177
05/28/2022 10:27:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=179
05/28/2022 10:28:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
05/28/2022 10:28:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=184
05/28/2022 10:28:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
05/28/2022 10:28:06 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.5734126984126985 on epoch=187
05/28/2022 10:28:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=189
05/28/2022 10:28:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=192
05/28/2022 10:28:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=194
05/28/2022 10:28:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=197
05/28/2022 10:28:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=199
05/28/2022 10:28:19 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.4981811145510836 on epoch=199
05/28/2022 10:28:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
05/28/2022 10:28:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=204
05/28/2022 10:28:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
05/28/2022 10:28:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=209
05/28/2022 10:28:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=212
05/28/2022 10:28:32 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.5480995475113122 on epoch=212
05/28/2022 10:28:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
05/28/2022 10:28:37 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=217
05/28/2022 10:28:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=219
05/28/2022 10:28:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=222
05/28/2022 10:28:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=224
05/28/2022 10:28:45 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.5547304982599099 on epoch=224
05/28/2022 10:28:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=227
05/28/2022 10:28:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=229
05/28/2022 10:28:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=232
05/28/2022 10:28:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=234
05/28/2022 10:28:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=237
05/28/2022 10:28:58 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.7341269841269842 on epoch=237
05/28/2022 10:28:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6000763941940412 -> 0.7341269841269842 on epoch=237, global_step=950
05/28/2022 10:29:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=239
05/28/2022 10:29:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=242
05/28/2022 10:29:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=244
05/28/2022 10:29:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=247
05/28/2022 10:29:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
05/28/2022 10:29:11 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7097447447447447 on epoch=249
05/28/2022 10:29:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=252
05/28/2022 10:29:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=254
05/28/2022 10:29:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
05/28/2022 10:29:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=259
05/28/2022 10:29:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=262
05/28/2022 10:29:24 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.6163152592012465 on epoch=262
05/28/2022 10:29:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
05/28/2022 10:29:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=267
05/28/2022 10:29:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/28/2022 10:29:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=272
05/28/2022 10:29:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=274
05/28/2022 10:29:37 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.5056063122923588 on epoch=274
05/28/2022 10:29:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=277
05/28/2022 10:29:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=279
05/28/2022 10:29:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/28/2022 10:29:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=284
05/28/2022 10:29:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
05/28/2022 10:29:51 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.6588023088023087 on epoch=287
05/28/2022 10:29:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=289
05/28/2022 10:29:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/28/2022 10:29:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=294
05/28/2022 10:30:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
05/28/2022 10:30:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
05/28/2022 10:30:04 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.6536133221617093 on epoch=299
05/28/2022 10:30:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/28/2022 10:30:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/28/2022 10:30:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/28/2022 10:30:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
05/28/2022 10:30:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
05/28/2022 10:30:17 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.651580459770115 on epoch=312
05/28/2022 10:30:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=314
05/28/2022 10:30:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=317
05/28/2022 10:30:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/28/2022 10:30:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/28/2022 10:30:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
05/28/2022 10:30:30 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.6992447516641065 on epoch=324
05/28/2022 10:30:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/28/2022 10:30:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/28/2022 10:30:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=332
05/28/2022 10:30:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/28/2022 10:30:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/28/2022 10:30:43 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.6153781512605042 on epoch=337
05/28/2022 10:30:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
05/28/2022 10:30:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/28/2022 10:30:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=344
05/28/2022 10:30:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/28/2022 10:30:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/28/2022 10:30:57 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6640188664382213 on epoch=349
05/28/2022 10:30:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=352
05/28/2022 10:31:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/28/2022 10:31:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
05/28/2022 10:31:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=359
05/28/2022 10:31:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/28/2022 10:31:10 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7362745098039215 on epoch=362
05/28/2022 10:31:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7341269841269842 -> 0.7362745098039215 on epoch=362, global_step=1450
05/28/2022 10:31:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/28/2022 10:31:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=367
05/28/2022 10:31:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/28/2022 10:31:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/28/2022 10:31:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/28/2022 10:31:24 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6730158730158731 on epoch=374
05/28/2022 10:31:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/28/2022 10:31:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=379
05/28/2022 10:31:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/28/2022 10:31:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/28/2022 10:31:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/28/2022 10:31:37 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.6369546582449808 on epoch=387
05/28/2022 10:31:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
05/28/2022 10:31:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/28/2022 10:31:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/28/2022 10:31:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/28/2022 10:31:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/28/2022 10:31:50 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7290806754221388 on epoch=399
05/28/2022 10:31:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/28/2022 10:31:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/28/2022 10:31:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/28/2022 10:32:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/28/2022 10:32:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 10:32:04 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6558862433862434 on epoch=412
05/28/2022 10:32:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
05/28/2022 10:32:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/28/2022 10:32:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=419
05/28/2022 10:32:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
05/28/2022 10:32:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/28/2022 10:32:17 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6964825356665963 on epoch=424
05/28/2022 10:32:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/28/2022 10:32:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=429
05/28/2022 10:32:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/28/2022 10:32:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/28/2022 10:32:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/28/2022 10:32:31 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6811305811305812 on epoch=437
05/28/2022 10:32:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/28/2022 10:32:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/28/2022 10:32:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/28/2022 10:32:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/28/2022 10:32:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/28/2022 10:32:44 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.666285403050109 on epoch=449
05/28/2022 10:32:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/28/2022 10:32:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/28/2022 10:32:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=457
05/28/2022 10:32:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 10:32:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/28/2022 10:32:57 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.724820528332325 on epoch=462
05/28/2022 10:33:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/28/2022 10:33:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/28/2022 10:33:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/28/2022 10:33:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/28/2022 10:33:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/28/2022 10:33:11 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6920337301587302 on epoch=474
05/28/2022 10:33:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/28/2022 10:33:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/28/2022 10:33:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/28/2022 10:33:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/28/2022 10:33:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 10:33:24 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6975353499547048 on epoch=487
05/28/2022 10:33:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/28/2022 10:33:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/28/2022 10:33:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/28/2022 10:33:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/28/2022 10:33:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/28/2022 10:33:38 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.663564675035754 on epoch=499
05/28/2022 10:33:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/28/2022 10:33:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
05/28/2022 10:33:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/28/2022 10:33:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 10:33:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 10:33:51 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6316197316197316 on epoch=512
05/28/2022 10:33:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 10:33:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 10:33:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 10:34:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/28/2022 10:34:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/28/2022 10:34:05 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6067749385541147 on epoch=524
05/28/2022 10:34:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
05/28/2022 10:34:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/28/2022 10:34:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
05/28/2022 10:34:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/28/2022 10:34:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 10:34:18 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.6569876512668273 on epoch=537
05/28/2022 10:34:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/28/2022 10:34:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/28/2022 10:34:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/28/2022 10:34:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 10:34:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/28/2022 10:34:32 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7130781799899447 on epoch=549
05/28/2022 10:34:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/28/2022 10:34:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/28/2022 10:34:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/28/2022 10:34:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 10:34:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 10:34:45 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7486764193660744 on epoch=562
05/28/2022 10:34:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7362745098039215 -> 0.7486764193660744 on epoch=562, global_step=2250
05/28/2022 10:34:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 10:34:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/28/2022 10:34:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 10:34:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 10:34:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/28/2022 10:34:59 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7158613445378151 on epoch=574
05/28/2022 10:35:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 10:35:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 10:35:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 10:35:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 10:35:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/28/2022 10:35:12 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7125550919668566 on epoch=587
05/28/2022 10:35:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 10:35:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 10:35:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 10:35:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 10:35:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/28/2022 10:35:26 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6978492096139155 on epoch=599
05/28/2022 10:35:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/28/2022 10:35:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/28/2022 10:35:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/28/2022 10:35:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/28/2022 10:35:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/28/2022 10:35:39 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7308982683982684 on epoch=612
05/28/2022 10:35:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 10:35:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 10:35:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=619
05/28/2022 10:35:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 10:35:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/28/2022 10:35:52 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7163678597134071 on epoch=624
05/28/2022 10:35:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/28/2022 10:35:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 10:36:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/28/2022 10:36:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/28/2022 10:36:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 10:36:06 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7312253139839346 on epoch=637
05/28/2022 10:36:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/28/2022 10:36:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/28/2022 10:36:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/28/2022 10:36:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 10:36:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/28/2022 10:36:19 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6851425601425601 on epoch=649
05/28/2022 10:36:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 10:36:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 10:36:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 10:36:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 10:36:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/28/2022 10:36:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7158349572142675 on epoch=662
05/28/2022 10:36:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 10:36:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/28/2022 10:36:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/28/2022 10:36:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 10:36:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/28/2022 10:36:46 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7163515406162465 on epoch=674
05/28/2022 10:36:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/28/2022 10:36:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/28/2022 10:36:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 10:36:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/28/2022 10:36:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=687
05/28/2022 10:36:59 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6985012395762903 on epoch=687
05/28/2022 10:37:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 10:37:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 10:37:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 10:37:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 10:37:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 10:37:13 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6852743427287038 on epoch=699
05/28/2022 10:37:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/28/2022 10:37:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
05/28/2022 10:37:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/28/2022 10:37:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/28/2022 10:37:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 10:37:26 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7032467532467532 on epoch=712
05/28/2022 10:37:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 10:37:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
05/28/2022 10:37:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 10:37:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 10:37:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/28/2022 10:37:40 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6874034749034749 on epoch=724
05/28/2022 10:37:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 10:37:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 10:37:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 10:37:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 10:37:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/28/2022 10:37:53 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6847027069937287 on epoch=737
05/28/2022 10:37:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/28/2022 10:37:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 10:38:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/28/2022 10:38:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/28/2022 10:38:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 10:38:07 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7479155885227992 on epoch=749
05/28/2022 10:38:07 - INFO - __main__ - save last model!
05/28/2022 10:38:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 10:38:07 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 10:38:07 - INFO - __main__ - Printing 3 examples
05/28/2022 10:38:07 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:38:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:38:07 - INFO - __main__ - Printing 3 examples
05/28/2022 10:38:07 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:38:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:38:07 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:38:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:38:07 - INFO - __main__ - Printing 3 examples
05/28/2022 10:38:07 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:38:07 - INFO - __main__ - ['others']
05/28/2022 10:38:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:38:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:38:07 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:38:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:38:14 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 10:38:22 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:38:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:38:23 - INFO - __main__ - Starting training!
05/28/2022 10:39:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/28/2022 10:39:49 - INFO - __main__ - Classification-F1 on test data: 0.3735
05/28/2022 10:39:49 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.7486764193660744, test_performance=0.3735151679114044
05/28/2022 10:39:49 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/28/2022 10:39:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:39:50 - INFO - __main__ - Printing 3 examples
05/28/2022 10:39:50 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:39:50 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:39:50 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:39:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:39:50 - INFO - __main__ - Printing 3 examples
05/28/2022 10:39:50 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:39:50 - INFO - __main__ - ['others']
05/28/2022 10:39:50 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:39:50 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:39:50 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:40:05 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:40:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:40:06 - INFO - __main__ - Starting training!
05/28/2022 10:40:09 - INFO - __main__ - Step 10 Global step 10 Train loss 4.54 on epoch=2
05/28/2022 10:40:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.26 on epoch=4
05/28/2022 10:40:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.38 on epoch=7
05/28/2022 10:40:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=9
05/28/2022 10:40:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=12
05/28/2022 10:40:20 - INFO - __main__ - Global step 50 Train loss 2.05 Classification-F1 0.18500948766603414 on epoch=12
05/28/2022 10:40:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18500948766603414 on epoch=12, global_step=50
05/28/2022 10:40:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=14
05/28/2022 10:40:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.87 on epoch=17
05/28/2022 10:40:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=19
05/28/2022 10:40:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=22
05/28/2022 10:40:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/28/2022 10:40:33 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.12368421052631579 on epoch=24
05/28/2022 10:40:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.82 on epoch=27
05/28/2022 10:40:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/28/2022 10:40:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.87 on epoch=32
05/28/2022 10:40:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=34
05/28/2022 10:40:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=37
05/28/2022 10:40:46 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.17148157487122223 on epoch=37
05/28/2022 10:40:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.92 on epoch=39
05/28/2022 10:40:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=42
05/28/2022 10:40:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=44
05/28/2022 10:40:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.83 on epoch=47
05/28/2022 10:40:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=49
05/28/2022 10:40:59 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.1 on epoch=49
05/28/2022 10:41:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=52
05/28/2022 10:41:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.70 on epoch=54
05/28/2022 10:41:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.69 on epoch=57
05/28/2022 10:41:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=59
05/28/2022 10:41:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=62
05/28/2022 10:41:12 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.2889655172413793 on epoch=62
05/28/2022 10:41:12 - INFO - __main__ - Saving model with best Classification-F1: 0.18500948766603414 -> 0.2889655172413793 on epoch=62, global_step=250
05/28/2022 10:41:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=64
05/28/2022 10:41:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=67
05/28/2022 10:41:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=69
05/28/2022 10:41:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=72
05/28/2022 10:41:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=74
05/28/2022 10:41:26 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.20894844424256187 on epoch=74
05/28/2022 10:41:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=77
05/28/2022 10:41:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=79
05/28/2022 10:41:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=82
05/28/2022 10:41:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=84
05/28/2022 10:41:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=87
05/28/2022 10:41:39 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.33840326340326343 on epoch=87
05/28/2022 10:41:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2889655172413793 -> 0.33840326340326343 on epoch=87, global_step=350
05/28/2022 10:41:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=89
05/28/2022 10:41:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=92
05/28/2022 10:41:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=94
05/28/2022 10:41:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=97
05/28/2022 10:41:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=99
05/28/2022 10:41:52 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.36944444444444446 on epoch=99
05/28/2022 10:41:52 - INFO - __main__ - Saving model with best Classification-F1: 0.33840326340326343 -> 0.36944444444444446 on epoch=99, global_step=400
05/28/2022 10:41:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=102
05/28/2022 10:41:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=104
05/28/2022 10:41:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=107
05/28/2022 10:42:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=109
05/28/2022 10:42:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/28/2022 10:42:05 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.39630681818181823 on epoch=112
05/28/2022 10:42:05 - INFO - __main__ - Saving model with best Classification-F1: 0.36944444444444446 -> 0.39630681818181823 on epoch=112, global_step=450
05/28/2022 10:42:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=114
05/28/2022 10:42:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=117
05/28/2022 10:42:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=119
05/28/2022 10:42:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=122
05/28/2022 10:42:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=124
05/28/2022 10:42:18 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.4407457729468599 on epoch=124
05/28/2022 10:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.39630681818181823 -> 0.4407457729468599 on epoch=124, global_step=500
05/28/2022 10:42:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=127
05/28/2022 10:42:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=129
05/28/2022 10:42:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=132
05/28/2022 10:42:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
05/28/2022 10:42:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=137
05/28/2022 10:42:31 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.33545171045171046 on epoch=137
05/28/2022 10:42:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=139
05/28/2022 10:42:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
05/28/2022 10:42:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
05/28/2022 10:42:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=147
05/28/2022 10:42:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=149
05/28/2022 10:42:44 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.36444452220977974 on epoch=149
05/28/2022 10:42:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
05/28/2022 10:42:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/28/2022 10:42:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
05/28/2022 10:42:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
05/28/2022 10:42:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
05/28/2022 10:42:58 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.44003710575139154 on epoch=162
05/28/2022 10:43:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/28/2022 10:43:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=167
05/28/2022 10:43:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=169
05/28/2022 10:43:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=172
05/28/2022 10:43:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/28/2022 10:43:11 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.4110965045336569 on epoch=174
05/28/2022 10:43:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
05/28/2022 10:43:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=179
05/28/2022 10:43:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=182
05/28/2022 10:43:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=184
05/28/2022 10:43:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=187
05/28/2022 10:43:24 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.3817789481383952 on epoch=187
05/28/2022 10:43:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
05/28/2022 10:43:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=192
05/28/2022 10:43:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
05/28/2022 10:43:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=197
05/28/2022 10:43:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
05/28/2022 10:43:37 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.45416304452815615 on epoch=199
05/28/2022 10:43:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4407457729468599 -> 0.45416304452815615 on epoch=199, global_step=800
05/28/2022 10:43:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/28/2022 10:43:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.12 on epoch=204
05/28/2022 10:43:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
05/28/2022 10:43:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
05/28/2022 10:43:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/28/2022 10:43:50 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.3890863689776733 on epoch=212
05/28/2022 10:43:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/28/2022 10:43:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
05/28/2022 10:43:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=219
05/28/2022 10:44:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/28/2022 10:44:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
05/28/2022 10:44:03 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.5040481577246283 on epoch=224
05/28/2022 10:44:03 - INFO - __main__ - Saving model with best Classification-F1: 0.45416304452815615 -> 0.5040481577246283 on epoch=224, global_step=900
05/28/2022 10:44:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/28/2022 10:44:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
05/28/2022 10:44:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
05/28/2022 10:44:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/28/2022 10:44:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
05/28/2022 10:44:16 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.49978991596638656 on epoch=237
05/28/2022 10:44:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
05/28/2022 10:44:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
05/28/2022 10:44:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
05/28/2022 10:44:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/28/2022 10:44:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/28/2022 10:44:30 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.4967375052764047 on epoch=249
05/28/2022 10:44:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/28/2022 10:44:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/28/2022 10:44:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
05/28/2022 10:44:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
05/28/2022 10:44:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/28/2022 10:44:43 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.5938899063899064 on epoch=262
05/28/2022 10:44:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5040481577246283 -> 0.5938899063899064 on epoch=262, global_step=1050
05/28/2022 10:44:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/28/2022 10:44:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
05/28/2022 10:44:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/28/2022 10:44:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/28/2022 10:44:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/28/2022 10:44:56 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6340983840983841 on epoch=274
05/28/2022 10:44:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5938899063899064 -> 0.6340983840983841 on epoch=274, global_step=1100
05/28/2022 10:44:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/28/2022 10:45:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
05/28/2022 10:45:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/28/2022 10:45:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=284
05/28/2022 10:45:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
05/28/2022 10:45:09 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5672068829963566 on epoch=287
05/28/2022 10:45:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/28/2022 10:45:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 10:45:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/28/2022 10:45:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/28/2022 10:45:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=299
05/28/2022 10:45:22 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5281439393939393 on epoch=299
05/28/2022 10:45:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/28/2022 10:45:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/28/2022 10:45:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/28/2022 10:45:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/28/2022 10:45:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/28/2022 10:45:35 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.5640257904963788 on epoch=312
05/28/2022 10:45:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/28/2022 10:45:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/28/2022 10:45:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/28/2022 10:45:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/28/2022 10:45:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
05/28/2022 10:45:48 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.603490990990991 on epoch=324
05/28/2022 10:45:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/28/2022 10:45:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/28/2022 10:45:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
05/28/2022 10:45:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/28/2022 10:46:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/28/2022 10:46:02 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6041702993315896 on epoch=337
05/28/2022 10:46:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/28/2022 10:46:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/28/2022 10:46:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 10:46:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/28/2022 10:46:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=349
05/28/2022 10:46:15 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.567857142857143 on epoch=349
05/28/2022 10:46:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 10:46:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/28/2022 10:46:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/28/2022 10:46:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=359
05/28/2022 10:46:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/28/2022 10:46:28 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5902319902319902 on epoch=362
05/28/2022 10:46:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/28/2022 10:46:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/28/2022 10:46:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/28/2022 10:46:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/28/2022 10:46:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 10:46:41 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5167589516426726 on epoch=374
05/28/2022 10:46:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/28/2022 10:46:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/28/2022 10:46:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/28/2022 10:46:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/28/2022 10:46:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/28/2022 10:46:54 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.4924970963995355 on epoch=387
05/28/2022 10:46:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/28/2022 10:46:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/28/2022 10:47:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/28/2022 10:47:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/28/2022 10:47:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/28/2022 10:47:08 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6058424048736643 on epoch=399
05/28/2022 10:47:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 10:47:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=404
05/28/2022 10:47:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/28/2022 10:47:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/28/2022 10:47:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/28/2022 10:47:21 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.650554151888624 on epoch=412
05/28/2022 10:47:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6340983840983841 -> 0.650554151888624 on epoch=412, global_step=1650
05/28/2022 10:47:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 10:47:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/28/2022 10:47:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/28/2022 10:47:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/28/2022 10:47:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 10:47:34 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6353831154477707 on epoch=424
05/28/2022 10:47:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/28/2022 10:47:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 10:47:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/28/2022 10:47:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/28/2022 10:47:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/28/2022 10:47:47 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.618292339844064 on epoch=437
05/28/2022 10:47:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 10:47:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
05/28/2022 10:47:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/28/2022 10:47:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/28/2022 10:47:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/28/2022 10:48:00 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6709000186822767 on epoch=449
05/28/2022 10:48:00 - INFO - __main__ - Saving model with best Classification-F1: 0.650554151888624 -> 0.6709000186822767 on epoch=449, global_step=1800
05/28/2022 10:48:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/28/2022 10:48:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 10:48:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/28/2022 10:48:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 10:48:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/28/2022 10:48:13 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.653335074886799 on epoch=462
05/28/2022 10:48:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/28/2022 10:48:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 10:48:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
05/28/2022 10:48:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/28/2022 10:48:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/28/2022 10:48:27 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5867201989221624 on epoch=474
05/28/2022 10:48:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 10:48:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 10:48:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/28/2022 10:48:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 10:48:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 10:48:40 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.596281739997711 on epoch=487
05/28/2022 10:48:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/28/2022 10:48:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 10:48:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 10:48:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/28/2022 10:48:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/28/2022 10:48:53 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6083723817533742 on epoch=499
05/28/2022 10:48:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/28/2022 10:48:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/28/2022 10:49:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 10:49:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/28/2022 10:49:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=512
05/28/2022 10:49:06 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6216371591371591 on epoch=512
05/28/2022 10:49:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/28/2022 10:49:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 10:49:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 10:49:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 10:49:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 10:49:20 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6505291005291005 on epoch=524
05/28/2022 10:49:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 10:49:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/28/2022 10:49:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 10:49:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 10:49:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 10:49:33 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.5876582464817759 on epoch=537
05/28/2022 10:49:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 10:49:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/28/2022 10:49:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/28/2022 10:49:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/28/2022 10:49:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 10:49:46 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.5584584781515728 on epoch=549
05/28/2022 10:49:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 10:49:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 10:49:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 10:49:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 10:49:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 10:49:59 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5852528649403731 on epoch=562
05/28/2022 10:50:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
05/28/2022 10:50:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 10:50:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 10:50:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 10:50:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
05/28/2022 10:50:12 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6383415435139573 on epoch=574
05/28/2022 10:50:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 10:50:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 10:50:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 10:50:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 10:50:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/28/2022 10:50:26 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.606365934679719 on epoch=587
05/28/2022 10:50:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 10:50:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 10:50:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/28/2022 10:50:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/28/2022 10:50:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 10:50:39 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6050246050246051 on epoch=599
05/28/2022 10:50:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 10:50:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 10:50:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 10:50:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/28/2022 10:50:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 10:50:52 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6000891265597148 on epoch=612
05/28/2022 10:50:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 10:50:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 10:51:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 10:51:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 10:51:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/28/2022 10:51:05 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.5297888386123681 on epoch=624
05/28/2022 10:51:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 10:51:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 10:51:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/28/2022 10:51:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 10:51:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 10:51:19 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6178404855297308 on epoch=637
05/28/2022 10:51:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 10:51:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 10:51:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/28/2022 10:51:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 10:51:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 10:51:32 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6155712362608914 on epoch=649
05/28/2022 10:51:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 10:51:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 10:51:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 10:51:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 10:51:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 10:51:45 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6141414141414142 on epoch=662
05/28/2022 10:51:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 10:51:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 10:51:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 10:51:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 10:51:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 10:51:58 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6729792429792429 on epoch=674
05/28/2022 10:51:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6709000186822767 -> 0.6729792429792429 on epoch=674, global_step=2700
05/28/2022 10:52:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 10:52:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 10:52:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 10:52:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 10:52:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 10:52:11 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6016281512605043 on epoch=687
05/28/2022 10:52:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/28/2022 10:52:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=692
05/28/2022 10:52:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 10:52:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/28/2022 10:52:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 10:52:25 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.5523515106636318 on epoch=699
05/28/2022 10:52:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 10:52:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 10:52:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 10:52:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 10:52:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 10:52:38 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6028454116689411 on epoch=712
05/28/2022 10:52:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 10:52:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 10:52:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 10:52:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 10:52:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 10:52:51 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6196969696969697 on epoch=724
05/28/2022 10:52:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 10:52:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 10:52:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 10:53:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/28/2022 10:53:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 10:53:04 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.649043829733485 on epoch=737
05/28/2022 10:53:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 10:53:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 10:53:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 10:53:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 10:53:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 10:53:18 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6201121794871794 on epoch=749
05/28/2022 10:53:18 - INFO - __main__ - save last model!
05/28/2022 10:53:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 10:53:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 10:53:18 - INFO - __main__ - Printing 3 examples
05/28/2022 10:53:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:53:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:53:18 - INFO - __main__ - Printing 3 examples
05/28/2022 10:53:18 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:53:18 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:53:18 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:53:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:53:18 - INFO - __main__ - Printing 3 examples
05/28/2022 10:53:18 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:53:18 - INFO - __main__ - ['others']
05/28/2022 10:53:18 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:53:18 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:53:18 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:53:20 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:53:25 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 10:53:33 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:53:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:53:34 - INFO - __main__ - Starting training!
05/28/2022 10:54:45 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/28/2022 10:54:45 - INFO - __main__ - Classification-F1 on test data: 0.2750
05/28/2022 10:54:46 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.6729792429792429, test_performance=0.2749624084215707
05/28/2022 10:54:46 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/28/2022 10:54:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:54:47 - INFO - __main__ - Printing 3 examples
05/28/2022 10:54:47 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:54:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:54:47 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 10:54:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 10:54:47 - INFO - __main__ - Printing 3 examples
05/28/2022 10:54:47 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 10:54:47 - INFO - __main__ - ['others']
05/28/2022 10:54:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 10:54:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 10:54:47 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 10:55:05 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 10:55:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 10:55:06 - INFO - __main__ - Starting training!
05/28/2022 10:55:09 - INFO - __main__ - Step 10 Global step 10 Train loss 4.87 on epoch=2
05/28/2022 10:55:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.91 on epoch=4
05/28/2022 10:55:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.74 on epoch=7
05/28/2022 10:55:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=9
05/28/2022 10:55:19 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=12
05/28/2022 10:55:20 - INFO - __main__ - Global step 50 Train loss 2.35 Classification-F1 0.11078022632519356 on epoch=12
05/28/2022 10:55:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11078022632519356 on epoch=12, global_step=50
05/28/2022 10:55:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
05/28/2022 10:55:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
05/28/2022 10:55:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=19
05/28/2022 10:55:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=22
05/28/2022 10:55:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=24
05/28/2022 10:55:33 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.1 on epoch=24
05/28/2022 10:55:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
05/28/2022 10:55:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=29
05/28/2022 10:55:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
05/28/2022 10:55:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 10:55:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=37
05/28/2022 10:55:46 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.1 on epoch=37
05/28/2022 10:55:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=39
05/28/2022 10:55:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
05/28/2022 10:55:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=44
05/28/2022 10:55:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=47
05/28/2022 10:55:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=49
05/28/2022 10:55:59 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.1 on epoch=49
05/28/2022 10:56:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
05/28/2022 10:56:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=54
05/28/2022 10:56:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=57
05/28/2022 10:56:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=59
05/28/2022 10:56:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=62
05/28/2022 10:56:12 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.1 on epoch=62
05/28/2022 10:56:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=64
05/28/2022 10:56:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.79 on epoch=67
05/28/2022 10:56:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=69
05/28/2022 10:56:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=72
05/28/2022 10:56:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.83 on epoch=74
05/28/2022 10:56:25 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.1565276828434723 on epoch=74
05/28/2022 10:56:25 - INFO - __main__ - Saving model with best Classification-F1: 0.11078022632519356 -> 0.1565276828434723 on epoch=74, global_step=300
05/28/2022 10:56:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=77
05/28/2022 10:56:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=79
05/28/2022 10:56:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=82
05/28/2022 10:56:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.76 on epoch=84
05/28/2022 10:56:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
05/28/2022 10:56:37 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.24371980676328503 on epoch=87
05/28/2022 10:56:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1565276828434723 -> 0.24371980676328503 on epoch=87, global_step=350
05/28/2022 10:56:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=89
05/28/2022 10:56:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=92
05/28/2022 10:56:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=94
05/28/2022 10:56:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.74 on epoch=97
05/28/2022 10:56:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
05/28/2022 10:56:50 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.2327579680520857 on epoch=99
05/28/2022 10:56:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.67 on epoch=102
05/28/2022 10:56:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=104
05/28/2022 10:56:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
05/28/2022 10:57:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=109
05/28/2022 10:57:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=112
05/28/2022 10:57:03 - INFO - __main__ - Global step 450 Train loss 0.67 Classification-F1 0.44137362637362637 on epoch=112
05/28/2022 10:57:03 - INFO - __main__ - Saving model with best Classification-F1: 0.24371980676328503 -> 0.44137362637362637 on epoch=112, global_step=450
05/28/2022 10:57:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=114
05/28/2022 10:57:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=117
05/28/2022 10:57:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.66 on epoch=119
05/28/2022 10:57:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=122
05/28/2022 10:57:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.57 on epoch=124
05/28/2022 10:57:16 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.4377016535591853 on epoch=124
05/28/2022 10:57:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
05/28/2022 10:57:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=129
05/28/2022 10:57:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=132
05/28/2022 10:57:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=134
05/28/2022 10:57:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=137
05/28/2022 10:57:29 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.40774395882755016 on epoch=137
05/28/2022 10:57:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=139
05/28/2022 10:57:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=142
05/28/2022 10:57:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=144
05/28/2022 10:57:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=147
05/28/2022 10:57:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=149
05/28/2022 10:57:42 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.5484848484848485 on epoch=149
05/28/2022 10:57:42 - INFO - __main__ - Saving model with best Classification-F1: 0.44137362637362637 -> 0.5484848484848485 on epoch=149, global_step=600
05/28/2022 10:57:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=152
05/28/2022 10:57:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=154
05/28/2022 10:57:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=157
05/28/2022 10:57:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=159
05/28/2022 10:57:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=162
05/28/2022 10:57:55 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.6032323629687909 on epoch=162
05/28/2022 10:57:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5484848484848485 -> 0.6032323629687909 on epoch=162, global_step=650
05/28/2022 10:57:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=164
05/28/2022 10:58:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=167
05/28/2022 10:58:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=169
05/28/2022 10:58:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=172
05/28/2022 10:58:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=174
05/28/2022 10:58:08 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.5132676518883416 on epoch=174
05/28/2022 10:58:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=177
05/28/2022 10:58:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=179
05/28/2022 10:58:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=182
05/28/2022 10:58:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
05/28/2022 10:58:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=187
05/28/2022 10:58:21 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.6101712928232057 on epoch=187
05/28/2022 10:58:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6032323629687909 -> 0.6101712928232057 on epoch=187, global_step=750
05/28/2022 10:58:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
05/28/2022 10:58:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=192
05/28/2022 10:58:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/28/2022 10:58:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
05/28/2022 10:58:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/28/2022 10:58:33 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.5075443602076031 on epoch=199
05/28/2022 10:58:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=202
05/28/2022 10:58:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=204
05/28/2022 10:58:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
05/28/2022 10:58:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=209
05/28/2022 10:58:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=212
05/28/2022 10:58:46 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.6432242326638878 on epoch=212
05/28/2022 10:58:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6101712928232057 -> 0.6432242326638878 on epoch=212, global_step=850
05/28/2022 10:58:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=214
05/28/2022 10:58:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
05/28/2022 10:58:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=219
05/28/2022 10:58:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=222
05/28/2022 10:58:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/28/2022 10:58:59 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.45671683389074696 on epoch=224
05/28/2022 10:59:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.13 on epoch=227
05/28/2022 10:59:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
05/28/2022 10:59:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=232
05/28/2022 10:59:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
05/28/2022 10:59:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
05/28/2022 10:59:12 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.6348810561713788 on epoch=237
05/28/2022 10:59:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
05/28/2022 10:59:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=242
05/28/2022 10:59:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=244
05/28/2022 10:59:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=247
05/28/2022 10:59:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/28/2022 10:59:25 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.6395815170008718 on epoch=249
05/28/2022 10:59:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=252
05/28/2022 10:59:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/28/2022 10:59:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/28/2022 10:59:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
05/28/2022 10:59:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=262
05/28/2022 10:59:38 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.5595955222957512 on epoch=262
05/28/2022 10:59:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
05/28/2022 10:59:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
05/28/2022 10:59:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/28/2022 10:59:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/28/2022 10:59:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=274
05/28/2022 10:59:51 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.6419191919191919 on epoch=274
05/28/2022 10:59:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/28/2022 10:59:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/28/2022 10:59:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
05/28/2022 11:00:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/28/2022 11:00:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/28/2022 11:00:04 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6089138372773305 on epoch=287
05/28/2022 11:00:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/28/2022 11:00:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 11:00:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
05/28/2022 11:00:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/28/2022 11:00:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/28/2022 11:00:17 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.6541364127571024 on epoch=299
05/28/2022 11:00:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6432242326638878 -> 0.6541364127571024 on epoch=299, global_step=1200
05/28/2022 11:00:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/28/2022 11:00:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=304
05/28/2022 11:00:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 11:00:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/28/2022 11:00:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/28/2022 11:00:30 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.6009497653051035 on epoch=312
05/28/2022 11:00:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=314
05/28/2022 11:00:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/28/2022 11:00:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/28/2022 11:00:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/28/2022 11:00:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/28/2022 11:00:43 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6678414786967418 on epoch=324
05/28/2022 11:00:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6541364127571024 -> 0.6678414786967418 on epoch=324, global_step=1300
05/28/2022 11:00:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/28/2022 11:00:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/28/2022 11:00:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=332
05/28/2022 11:00:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/28/2022 11:00:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/28/2022 11:00:56 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6697232947232946 on epoch=337
05/28/2022 11:00:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6678414786967418 -> 0.6697232947232946 on epoch=337, global_step=1350
05/28/2022 11:00:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/28/2022 11:01:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/28/2022 11:01:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/28/2022 11:01:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/28/2022 11:01:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/28/2022 11:01:09 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6391218430692114 on epoch=349
05/28/2022 11:01:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
05/28/2022 11:01:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/28/2022 11:01:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/28/2022 11:01:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
05/28/2022 11:01:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/28/2022 11:01:22 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6024305555555556 on epoch=362
05/28/2022 11:01:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/28/2022 11:01:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 11:01:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/28/2022 11:01:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/28/2022 11:01:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 11:01:36 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.673611111111111 on epoch=374
05/28/2022 11:01:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6697232947232946 -> 0.673611111111111 on epoch=374, global_step=1500
05/28/2022 11:01:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/28/2022 11:01:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
05/28/2022 11:01:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/28/2022 11:01:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/28/2022 11:01:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/28/2022 11:01:49 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6705627705627706 on epoch=387
05/28/2022 11:01:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/28/2022 11:01:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/28/2022 11:01:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 11:01:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/28/2022 11:02:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/28/2022 11:02:02 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6079065246788371 on epoch=399
05/28/2022 11:02:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 11:02:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
05/28/2022 11:02:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/28/2022 11:02:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/28/2022 11:02:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/28/2022 11:02:15 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6101402046229633 on epoch=412
05/28/2022 11:02:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 11:02:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/28/2022 11:02:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/28/2022 11:02:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 11:02:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/28/2022 11:02:29 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6204471390678288 on epoch=424
05/28/2022 11:02:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
05/28/2022 11:02:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/28/2022 11:02:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/28/2022 11:02:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
05/28/2022 11:02:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/28/2022 11:02:42 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.5640881931204512 on epoch=437
05/28/2022 11:02:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=439
05/28/2022 11:02:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/28/2022 11:02:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/28/2022 11:02:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/28/2022 11:02:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 11:02:56 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6184488370246884 on epoch=449
05/28/2022 11:02:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/28/2022 11:03:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/28/2022 11:03:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/28/2022 11:03:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 11:03:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/28/2022 11:03:09 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6184534534534535 on epoch=462
05/28/2022 11:03:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/28/2022 11:03:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 11:03:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/28/2022 11:03:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/28/2022 11:03:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 11:03:23 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.651376424933301 on epoch=474
05/28/2022 11:03:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=477
05/28/2022 11:03:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 11:03:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 11:03:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 11:03:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 11:03:36 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6170700111876583 on epoch=487
05/28/2022 11:03:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/28/2022 11:03:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 11:03:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 11:03:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/28/2022 11:03:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/28/2022 11:03:49 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6686507936507936 on epoch=499
05/28/2022 11:03:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 11:03:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/28/2022 11:03:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 11:03:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 11:04:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 11:04:03 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6688932919293041 on epoch=512
05/28/2022 11:04:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/28/2022 11:04:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 11:04:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 11:04:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 11:04:15 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 11:04:16 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.5530982905982906 on epoch=524
05/28/2022 11:04:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 11:04:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
05/28/2022 11:04:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/28/2022 11:04:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 11:04:28 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 11:04:29 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6396626984126984 on epoch=537
05/28/2022 11:04:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 11:04:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/28/2022 11:04:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/28/2022 11:04:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/28/2022 11:04:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 11:04:43 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6221397434300661 on epoch=549
05/28/2022 11:04:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 11:04:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
05/28/2022 11:04:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 11:04:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 11:04:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 11:04:56 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6705627705627706 on epoch=562
05/28/2022 11:04:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 11:05:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 11:05:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 11:05:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/28/2022 11:05:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
05/28/2022 11:05:09 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6131449973773931 on epoch=574
05/28/2022 11:05:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/28/2022 11:05:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 11:05:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 11:05:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 11:05:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/28/2022 11:05:22 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6255092946269417 on epoch=587
05/28/2022 11:05:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 11:05:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/28/2022 11:05:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 11:05:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 11:05:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/28/2022 11:05:36 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.641991341991342 on epoch=599
05/28/2022 11:05:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/28/2022 11:05:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=604
05/28/2022 11:05:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 11:05:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 11:05:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 11:05:49 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6393353097207053 on epoch=612
05/28/2022 11:05:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 11:05:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 11:05:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 11:05:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 11:06:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/28/2022 11:06:02 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6444680045729824 on epoch=624
05/28/2022 11:06:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 11:06:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 11:06:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=632
05/28/2022 11:06:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 11:06:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/28/2022 11:06:16 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.644154680445003 on epoch=637
05/28/2022 11:06:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 11:06:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/28/2022 11:06:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 11:06:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/28/2022 11:06:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 11:06:29 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6207611832611832 on epoch=649
05/28/2022 11:06:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 11:06:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 11:06:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 11:06:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 11:06:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 11:06:42 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6087693141772839 on epoch=662
05/28/2022 11:06:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 11:06:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/28/2022 11:06:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/28/2022 11:06:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 11:06:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 11:06:56 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.609375 on epoch=674
05/28/2022 11:06:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/28/2022 11:07:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/28/2022 11:07:03 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 11:07:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 11:07:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/28/2022 11:07:09 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6207347670250897 on epoch=687
05/28/2022 11:07:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 11:07:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 11:07:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/28/2022 11:07:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/28/2022 11:07:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 11:07:23 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6593569574659675 on epoch=699
05/28/2022 11:07:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 11:07:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 11:07:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 11:07:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 11:07:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 11:07:36 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5774628639334523 on epoch=712
05/28/2022 11:07:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 11:07:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 11:07:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 11:07:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/28/2022 11:07:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 11:07:49 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6136712749615976 on epoch=724
05/28/2022 11:07:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 11:07:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
05/28/2022 11:07:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 11:07:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 11:08:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 11:08:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6242688683865154 on epoch=737
05/28/2022 11:08:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/28/2022 11:08:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 11:08:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 11:08:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/28/2022 11:08:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 11:08:16 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6097285067873304 on epoch=749
05/28/2022 11:08:16 - INFO - __main__ - save last model!
05/28/2022 11:08:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 11:08:16 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 11:08:16 - INFO - __main__ - Printing 3 examples
05/28/2022 11:08:16 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:08:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:08:16 - INFO - __main__ - Printing 3 examples
05/28/2022 11:08:16 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:08:16 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:08:16 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:08:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:08:16 - INFO - __main__ - Printing 3 examples
05/28/2022 11:08:16 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 11:08:16 - INFO - __main__ - ['others']
05/28/2022 11:08:16 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:08:16 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:08:16 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:08:18 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:08:24 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 11:08:35 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:08:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:08:36 - INFO - __main__ - Starting training!
05/28/2022 11:09:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/28/2022 11:09:56 - INFO - __main__ - Classification-F1 on test data: 0.3440
05/28/2022 11:09:56 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.673611111111111, test_performance=0.3439975115072997
05/28/2022 11:09:56 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/28/2022 11:09:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:09:57 - INFO - __main__ - Printing 3 examples
05/28/2022 11:09:57 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:09:57 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:09:57 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:09:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:09:57 - INFO - __main__ - Printing 3 examples
05/28/2022 11:09:57 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/28/2022 11:09:57 - INFO - __main__ - ['others']
05/28/2022 11:09:57 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:09:57 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:09:58 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:10:16 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:10:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:10:17 - INFO - __main__ - Starting training!
05/28/2022 11:10:20 - INFO - __main__ - Step 10 Global step 10 Train loss 5.19 on epoch=2
05/28/2022 11:10:22 - INFO - __main__ - Step 20 Global step 20 Train loss 3.71 on epoch=4
05/28/2022 11:10:25 - INFO - __main__ - Step 30 Global step 30 Train loss 2.81 on epoch=7
05/28/2022 11:10:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.76 on epoch=9
05/28/2022 11:10:30 - INFO - __main__ - Step 50 Global step 50 Train loss 1.43 on epoch=12
05/28/2022 11:10:30 - INFO - __main__ - Global step 50 Train loss 2.98 Classification-F1 0.1 on epoch=12
05/28/2022 11:10:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 11:10:33 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=14
05/28/2022 11:10:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=17
05/28/2022 11:10:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=19
05/28/2022 11:10:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=22
05/28/2022 11:10:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=24
05/28/2022 11:10:43 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.1468058968058968 on epoch=24
05/28/2022 11:10:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1468058968058968 on epoch=24, global_step=100
05/28/2022 11:10:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=27
05/28/2022 11:10:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=29
05/28/2022 11:10:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=32
05/28/2022 11:10:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=34
05/28/2022 11:10:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=37
05/28/2022 11:10:56 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.11840411840411841 on epoch=37
05/28/2022 11:10:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=39
05/28/2022 11:11:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
05/28/2022 11:11:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
05/28/2022 11:11:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=47
05/28/2022 11:11:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=49
05/28/2022 11:11:09 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.1807810443176832 on epoch=49
05/28/2022 11:11:09 - INFO - __main__ - Saving model with best Classification-F1: 0.1468058968058968 -> 0.1807810443176832 on epoch=49, global_step=200
05/28/2022 11:11:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=52
05/28/2022 11:11:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
05/28/2022 11:11:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=57
05/28/2022 11:11:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=59
05/28/2022 11:11:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
05/28/2022 11:11:22 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.24205105239587998 on epoch=62
05/28/2022 11:11:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1807810443176832 -> 0.24205105239587998 on epoch=62, global_step=250
05/28/2022 11:11:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=64
05/28/2022 11:11:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=67
05/28/2022 11:11:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
05/28/2022 11:11:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=72
05/28/2022 11:11:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=74
05/28/2022 11:11:35 - INFO - __main__ - Global step 300 Train loss 0.77 Classification-F1 0.3041125541125541 on epoch=74
05/28/2022 11:11:35 - INFO - __main__ - Saving model with best Classification-F1: 0.24205105239587998 -> 0.3041125541125541 on epoch=74, global_step=300
05/28/2022 11:11:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.73 on epoch=77
05/28/2022 11:11:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=79
05/28/2022 11:11:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=82
05/28/2022 11:11:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=84
05/28/2022 11:11:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.63 on epoch=87
05/28/2022 11:11:48 - INFO - __main__ - Global step 350 Train loss 0.69 Classification-F1 0.3066396761133603 on epoch=87
05/28/2022 11:11:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3041125541125541 -> 0.3066396761133603 on epoch=87, global_step=350
05/28/2022 11:11:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.65 on epoch=89
05/28/2022 11:11:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.64 on epoch=92
05/28/2022 11:11:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=94
05/28/2022 11:11:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=97
05/28/2022 11:12:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=99
05/28/2022 11:12:01 - INFO - __main__ - Global step 400 Train loss 0.63 Classification-F1 0.2824867724867725 on epoch=99
05/28/2022 11:12:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.67 on epoch=102
05/28/2022 11:12:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=104
05/28/2022 11:12:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=107
05/28/2022 11:12:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=109
05/28/2022 11:12:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=112
05/28/2022 11:12:14 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.4263684767717026 on epoch=112
05/28/2022 11:12:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3066396761133603 -> 0.4263684767717026 on epoch=112, global_step=450
05/28/2022 11:12:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=114
05/28/2022 11:12:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=117
05/28/2022 11:12:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=119
05/28/2022 11:12:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=122
05/28/2022 11:12:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=124
05/28/2022 11:12:27 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.3808404558404558 on epoch=124
05/28/2022 11:12:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=127
05/28/2022 11:12:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=129
05/28/2022 11:12:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=132
05/28/2022 11:12:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=134
05/28/2022 11:12:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=137
05/28/2022 11:12:40 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.454780548895168 on epoch=137
05/28/2022 11:12:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4263684767717026 -> 0.454780548895168 on epoch=137, global_step=550
05/28/2022 11:12:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=139
05/28/2022 11:12:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=142
05/28/2022 11:12:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=144
05/28/2022 11:12:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=147
05/28/2022 11:12:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=149
05/28/2022 11:12:53 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.42850574712643674 on epoch=149
05/28/2022 11:12:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=152
05/28/2022 11:12:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=154
05/28/2022 11:13:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=157
05/28/2022 11:13:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=159
05/28/2022 11:13:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=162
05/28/2022 11:13:06 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.39602359099196044 on epoch=162
05/28/2022 11:13:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=164
05/28/2022 11:13:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=167
05/28/2022 11:13:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=169
05/28/2022 11:13:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=172
05/28/2022 11:13:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=174
05/28/2022 11:13:19 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.47427305722500224 on epoch=174
05/28/2022 11:13:19 - INFO - __main__ - Saving model with best Classification-F1: 0.454780548895168 -> 0.47427305722500224 on epoch=174, global_step=700
05/28/2022 11:13:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=177
05/28/2022 11:13:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=179
05/28/2022 11:13:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=182
05/28/2022 11:13:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=184
05/28/2022 11:13:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=187
05/28/2022 11:13:32 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.4914559454882036 on epoch=187
05/28/2022 11:13:32 - INFO - __main__ - Saving model with best Classification-F1: 0.47427305722500224 -> 0.4914559454882036 on epoch=187, global_step=750
05/28/2022 11:13:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=189
05/28/2022 11:13:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=192
05/28/2022 11:13:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=194
05/28/2022 11:13:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
05/28/2022 11:13:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=199
05/28/2022 11:13:45 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.4829279746820976 on epoch=199
05/28/2022 11:13:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
05/28/2022 11:13:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=204
05/28/2022 11:13:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=207
05/28/2022 11:13:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
05/28/2022 11:13:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=212
05/28/2022 11:13:58 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.527014652014652 on epoch=212
05/28/2022 11:13:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4914559454882036 -> 0.527014652014652 on epoch=212, global_step=850
05/28/2022 11:14:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=214
05/28/2022 11:14:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=217
05/28/2022 11:14:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=219
05/28/2022 11:14:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=222
05/28/2022 11:14:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
05/28/2022 11:14:11 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.4958152173913043 on epoch=224
05/28/2022 11:14:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
05/28/2022 11:14:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=229
05/28/2022 11:14:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/28/2022 11:14:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
05/28/2022 11:14:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=237
05/28/2022 11:14:24 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.47772657450076805 on epoch=237
05/28/2022 11:14:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=239
05/28/2022 11:14:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=242
05/28/2022 11:14:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=244
05/28/2022 11:14:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=247
05/28/2022 11:14:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
05/28/2022 11:14:37 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5396830680465613 on epoch=249
05/28/2022 11:14:37 - INFO - __main__ - Saving model with best Classification-F1: 0.527014652014652 -> 0.5396830680465613 on epoch=249, global_step=1000
05/28/2022 11:14:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/28/2022 11:14:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
05/28/2022 11:14:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/28/2022 11:14:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=259
05/28/2022 11:14:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=262
05/28/2022 11:14:50 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.557109557109557 on epoch=262
05/28/2022 11:14:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5396830680465613 -> 0.557109557109557 on epoch=262, global_step=1050
05/28/2022 11:14:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
05/28/2022 11:14:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=267
05/28/2022 11:14:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=269
05/28/2022 11:15:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
05/28/2022 11:15:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=274
05/28/2022 11:15:03 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.5582027168234065 on epoch=274
05/28/2022 11:15:03 - INFO - __main__ - Saving model with best Classification-F1: 0.557109557109557 -> 0.5582027168234065 on epoch=274, global_step=1100
05/28/2022 11:15:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
05/28/2022 11:15:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
05/28/2022 11:15:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=282
05/28/2022 11:15:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
05/28/2022 11:15:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=287
05/28/2022 11:15:16 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.5852826510721247 on epoch=287
05/28/2022 11:15:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5582027168234065 -> 0.5852826510721247 on epoch=287, global_step=1150
05/28/2022 11:15:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/28/2022 11:15:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/28/2022 11:15:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
05/28/2022 11:15:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
05/28/2022 11:15:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=299
05/28/2022 11:15:29 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.6584926724312913 on epoch=299
05/28/2022 11:15:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5852826510721247 -> 0.6584926724312913 on epoch=299, global_step=1200
05/28/2022 11:15:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/28/2022 11:15:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
05/28/2022 11:15:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/28/2022 11:15:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
05/28/2022 11:15:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=312
05/28/2022 11:15:42 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5754357298474946 on epoch=312
05/28/2022 11:15:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/28/2022 11:15:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
05/28/2022 11:15:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=319
05/28/2022 11:15:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/28/2022 11:15:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/28/2022 11:15:56 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6269980506822612 on epoch=324
05/28/2022 11:15:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=327
05/28/2022 11:16:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/28/2022 11:16:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=332
05/28/2022 11:16:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
05/28/2022 11:16:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=337
05/28/2022 11:16:09 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.588523483684774 on epoch=337
05/28/2022 11:16:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=339
05/28/2022 11:16:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
05/28/2022 11:16:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=344
05/28/2022 11:16:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/28/2022 11:16:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/28/2022 11:16:22 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.6157635467980296 on epoch=349
05/28/2022 11:16:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/28/2022 11:16:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
05/28/2022 11:16:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=357
05/28/2022 11:16:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=359
05/28/2022 11:16:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
05/28/2022 11:16:35 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.5861055935166057 on epoch=362
05/28/2022 11:16:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/28/2022 11:16:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/28/2022 11:16:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/28/2022 11:16:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/28/2022 11:16:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/28/2022 11:16:49 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.5689221085759244 on epoch=374
05/28/2022 11:16:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/28/2022 11:16:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
05/28/2022 11:16:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/28/2022 11:16:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/28/2022 11:17:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/28/2022 11:17:02 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.5393446931266731 on epoch=387
05/28/2022 11:17:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/28/2022 11:17:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
05/28/2022 11:17:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 11:17:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
05/28/2022 11:17:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.08 on epoch=399
05/28/2022 11:17:15 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.6291556262144498 on epoch=399
05/28/2022 11:17:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=402
05/28/2022 11:17:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/28/2022 11:17:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
05/28/2022 11:17:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/28/2022 11:17:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
05/28/2022 11:17:28 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.6230989636252795 on epoch=412
05/28/2022 11:17:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/28/2022 11:17:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/28/2022 11:17:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/28/2022 11:17:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/28/2022 11:17:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 11:17:42 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6358405108405109 on epoch=424
05/28/2022 11:17:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/28/2022 11:17:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 11:17:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/28/2022 11:17:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/28/2022 11:17:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/28/2022 11:17:55 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6114935746131656 on epoch=437
05/28/2022 11:17:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/28/2022 11:18:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/28/2022 11:18:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/28/2022 11:18:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/28/2022 11:18:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/28/2022 11:18:09 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6404970760233919 on epoch=449
05/28/2022 11:18:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/28/2022 11:18:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 11:18:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/28/2022 11:18:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/28/2022 11:18:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/28/2022 11:18:22 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.634364478114478 on epoch=462
05/28/2022 11:18:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=464
05/28/2022 11:18:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 11:18:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=469
05/28/2022 11:18:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/28/2022 11:18:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/28/2022 11:18:36 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6281855168425041 on epoch=474
05/28/2022 11:18:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 11:18:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/28/2022 11:18:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/28/2022 11:18:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 11:18:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 11:18:49 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5324732905982905 on epoch=487
05/28/2022 11:18:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/28/2022 11:18:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/28/2022 11:18:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 11:18:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/28/2022 11:19:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/28/2022 11:19:02 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5685804533630621 on epoch=499
05/28/2022 11:19:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/28/2022 11:19:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 11:19:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/28/2022 11:19:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 11:19:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/28/2022 11:19:15 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6348060344827586 on epoch=512
05/28/2022 11:19:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 11:19:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 11:19:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 11:19:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/28/2022 11:19:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/28/2022 11:19:29 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.628984753984754 on epoch=524
05/28/2022 11:19:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 11:19:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 11:19:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=532
05/28/2022 11:19:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/28/2022 11:19:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/28/2022 11:19:42 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6370195278704733 on epoch=537
05/28/2022 11:19:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 11:19:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/28/2022 11:19:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/28/2022 11:19:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/28/2022 11:19:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 11:19:55 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.5775207747170058 on epoch=549
05/28/2022 11:19:58 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 11:20:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/28/2022 11:20:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/28/2022 11:20:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 11:20:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/28/2022 11:20:09 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5978170189098999 on epoch=562
05/28/2022 11:20:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 11:20:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/28/2022 11:20:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 11:20:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/28/2022 11:20:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/28/2022 11:20:22 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6195952740070387 on epoch=574
05/28/2022 11:20:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/28/2022 11:20:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 11:20:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/28/2022 11:20:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 11:20:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 11:20:35 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6362637362637363 on epoch=587
05/28/2022 11:20:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/28/2022 11:20:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
05/28/2022 11:20:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 11:20:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/28/2022 11:20:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
05/28/2022 11:20:49 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6849972943722944 on epoch=599
05/28/2022 11:20:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6584926724312913 -> 0.6849972943722944 on epoch=599, global_step=2400
05/28/2022 11:20:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 11:20:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 11:20:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 11:20:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/28/2022 11:21:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 11:21:02 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.66453065969195 on epoch=612
05/28/2022 11:21:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 11:21:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 11:21:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
05/28/2022 11:21:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 11:21:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 11:21:15 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6427706552706552 on epoch=624
05/28/2022 11:21:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=627
05/28/2022 11:21:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/28/2022 11:21:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/28/2022 11:21:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 11:21:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/28/2022 11:21:29 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.629985754985755 on epoch=637
05/28/2022 11:21:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 11:21:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 11:21:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/28/2022 11:21:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 11:21:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 11:21:42 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6781778595233603 on epoch=649
05/28/2022 11:21:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
05/28/2022 11:21:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 11:21:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/28/2022 11:21:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/28/2022 11:21:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 11:21:55 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6733469350636 on epoch=662
05/28/2022 11:21:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 11:22:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/28/2022 11:22:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 11:22:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 11:22:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 11:22:09 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6644890214150176 on epoch=674
05/28/2022 11:22:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=677
05/28/2022 11:22:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 11:22:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/28/2022 11:22:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/28/2022 11:22:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/28/2022 11:22:22 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6253192848020435 on epoch=687
05/28/2022 11:22:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/28/2022 11:22:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 11:22:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 11:22:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/28/2022 11:22:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=699
05/28/2022 11:22:35 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6782211358824262 on epoch=699
05/28/2022 11:22:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 11:22:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 11:22:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 11:22:45 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/28/2022 11:22:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 11:22:48 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6470588235294118 on epoch=712
05/28/2022 11:22:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 11:22:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 11:22:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/28/2022 11:22:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/28/2022 11:23:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/28/2022 11:23:02 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6294198139025726 on epoch=724
05/28/2022 11:23:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 11:23:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
05/28/2022 11:23:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/28/2022 11:23:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/28/2022 11:23:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/28/2022 11:23:15 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6359530125047367 on epoch=737
05/28/2022 11:23:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/28/2022 11:23:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/28/2022 11:23:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 11:23:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 11:23:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 11:23:28 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6809003360727498 on epoch=749
05/28/2022 11:23:28 - INFO - __main__ - save last model!
05/28/2022 11:23:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 11:23:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 11:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 11:23:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 11:23:29 - INFO - __main__ - ['others']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 11:23:29 - INFO - __main__ - ['others']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 11:23:29 - INFO - __main__ - ['others']
05/28/2022 11:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 11:23:29 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:23:29 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:23:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:23:29 - INFO - __main__ - Printing 3 examples
05/28/2022 11:23:29 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:23:29 - INFO - __main__ - ['sad']
05/28/2022 11:23:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:23:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:23:29 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:23:31 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:23:36 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 11:23:44 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:23:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:23:45 - INFO - __main__ - Starting training!
05/28/2022 11:25:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/28/2022 11:25:09 - INFO - __main__ - Classification-F1 on test data: 0.3576
05/28/2022 11:25:09 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.6849972943722944, test_performance=0.3576454904277877
05/28/2022 11:25:09 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/28/2022 11:25:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:25:10 - INFO - __main__ - Printing 3 examples
05/28/2022 11:25:10 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:25:10 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:25:10 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:25:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:25:10 - INFO - __main__ - Printing 3 examples
05/28/2022 11:25:10 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:25:10 - INFO - __main__ - ['sad']
05/28/2022 11:25:10 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:25:10 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:25:10 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:25:26 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:25:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:25:27 - INFO - __main__ - Starting training!
05/28/2022 11:25:30 - INFO - __main__ - Step 10 Global step 10 Train loss 4.39 on epoch=2
05/28/2022 11:25:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.80 on epoch=4
05/28/2022 11:25:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=7
05/28/2022 11:25:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=9
05/28/2022 11:25:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=12
05/28/2022 11:25:40 - INFO - __main__ - Global step 50 Train loss 1.88 Classification-F1 0.1 on epoch=12
05/28/2022 11:25:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 11:25:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
05/28/2022 11:25:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=17
05/28/2022 11:25:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=19
05/28/2022 11:25:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=22
05/28/2022 11:25:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=24
05/28/2022 11:25:54 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.12393162393162392 on epoch=24
05/28/2022 11:25:54 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.12393162393162392 on epoch=24, global_step=100
05/28/2022 11:25:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
05/28/2022 11:25:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=29
05/28/2022 11:26:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=32
05/28/2022 11:26:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=34
05/28/2022 11:26:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.97 on epoch=37
05/28/2022 11:26:07 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.1565276828434723 on epoch=37
05/28/2022 11:26:07 - INFO - __main__ - Saving model with best Classification-F1: 0.12393162393162392 -> 0.1565276828434723 on epoch=37, global_step=150
05/28/2022 11:26:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=39
05/28/2022 11:26:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.80 on epoch=42
05/28/2022 11:26:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=44
05/28/2022 11:26:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=47
05/28/2022 11:26:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
05/28/2022 11:26:20 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.38245257452574527 on epoch=49
05/28/2022 11:26:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1565276828434723 -> 0.38245257452574527 on epoch=49, global_step=200
05/28/2022 11:26:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=52
05/28/2022 11:26:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=54
05/28/2022 11:26:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=57
05/28/2022 11:26:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=59
05/28/2022 11:26:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=62
05/28/2022 11:26:33 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.2569444444444445 on epoch=62
05/28/2022 11:26:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=64
05/28/2022 11:26:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.63 on epoch=67
05/28/2022 11:26:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=69
05/28/2022 11:26:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=72
05/28/2022 11:26:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=74
05/28/2022 11:26:46 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.460727969348659 on epoch=74
05/28/2022 11:26:46 - INFO - __main__ - Saving model with best Classification-F1: 0.38245257452574527 -> 0.460727969348659 on epoch=74, global_step=300
05/28/2022 11:26:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=77
05/28/2022 11:26:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=79
05/28/2022 11:26:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=82
05/28/2022 11:26:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
05/28/2022 11:26:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=87
05/28/2022 11:26:59 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.6635294117647058 on epoch=87
05/28/2022 11:26:59 - INFO - __main__ - Saving model with best Classification-F1: 0.460727969348659 -> 0.6635294117647058 on epoch=87, global_step=350
05/28/2022 11:27:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=89
05/28/2022 11:27:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=92
05/28/2022 11:27:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=94
05/28/2022 11:27:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=97
05/28/2022 11:27:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=99
05/28/2022 11:27:12 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.7334975369458128 on epoch=99
05/28/2022 11:27:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6635294117647058 -> 0.7334975369458128 on epoch=99, global_step=400
05/28/2022 11:27:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=102
05/28/2022 11:27:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=104
05/28/2022 11:27:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=107
05/28/2022 11:27:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=109
05/28/2022 11:27:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=112
05/28/2022 11:27:26 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.7053460342934026 on epoch=112
05/28/2022 11:27:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=114
05/28/2022 11:27:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=117
05/28/2022 11:27:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/28/2022 11:27:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/28/2022 11:27:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=124
05/28/2022 11:27:39 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.7020973227869779 on epoch=124
05/28/2022 11:27:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=127
05/28/2022 11:27:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=129
05/28/2022 11:27:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=132
05/28/2022 11:27:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=134
05/28/2022 11:27:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=137
05/28/2022 11:27:52 - INFO - __main__ - Global step 550 Train loss 0.18 Classification-F1 0.7479124493927125 on epoch=137
05/28/2022 11:27:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7334975369458128 -> 0.7479124493927125 on epoch=137, global_step=550
05/28/2022 11:27:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=139
05/28/2022 11:27:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=142
05/28/2022 11:27:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=144
05/28/2022 11:28:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=147
05/28/2022 11:28:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=149
05/28/2022 11:28:05 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.7116311684622879 on epoch=149
05/28/2022 11:28:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=152
05/28/2022 11:28:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=154
05/28/2022 11:28:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=157
05/28/2022 11:28:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=159
05/28/2022 11:28:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=162
05/28/2022 11:28:18 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.7323529411764705 on epoch=162
05/28/2022 11:28:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/28/2022 11:28:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.10 on epoch=167
05/28/2022 11:28:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
05/28/2022 11:28:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=172
05/28/2022 11:28:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=174
05/28/2022 11:28:32 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.7507227249874309 on epoch=174
05/28/2022 11:28:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7479124493927125 -> 0.7507227249874309 on epoch=174, global_step=700
05/28/2022 11:28:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=177
05/28/2022 11:28:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
05/28/2022 11:28:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/28/2022 11:28:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=184
05/28/2022 11:28:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=187
05/28/2022 11:28:45 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.7324983260467133 on epoch=187
05/28/2022 11:28:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
05/28/2022 11:28:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=192
05/28/2022 11:28:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
05/28/2022 11:28:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/28/2022 11:28:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
05/28/2022 11:28:58 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.6818657897941786 on epoch=199
05/28/2022 11:29:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
05/28/2022 11:29:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
05/28/2022 11:29:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
05/28/2022 11:29:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=209
05/28/2022 11:29:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/28/2022 11:29:11 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.6530092337122466 on epoch=212
05/28/2022 11:29:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/28/2022 11:29:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=217
05/28/2022 11:29:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/28/2022 11:29:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
05/28/2022 11:29:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=224
05/28/2022 11:29:25 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.7526265147969002 on epoch=224
05/28/2022 11:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7507227249874309 -> 0.7526265147969002 on epoch=224, global_step=900
05/28/2022 11:29:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/28/2022 11:29:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
05/28/2022 11:29:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/28/2022 11:29:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
05/28/2022 11:29:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=237
05/28/2022 11:29:38 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7616432587020823 on epoch=237
05/28/2022 11:29:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7526265147969002 -> 0.7616432587020823 on epoch=237, global_step=950
05/28/2022 11:29:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=239
05/28/2022 11:29:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/28/2022 11:29:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/28/2022 11:29:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
05/28/2022 11:29:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/28/2022 11:29:51 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.7497519841269842 on epoch=249
05/28/2022 11:29:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/28/2022 11:29:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/28/2022 11:29:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/28/2022 11:30:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/28/2022 11:30:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/28/2022 11:30:04 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.7285557184750733 on epoch=262
05/28/2022 11:30:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/28/2022 11:30:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/28/2022 11:30:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/28/2022 11:30:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
05/28/2022 11:30:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/28/2022 11:30:18 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6874266862170088 on epoch=274
05/28/2022 11:30:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
05/28/2022 11:30:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/28/2022 11:30:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=282
05/28/2022 11:30:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/28/2022 11:30:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
05/28/2022 11:30:31 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.749343487394958 on epoch=287
05/28/2022 11:30:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/28/2022 11:30:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 11:30:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/28/2022 11:30:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/28/2022 11:30:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/28/2022 11:30:44 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.7820701680545952 on epoch=299
05/28/2022 11:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7616432587020823 -> 0.7820701680545952 on epoch=299, global_step=1200
05/28/2022 11:30:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/28/2022 11:30:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/28/2022 11:30:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=307
05/28/2022 11:30:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/28/2022 11:30:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/28/2022 11:30:58 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.693732492997199 on epoch=312
05/28/2022 11:31:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/28/2022 11:31:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=317
05/28/2022 11:31:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/28/2022 11:31:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/28/2022 11:31:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/28/2022 11:31:11 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7139915264915265 on epoch=324
05/28/2022 11:31:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/28/2022 11:31:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/28/2022 11:31:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/28/2022 11:31:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/28/2022 11:31:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/28/2022 11:31:24 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7955375253549695 on epoch=337
05/28/2022 11:31:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7820701680545952 -> 0.7955375253549695 on epoch=337, global_step=1350
05/28/2022 11:31:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/28/2022 11:31:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/28/2022 11:31:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 11:31:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/28/2022 11:31:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/28/2022 11:31:38 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.7214229515567046 on epoch=349
05/28/2022 11:31:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 11:31:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/28/2022 11:31:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/28/2022 11:31:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/28/2022 11:31:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/28/2022 11:31:51 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6058379792922972 on epoch=362
05/28/2022 11:31:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/28/2022 11:31:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/28/2022 11:31:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/28/2022 11:32:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/28/2022 11:32:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 11:32:05 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.751445347786811 on epoch=374
05/28/2022 11:32:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=377
05/28/2022 11:32:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/28/2022 11:32:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=382
05/28/2022 11:32:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 11:32:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/28/2022 11:32:18 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.7564795008912657 on epoch=387
05/28/2022 11:32:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/28/2022 11:32:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
05/28/2022 11:32:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/28/2022 11:32:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/28/2022 11:32:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/28/2022 11:32:32 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6709645177411294 on epoch=399
05/28/2022 11:32:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/28/2022 11:32:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/28/2022 11:32:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/28/2022 11:32:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/28/2022 11:32:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 11:32:45 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7176105318485742 on epoch=412
05/28/2022 11:32:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/28/2022 11:32:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/28/2022 11:32:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/28/2022 11:32:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
05/28/2022 11:32:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/28/2022 11:32:58 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7635146103896103 on epoch=424
05/28/2022 11:33:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/28/2022 11:33:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 11:33:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/28/2022 11:33:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 11:33:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/28/2022 11:33:12 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.8246326395732169 on epoch=437
05/28/2022 11:33:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7955375253549695 -> 0.8246326395732169 on epoch=437, global_step=1750
05/28/2022 11:33:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/28/2022 11:33:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/28/2022 11:33:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/28/2022 11:33:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/28/2022 11:33:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 11:33:25 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7515397184514832 on epoch=449
05/28/2022 11:33:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/28/2022 11:33:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 11:33:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 11:33:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/28/2022 11:33:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/28/2022 11:33:39 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.733718487394958 on epoch=462
05/28/2022 11:33:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/28/2022 11:33:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 11:33:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/28/2022 11:33:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/28/2022 11:33:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/28/2022 11:33:52 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6756911691076545 on epoch=474
05/28/2022 11:33:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/28/2022 11:33:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 11:33:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/28/2022 11:34:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 11:34:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 11:34:06 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7689841169442688 on epoch=487
05/28/2022 11:34:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 11:34:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/28/2022 11:34:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=494
05/28/2022 11:34:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 11:34:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 11:34:19 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7668026418026419 on epoch=499
05/28/2022 11:34:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/28/2022 11:34:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/28/2022 11:34:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=507
05/28/2022 11:34:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 11:34:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 11:34:32 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7936132154882155 on epoch=512
05/28/2022 11:34:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 11:34:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/28/2022 11:34:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/28/2022 11:34:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/28/2022 11:34:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 11:34:45 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7452107279693485 on epoch=524
05/28/2022 11:34:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 11:34:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 11:34:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/28/2022 11:34:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 11:34:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 11:34:59 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8069268366042559 on epoch=537
05/28/2022 11:35:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 11:35:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 11:35:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/28/2022 11:35:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 11:35:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 11:35:12 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7664596273291925 on epoch=549
05/28/2022 11:35:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 11:35:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=554
05/28/2022 11:35:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 11:35:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 11:35:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/28/2022 11:35:26 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7921212121212122 on epoch=562
05/28/2022 11:35:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 11:35:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 11:35:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/28/2022 11:35:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/28/2022 11:35:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/28/2022 11:35:39 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7927871148459383 on epoch=574
05/28/2022 11:35:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/28/2022 11:35:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 11:35:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 11:35:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/28/2022 11:35:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 11:35:52 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7619658119658119 on epoch=587
05/28/2022 11:35:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 11:35:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 11:36:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 11:36:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 11:36:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/28/2022 11:36:06 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.8096865193639388 on epoch=599
05/28/2022 11:36:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 11:36:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 11:36:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 11:36:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/28/2022 11:36:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 11:36:19 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.8275445992179864 on epoch=612
05/28/2022 11:36:19 - INFO - __main__ - Saving model with best Classification-F1: 0.8246326395732169 -> 0.8275445992179864 on epoch=612, global_step=2450
05/28/2022 11:36:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/28/2022 11:36:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 11:36:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/28/2022 11:36:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 11:36:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/28/2022 11:36:33 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.787354644090887 on epoch=624
05/28/2022 11:36:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 11:36:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 11:36:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/28/2022 11:36:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/28/2022 11:36:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 11:36:46 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.8430680740037951 on epoch=637
05/28/2022 11:36:46 - INFO - __main__ - Saving model with best Classification-F1: 0.8275445992179864 -> 0.8430680740037951 on epoch=637, global_step=2550
05/28/2022 11:36:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 11:36:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 11:36:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 11:36:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 11:36:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 11:36:59 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7135287081339713 on epoch=649
05/28/2022 11:37:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 11:37:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 11:37:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 11:37:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 11:37:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 11:37:13 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.8577739563567363 on epoch=662
05/28/2022 11:37:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8430680740037951 -> 0.8577739563567363 on epoch=662, global_step=2650
05/28/2022 11:37:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 11:37:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 11:37:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/28/2022 11:37:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 11:37:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 11:37:26 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.8126564064064065 on epoch=674
05/28/2022 11:37:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 11:37:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 11:37:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 11:37:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/28/2022 11:37:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 11:37:40 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.8416491596638656 on epoch=687
05/28/2022 11:37:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 11:37:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/28/2022 11:37:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 11:37:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/28/2022 11:37:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=699
05/28/2022 11:37:53 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7169191919191918 on epoch=699
05/28/2022 11:37:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/28/2022 11:37:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/28/2022 11:38:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 11:38:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 11:38:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 11:38:07 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.8417185513959707 on epoch=712
05/28/2022 11:38:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 11:38:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 11:38:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 11:38:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 11:38:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/28/2022 11:38:20 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8583559991394677 on epoch=724
05/28/2022 11:38:20 - INFO - __main__ - Saving model with best Classification-F1: 0.8577739563567363 -> 0.8583559991394677 on epoch=724, global_step=2900
05/28/2022 11:38:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 11:38:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 11:38:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 11:38:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 11:38:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 11:38:34 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7582869335500914 on epoch=737
05/28/2022 11:38:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 11:38:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/28/2022 11:38:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/28/2022 11:38:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/28/2022 11:38:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 11:38:47 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.8572666117716173 on epoch=749
05/28/2022 11:38:47 - INFO - __main__ - save last model!
05/28/2022 11:38:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 11:38:47 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 11:38:47 - INFO - __main__ - Printing 3 examples
05/28/2022 11:38:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 11:38:47 - INFO - __main__ - ['others']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 11:38:47 - INFO - __main__ - ['others']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 11:38:47 - INFO - __main__ - ['others']
05/28/2022 11:38:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:38:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:38:47 - INFO - __main__ - Printing 3 examples
05/28/2022 11:38:47 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:38:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:38:47 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:38:47 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:38:47 - INFO - __main__ - Printing 3 examples
05/28/2022 11:38:47 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:38:47 - INFO - __main__ - ['sad']
05/28/2022 11:38:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:38:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:38:47 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:38:49 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:38:54 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 11:39:06 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:39:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:39:07 - INFO - __main__ - Starting training!
05/28/2022 11:40:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/28/2022 11:40:27 - INFO - __main__ - Classification-F1 on test data: 0.4124
05/28/2022 11:40:28 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.8583559991394677, test_performance=0.4124374469082885
05/28/2022 11:40:28 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/28/2022 11:40:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:40:29 - INFO - __main__ - Printing 3 examples
05/28/2022 11:40:29 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:40:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:40:29 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:40:29 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:40:29 - INFO - __main__ - Printing 3 examples
05/28/2022 11:40:29 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:40:29 - INFO - __main__ - ['sad']
05/28/2022 11:40:29 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:40:29 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:40:29 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:40:44 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:40:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:40:45 - INFO - __main__ - Starting training!
05/28/2022 11:40:48 - INFO - __main__ - Step 10 Global step 10 Train loss 4.73 on epoch=2
05/28/2022 11:40:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.58 on epoch=4
05/28/2022 11:40:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.61 on epoch=7
05/28/2022 11:40:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=9
05/28/2022 11:40:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=12
05/28/2022 11:40:58 - INFO - __main__ - Global step 50 Train loss 2.20 Classification-F1 0.13067758749069247 on epoch=12
05/28/2022 11:40:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
05/28/2022 11:41:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.96 on epoch=14
05/28/2022 11:41:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=17
05/28/2022 11:41:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
05/28/2022 11:41:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=22
05/28/2022 11:41:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=24
05/28/2022 11:41:12 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.1 on epoch=24
05/28/2022 11:41:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=27
05/28/2022 11:41:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=29
05/28/2022 11:41:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=32
05/28/2022 11:41:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 11:41:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
05/28/2022 11:41:25 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.28435409161091096 on epoch=37
05/28/2022 11:41:25 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.28435409161091096 on epoch=37, global_step=150
05/28/2022 11:41:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=39
05/28/2022 11:41:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=42
05/28/2022 11:41:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=44
05/28/2022 11:41:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
05/28/2022 11:41:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=49
05/28/2022 11:41:38 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.27320261437908494 on epoch=49
05/28/2022 11:41:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=52
05/28/2022 11:41:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=54
05/28/2022 11:41:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=57
05/28/2022 11:41:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
05/28/2022 11:41:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=62
05/28/2022 11:41:51 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.30637254901960786 on epoch=62
05/28/2022 11:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.28435409161091096 -> 0.30637254901960786 on epoch=62, global_step=250
05/28/2022 11:41:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.61 on epoch=64
05/28/2022 11:41:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=67
05/28/2022 11:41:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=69
05/28/2022 11:42:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.60 on epoch=72
05/28/2022 11:42:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=74
05/28/2022 11:42:05 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.456596035543404 on epoch=74
05/28/2022 11:42:05 - INFO - __main__ - Saving model with best Classification-F1: 0.30637254901960786 -> 0.456596035543404 on epoch=74, global_step=300
05/28/2022 11:42:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=77
05/28/2022 11:42:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=79
05/28/2022 11:42:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=82
05/28/2022 11:42:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=84
05/28/2022 11:42:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=87
05/28/2022 11:42:18 - INFO - __main__ - Global step 350 Train loss 0.54 Classification-F1 0.4908479349186483 on epoch=87
05/28/2022 11:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.456596035543404 -> 0.4908479349186483 on epoch=87, global_step=350
05/28/2022 11:42:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=89
05/28/2022 11:42:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=92
05/28/2022 11:42:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=94
05/28/2022 11:42:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
05/28/2022 11:42:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=99
05/28/2022 11:42:31 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.45972644376899696 on epoch=99
05/28/2022 11:42:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=102
05/28/2022 11:42:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=104
05/28/2022 11:42:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=107
05/28/2022 11:42:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=109
05/28/2022 11:42:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=112
05/28/2022 11:42:44 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.6092490842490843 on epoch=112
05/28/2022 11:42:44 - INFO - __main__ - Saving model with best Classification-F1: 0.4908479349186483 -> 0.6092490842490843 on epoch=112, global_step=450
05/28/2022 11:42:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=114
05/28/2022 11:42:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=117
05/28/2022 11:42:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=119
05/28/2022 11:42:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=122
05/28/2022 11:42:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=124
05/28/2022 11:42:58 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.5255280662854476 on epoch=124
05/28/2022 11:43:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=127
05/28/2022 11:43:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=129
05/28/2022 11:43:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=132
05/28/2022 11:43:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
05/28/2022 11:43:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=137
05/28/2022 11:43:11 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.5958823968458856 on epoch=137
05/28/2022 11:43:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=139
05/28/2022 11:43:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=142
05/28/2022 11:43:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=144
05/28/2022 11:43:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=147
05/28/2022 11:43:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
05/28/2022 11:43:24 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.6855057514378595 on epoch=149
05/28/2022 11:43:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6092490842490843 -> 0.6855057514378595 on epoch=149, global_step=600
05/28/2022 11:43:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=152
05/28/2022 11:43:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=154
05/28/2022 11:43:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/28/2022 11:43:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
05/28/2022 11:43:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
05/28/2022 11:43:38 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.6393138111888111 on epoch=162
05/28/2022 11:43:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=164
05/28/2022 11:43:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=167
05/28/2022 11:43:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.11 on epoch=169
05/28/2022 11:43:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
05/28/2022 11:43:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
05/28/2022 11:43:51 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.580841307814992 on epoch=174
05/28/2022 11:43:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=177
05/28/2022 11:43:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
05/28/2022 11:43:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/28/2022 11:44:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
05/28/2022 11:44:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
05/28/2022 11:44:04 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.6533613445378152 on epoch=187
05/28/2022 11:44:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
05/28/2022 11:44:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/28/2022 11:44:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/28/2022 11:44:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/28/2022 11:44:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
05/28/2022 11:44:17 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.6880855941294762 on epoch=199
05/28/2022 11:44:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6855057514378595 -> 0.6880855941294762 on epoch=199, global_step=800
05/28/2022 11:44:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=202
05/28/2022 11:44:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
05/28/2022 11:44:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=207
05/28/2022 11:44:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
05/28/2022 11:44:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.16 on epoch=212
05/28/2022 11:44:31 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.6941853877337748 on epoch=212
05/28/2022 11:44:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6880855941294762 -> 0.6941853877337748 on epoch=212, global_step=850
05/28/2022 11:44:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/28/2022 11:44:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=217
05/28/2022 11:44:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/28/2022 11:44:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=222
05/28/2022 11:44:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
05/28/2022 11:44:44 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7036799027288156 on epoch=224
05/28/2022 11:44:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6941853877337748 -> 0.7036799027288156 on epoch=224, global_step=900
05/28/2022 11:44:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
05/28/2022 11:44:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=229
05/28/2022 11:44:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/28/2022 11:44:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=234
05/28/2022 11:44:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/28/2022 11:44:57 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.6516857766857767 on epoch=237
05/28/2022 11:45:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/28/2022 11:45:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=242
05/28/2022 11:45:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/28/2022 11:45:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
05/28/2022 11:45:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/28/2022 11:45:10 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.6299446073639622 on epoch=249
05/28/2022 11:45:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
05/28/2022 11:45:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/28/2022 11:45:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
05/28/2022 11:45:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/28/2022 11:45:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
05/28/2022 11:45:24 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6940359477124184 on epoch=262
05/28/2022 11:45:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/28/2022 11:45:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
05/28/2022 11:45:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/28/2022 11:45:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/28/2022 11:45:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/28/2022 11:45:37 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6793521462639109 on epoch=274
05/28/2022 11:45:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
05/28/2022 11:45:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/28/2022 11:45:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/28/2022 11:45:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/28/2022 11:45:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/28/2022 11:45:50 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7192439648181648 on epoch=287
05/28/2022 11:45:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7036799027288156 -> 0.7192439648181648 on epoch=287, global_step=1150
05/28/2022 11:45:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/28/2022 11:45:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/28/2022 11:45:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/28/2022 11:46:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
05/28/2022 11:46:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=299
05/28/2022 11:46:04 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6910262748677384 on epoch=299
05/28/2022 11:46:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
05/28/2022 11:46:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=304
05/28/2022 11:46:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=307
05/28/2022 11:46:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
05/28/2022 11:46:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/28/2022 11:46:17 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7606177606177605 on epoch=312
05/28/2022 11:46:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7192439648181648 -> 0.7606177606177605 on epoch=312, global_step=1250
05/28/2022 11:46:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/28/2022 11:46:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
05/28/2022 11:46:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
05/28/2022 11:46:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/28/2022 11:46:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/28/2022 11:46:30 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.7459903537489745 on epoch=324
05/28/2022 11:46:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/28/2022 11:46:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
05/28/2022 11:46:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/28/2022 11:46:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/28/2022 11:46:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/28/2022 11:46:43 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.7374299719887956 on epoch=337
05/28/2022 11:46:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/28/2022 11:46:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/28/2022 11:46:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/28/2022 11:46:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/28/2022 11:46:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/28/2022 11:46:56 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.7897435897435897 on epoch=349
05/28/2022 11:46:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7606177606177605 -> 0.7897435897435897 on epoch=349, global_step=1400
05/28/2022 11:46:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 11:47:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/28/2022 11:47:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/28/2022 11:47:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/28/2022 11:47:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
05/28/2022 11:47:10 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7752103658175765 on epoch=362
05/28/2022 11:47:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/28/2022 11:47:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 11:47:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/28/2022 11:47:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/28/2022 11:47:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/28/2022 11:47:23 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.7454453441295547 on epoch=374
05/28/2022 11:47:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/28/2022 11:47:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/28/2022 11:47:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/28/2022 11:47:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 11:47:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/28/2022 11:47:36 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7416965352449224 on epoch=387
05/28/2022 11:47:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/28/2022 11:47:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/28/2022 11:47:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/28/2022 11:47:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/28/2022 11:47:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/28/2022 11:47:50 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.7289525695597803 on epoch=399
05/28/2022 11:47:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 11:47:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/28/2022 11:47:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/28/2022 11:47:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/28/2022 11:48:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 11:48:03 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7003968253968254 on epoch=412
05/28/2022 11:48:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/28/2022 11:48:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/28/2022 11:48:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/28/2022 11:48:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 11:48:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=424
05/28/2022 11:48:16 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7798406862745099 on epoch=424
05/28/2022 11:48:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/28/2022 11:48:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/28/2022 11:48:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/28/2022 11:48:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/28/2022 11:48:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/28/2022 11:48:29 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7757575757575758 on epoch=437
05/28/2022 11:48:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 11:48:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 11:48:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/28/2022 11:48:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/28/2022 11:48:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/28/2022 11:48:43 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7626780626780627 on epoch=449
05/28/2022 11:48:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/28/2022 11:48:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 11:48:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 11:48:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/28/2022 11:48:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/28/2022 11:48:56 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7614718614718615 on epoch=462
05/28/2022 11:48:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/28/2022 11:49:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/28/2022 11:49:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/28/2022 11:49:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/28/2022 11:49:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/28/2022 11:49:10 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7072704388880859 on epoch=474
05/28/2022 11:49:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 11:49:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=479
05/28/2022 11:49:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 11:49:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/28/2022 11:49:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 11:49:23 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7071886446886447 on epoch=487
05/28/2022 11:49:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/28/2022 11:49:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 11:49:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=494
05/28/2022 11:49:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 11:49:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/28/2022 11:49:37 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7957315162907268 on epoch=499
05/28/2022 11:49:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7897435897435897 -> 0.7957315162907268 on epoch=499, global_step=2000
05/28/2022 11:49:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/28/2022 11:49:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/28/2022 11:49:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 11:49:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/28/2022 11:49:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 11:49:50 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7364795008912657 on epoch=512
05/28/2022 11:49:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/28/2022 11:49:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 11:49:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/28/2022 11:50:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/28/2022 11:50:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 11:50:04 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7590996168582376 on epoch=524
05/28/2022 11:50:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 11:50:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 11:50:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 11:50:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 11:50:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 11:50:17 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.6910749774164409 on epoch=537
05/28/2022 11:50:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 11:50:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/28/2022 11:50:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/28/2022 11:50:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/28/2022 11:50:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=549
05/28/2022 11:50:31 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.8054018703424477 on epoch=549
05/28/2022 11:50:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7957315162907268 -> 0.8054018703424477 on epoch=549, global_step=2200
05/28/2022 11:50:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 11:50:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 11:50:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 11:50:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 11:50:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/28/2022 11:50:44 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7370098039215686 on epoch=562
05/28/2022 11:50:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 11:50:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 11:50:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 11:50:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/28/2022 11:50:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 11:50:57 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7002353524092654 on epoch=574
05/28/2022 11:51:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 11:51:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 11:51:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 11:51:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
05/28/2022 11:51:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/28/2022 11:51:11 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.779060909427984 on epoch=587
05/28/2022 11:51:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 11:51:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 11:51:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 11:51:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/28/2022 11:51:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
05/28/2022 11:51:24 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.7759289729877965 on epoch=599
05/28/2022 11:51:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 11:51:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 11:51:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 11:51:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 11:51:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 11:51:38 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.7606951871657754 on epoch=612
05/28/2022 11:51:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 11:51:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 11:51:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/28/2022 11:51:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 11:51:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/28/2022 11:51:51 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7024044795783926 on epoch=624
05/28/2022 11:51:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/28/2022 11:51:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 11:51:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 11:52:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 11:52:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 11:52:05 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7323388685617788 on epoch=637
05/28/2022 11:52:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 11:52:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 11:52:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 11:52:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 11:52:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 11:52:18 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.792448009506833 on epoch=649
05/28/2022 11:52:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 11:52:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 11:52:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 11:52:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/28/2022 11:52:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 11:52:32 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7890406162464986 on epoch=662
05/28/2022 11:52:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=664
05/28/2022 11:52:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 11:52:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 11:52:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 11:52:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 11:52:45 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.747357544416368 on epoch=674
05/28/2022 11:52:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/28/2022 11:52:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/28/2022 11:52:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 11:52:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 11:52:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/28/2022 11:52:58 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7455436720142602 on epoch=687
05/28/2022 11:53:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 11:53:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/28/2022 11:53:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 11:53:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/28/2022 11:53:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 11:53:12 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7505827505827506 on epoch=699
05/28/2022 11:53:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 11:53:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/28/2022 11:53:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/28/2022 11:53:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 11:53:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 11:53:25 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.8067320067320068 on epoch=712
05/28/2022 11:53:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8054018703424477 -> 0.8067320067320068 on epoch=712, global_step=2850
05/28/2022 11:53:28 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 11:53:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 11:53:33 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/28/2022 11:53:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 11:53:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/28/2022 11:53:39 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8088984204793027 on epoch=724
05/28/2022 11:53:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8067320067320068 -> 0.8088984204793027 on epoch=724, global_step=2900
05/28/2022 11:53:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/28/2022 11:53:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/28/2022 11:53:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/28/2022 11:53:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 11:53:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=737
05/28/2022 11:53:52 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7171869193608322 on epoch=737
05/28/2022 11:53:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 11:53:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 11:53:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/28/2022 11:54:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/28/2022 11:54:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/28/2022 11:54:05 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.682627276105537 on epoch=749
05/28/2022 11:54:05 - INFO - __main__ - save last model!
05/28/2022 11:54:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 11:54:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 11:54:05 - INFO - __main__ - Printing 3 examples
05/28/2022 11:54:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 11:54:05 - INFO - __main__ - ['others']
05/28/2022 11:54:05 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 11:54:05 - INFO - __main__ - ['others']
05/28/2022 11:54:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 11:54:05 - INFO - __main__ - ['others']
05/28/2022 11:54:05 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:54:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:54:06 - INFO - __main__ - Printing 3 examples
05/28/2022 11:54:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:54:06 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:54:06 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:54:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:54:06 - INFO - __main__ - Printing 3 examples
05/28/2022 11:54:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:54:06 - INFO - __main__ - ['sad']
05/28/2022 11:54:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:54:06 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:54:06 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:54:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:54:13 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 11:54:24 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:54:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:54:25 - INFO - __main__ - Starting training!
05/28/2022 11:55:52 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/28/2022 11:55:52 - INFO - __main__ - Classification-F1 on test data: 0.3119
05/28/2022 11:55:53 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.8088984204793027, test_performance=0.31188486669285553
05/28/2022 11:55:53 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/28/2022 11:55:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:55:54 - INFO - __main__ - Printing 3 examples
05/28/2022 11:55:54 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:55:54 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:55:54 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 11:55:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 11:55:54 - INFO - __main__ - Printing 3 examples
05/28/2022 11:55:54 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 11:55:54 - INFO - __main__ - ['sad']
05/28/2022 11:55:54 - INFO - __main__ - Tokenizing Input ...
05/28/2022 11:55:54 - INFO - __main__ - Tokenizing Output ...
05/28/2022 11:55:54 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 11:56:12 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 11:56:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 11:56:13 - INFO - __main__ - Starting training!
05/28/2022 11:56:16 - INFO - __main__ - Step 10 Global step 10 Train loss 5.15 on epoch=2
05/28/2022 11:56:18 - INFO - __main__ - Step 20 Global step 20 Train loss 3.11 on epoch=4
05/28/2022 11:56:21 - INFO - __main__ - Step 30 Global step 30 Train loss 2.03 on epoch=7
05/28/2022 11:56:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.32 on epoch=9
05/28/2022 11:56:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=12
05/28/2022 11:56:27 - INFO - __main__ - Global step 50 Train loss 2.53 Classification-F1 0.1 on epoch=12
05/28/2022 11:56:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 11:56:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.96 on epoch=14
05/28/2022 11:56:31 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=17
05/28/2022 11:56:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=19
05/28/2022 11:56:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
05/28/2022 11:56:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=24
05/28/2022 11:56:39 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.24430323299888518 on epoch=24
05/28/2022 11:56:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.24430323299888518 on epoch=24, global_step=100
05/28/2022 11:56:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=27
05/28/2022 11:56:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=29
05/28/2022 11:56:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=32
05/28/2022 11:56:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 11:56:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
05/28/2022 11:56:53 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.1 on epoch=37
05/28/2022 11:56:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=39
05/28/2022 11:56:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=42
05/28/2022 11:57:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=44
05/28/2022 11:57:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=47
05/28/2022 11:57:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/28/2022 11:57:05 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.23210514975220856 on epoch=49
05/28/2022 11:57:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=52
05/28/2022 11:57:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=54
05/28/2022 11:57:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
05/28/2022 11:57:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=59
05/28/2022 11:57:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=62
05/28/2022 11:57:18 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.2340314769975787 on epoch=62
05/28/2022 11:57:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=64
05/28/2022 11:57:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=67
05/28/2022 11:57:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/28/2022 11:57:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.83 on epoch=72
05/28/2022 11:57:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=74
05/28/2022 11:57:31 - INFO - __main__ - Global step 300 Train loss 0.82 Classification-F1 0.3954545454545455 on epoch=74
05/28/2022 11:57:31 - INFO - __main__ - Saving model with best Classification-F1: 0.24430323299888518 -> 0.3954545454545455 on epoch=74, global_step=300
05/28/2022 11:57:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=77
05/28/2022 11:57:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=79
05/28/2022 11:57:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.75 on epoch=82
05/28/2022 11:57:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=84
05/28/2022 11:57:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=87
05/28/2022 11:57:44 - INFO - __main__ - Global step 350 Train loss 0.75 Classification-F1 0.43476747424115847 on epoch=87
05/28/2022 11:57:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3954545454545455 -> 0.43476747424115847 on epoch=87, global_step=350
05/28/2022 11:57:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=89
05/28/2022 11:57:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.72 on epoch=92
05/28/2022 11:57:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=94
05/28/2022 11:57:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.69 on epoch=97
05/28/2022 11:57:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.67 on epoch=99
05/28/2022 11:57:57 - INFO - __main__ - Global step 400 Train loss 0.71 Classification-F1 0.4361841304417464 on epoch=99
05/28/2022 11:57:57 - INFO - __main__ - Saving model with best Classification-F1: 0.43476747424115847 -> 0.4361841304417464 on epoch=99, global_step=400
05/28/2022 11:57:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=102
05/28/2022 11:58:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=104
05/28/2022 11:58:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=107
05/28/2022 11:58:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=109
05/28/2022 11:58:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=112
05/28/2022 11:58:10 - INFO - __main__ - Global step 450 Train loss 0.62 Classification-F1 0.42275132275132277 on epoch=112
05/28/2022 11:58:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=114
05/28/2022 11:58:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.57 on epoch=117
05/28/2022 11:58:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=119
05/28/2022 11:58:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=122
05/28/2022 11:58:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=124
05/28/2022 11:58:23 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.541871921182266 on epoch=124
05/28/2022 11:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4361841304417464 -> 0.541871921182266 on epoch=124, global_step=500
05/28/2022 11:58:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
05/28/2022 11:58:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=129
05/28/2022 11:58:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=132
05/28/2022 11:58:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=134
05/28/2022 11:58:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=137
05/28/2022 11:58:35 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.5723802060008956 on epoch=137
05/28/2022 11:58:36 - INFO - __main__ - Saving model with best Classification-F1: 0.541871921182266 -> 0.5723802060008956 on epoch=137, global_step=550
05/28/2022 11:58:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=139
05/28/2022 11:58:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=142
05/28/2022 11:58:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=144
05/28/2022 11:58:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=147
05/28/2022 11:58:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/28/2022 11:58:48 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.7164144335196968 on epoch=149
05/28/2022 11:58:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5723802060008956 -> 0.7164144335196968 on epoch=149, global_step=600
05/28/2022 11:58:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
05/28/2022 11:58:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=154
05/28/2022 11:58:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=157
05/28/2022 11:58:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=159
05/28/2022 11:59:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=162
05/28/2022 11:59:01 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.7199638663053296 on epoch=162
05/28/2022 11:59:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7164144335196968 -> 0.7199638663053296 on epoch=162, global_step=650
05/28/2022 11:59:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=164
05/28/2022 11:59:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=167
05/28/2022 11:59:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
05/28/2022 11:59:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=172
05/28/2022 11:59:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=174
05/28/2022 11:59:14 - INFO - __main__ - Global step 700 Train loss 0.28 Classification-F1 0.7287496287496287 on epoch=174
05/28/2022 11:59:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7199638663053296 -> 0.7287496287496287 on epoch=174, global_step=700
05/28/2022 11:59:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=177
05/28/2022 11:59:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=179
05/28/2022 11:59:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=182
05/28/2022 11:59:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=184
05/28/2022 11:59:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=187
05/28/2022 11:59:27 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.6871678314470077 on epoch=187
05/28/2022 11:59:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=189
05/28/2022 11:59:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=192
05/28/2022 11:59:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=194
05/28/2022 11:59:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=197
05/28/2022 11:59:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=199
05/28/2022 11:59:40 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.7087203302373581 on epoch=199
05/28/2022 11:59:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=202
05/28/2022 11:59:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=204
05/28/2022 11:59:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
05/28/2022 11:59:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
05/28/2022 11:59:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/28/2022 11:59:53 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.7636071084346946 on epoch=212
05/28/2022 11:59:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7287496287496287 -> 0.7636071084346946 on epoch=212, global_step=850
05/28/2022 11:59:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/28/2022 11:59:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=217
05/28/2022 12:00:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
05/28/2022 12:00:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
05/28/2022 12:00:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=224
05/28/2022 12:00:06 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.7312253139839346 on epoch=224
05/28/2022 12:00:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=227
05/28/2022 12:00:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=229
05/28/2022 12:00:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=232
05/28/2022 12:00:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=234
05/28/2022 12:00:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=237
05/28/2022 12:00:19 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.7238553113553113 on epoch=237
05/28/2022 12:00:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=239
05/28/2022 12:00:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=242
05/28/2022 12:00:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=244
05/28/2022 12:00:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=247
05/28/2022 12:00:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/28/2022 12:00:32 - INFO - __main__ - Global step 1000 Train loss 0.15 Classification-F1 0.6920422714540361 on epoch=249
05/28/2022 12:00:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
05/28/2022 12:00:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
05/28/2022 12:00:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
05/28/2022 12:00:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/28/2022 12:00:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=262
05/28/2022 12:00:45 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.7610703441295547 on epoch=262
05/28/2022 12:00:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
05/28/2022 12:00:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
05/28/2022 12:00:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=269
05/28/2022 12:00:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/28/2022 12:00:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
05/28/2022 12:00:58 - INFO - __main__ - Global step 1100 Train loss 0.10 Classification-F1 0.7574631180223286 on epoch=274
05/28/2022 12:01:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/28/2022 12:01:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=279
05/28/2022 12:01:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/28/2022 12:01:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/28/2022 12:01:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/28/2022 12:01:11 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.7454326923076923 on epoch=287
05/28/2022 12:01:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/28/2022 12:01:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=292
05/28/2022 12:01:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=294
05/28/2022 12:01:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/28/2022 12:01:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
05/28/2022 12:01:24 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.7024044795783926 on epoch=299
05/28/2022 12:01:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=302
05/28/2022 12:01:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=304
05/28/2022 12:01:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/28/2022 12:01:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/28/2022 12:01:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/28/2022 12:01:37 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7574027464919306 on epoch=312
05/28/2022 12:01:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/28/2022 12:01:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/28/2022 12:01:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/28/2022 12:01:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/28/2022 12:01:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
05/28/2022 12:01:50 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.7410185535185536 on epoch=324
05/28/2022 12:01:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/28/2022 12:01:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/28/2022 12:01:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/28/2022 12:02:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=334
05/28/2022 12:02:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/28/2022 12:02:03 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.79407655062897 on epoch=337
05/28/2022 12:02:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7636071084346946 -> 0.79407655062897 on epoch=337, global_step=1350
05/28/2022 12:02:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/28/2022 12:02:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/28/2022 12:02:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 12:02:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/28/2022 12:02:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/28/2022 12:02:16 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.79407655062897 on epoch=349
05/28/2022 12:02:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 12:02:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
05/28/2022 12:02:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/28/2022 12:02:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=359
05/28/2022 12:02:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=362
05/28/2022 12:02:29 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.7530260875849112 on epoch=362
05/28/2022 12:02:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/28/2022 12:02:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/28/2022 12:02:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=369
05/28/2022 12:02:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=372
05/28/2022 12:02:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/28/2022 12:02:42 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.7782223109957056 on epoch=374
05/28/2022 12:02:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
05/28/2022 12:02:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=379
05/28/2022 12:02:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/28/2022 12:02:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/28/2022 12:02:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/28/2022 12:02:55 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7871212121212121 on epoch=387
05/28/2022 12:02:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/28/2022 12:03:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/28/2022 12:03:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 12:03:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/28/2022 12:03:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/28/2022 12:03:09 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7585585585585585 on epoch=399
05/28/2022 12:03:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/28/2022 12:03:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=404
05/28/2022 12:03:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/28/2022 12:03:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/28/2022 12:03:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/28/2022 12:03:22 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7395094393592677 on epoch=412
05/28/2022 12:03:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
05/28/2022 12:03:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/28/2022 12:03:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/28/2022 12:03:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
05/28/2022 12:03:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/28/2022 12:03:35 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.759828248917433 on epoch=424
05/28/2022 12:03:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/28/2022 12:03:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 12:03:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/28/2022 12:03:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/28/2022 12:03:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/28/2022 12:03:48 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7301963088408101 on epoch=437
05/28/2022 12:03:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/28/2022 12:03:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/28/2022 12:03:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/28/2022 12:03:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/28/2022 12:04:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 12:04:01 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7734912331686525 on epoch=449
05/28/2022 12:04:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/28/2022 12:04:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/28/2022 12:04:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 12:04:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=459
05/28/2022 12:04:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/28/2022 12:04:14 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7560338680926916 on epoch=462
05/28/2022 12:04:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
05/28/2022 12:04:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 12:04:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/28/2022 12:04:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/28/2022 12:04:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 12:04:28 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7618058464832659 on epoch=474
05/28/2022 12:04:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 12:04:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
05/28/2022 12:04:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 12:04:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 12:04:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/28/2022 12:04:41 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7949104949104949 on epoch=487
05/28/2022 12:04:41 - INFO - __main__ - Saving model with best Classification-F1: 0.79407655062897 -> 0.7949104949104949 on epoch=487, global_step=1950
05/28/2022 12:04:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/28/2022 12:04:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 12:04:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/28/2022 12:04:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/28/2022 12:04:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/28/2022 12:04:54 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7862460127591707 on epoch=499
05/28/2022 12:04:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/28/2022 12:04:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/28/2022 12:05:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/28/2022 12:05:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/28/2022 12:05:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 12:05:07 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7760164856939051 on epoch=512
05/28/2022 12:05:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 12:05:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 12:05:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/28/2022 12:05:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/28/2022 12:05:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/28/2022 12:05:20 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.8050128362628364 on epoch=524
05/28/2022 12:05:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7949104949104949 -> 0.8050128362628364 on epoch=524, global_step=2100
05/28/2022 12:05:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 12:05:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/28/2022 12:05:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 12:05:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 12:05:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/28/2022 12:05:34 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.8018253353973168 on epoch=537
05/28/2022 12:05:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 12:05:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/28/2022 12:05:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/28/2022 12:05:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 12:05:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
05/28/2022 12:05:47 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7814502288329519 on epoch=549
05/28/2022 12:05:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/28/2022 12:05:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 12:05:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 12:05:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 12:05:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 12:06:00 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7881193693693693 on epoch=562
05/28/2022 12:06:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 12:06:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 12:06:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/28/2022 12:06:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
05/28/2022 12:06:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/28/2022 12:06:13 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7931944444444443 on epoch=574
05/28/2022 12:06:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/28/2022 12:06:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 12:06:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/28/2022 12:06:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 12:06:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/28/2022 12:06:26 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7602418414918415 on epoch=587
05/28/2022 12:06:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/28/2022 12:06:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/28/2022 12:06:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/28/2022 12:06:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/28/2022 12:06:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/28/2022 12:06:40 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7339133488327036 on epoch=599
05/28/2022 12:06:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 12:06:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/28/2022 12:06:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/28/2022 12:06:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 12:06:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 12:06:53 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7583017077798861 on epoch=612
05/28/2022 12:06:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 12:06:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 12:07:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/28/2022 12:07:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 12:07:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/28/2022 12:07:06 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.8050128362628364 on epoch=624
05/28/2022 12:07:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 12:07:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 12:07:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=632
05/28/2022 12:07:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/28/2022 12:07:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 12:07:19 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.8018253353973168 on epoch=637
05/28/2022 12:07:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/28/2022 12:07:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 12:07:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/28/2022 12:07:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 12:07:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/28/2022 12:07:32 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7819344711128599 on epoch=649
05/28/2022 12:07:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/28/2022 12:07:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 12:07:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 12:07:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 12:07:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 12:07:46 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7902503551909326 on epoch=662
05/28/2022 12:07:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 12:07:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/28/2022 12:07:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 12:07:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=672
05/28/2022 12:07:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
05/28/2022 12:07:59 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.8054782388663968 on epoch=674
05/28/2022 12:07:59 - INFO - __main__ - Saving model with best Classification-F1: 0.8050128362628364 -> 0.8054782388663968 on epoch=674, global_step=2700
05/28/2022 12:08:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 12:08:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 12:08:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 12:08:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/28/2022 12:08:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/28/2022 12:08:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7666812354312355 on epoch=687
05/28/2022 12:08:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/28/2022 12:08:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/28/2022 12:08:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/28/2022 12:08:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/28/2022 12:08:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 12:08:26 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7682923340961099 on epoch=699
05/28/2022 12:08:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/28/2022 12:08:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 12:08:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/28/2022 12:08:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/28/2022 12:08:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 12:08:39 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7819344711128599 on epoch=712
05/28/2022 12:08:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 12:08:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/28/2022 12:08:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 12:08:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 12:08:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 12:08:53 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.8050128362628364 on epoch=724
05/28/2022 12:08:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 12:08:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 12:09:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 12:09:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/28/2022 12:09:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 12:09:07 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.790719696969697 on epoch=737
05/28/2022 12:09:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 12:09:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 12:09:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 12:09:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 12:09:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 12:09:21 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7716310160427807 on epoch=749
05/28/2022 12:09:21 - INFO - __main__ - save last model!
05/28/2022 12:09:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 12:09:21 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 12:09:21 - INFO - __main__ - Printing 3 examples
05/28/2022 12:09:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 12:09:21 - INFO - __main__ - ['others']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 12:09:21 - INFO - __main__ - ['others']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 12:09:21 - INFO - __main__ - ['others']
05/28/2022 12:09:21 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:09:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:09:21 - INFO - __main__ - Printing 3 examples
05/28/2022 12:09:21 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:09:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:09:21 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:09:21 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:09:21 - INFO - __main__ - Printing 3 examples
05/28/2022 12:09:21 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 12:09:21 - INFO - __main__ - ['sad']
05/28/2022 12:09:21 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:09:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:09:21 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:09:23 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:09:28 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 12:09:37 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:09:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:09:37 - INFO - __main__ - Starting training!
05/28/2022 12:11:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/28/2022 12:11:03 - INFO - __main__ - Classification-F1 on test data: 0.2570
05/28/2022 12:11:04 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.8054782388663968, test_performance=0.25696520931808003
05/28/2022 12:11:04 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/28/2022 12:11:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:11:05 - INFO - __main__ - Printing 3 examples
05/28/2022 12:11:05 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:11:05 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:11:05 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:11:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:11:05 - INFO - __main__ - Printing 3 examples
05/28/2022 12:11:05 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/28/2022 12:11:05 - INFO - __main__ - ['sad']
05/28/2022 12:11:05 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:11:05 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:11:05 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:11:23 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:11:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:11:24 - INFO - __main__ - Starting training!
05/28/2022 12:11:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.46 on epoch=2
05/28/2022 12:11:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.95 on epoch=4
05/28/2022 12:11:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.94 on epoch=7
05/28/2022 12:11:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.92 on epoch=9
05/28/2022 12:11:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.48 on epoch=12
05/28/2022 12:11:37 - INFO - __main__ - Global step 50 Train loss 3.15 Classification-F1 0.1 on epoch=12
05/28/2022 12:11:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 12:11:40 - INFO - __main__ - Step 60 Global step 60 Train loss 1.08 on epoch=14
05/28/2022 12:11:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=17
05/28/2022 12:11:45 - INFO - __main__ - Step 80 Global step 80 Train loss 1.12 on epoch=19
05/28/2022 12:11:47 - INFO - __main__ - Step 90 Global step 90 Train loss 1.02 on epoch=22
05/28/2022 12:11:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=24
05/28/2022 12:11:50 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.2000907441016334 on epoch=24
05/28/2022 12:11:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.2000907441016334 on epoch=24, global_step=100
05/28/2022 12:11:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=27
05/28/2022 12:11:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=29
05/28/2022 12:11:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=32
05/28/2022 12:12:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.93 on epoch=34
05/28/2022 12:12:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=37
05/28/2022 12:12:03 - INFO - __main__ - Global step 150 Train loss 0.93 Classification-F1 0.14600840336134455 on epoch=37
05/28/2022 12:12:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/28/2022 12:12:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=42
05/28/2022 12:12:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=44
05/28/2022 12:12:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=47
05/28/2022 12:12:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/28/2022 12:12:16 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.16563380281690143 on epoch=49
05/28/2022 12:12:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=52
05/28/2022 12:12:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.82 on epoch=54
05/28/2022 12:12:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
05/28/2022 12:12:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.84 on epoch=59
05/28/2022 12:12:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=62
05/28/2022 12:12:29 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.16666666666666666 on epoch=62
05/28/2022 12:12:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=64
05/28/2022 12:12:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.80 on epoch=67
05/28/2022 12:12:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.79 on epoch=69
05/28/2022 12:12:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=72
05/28/2022 12:12:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
05/28/2022 12:12:42 - INFO - __main__ - Global step 300 Train loss 0.83 Classification-F1 0.2502462707571067 on epoch=74
05/28/2022 12:12:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2000907441016334 -> 0.2502462707571067 on epoch=74, global_step=300
05/28/2022 12:12:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=77
05/28/2022 12:12:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/28/2022 12:12:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=82
05/28/2022 12:12:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=84
05/28/2022 12:12:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=87
05/28/2022 12:12:55 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.20190476190476192 on epoch=87
05/28/2022 12:12:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.74 on epoch=89
05/28/2022 12:13:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=92
05/28/2022 12:13:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=94
05/28/2022 12:13:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=97
05/28/2022 12:13:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=99
05/28/2022 12:13:08 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.5504464285714286 on epoch=99
05/28/2022 12:13:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2502462707571067 -> 0.5504464285714286 on epoch=99, global_step=400
05/28/2022 12:13:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.76 on epoch=102
05/28/2022 12:13:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=104
05/28/2022 12:13:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
05/28/2022 12:13:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.72 on epoch=109
05/28/2022 12:13:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/28/2022 12:13:21 - INFO - __main__ - Global step 450 Train loss 0.75 Classification-F1 0.33027863777089783 on epoch=112
05/28/2022 12:13:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=114
05/28/2022 12:13:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=117
05/28/2022 12:13:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.58 on epoch=119
05/28/2022 12:13:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=122
05/28/2022 12:13:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.66 on epoch=124
05/28/2022 12:13:33 - INFO - __main__ - Global step 500 Train loss 0.63 Classification-F1 0.43416005291005283 on epoch=124
05/28/2022 12:13:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=127
05/28/2022 12:13:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=129
05/28/2022 12:13:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=132
05/28/2022 12:13:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.65 on epoch=134
05/28/2022 12:13:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=137
05/28/2022 12:13:46 - INFO - __main__ - Global step 550 Train loss 0.63 Classification-F1 0.40386363636363637 on epoch=137
05/28/2022 12:13:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.57 on epoch=139
05/28/2022 12:13:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=142
05/28/2022 12:13:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=144
05/28/2022 12:13:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=147
05/28/2022 12:13:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=149
05/28/2022 12:13:59 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.44447261663286003 on epoch=149
05/28/2022 12:14:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=152
05/28/2022 12:14:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=154
05/28/2022 12:14:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=157
05/28/2022 12:14:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=159
05/28/2022 12:14:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=162
05/28/2022 12:14:12 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.5154893524179677 on epoch=162
05/28/2022 12:14:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=164
05/28/2022 12:14:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=167
05/28/2022 12:14:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=169
05/28/2022 12:14:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=172
05/28/2022 12:14:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=174
05/28/2022 12:14:25 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.5467366302472685 on epoch=174
05/28/2022 12:14:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=177
05/28/2022 12:14:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=179
05/28/2022 12:14:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=182
05/28/2022 12:14:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=184
05/28/2022 12:14:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=187
05/28/2022 12:14:38 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.4542956891317548 on epoch=187
05/28/2022 12:14:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=189
05/28/2022 12:14:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=192
05/28/2022 12:14:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=194
05/28/2022 12:14:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=197
05/28/2022 12:14:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=199
05/28/2022 12:14:51 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.5609019029750737 on epoch=199
05/28/2022 12:14:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5504464285714286 -> 0.5609019029750737 on epoch=199, global_step=800
05/28/2022 12:14:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=202
05/28/2022 12:14:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=204
05/28/2022 12:14:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=207
05/28/2022 12:15:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=209
05/28/2022 12:15:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=212
05/28/2022 12:15:04 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.7574725885904974 on epoch=212
05/28/2022 12:15:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5609019029750737 -> 0.7574725885904974 on epoch=212, global_step=850
05/28/2022 12:15:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=214
05/28/2022 12:15:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=217
05/28/2022 12:15:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=219
05/28/2022 12:15:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=222
05/28/2022 12:15:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=224
05/28/2022 12:15:17 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.6552178973231604 on epoch=224
05/28/2022 12:15:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=227
05/28/2022 12:15:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
05/28/2022 12:15:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=232
05/28/2022 12:15:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=234
05/28/2022 12:15:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=237
05/28/2022 12:15:30 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.6885416666666666 on epoch=237
05/28/2022 12:15:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=239
05/28/2022 12:15:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
05/28/2022 12:15:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=244
05/28/2022 12:15:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=247
05/28/2022 12:15:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=249
05/28/2022 12:15:43 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.7039969834087482 on epoch=249
05/28/2022 12:15:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=252
05/28/2022 12:15:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=254
05/28/2022 12:15:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=257
05/28/2022 12:15:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=259
05/28/2022 12:15:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=262
05/28/2022 12:15:56 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7149985431235432 on epoch=262
05/28/2022 12:15:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
05/28/2022 12:16:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=267
05/28/2022 12:16:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=269
05/28/2022 12:16:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/28/2022 12:16:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=274
05/28/2022 12:16:09 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.7191977377461249 on epoch=274
05/28/2022 12:16:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=277
05/28/2022 12:16:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/28/2022 12:16:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=282
05/28/2022 12:16:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=284
05/28/2022 12:16:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=287
05/28/2022 12:16:22 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.7005645969060603 on epoch=287
05/28/2022 12:16:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=289
05/28/2022 12:16:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/28/2022 12:16:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
05/28/2022 12:16:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
05/28/2022 12:16:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/28/2022 12:16:35 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.7229034531360112 on epoch=299
05/28/2022 12:16:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=302
05/28/2022 12:16:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/28/2022 12:16:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/28/2022 12:16:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/28/2022 12:16:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=312
05/28/2022 12:16:48 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.7208174178762414 on epoch=312
05/28/2022 12:16:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=314
05/28/2022 12:16:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=317
05/28/2022 12:16:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/28/2022 12:16:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=322
05/28/2022 12:17:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/28/2022 12:17:01 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.6930407298054356 on epoch=324
05/28/2022 12:17:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=327
05/28/2022 12:17:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/28/2022 12:17:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/28/2022 12:17:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=334
05/28/2022 12:17:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/28/2022 12:17:14 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.6706484348125216 on epoch=337
05/28/2022 12:17:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
05/28/2022 12:17:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/28/2022 12:17:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=344
05/28/2022 12:17:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=347
05/28/2022 12:17:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
05/28/2022 12:17:27 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.688782991202346 on epoch=349
05/28/2022 12:17:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/28/2022 12:17:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/28/2022 12:17:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=357
05/28/2022 12:17:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/28/2022 12:17:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=362
05/28/2022 12:17:40 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.6976849413131272 on epoch=362
05/28/2022 12:17:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/28/2022 12:17:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=367
05/28/2022 12:17:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
05/28/2022 12:17:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/28/2022 12:17:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/28/2022 12:17:53 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.7043470043470044 on epoch=374
05/28/2022 12:17:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/28/2022 12:17:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=379
05/28/2022 12:18:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/28/2022 12:18:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=384
05/28/2022 12:18:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/28/2022 12:18:06 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.7175014969132616 on epoch=387
05/28/2022 12:18:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=389
05/28/2022 12:18:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
05/28/2022 12:18:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/28/2022 12:18:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/28/2022 12:18:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/28/2022 12:18:19 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7161620082815736 on epoch=399
05/28/2022 12:18:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/28/2022 12:18:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/28/2022 12:18:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/28/2022 12:18:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/28/2022 12:18:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/28/2022 12:18:32 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7105054541336401 on epoch=412
05/28/2022 12:18:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
05/28/2022 12:18:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/28/2022 12:18:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/28/2022 12:18:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/28/2022 12:18:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=424
05/28/2022 12:18:45 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7229885057471264 on epoch=424
05/28/2022 12:18:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/28/2022 12:18:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/28/2022 12:18:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/28/2022 12:18:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=434
05/28/2022 12:18:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/28/2022 12:18:58 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7058823529411766 on epoch=437
05/28/2022 12:19:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/28/2022 12:19:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 12:19:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=444
05/28/2022 12:19:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/28/2022 12:19:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/28/2022 12:19:12 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7615254550738422 on epoch=449
05/28/2022 12:19:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7574725885904974 -> 0.7615254550738422 on epoch=449, global_step=1800
05/28/2022 12:19:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/28/2022 12:19:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 12:19:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/28/2022 12:19:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/28/2022 12:19:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/28/2022 12:19:25 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.7340960240060015 on epoch=462
05/28/2022 12:19:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/28/2022 12:19:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/28/2022 12:19:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/28/2022 12:19:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
05/28/2022 12:19:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/28/2022 12:19:38 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7610047846889952 on epoch=474
05/28/2022 12:19:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 12:19:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=479
05/28/2022 12:19:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 12:19:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 12:19:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 12:19:51 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7786661255411256 on epoch=487
05/28/2022 12:19:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7615254550738422 -> 0.7786661255411256 on epoch=487, global_step=1950
05/28/2022 12:19:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/28/2022 12:19:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/28/2022 12:19:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 12:20:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/28/2022 12:20:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/28/2022 12:20:05 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7277150385846038 on epoch=499
05/28/2022 12:20:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/28/2022 12:20:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 12:20:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/28/2022 12:20:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 12:20:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/28/2022 12:20:18 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6550791424564215 on epoch=512
05/28/2022 12:20:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=514
05/28/2022 12:20:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/28/2022 12:20:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 12:20:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/28/2022 12:20:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/28/2022 12:20:31 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6983832224685883 on epoch=524
05/28/2022 12:20:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/28/2022 12:20:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/28/2022 12:20:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/28/2022 12:20:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/28/2022 12:20:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/28/2022 12:20:45 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6765660809778457 on epoch=537
05/28/2022 12:20:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/28/2022 12:20:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/28/2022 12:20:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=544
05/28/2022 12:20:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 12:20:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 12:20:58 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7668650593010993 on epoch=549
05/28/2022 12:21:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 12:21:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/28/2022 12:21:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/28/2022 12:21:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/28/2022 12:21:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 12:21:11 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7464808558558558 on epoch=562
05/28/2022 12:21:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 12:21:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/28/2022 12:21:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 12:21:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=572
05/28/2022 12:21:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/28/2022 12:21:24 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7492749880819959 on epoch=574
05/28/2022 12:21:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
05/28/2022 12:21:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 12:21:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/28/2022 12:21:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 12:21:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/28/2022 12:21:38 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7284518620002491 on epoch=587
05/28/2022 12:21:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/28/2022 12:21:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/28/2022 12:21:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 12:21:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/28/2022 12:21:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/28/2022 12:21:51 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7961009711009711 on epoch=599
05/28/2022 12:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7786661255411256 -> 0.7961009711009711 on epoch=599, global_step=2400
05/28/2022 12:21:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/28/2022 12:21:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/28/2022 12:21:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/28/2022 12:22:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 12:22:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/28/2022 12:22:04 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.658974358974359 on epoch=612
05/28/2022 12:22:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/28/2022 12:22:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 12:22:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/28/2022 12:22:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 12:22:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/28/2022 12:22:18 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.667227485775873 on epoch=624
05/28/2022 12:22:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/28/2022 12:22:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 12:22:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/28/2022 12:22:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 12:22:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/28/2022 12:22:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6760576923076923 on epoch=637
05/28/2022 12:22:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/28/2022 12:22:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 12:22:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 12:22:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 12:22:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
05/28/2022 12:22:44 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6326238390092879 on epoch=649
05/28/2022 12:22:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/28/2022 12:22:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/28/2022 12:22:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 12:22:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 12:22:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 12:22:57 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6833842627960275 on epoch=662
05/28/2022 12:23:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 12:23:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/28/2022 12:23:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/28/2022 12:23:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 12:23:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 12:23:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.649800099950025 on epoch=674
05/28/2022 12:23:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 12:23:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/28/2022 12:23:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/28/2022 12:23:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/28/2022 12:23:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 12:23:24 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7038796959434594 on epoch=687
05/28/2022 12:23:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/28/2022 12:23:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/28/2022 12:23:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 12:23:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/28/2022 12:23:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 12:23:37 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6814301513790005 on epoch=699
05/28/2022 12:23:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/28/2022 12:23:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/28/2022 12:23:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
05/28/2022 12:23:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 12:23:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 12:23:51 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6511654363434118 on epoch=712
05/28/2022 12:23:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 12:23:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
05/28/2022 12:23:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/28/2022 12:24:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=722
05/28/2022 12:24:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/28/2022 12:24:04 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.633414816772375 on epoch=724
05/28/2022 12:24:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
05/28/2022 12:24:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/28/2022 12:24:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/28/2022 12:24:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 12:24:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/28/2022 12:24:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.721801251213016 on epoch=737
05/28/2022 12:24:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/28/2022 12:24:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/28/2022 12:24:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 12:24:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/28/2022 12:24:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/28/2022 12:24:31 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7765931372549019 on epoch=749
05/28/2022 12:24:31 - INFO - __main__ - save last model!
05/28/2022 12:24:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 12:24:31 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 12:24:31 - INFO - __main__ - Printing 3 examples
05/28/2022 12:24:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 12:24:31 - INFO - __main__ - ['others']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 12:24:31 - INFO - __main__ - ['others']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 12:24:31 - INFO - __main__ - ['others']
05/28/2022 12:24:31 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:24:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:24:31 - INFO - __main__ - Printing 3 examples
05/28/2022 12:24:31 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:24:31 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:24:31 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:24:31 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:24:31 - INFO - __main__ - Printing 3 examples
05/28/2022 12:24:31 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:24:31 - INFO - __main__ - ['happy']
05/28/2022 12:24:31 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:24:31 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:24:31 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:24:33 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:24:38 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 12:24:46 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:24:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:24:47 - INFO - __main__ - Starting training!
05/28/2022 12:26:13 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/28/2022 12:26:13 - INFO - __main__ - Classification-F1 on test data: 0.3639
05/28/2022 12:26:13 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7961009711009711, test_performance=0.36389172892901367
05/28/2022 12:26:13 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/28/2022 12:26:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:26:14 - INFO - __main__ - Printing 3 examples
05/28/2022 12:26:14 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:26:14 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:26:14 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:26:14 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:26:14 - INFO - __main__ - Printing 3 examples
05/28/2022 12:26:14 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:26:14 - INFO - __main__ - ['happy']
05/28/2022 12:26:14 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:26:14 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:26:14 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:26:29 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:26:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:26:30 - INFO - __main__ - Starting training!
05/28/2022 12:26:33 - INFO - __main__ - Step 10 Global step 10 Train loss 4.53 on epoch=2
05/28/2022 12:26:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.13 on epoch=4
05/28/2022 12:26:38 - INFO - __main__ - Step 30 Global step 30 Train loss 1.24 on epoch=7
05/28/2022 12:26:41 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/28/2022 12:26:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=12
05/28/2022 12:26:44 - INFO - __main__ - Global step 50 Train loss 1.98 Classification-F1 0.1581196581196581 on epoch=12
05/28/2022 12:26:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1581196581196581 on epoch=12, global_step=50
05/28/2022 12:26:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=14
05/28/2022 12:26:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=17
05/28/2022 12:26:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
05/28/2022 12:26:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
05/28/2022 12:26:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=24
05/28/2022 12:26:57 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.21743697478991597 on epoch=24
05/28/2022 12:26:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1581196581196581 -> 0.21743697478991597 on epoch=24, global_step=100
05/28/2022 12:27:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.85 on epoch=27
05/28/2022 12:27:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=29
05/28/2022 12:27:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.78 on epoch=32
05/28/2022 12:27:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
05/28/2022 12:27:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=37
05/28/2022 12:27:10 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.26329787234042556 on epoch=37
05/28/2022 12:27:10 - INFO - __main__ - Saving model with best Classification-F1: 0.21743697478991597 -> 0.26329787234042556 on epoch=37, global_step=150
05/28/2022 12:27:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=39
05/28/2022 12:27:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=42
05/28/2022 12:27:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=44
05/28/2022 12:27:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=47
05/28/2022 12:27:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
05/28/2022 12:27:23 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.1581196581196581 on epoch=49
05/28/2022 12:27:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=52
05/28/2022 12:27:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=54
05/28/2022 12:27:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=57
05/28/2022 12:27:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.74 on epoch=59
05/28/2022 12:27:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=62
05/28/2022 12:27:37 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.4572301967038809 on epoch=62
05/28/2022 12:27:37 - INFO - __main__ - Saving model with best Classification-F1: 0.26329787234042556 -> 0.4572301967038809 on epoch=62, global_step=250
05/28/2022 12:27:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.70 on epoch=64
05/28/2022 12:27:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=67
05/28/2022 12:27:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.66 on epoch=69
05/28/2022 12:27:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=72
05/28/2022 12:27:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=74
05/28/2022 12:27:50 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.4672140762463343 on epoch=74
05/28/2022 12:27:50 - INFO - __main__ - Saving model with best Classification-F1: 0.4572301967038809 -> 0.4672140762463343 on epoch=74, global_step=300
05/28/2022 12:27:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=77
05/28/2022 12:27:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=79
05/28/2022 12:27:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=82
05/28/2022 12:28:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=84
05/28/2022 12:28:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
05/28/2022 12:28:03 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.4533720355731225 on epoch=87
05/28/2022 12:28:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=89
05/28/2022 12:28:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=92
05/28/2022 12:28:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=94
05/28/2022 12:28:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=97
05/28/2022 12:28:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=99
05/28/2022 12:28:16 - INFO - __main__ - Global step 400 Train loss 0.32 Classification-F1 0.5270776022007548 on epoch=99
05/28/2022 12:28:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4672140762463343 -> 0.5270776022007548 on epoch=99, global_step=400
05/28/2022 12:28:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/28/2022 12:28:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=104
05/28/2022 12:28:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=107
05/28/2022 12:28:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.16 on epoch=109
05/28/2022 12:28:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=112
05/28/2022 12:28:29 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.557498952660243 on epoch=112
05/28/2022 12:28:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5270776022007548 -> 0.557498952660243 on epoch=112, global_step=450
05/28/2022 12:28:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=114
05/28/2022 12:28:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=117
05/28/2022 12:28:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/28/2022 12:28:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=122
05/28/2022 12:28:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=124
05/28/2022 12:28:42 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.5276508859357696 on epoch=124
05/28/2022 12:28:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=127
05/28/2022 12:28:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/28/2022 12:28:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=132
05/28/2022 12:28:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=134
05/28/2022 12:28:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
05/28/2022 12:28:56 - INFO - __main__ - Global step 550 Train loss 0.14 Classification-F1 0.46573671497584535 on epoch=137
05/28/2022 12:28:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=139
05/28/2022 12:29:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.10 on epoch=142
05/28/2022 12:29:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.10 on epoch=144
05/28/2022 12:29:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=147
05/28/2022 12:29:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.12 on epoch=149
05/28/2022 12:29:09 - INFO - __main__ - Global step 600 Train loss 0.11 Classification-F1 0.503968253968254 on epoch=149
05/28/2022 12:29:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=152
05/28/2022 12:29:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/28/2022 12:29:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.06 on epoch=157
05/28/2022 12:29:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=159
05/28/2022 12:29:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=162
05/28/2022 12:29:22 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.5615421455938697 on epoch=162
05/28/2022 12:29:22 - INFO - __main__ - Saving model with best Classification-F1: 0.557498952660243 -> 0.5615421455938697 on epoch=162, global_step=650
05/28/2022 12:29:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=164
05/28/2022 12:29:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=167
05/28/2022 12:29:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
05/28/2022 12:29:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=172
05/28/2022 12:29:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=174
05/28/2022 12:29:36 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.5701052345448897 on epoch=174
05/28/2022 12:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5615421455938697 -> 0.5701052345448897 on epoch=174, global_step=700
05/28/2022 12:29:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
05/28/2022 12:29:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/28/2022 12:29:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/28/2022 12:29:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=184
05/28/2022 12:29:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
05/28/2022 12:29:49 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.4394908386187456 on epoch=187
05/28/2022 12:29:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=189
05/28/2022 12:29:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/28/2022 12:29:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
05/28/2022 12:29:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/28/2022 12:30:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=199
05/28/2022 12:30:02 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.5959595202398801 on epoch=199
05/28/2022 12:30:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5701052345448897 -> 0.5959595202398801 on epoch=199, global_step=800
05/28/2022 12:30:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
05/28/2022 12:30:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=204
05/28/2022 12:30:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
05/28/2022 12:30:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
05/28/2022 12:30:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/28/2022 12:30:16 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.5550883095037847 on epoch=212
05/28/2022 12:30:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
05/28/2022 12:30:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/28/2022 12:30:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
05/28/2022 12:30:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
05/28/2022 12:30:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=224
05/28/2022 12:30:29 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.5890585839598999 on epoch=224
05/28/2022 12:30:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/28/2022 12:30:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/28/2022 12:30:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/28/2022 12:30:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
05/28/2022 12:30:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
05/28/2022 12:30:42 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.5437852412798322 on epoch=237
05/28/2022 12:30:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/28/2022 12:30:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
05/28/2022 12:30:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=244
05/28/2022 12:30:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/28/2022 12:30:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/28/2022 12:30:56 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5815019117745275 on epoch=249
05/28/2022 12:30:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/28/2022 12:31:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/28/2022 12:31:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/28/2022 12:31:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/28/2022 12:31:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/28/2022 12:31:09 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.5961188472583985 on epoch=262
05/28/2022 12:31:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5959595202398801 -> 0.5961188472583985 on epoch=262, global_step=1050
05/28/2022 12:31:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=264
05/28/2022 12:31:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/28/2022 12:31:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/28/2022 12:31:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/28/2022 12:31:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/28/2022 12:31:23 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.6037202380952381 on epoch=274
05/28/2022 12:31:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5961188472583985 -> 0.6037202380952381 on epoch=274, global_step=1100
05/28/2022 12:31:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/28/2022 12:31:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/28/2022 12:31:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=282
05/28/2022 12:31:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/28/2022 12:31:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=287
05/28/2022 12:31:36 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.6202546296296296 on epoch=287
05/28/2022 12:31:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6037202380952381 -> 0.6202546296296296 on epoch=287, global_step=1150
05/28/2022 12:31:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=289
05/28/2022 12:31:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 12:31:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/28/2022 12:31:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=297
05/28/2022 12:31:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/28/2022 12:31:50 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5957386363636364 on epoch=299
05/28/2022 12:31:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=302
05/28/2022 12:31:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
05/28/2022 12:31:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=307
05/28/2022 12:32:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=309
05/28/2022 12:32:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
05/28/2022 12:32:04 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.6054324624958664 on epoch=312
05/28/2022 12:32:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/28/2022 12:32:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/28/2022 12:32:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/28/2022 12:32:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/28/2022 12:32:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
05/28/2022 12:32:17 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6213355492589363 on epoch=324
05/28/2022 12:32:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6202546296296296 -> 0.6213355492589363 on epoch=324, global_step=1300
05/28/2022 12:32:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
05/28/2022 12:32:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/28/2022 12:32:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/28/2022 12:32:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/28/2022 12:32:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/28/2022 12:32:31 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5843362282878413 on epoch=337
05/28/2022 12:32:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
05/28/2022 12:32:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/28/2022 12:32:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 12:32:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
05/28/2022 12:32:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
05/28/2022 12:32:44 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5634072580645161 on epoch=349
05/28/2022 12:32:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 12:32:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
05/28/2022 12:32:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/28/2022 12:32:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/28/2022 12:32:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/28/2022 12:32:58 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.591590477833362 on epoch=362
05/28/2022 12:33:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/28/2022 12:33:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/28/2022 12:33:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/28/2022 12:33:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/28/2022 12:33:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 12:33:12 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6374637310121182 on epoch=374
05/28/2022 12:33:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6213355492589363 -> 0.6374637310121182 on epoch=374, global_step=1500
05/28/2022 12:33:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/28/2022 12:33:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/28/2022 12:33:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/28/2022 12:33:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/28/2022 12:33:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/28/2022 12:33:26 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5670216362407032 on epoch=387
05/28/2022 12:33:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/28/2022 12:33:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/28/2022 12:33:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/28/2022 12:33:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/28/2022 12:33:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/28/2022 12:33:39 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6516093892754234 on epoch=399
05/28/2022 12:33:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6374637310121182 -> 0.6516093892754234 on epoch=399, global_step=1600
05/28/2022 12:33:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 12:33:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/28/2022 12:33:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/28/2022 12:33:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/28/2022 12:33:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 12:33:53 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6061781609195402 on epoch=412
05/28/2022 12:33:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/28/2022 12:33:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/28/2022 12:34:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/28/2022 12:34:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/28/2022 12:34:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/28/2022 12:34:06 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6080347211187034 on epoch=424
05/28/2022 12:34:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/28/2022 12:34:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/28/2022 12:34:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/28/2022 12:34:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 12:34:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/28/2022 12:34:20 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.6123949579831933 on epoch=437
05/28/2022 12:34:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 12:34:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/28/2022 12:34:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/28/2022 12:34:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/28/2022 12:34:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/28/2022 12:34:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6425213675213676 on epoch=449
05/28/2022 12:34:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/28/2022 12:34:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 12:34:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 12:34:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/28/2022 12:34:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/28/2022 12:34:47 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.6396103896103896 on epoch=462
05/28/2022 12:34:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/28/2022 12:34:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 12:34:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/28/2022 12:34:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/28/2022 12:34:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/28/2022 12:35:01 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.63996138996139 on epoch=474
05/28/2022 12:35:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/28/2022 12:35:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 12:35:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/28/2022 12:35:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/28/2022 12:35:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/28/2022 12:35:14 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6235823934837093 on epoch=487
05/28/2022 12:35:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 12:35:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 12:35:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 12:35:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 12:35:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 12:35:27 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6047054597701149 on epoch=499
05/28/2022 12:35:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 12:35:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/28/2022 12:35:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 12:35:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 12:35:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 12:35:41 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.592857142857143 on epoch=512
05/28/2022 12:35:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 12:35:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 12:35:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 12:35:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 12:35:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 12:35:54 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.5947831738154319 on epoch=524
05/28/2022 12:35:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 12:35:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 12:36:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 12:36:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 12:36:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 12:36:08 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.5909499834123686 on epoch=537
05/28/2022 12:36:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/28/2022 12:36:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/28/2022 12:36:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/28/2022 12:36:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 12:36:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 12:36:21 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6267174432497014 on epoch=549
05/28/2022 12:36:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 12:36:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=554
05/28/2022 12:36:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 12:36:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 12:36:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 12:36:35 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6222162097162097 on epoch=562
05/28/2022 12:36:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 12:36:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 12:36:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 12:36:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 12:36:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 12:36:48 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.6366324307500778 on epoch=574
05/28/2022 12:36:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 12:36:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 12:36:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 12:36:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 12:37:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 12:37:02 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6076885762794082 on epoch=587
05/28/2022 12:37:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 12:37:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 12:37:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 12:37:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 12:37:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 12:37:15 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6047054597701149 on epoch=599
05/28/2022 12:37:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 12:37:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 12:37:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/28/2022 12:37:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/28/2022 12:37:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 12:37:29 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6395400265369307 on epoch=612
05/28/2022 12:37:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 12:37:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 12:37:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 12:37:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 12:37:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 12:37:42 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6579196843902726 on epoch=624
05/28/2022 12:37:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6516093892754234 -> 0.6579196843902726 on epoch=624, global_step=2500
05/28/2022 12:37:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 12:37:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 12:37:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 12:37:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 12:37:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 12:37:56 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.5927738927738928 on epoch=637
05/28/2022 12:37:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 12:38:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 12:38:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 12:38:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 12:38:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 12:38:09 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.5804945054945054 on epoch=649
05/28/2022 12:38:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/28/2022 12:38:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 12:38:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
05/28/2022 12:38:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 12:38:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 12:38:23 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6411883781852823 on epoch=662
05/28/2022 12:38:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 12:38:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 12:38:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 12:38:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 12:38:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 12:38:36 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6257412351162351 on epoch=674
05/28/2022 12:38:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 12:38:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 12:38:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 12:38:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 12:38:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 12:38:49 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6097704714640199 on epoch=687
05/28/2022 12:38:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 12:38:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 12:38:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 12:38:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 12:39:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 12:39:03 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6122277216027215 on epoch=699
05/28/2022 12:39:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 12:39:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/28/2022 12:39:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 12:39:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/28/2022 12:39:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 12:39:16 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.593441465432024 on epoch=712
05/28/2022 12:39:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 12:39:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
05/28/2022 12:39:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 12:39:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 12:39:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 12:39:30 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5752032520325203 on epoch=724
05/28/2022 12:39:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 12:39:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 12:39:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 12:39:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 12:39:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 12:39:43 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6004496578690127 on epoch=737
05/28/2022 12:39:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 12:39:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 12:39:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 12:39:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/28/2022 12:39:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 12:39:57 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6433431085043988 on epoch=749
05/28/2022 12:39:57 - INFO - __main__ - save last model!
05/28/2022 12:39:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 12:39:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 12:39:57 - INFO - __main__ - Printing 3 examples
05/28/2022 12:39:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 12:39:57 - INFO - __main__ - ['others']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 12:39:57 - INFO - __main__ - ['others']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 12:39:57 - INFO - __main__ - ['others']
05/28/2022 12:39:57 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:39:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:39:57 - INFO - __main__ - Printing 3 examples
05/28/2022 12:39:57 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:39:57 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:39:57 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:39:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:39:57 - INFO - __main__ - Printing 3 examples
05/28/2022 12:39:57 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:39:57 - INFO - __main__ - ['happy']
05/28/2022 12:39:57 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:39:57 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:39:57 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:39:59 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:40:04 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 12:40:12 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:40:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:40:13 - INFO - __main__ - Starting training!
05/28/2022 12:41:40 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/28/2022 12:41:40 - INFO - __main__ - Classification-F1 on test data: 0.1214
05/28/2022 12:41:40 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.6579196843902726, test_performance=0.12139426362168686
05/28/2022 12:41:40 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/28/2022 12:41:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:41:41 - INFO - __main__ - Printing 3 examples
05/28/2022 12:41:41 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:41:41 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:41:41 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:41:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:41:41 - INFO - __main__ - Printing 3 examples
05/28/2022 12:41:41 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:41:41 - INFO - __main__ - ['happy']
05/28/2022 12:41:41 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:41:41 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:41:41 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:41:56 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:41:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:41:57 - INFO - __main__ - Starting training!
05/28/2022 12:42:00 - INFO - __main__ - Step 10 Global step 10 Train loss 4.48 on epoch=2
05/28/2022 12:42:03 - INFO - __main__ - Step 20 Global step 20 Train loss 2.30 on epoch=4
05/28/2022 12:42:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.25 on epoch=7
05/28/2022 12:42:08 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=9
05/28/2022 12:42:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=12
05/28/2022 12:42:11 - INFO - __main__ - Global step 50 Train loss 2.04 Classification-F1 0.1 on epoch=12
05/28/2022 12:42:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 12:42:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.96 on epoch=14
05/28/2022 12:42:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=17
05/28/2022 12:42:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=19
05/28/2022 12:42:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=22
05/28/2022 12:42:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=24
05/28/2022 12:42:24 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.1 on epoch=24
05/28/2022 12:42:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
05/28/2022 12:42:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=29
05/28/2022 12:42:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
05/28/2022 12:42:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=34
05/28/2022 12:42:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=37
05/28/2022 12:42:37 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.24578785436964323 on epoch=37
05/28/2022 12:42:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.24578785436964323 on epoch=37, global_step=150
05/28/2022 12:42:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=39
05/28/2022 12:42:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.76 on epoch=42
05/28/2022 12:42:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=44
05/28/2022 12:42:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=47
05/28/2022 12:42:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/28/2022 12:42:50 - INFO - __main__ - Global step 200 Train loss 0.83 Classification-F1 0.22113289760348584 on epoch=49
05/28/2022 12:42:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=52
05/28/2022 12:42:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=54
05/28/2022 12:42:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.77 on epoch=57
05/28/2022 12:43:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=59
05/28/2022 12:43:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=62
05/28/2022 12:43:04 - INFO - __main__ - Global step 250 Train loss 0.74 Classification-F1 0.28335889891953964 on epoch=62
05/28/2022 12:43:04 - INFO - __main__ - Saving model with best Classification-F1: 0.24578785436964323 -> 0.28335889891953964 on epoch=62, global_step=250
05/28/2022 12:43:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
05/28/2022 12:43:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.70 on epoch=67
05/28/2022 12:43:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
05/28/2022 12:43:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=72
05/28/2022 12:43:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=74
05/28/2022 12:43:17 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.4476890756302521 on epoch=74
05/28/2022 12:43:17 - INFO - __main__ - Saving model with best Classification-F1: 0.28335889891953964 -> 0.4476890756302521 on epoch=74, global_step=300
05/28/2022 12:43:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=77
05/28/2022 12:43:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.69 on epoch=79
05/28/2022 12:43:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=82
05/28/2022 12:43:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.63 on epoch=84
05/28/2022 12:43:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
05/28/2022 12:43:30 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.4744076711818648 on epoch=87
05/28/2022 12:43:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4476890756302521 -> 0.4744076711818648 on epoch=87, global_step=350
05/28/2022 12:43:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=89
05/28/2022 12:43:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=92
05/28/2022 12:43:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=94
05/28/2022 12:43:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=97
05/28/2022 12:43:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=99
05/28/2022 12:43:44 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.5662515664160401 on epoch=99
05/28/2022 12:43:44 - INFO - __main__ - Saving model with best Classification-F1: 0.4744076711818648 -> 0.5662515664160401 on epoch=99, global_step=400
05/28/2022 12:43:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=102
05/28/2022 12:43:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=104
05/28/2022 12:43:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=107
05/28/2022 12:43:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=109
05/28/2022 12:43:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=112
05/28/2022 12:43:57 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.5142323875283202 on epoch=112
05/28/2022 12:43:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=114
05/28/2022 12:44:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=117
05/28/2022 12:44:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=119
05/28/2022 12:44:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=122
05/28/2022 12:44:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=124
05/28/2022 12:44:10 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.5736010226332807 on epoch=124
05/28/2022 12:44:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5662515664160401 -> 0.5736010226332807 on epoch=124, global_step=500
05/28/2022 12:44:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=127
05/28/2022 12:44:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=129
05/28/2022 12:44:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=132
05/28/2022 12:44:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=134
05/28/2022 12:44:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
05/28/2022 12:44:24 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.6525673400673401 on epoch=137
05/28/2022 12:44:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5736010226332807 -> 0.6525673400673401 on epoch=137, global_step=550
05/28/2022 12:44:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=139
05/28/2022 12:44:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
05/28/2022 12:44:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
05/28/2022 12:44:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=147
05/28/2022 12:44:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
05/28/2022 12:44:37 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.6301805953418855 on epoch=149
05/28/2022 12:44:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=152
05/28/2022 12:44:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
05/28/2022 12:44:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/28/2022 12:44:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=159
05/28/2022 12:44:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
05/28/2022 12:44:50 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.5545107466063348 on epoch=162
05/28/2022 12:44:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=164
05/28/2022 12:44:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
05/28/2022 12:44:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=169
05/28/2022 12:45:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=172
05/28/2022 12:45:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=174
05/28/2022 12:45:03 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.598714208089208 on epoch=174
05/28/2022 12:45:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=177
05/28/2022 12:45:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
05/28/2022 12:45:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=182
05/28/2022 12:45:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
05/28/2022 12:45:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=187
05/28/2022 12:45:16 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.6630909658293025 on epoch=187
05/28/2022 12:45:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6525673400673401 -> 0.6630909658293025 on epoch=187, global_step=750
05/28/2022 12:45:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=189
05/28/2022 12:45:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/28/2022 12:45:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/28/2022 12:45:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/28/2022 12:45:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=199
05/28/2022 12:45:30 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.5996572580645161 on epoch=199
05/28/2022 12:45:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
05/28/2022 12:45:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=204
05/28/2022 12:45:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/28/2022 12:45:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/28/2022 12:45:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/28/2022 12:45:43 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.6330214500946209 on epoch=212
05/28/2022 12:45:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
05/28/2022 12:45:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/28/2022 12:45:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=219
05/28/2022 12:45:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/28/2022 12:45:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=224
05/28/2022 12:45:57 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.5695348837209302 on epoch=224
05/28/2022 12:45:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=227
05/28/2022 12:46:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
05/28/2022 12:46:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/28/2022 12:46:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/28/2022 12:46:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/28/2022 12:46:10 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6476689976689977 on epoch=237
05/28/2022 12:46:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=239
05/28/2022 12:46:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
05/28/2022 12:46:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/28/2022 12:46:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/28/2022 12:46:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/28/2022 12:46:23 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.5820118949151207 on epoch=249
05/28/2022 12:46:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
05/28/2022 12:46:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/28/2022 12:46:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/28/2022 12:46:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/28/2022 12:46:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/28/2022 12:46:37 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.5645436321187155 on epoch=262
05/28/2022 12:46:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/28/2022 12:46:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/28/2022 12:46:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
05/28/2022 12:46:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=272
05/28/2022 12:46:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=274
05/28/2022 12:46:50 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6154771944620141 on epoch=274
05/28/2022 12:46:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/28/2022 12:46:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/28/2022 12:46:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/28/2022 12:47:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/28/2022 12:47:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/28/2022 12:47:04 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.49708463949843257 on epoch=287
05/28/2022 12:47:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/28/2022 12:47:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 12:47:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/28/2022 12:47:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/28/2022 12:47:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/28/2022 12:47:17 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.5860015898251192 on epoch=299
05/28/2022 12:47:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=302
05/28/2022 12:47:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/28/2022 12:47:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 12:47:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/28/2022 12:47:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
05/28/2022 12:47:30 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.55451054692434 on epoch=312
05/28/2022 12:47:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/28/2022 12:47:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=317
05/28/2022 12:47:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/28/2022 12:47:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/28/2022 12:47:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=324
05/28/2022 12:47:44 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.5580303030303031 on epoch=324
05/28/2022 12:47:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/28/2022 12:47:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/28/2022 12:47:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
05/28/2022 12:47:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/28/2022 12:47:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/28/2022 12:47:57 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5558911033048963 on epoch=337
05/28/2022 12:48:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/28/2022 12:48:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/28/2022 12:48:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 12:48:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/28/2022 12:48:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/28/2022 12:48:11 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6315151515151516 on epoch=349
05/28/2022 12:48:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/28/2022 12:48:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=354
05/28/2022 12:48:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/28/2022 12:48:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=359
05/28/2022 12:48:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/28/2022 12:48:24 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6337388102093984 on epoch=362
05/28/2022 12:48:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/28/2022 12:48:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 12:48:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=369
05/28/2022 12:48:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/28/2022 12:48:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 12:48:38 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5885416666666666 on epoch=374
05/28/2022 12:48:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/28/2022 12:48:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/28/2022 12:48:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
05/28/2022 12:48:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/28/2022 12:48:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/28/2022 12:48:51 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.618880404725993 on epoch=387
05/28/2022 12:48:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/28/2022 12:48:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
05/28/2022 12:48:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/28/2022 12:49:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/28/2022 12:49:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/28/2022 12:49:05 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6044314638064638 on epoch=399
05/28/2022 12:49:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/28/2022 12:49:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/28/2022 12:49:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/28/2022 12:49:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/28/2022 12:49:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/28/2022 12:49:19 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.599859943977591 on epoch=412
05/28/2022 12:49:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/28/2022 12:49:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/28/2022 12:49:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/28/2022 12:49:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 12:49:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/28/2022 12:49:32 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6048815700428604 on epoch=424
05/28/2022 12:49:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/28/2022 12:49:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 12:49:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/28/2022 12:49:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/28/2022 12:49:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/28/2022 12:49:46 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5430059523809524 on epoch=437
05/28/2022 12:49:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 12:49:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 12:49:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/28/2022 12:49:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/28/2022 12:49:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 12:49:59 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5697799921937853 on epoch=449
05/28/2022 12:50:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/28/2022 12:50:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 12:50:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 12:50:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/28/2022 12:50:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/28/2022 12:50:13 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.619582978957979 on epoch=462
05/28/2022 12:50:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/28/2022 12:50:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/28/2022 12:50:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/28/2022 12:50:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
05/28/2022 12:50:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 12:50:26 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6325019936204147 on epoch=474
05/28/2022 12:50:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/28/2022 12:50:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/28/2022 12:50:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
05/28/2022 12:50:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 12:50:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 12:50:40 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6608630952380953 on epoch=487
05/28/2022 12:50:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 12:50:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 12:50:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 12:50:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 12:50:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 12:50:54 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6450772200772201 on epoch=499
05/28/2022 12:50:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 12:50:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/28/2022 12:51:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 12:51:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/28/2022 12:51:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 12:51:07 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6106597138006307 on epoch=512
05/28/2022 12:51:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/28/2022 12:51:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/28/2022 12:51:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 12:51:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 12:51:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/28/2022 12:51:21 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.5726406926406926 on epoch=524
05/28/2022 12:51:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 12:51:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 12:51:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/28/2022 12:51:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 12:51:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 12:51:34 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.6179960317460318 on epoch=537
05/28/2022 12:51:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 12:51:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 12:51:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/28/2022 12:51:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/28/2022 12:51:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 12:51:48 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.5722108575924469 on epoch=549
05/28/2022 12:51:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/28/2022 12:51:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 12:51:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 12:51:58 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 12:52:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 12:52:02 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6315151515151516 on epoch=562
05/28/2022 12:52:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 12:52:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 12:52:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 12:52:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 12:52:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 12:52:15 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.5894551282051281 on epoch=574
05/28/2022 12:52:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 12:52:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/28/2022 12:52:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/28/2022 12:52:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 12:52:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 12:52:29 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6143812434135014 on epoch=587
05/28/2022 12:52:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 12:52:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 12:52:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 12:52:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 12:52:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 12:52:43 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.617912912912913 on epoch=599
05/28/2022 12:52:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 12:52:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 12:52:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/28/2022 12:52:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/28/2022 12:52:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 12:52:56 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5100414078674947 on epoch=612
05/28/2022 12:52:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 12:53:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 12:53:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 12:53:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 12:53:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 12:53:10 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.5045701581027668 on epoch=624
05/28/2022 12:53:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/28/2022 12:53:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 12:53:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 12:53:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 12:53:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 12:53:24 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.5592428861788618 on epoch=637
05/28/2022 12:53:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/28/2022 12:53:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/28/2022 12:53:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 12:53:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 12:53:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/28/2022 12:53:37 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6037990612204134 on epoch=649
05/28/2022 12:53:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 12:53:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 12:53:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 12:53:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 12:53:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/28/2022 12:53:51 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.5665298273993926 on epoch=662
05/28/2022 12:53:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 12:53:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/28/2022 12:53:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 12:54:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/28/2022 12:54:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 12:54:05 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5735570735570736 on epoch=674
05/28/2022 12:54:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 12:54:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 12:54:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 12:54:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 12:54:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 12:54:18 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6329569892473119 on epoch=687
05/28/2022 12:54:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 12:54:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 12:54:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 12:54:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 12:54:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 12:54:32 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6329569892473119 on epoch=699
05/28/2022 12:54:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 12:54:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 12:54:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 12:54:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 12:54:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 12:54:45 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5372089596227527 on epoch=712
05/28/2022 12:54:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 12:54:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 12:54:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 12:54:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 12:54:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 12:54:59 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6207616396761133 on epoch=724
05/28/2022 12:55:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 12:55:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 12:55:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 12:55:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 12:55:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 12:55:12 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5891597479832773 on epoch=737
05/28/2022 12:55:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 12:55:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 12:55:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 12:55:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 12:55:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/28/2022 12:55:25 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5124458874458874 on epoch=749
05/28/2022 12:55:25 - INFO - __main__ - save last model!
05/28/2022 12:55:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 12:55:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 12:55:25 - INFO - __main__ - Printing 3 examples
05/28/2022 12:55:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 12:55:25 - INFO - __main__ - ['others']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 12:55:25 - INFO - __main__ - ['others']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 12:55:25 - INFO - __main__ - ['others']
05/28/2022 12:55:25 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:55:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:55:25 - INFO - __main__ - Printing 3 examples
05/28/2022 12:55:25 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:55:25 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:55:25 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:55:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:55:25 - INFO - __main__ - Printing 3 examples
05/28/2022 12:55:25 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:55:25 - INFO - __main__ - ['happy']
05/28/2022 12:55:25 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:55:25 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:55:25 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:55:27 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:55:33 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 12:55:41 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:55:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:55:41 - INFO - __main__ - Starting training!
05/28/2022 12:57:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/28/2022 12:57:07 - INFO - __main__ - Classification-F1 on test data: 0.1588
05/28/2022 12:57:07 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.6630909658293025, test_performance=0.15883099677299264
05/28/2022 12:57:08 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/28/2022 12:57:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:57:08 - INFO - __main__ - Printing 3 examples
05/28/2022 12:57:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 12:57:08 - INFO - __main__ - ['happy']
05/28/2022 12:57:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 12:57:08 - INFO - __main__ - ['happy']
05/28/2022 12:57:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 12:57:08 - INFO - __main__ - ['happy']
05/28/2022 12:57:08 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:57:08 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:57:09 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 12:57:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 12:57:09 - INFO - __main__ - Printing 3 examples
05/28/2022 12:57:09 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 12:57:09 - INFO - __main__ - ['happy']
05/28/2022 12:57:09 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 12:57:09 - INFO - __main__ - ['happy']
05/28/2022 12:57:09 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 12:57:09 - INFO - __main__ - ['happy']
05/28/2022 12:57:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 12:57:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 12:57:09 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 12:57:24 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 12:57:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 12:57:25 - INFO - __main__ - Starting training!
05/28/2022 12:57:28 - INFO - __main__ - Step 10 Global step 10 Train loss 4.90 on epoch=2
05/28/2022 12:57:30 - INFO - __main__ - Step 20 Global step 20 Train loss 3.15 on epoch=4
05/28/2022 12:57:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.78 on epoch=7
05/28/2022 12:57:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.33 on epoch=9
05/28/2022 12:57:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.19 on epoch=12
05/28/2022 12:57:38 - INFO - __main__ - Global step 50 Train loss 2.47 Classification-F1 0.19602977667493798 on epoch=12
05/28/2022 12:57:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19602977667493798 on epoch=12, global_step=50
05/28/2022 12:57:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=14
05/28/2022 12:57:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=17
05/28/2022 12:57:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=19
05/28/2022 12:57:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=22
05/28/2022 12:57:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
05/28/2022 12:57:51 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.19307400379506642 on epoch=24
05/28/2022 12:57:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=27
05/28/2022 12:57:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/28/2022 12:57:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
05/28/2022 12:58:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 12:58:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=37
05/28/2022 12:58:05 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.1500341763499658 on epoch=37
05/28/2022 12:58:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=39
05/28/2022 12:58:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
05/28/2022 12:58:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
05/28/2022 12:58:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=47
05/28/2022 12:58:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=49
05/28/2022 12:58:18 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.27900202634245186 on epoch=49
05/28/2022 12:58:18 - INFO - __main__ - Saving model with best Classification-F1: 0.19602977667493798 -> 0.27900202634245186 on epoch=49, global_step=200
05/28/2022 12:58:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.86 on epoch=52
05/28/2022 12:58:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=54
05/28/2022 12:58:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/28/2022 12:58:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
05/28/2022 12:58:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.79 on epoch=62
05/28/2022 12:58:31 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.1238095238095238 on epoch=62
05/28/2022 12:58:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=64
05/28/2022 12:58:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.79 on epoch=67
05/28/2022 12:58:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/28/2022 12:58:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
05/28/2022 12:58:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=74
05/28/2022 12:58:44 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.35668449197860963 on epoch=74
05/28/2022 12:58:44 - INFO - __main__ - Saving model with best Classification-F1: 0.27900202634245186 -> 0.35668449197860963 on epoch=74, global_step=300
05/28/2022 12:58:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=77
05/28/2022 12:58:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=79
05/28/2022 12:58:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=82
05/28/2022 12:58:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=84
05/28/2022 12:58:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
05/28/2022 12:58:57 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.4548720029547783 on epoch=87
05/28/2022 12:58:57 - INFO - __main__ - Saving model with best Classification-F1: 0.35668449197860963 -> 0.4548720029547783 on epoch=87, global_step=350
05/28/2022 12:58:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=89
05/28/2022 12:59:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.69 on epoch=92
05/28/2022 12:59:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.70 on epoch=94
05/28/2022 12:59:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.73 on epoch=97
05/28/2022 12:59:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=99
05/28/2022 12:59:10 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.5141666666666667 on epoch=99
05/28/2022 12:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4548720029547783 -> 0.5141666666666667 on epoch=99, global_step=400
05/28/2022 12:59:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=102
05/28/2022 12:59:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=104
05/28/2022 12:59:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=107
05/28/2022 12:59:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=109
05/28/2022 12:59:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=112
05/28/2022 12:59:23 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.4456481481481481 on epoch=112
05/28/2022 12:59:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=114
05/28/2022 12:59:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=117
05/28/2022 12:59:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=119
05/28/2022 12:59:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=122
05/28/2022 12:59:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=124
05/28/2022 12:59:36 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.44691766566766566 on epoch=124
05/28/2022 12:59:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=127
05/28/2022 12:59:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=129
05/28/2022 12:59:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=132
05/28/2022 12:59:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=134
05/28/2022 12:59:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=137
05/28/2022 12:59:49 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.4281784005468216 on epoch=137
05/28/2022 12:59:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=139
05/28/2022 12:59:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=142
05/28/2022 12:59:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=144
05/28/2022 12:59:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=147
05/28/2022 13:00:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=149
05/28/2022 13:00:02 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.44454044017997507 on epoch=149
05/28/2022 13:00:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/28/2022 13:00:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=154
05/28/2022 13:00:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=157
05/28/2022 13:00:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=159
05/28/2022 13:00:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=162
05/28/2022 13:00:15 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5145833333333334 on epoch=162
05/28/2022 13:00:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5141666666666667 -> 0.5145833333333334 on epoch=162, global_step=650
05/28/2022 13:00:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=164
05/28/2022 13:00:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=167
05/28/2022 13:00:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=169
05/28/2022 13:00:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=172
05/28/2022 13:00:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=174
05/28/2022 13:00:28 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.5837833892862356 on epoch=174
05/28/2022 13:00:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5145833333333334 -> 0.5837833892862356 on epoch=174, global_step=700
05/28/2022 13:00:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=177
05/28/2022 13:00:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=179
05/28/2022 13:00:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=182
05/28/2022 13:00:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
05/28/2022 13:00:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=187
05/28/2022 13:00:41 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.5380801369173462 on epoch=187
05/28/2022 13:00:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
05/28/2022 13:00:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=192
05/28/2022 13:00:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/28/2022 13:00:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
05/28/2022 13:00:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/28/2022 13:00:54 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.6283229562428673 on epoch=199
05/28/2022 13:00:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5837833892862356 -> 0.6283229562428673 on epoch=199, global_step=800
05/28/2022 13:00:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=202
05/28/2022 13:00:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
05/28/2022 13:01:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=207
05/28/2022 13:01:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
05/28/2022 13:01:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
05/28/2022 13:01:07 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.5475774225774226 on epoch=212
05/28/2022 13:01:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/28/2022 13:01:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/28/2022 13:01:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
05/28/2022 13:01:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
05/28/2022 13:01:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=224
05/28/2022 13:01:20 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.5056980056980057 on epoch=224
05/28/2022 13:01:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
05/28/2022 13:01:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
05/28/2022 13:01:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=232
05/28/2022 13:01:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=234
05/28/2022 13:01:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/28/2022 13:01:34 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.4565714285714286 on epoch=237
05/28/2022 13:01:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/28/2022 13:01:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=242
05/28/2022 13:01:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
05/28/2022 13:01:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=247
05/28/2022 13:01:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
05/28/2022 13:01:47 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.5368290250322918 on epoch=249
05/28/2022 13:01:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
05/28/2022 13:01:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/28/2022 13:01:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/28/2022 13:01:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/28/2022 13:01:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/28/2022 13:02:00 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.46401098901098903 on epoch=262
05/28/2022 13:02:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
05/28/2022 13:02:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/28/2022 13:02:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=269
05/28/2022 13:02:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/28/2022 13:02:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/28/2022 13:02:13 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.5131270903010033 on epoch=274
05/28/2022 13:02:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
05/28/2022 13:02:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/28/2022 13:02:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/28/2022 13:02:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/28/2022 13:02:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=287
05/28/2022 13:02:27 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5448319672457604 on epoch=287
05/28/2022 13:02:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/28/2022 13:02:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/28/2022 13:02:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/28/2022 13:02:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/28/2022 13:02:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/28/2022 13:02:40 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5187596311734243 on epoch=299
05/28/2022 13:02:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/28/2022 13:02:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/28/2022 13:02:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 13:02:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
05/28/2022 13:02:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/28/2022 13:02:53 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.4967884361390733 on epoch=312
05/28/2022 13:02:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/28/2022 13:02:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/28/2022 13:03:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/28/2022 13:03:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/28/2022 13:03:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/28/2022 13:03:06 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5954624911360675 on epoch=324
05/28/2022 13:03:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/28/2022 13:03:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/28/2022 13:03:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/28/2022 13:03:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/28/2022 13:03:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/28/2022 13:03:20 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5622668822668823 on epoch=337
05/28/2022 13:03:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/28/2022 13:03:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/28/2022 13:03:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/28/2022 13:03:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/28/2022 13:03:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/28/2022 13:03:33 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5682088338338338 on epoch=349
05/28/2022 13:03:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/28/2022 13:03:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/28/2022 13:03:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/28/2022 13:03:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/28/2022 13:03:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/28/2022 13:03:46 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5125244125244125 on epoch=362
05/28/2022 13:03:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/28/2022 13:03:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/28/2022 13:03:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 13:03:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/28/2022 13:03:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/28/2022 13:04:00 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5020833333333333 on epoch=374
05/28/2022 13:04:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/28/2022 13:04:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=379
05/28/2022 13:04:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/28/2022 13:04:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 13:04:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/28/2022 13:04:13 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.5618097813838179 on epoch=387
05/28/2022 13:04:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/28/2022 13:04:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/28/2022 13:04:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
05/28/2022 13:04:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/28/2022 13:04:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/28/2022 13:04:27 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.5355370120076003 on epoch=399
05/28/2022 13:04:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/28/2022 13:04:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/28/2022 13:04:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=407
05/28/2022 13:04:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/28/2022 13:04:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/28/2022 13:04:40 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.5651812180844439 on epoch=412
05/28/2022 13:04:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 13:04:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/28/2022 13:04:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/28/2022 13:04:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 13:04:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/28/2022 13:04:54 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5414705798465532 on epoch=424
05/28/2022 13:04:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/28/2022 13:04:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/28/2022 13:05:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/28/2022 13:05:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 13:05:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/28/2022 13:05:07 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5132742365238778 on epoch=437
05/28/2022 13:05:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 13:05:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 13:05:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/28/2022 13:05:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/28/2022 13:05:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 13:05:20 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5071875013735478 on epoch=449
05/28/2022 13:05:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=452
05/28/2022 13:05:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 13:05:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 13:05:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 13:05:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/28/2022 13:05:34 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.5199519230769231 on epoch=462
05/28/2022 13:05:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/28/2022 13:05:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 13:05:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/28/2022 13:05:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/28/2022 13:05:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/28/2022 13:05:47 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5038857223704423 on epoch=474
05/28/2022 13:05:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/28/2022 13:05:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/28/2022 13:05:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/28/2022 13:05:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 13:06:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/28/2022 13:06:01 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5174460955710956 on epoch=487
05/28/2022 13:06:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 13:06:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/28/2022 13:06:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 13:06:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/28/2022 13:06:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 13:06:14 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5301900584795322 on epoch=499
05/28/2022 13:06:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 13:06:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/28/2022 13:06:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/28/2022 13:06:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 13:06:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 13:06:28 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.5448639715230105 on epoch=512
05/28/2022 13:06:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 13:06:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 13:06:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 13:06:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 13:06:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 13:06:41 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.5502136752136753 on epoch=524
05/28/2022 13:06:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 13:06:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/28/2022 13:06:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/28/2022 13:06:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 13:06:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=537
05/28/2022 13:06:55 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.5038461538461538 on epoch=537
05/28/2022 13:06:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 13:07:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 13:07:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/28/2022 13:07:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/28/2022 13:07:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 13:07:08 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.4840544871794871 on epoch=549
05/28/2022 13:07:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
05/28/2022 13:07:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 13:07:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/28/2022 13:07:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 13:07:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/28/2022 13:07:22 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5705222734254992 on epoch=562
05/28/2022 13:07:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 13:07:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 13:07:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/28/2022 13:07:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 13:07:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 13:07:35 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.4998973727422003 on epoch=574
05/28/2022 13:07:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 13:07:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 13:07:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 13:07:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 13:07:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 13:07:49 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.5046115288220551 on epoch=587
05/28/2022 13:07:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/28/2022 13:07:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 13:07:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/28/2022 13:07:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 13:08:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 13:08:02 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5039298730425729 on epoch=599
05/28/2022 13:08:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/28/2022 13:08:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 13:08:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 13:08:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/28/2022 13:08:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 13:08:16 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5275403225806452 on epoch=612
05/28/2022 13:08:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 13:08:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 13:08:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 13:08:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 13:08:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/28/2022 13:08:29 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.47056498542067293 on epoch=624
05/28/2022 13:08:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/28/2022 13:08:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 13:08:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 13:08:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/28/2022 13:08:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 13:08:43 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.48115079365079366 on epoch=637
05/28/2022 13:08:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 13:08:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/28/2022 13:08:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 13:08:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 13:08:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/28/2022 13:08:56 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.5073099415204678 on epoch=649
05/28/2022 13:08:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/28/2022 13:09:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 13:09:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 13:09:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/28/2022 13:09:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 13:09:10 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.551981351981352 on epoch=662
05/28/2022 13:09:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 13:09:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 13:09:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 13:09:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 13:09:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 13:09:23 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.5184523809523809 on epoch=674
05/28/2022 13:09:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 13:09:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 13:09:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/28/2022 13:09:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 13:09:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 13:09:37 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5384255675954592 on epoch=687
05/28/2022 13:09:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 13:09:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 13:09:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/28/2022 13:09:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 13:09:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 13:09:51 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.47970594220594215 on epoch=699
05/28/2022 13:09:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 13:09:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 13:09:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/28/2022 13:10:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 13:10:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 13:10:04 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.4921913498351086 on epoch=712
05/28/2022 13:10:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 13:10:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 13:10:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 13:10:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 13:10:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 13:10:18 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.4631373256373256 on epoch=724
05/28/2022 13:10:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 13:10:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 13:10:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 13:10:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 13:10:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 13:10:31 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5363807899090157 on epoch=737
05/28/2022 13:10:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 13:10:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 13:10:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/28/2022 13:10:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 13:10:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 13:10:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:10:45 - INFO - __main__ - Printing 3 examples
05/28/2022 13:10:45 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:10:45 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:10:45 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:10:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:10:45 - INFO - __main__ - Printing 3 examples
05/28/2022 13:10:45 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 13:10:45 - INFO - __main__ - ['happy']
05/28/2022 13:10:45 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:10:45 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:10:45 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.5070608710314592 on epoch=749
05/28/2022 13:10:45 - INFO - __main__ - save last model!
05/28/2022 13:10:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 13:10:45 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 13:10:45 - INFO - __main__ - Printing 3 examples
05/28/2022 13:10:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 13:10:45 - INFO - __main__ - ['others']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 13:10:45 - INFO - __main__ - ['others']
05/28/2022 13:10:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 13:10:45 - INFO - __main__ - ['others']
05/28/2022 13:10:45 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:10:45 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:10:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:10:52 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 13:11:03 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:11:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:11:04 - INFO - __main__ - Starting training!
05/28/2022 13:12:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/28/2022 13:12:41 - INFO - __main__ - Classification-F1 on test data: 0.3146
05/28/2022 13:12:41 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.6283229562428673, test_performance=0.3145734957815004
05/28/2022 13:12:41 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/28/2022 13:12:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:12:42 - INFO - __main__ - Printing 3 examples
05/28/2022 13:12:42 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:12:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:12:42 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:12:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:12:42 - INFO - __main__ - Printing 3 examples
05/28/2022 13:12:42 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/28/2022 13:12:42 - INFO - __main__ - ['happy']
05/28/2022 13:12:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:12:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:12:42 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:13:01 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:13:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:13:02 - INFO - __main__ - Starting training!
05/28/2022 13:13:05 - INFO - __main__ - Step 10 Global step 10 Train loss 5.21 on epoch=2
05/28/2022 13:13:07 - INFO - __main__ - Step 20 Global step 20 Train loss 3.71 on epoch=4
05/28/2022 13:13:10 - INFO - __main__ - Step 30 Global step 30 Train loss 2.38 on epoch=7
05/28/2022 13:13:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.80 on epoch=9
05/28/2022 13:13:15 - INFO - __main__ - Step 50 Global step 50 Train loss 1.34 on epoch=12
05/28/2022 13:13:16 - INFO - __main__ - Global step 50 Train loss 2.89 Classification-F1 0.19957983193277312 on epoch=12
05/28/2022 13:13:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19957983193277312 on epoch=12, global_step=50
05/28/2022 13:13:18 - INFO - __main__ - Step 60 Global step 60 Train loss 1.20 on epoch=14
05/28/2022 13:13:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.05 on epoch=17
05/28/2022 13:13:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=19
05/28/2022 13:13:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=22
05/28/2022 13:13:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/28/2022 13:13:29 - INFO - __main__ - Global step 100 Train loss 1.02 Classification-F1 0.20984848484848484 on epoch=24
05/28/2022 13:13:29 - INFO - __main__ - Saving model with best Classification-F1: 0.19957983193277312 -> 0.20984848484848484 on epoch=24, global_step=100
05/28/2022 13:13:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.01 on epoch=27
05/28/2022 13:13:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=29
05/28/2022 13:13:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=32
05/28/2022 13:13:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=34
05/28/2022 13:13:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=37
05/28/2022 13:13:43 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.1875 on epoch=37
05/28/2022 13:13:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
05/28/2022 13:13:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=42
05/28/2022 13:13:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
05/28/2022 13:13:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=47
05/28/2022 13:13:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=49
05/28/2022 13:13:56 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.2481392876129718 on epoch=49
05/28/2022 13:13:56 - INFO - __main__ - Saving model with best Classification-F1: 0.20984848484848484 -> 0.2481392876129718 on epoch=49, global_step=200
05/28/2022 13:13:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
05/28/2022 13:14:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/28/2022 13:14:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=57
05/28/2022 13:14:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=59
05/28/2022 13:14:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=62
05/28/2022 13:14:09 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.17368421052631577 on epoch=62
05/28/2022 13:14:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.83 on epoch=64
05/28/2022 13:14:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=67
05/28/2022 13:14:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=69
05/28/2022 13:14:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=72
05/28/2022 13:14:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.71 on epoch=74
05/28/2022 13:14:23 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.17368421052631577 on epoch=74
05/28/2022 13:14:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=77
05/28/2022 13:14:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/28/2022 13:14:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=82
05/28/2022 13:14:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/28/2022 13:14:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.76 on epoch=87
05/28/2022 13:14:36 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.2618885448916409 on epoch=87
05/28/2022 13:14:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2481392876129718 -> 0.2618885448916409 on epoch=87, global_step=350
05/28/2022 13:14:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
05/28/2022 13:14:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.79 on epoch=92
05/28/2022 13:14:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
05/28/2022 13:14:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=97
05/28/2022 13:14:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
05/28/2022 13:14:50 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.26230899830220716 on epoch=99
05/28/2022 13:14:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2618885448916409 -> 0.26230899830220716 on epoch=99, global_step=400
05/28/2022 13:14:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
05/28/2022 13:14:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/28/2022 13:14:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
05/28/2022 13:15:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.73 on epoch=109
05/28/2022 13:15:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=112
05/28/2022 13:15:03 - INFO - __main__ - Global step 450 Train loss 0.73 Classification-F1 0.4078329297820824 on epoch=112
05/28/2022 13:15:03 - INFO - __main__ - Saving model with best Classification-F1: 0.26230899830220716 -> 0.4078329297820824 on epoch=112, global_step=450
05/28/2022 13:15:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.77 on epoch=114
05/28/2022 13:15:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=117
05/28/2022 13:15:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
05/28/2022 13:15:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.71 on epoch=122
05/28/2022 13:15:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=124
05/28/2022 13:15:17 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.49241718426501035 on epoch=124
05/28/2022 13:15:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4078329297820824 -> 0.49241718426501035 on epoch=124, global_step=500
05/28/2022 13:15:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=127
05/28/2022 13:15:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.68 on epoch=129
05/28/2022 13:15:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=132
05/28/2022 13:15:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=134
05/28/2022 13:15:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.64 on epoch=137
05/28/2022 13:15:30 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.4367504476200129 on epoch=137
05/28/2022 13:15:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=139
05/28/2022 13:15:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=142
05/28/2022 13:15:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.62 on epoch=144
05/28/2022 13:15:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=147
05/28/2022 13:15:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=149
05/28/2022 13:15:43 - INFO - __main__ - Global step 600 Train loss 0.61 Classification-F1 0.48035714285714287 on epoch=149
05/28/2022 13:15:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=152
05/28/2022 13:15:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.64 on epoch=154
05/28/2022 13:15:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=157
05/28/2022 13:15:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=159
05/28/2022 13:15:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
05/28/2022 13:15:57 - INFO - __main__ - Global step 650 Train loss 0.59 Classification-F1 0.47975146198830415 on epoch=162
05/28/2022 13:15:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=164
05/28/2022 13:16:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=167
05/28/2022 13:16:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=169
05/28/2022 13:16:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=172
05/28/2022 13:16:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=174
05/28/2022 13:16:10 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.5357517482517482 on epoch=174
05/28/2022 13:16:10 - INFO - __main__ - Saving model with best Classification-F1: 0.49241718426501035 -> 0.5357517482517482 on epoch=174, global_step=700
05/28/2022 13:16:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=177
05/28/2022 13:16:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=179
05/28/2022 13:16:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=182
05/28/2022 13:16:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=184
05/28/2022 13:16:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=187
05/28/2022 13:16:23 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.5336951540840875 on epoch=187
05/28/2022 13:16:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=189
05/28/2022 13:16:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.55 on epoch=192
05/28/2022 13:16:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=194
05/28/2022 13:16:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=197
05/28/2022 13:16:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.61 on epoch=199
05/28/2022 13:16:37 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.5811063218390804 on epoch=199
05/28/2022 13:16:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5357517482517482 -> 0.5811063218390804 on epoch=199, global_step=800
05/28/2022 13:16:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=202
05/28/2022 13:16:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=204
05/28/2022 13:16:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=207
05/28/2022 13:16:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=209
05/28/2022 13:16:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=212
05/28/2022 13:16:50 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.4840753503595796 on epoch=212
05/28/2022 13:16:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=214
05/28/2022 13:16:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=217
05/28/2022 13:16:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=219
05/28/2022 13:17:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=222
05/28/2022 13:17:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=224
05/28/2022 13:17:04 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.5797164281671922 on epoch=224
05/28/2022 13:17:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=227
05/28/2022 13:17:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=229
05/28/2022 13:17:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=232
05/28/2022 13:17:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=234
05/28/2022 13:17:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=237
05/28/2022 13:17:17 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.5663492063492064 on epoch=237
05/28/2022 13:17:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=239
05/28/2022 13:17:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
05/28/2022 13:17:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=244
05/28/2022 13:17:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=247
05/28/2022 13:17:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=249
05/28/2022 13:17:30 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.5691958431681091 on epoch=249
05/28/2022 13:17:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=252
05/28/2022 13:17:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=254
05/28/2022 13:17:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
05/28/2022 13:17:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=259
05/28/2022 13:17:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=262
05/28/2022 13:17:44 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.4194257355547678 on epoch=262
05/28/2022 13:17:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=264
05/28/2022 13:17:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=267
05/28/2022 13:17:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/28/2022 13:17:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=272
05/28/2022 13:17:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
05/28/2022 13:17:57 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.5324799453831712 on epoch=274
05/28/2022 13:18:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=277
05/28/2022 13:18:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=279
05/28/2022 13:18:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=282
05/28/2022 13:18:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=284
05/28/2022 13:18:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=287
05/28/2022 13:18:11 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.5763085036255768 on epoch=287
05/28/2022 13:18:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=289
05/28/2022 13:18:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=292
05/28/2022 13:18:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=294
05/28/2022 13:18:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=297
05/28/2022 13:18:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=299
05/28/2022 13:18:24 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.46609523809523806 on epoch=299
05/28/2022 13:18:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/28/2022 13:18:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=304
05/28/2022 13:18:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/28/2022 13:18:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=309
05/28/2022 13:18:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
05/28/2022 13:18:37 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.547676282051282 on epoch=312
05/28/2022 13:18:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/28/2022 13:18:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=317
05/28/2022 13:18:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=319
05/28/2022 13:18:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
05/28/2022 13:18:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=324
05/28/2022 13:18:51 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.5812179872488428 on epoch=324
05/28/2022 13:18:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5811063218390804 -> 0.5812179872488428 on epoch=324, global_step=1300
05/28/2022 13:18:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=327
05/28/2022 13:18:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
05/28/2022 13:18:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/28/2022 13:19:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
05/28/2022 13:19:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=337
05/28/2022 13:19:04 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.5261376020397672 on epoch=337
05/28/2022 13:19:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=339
05/28/2022 13:19:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/28/2022 13:19:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/28/2022 13:19:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=347
05/28/2022 13:19:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/28/2022 13:19:18 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.5462342535513267 on epoch=349
05/28/2022 13:19:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=352
05/28/2022 13:19:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
05/28/2022 13:19:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/28/2022 13:19:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
05/28/2022 13:19:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/28/2022 13:19:31 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.5385346360956117 on epoch=362
05/28/2022 13:19:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/28/2022 13:19:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/28/2022 13:19:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=369
05/28/2022 13:19:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
05/28/2022 13:19:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=374
05/28/2022 13:19:45 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.38697362429896165 on epoch=374
05/28/2022 13:19:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/28/2022 13:19:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/28/2022 13:19:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=382
05/28/2022 13:19:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/28/2022 13:19:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
05/28/2022 13:19:58 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.5693220608081289 on epoch=387
05/28/2022 13:20:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=389
05/28/2022 13:20:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/28/2022 13:20:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=394
05/28/2022 13:20:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/28/2022 13:20:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
05/28/2022 13:20:12 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.5759462759462759 on epoch=399
05/28/2022 13:20:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/28/2022 13:20:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
05/28/2022 13:20:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/28/2022 13:20:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/28/2022 13:20:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/28/2022 13:20:25 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.5824898580121705 on epoch=412
05/28/2022 13:20:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5812179872488428 -> 0.5824898580121705 on epoch=412, global_step=1650
05/28/2022 13:20:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/28/2022 13:20:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/28/2022 13:20:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/28/2022 13:20:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/28/2022 13:20:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
05/28/2022 13:20:38 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.6086486486486486 on epoch=424
05/28/2022 13:20:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5824898580121705 -> 0.6086486486486486 on epoch=424, global_step=1700
05/28/2022 13:20:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/28/2022 13:20:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=429
05/28/2022 13:20:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/28/2022 13:20:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
05/28/2022 13:20:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/28/2022 13:20:52 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.6024561403508772 on epoch=437
05/28/2022 13:20:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/28/2022 13:20:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/28/2022 13:20:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/28/2022 13:21:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=447
05/28/2022 13:21:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
05/28/2022 13:21:06 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.5977564102564102 on epoch=449
05/28/2022 13:21:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
05/28/2022 13:21:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/28/2022 13:21:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/28/2022 13:21:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/28/2022 13:21:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/28/2022 13:21:19 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.5909072249589491 on epoch=462
05/28/2022 13:21:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=464
05/28/2022 13:21:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/28/2022 13:21:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/28/2022 13:21:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/28/2022 13:21:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=474
05/28/2022 13:21:33 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.5931139501871209 on epoch=474
05/28/2022 13:21:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 13:21:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=479
05/28/2022 13:21:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/28/2022 13:21:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/28/2022 13:21:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
05/28/2022 13:21:46 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.6547619047619048 on epoch=487
05/28/2022 13:21:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6086486486486486 -> 0.6547619047619048 on epoch=487, global_step=1950
05/28/2022 13:21:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/28/2022 13:21:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 13:21:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 13:21:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/28/2022 13:21:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
05/28/2022 13:22:00 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5639112708862649 on epoch=499
05/28/2022 13:22:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/28/2022 13:22:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/28/2022 13:22:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=507
05/28/2022 13:22:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 13:22:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/28/2022 13:22:14 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6150961538461538 on epoch=512
05/28/2022 13:22:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/28/2022 13:22:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 13:22:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=519
05/28/2022 13:22:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/28/2022 13:22:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/28/2022 13:22:27 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.5753846153846154 on epoch=524
05/28/2022 13:22:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/28/2022 13:22:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/28/2022 13:22:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/28/2022 13:22:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/28/2022 13:22:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/28/2022 13:22:41 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.626736111111111 on epoch=537
05/28/2022 13:22:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 13:22:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/28/2022 13:22:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/28/2022 13:22:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 13:22:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/28/2022 13:22:55 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.5944474891405838 on epoch=549
05/28/2022 13:22:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/28/2022 13:23:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/28/2022 13:23:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 13:23:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 13:23:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/28/2022 13:23:08 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5949898580121704 on epoch=562
05/28/2022 13:23:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=564
05/28/2022 13:23:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/28/2022 13:23:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/28/2022 13:23:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 13:23:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
05/28/2022 13:23:22 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5820779220779221 on epoch=574
05/28/2022 13:23:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/28/2022 13:23:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/28/2022 13:23:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/28/2022 13:23:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/28/2022 13:23:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 13:23:36 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6012412196679437 on epoch=587
05/28/2022 13:23:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/28/2022 13:23:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/28/2022 13:23:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/28/2022 13:23:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/28/2022 13:23:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/28/2022 13:23:49 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6023710317460318 on epoch=599
05/28/2022 13:23:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
05/28/2022 13:23:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/28/2022 13:23:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
05/28/2022 13:24:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=609
05/28/2022 13:24:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/28/2022 13:24:03 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.5810487225193108 on epoch=612
05/28/2022 13:24:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/28/2022 13:24:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/28/2022 13:24:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 13:24:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/28/2022 13:24:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/28/2022 13:24:17 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5510597826086957 on epoch=624
05/28/2022 13:24:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/28/2022 13:24:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 13:24:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=632
05/28/2022 13:24:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 13:24:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 13:24:31 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.5907314196787881 on epoch=637
05/28/2022 13:24:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/28/2022 13:24:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/28/2022 13:24:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/28/2022 13:24:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 13:24:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/28/2022 13:24:45 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5764264264264265 on epoch=649
05/28/2022 13:24:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 13:24:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/28/2022 13:24:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 13:24:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/28/2022 13:24:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 13:24:59 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.590956340956341 on epoch=662
05/28/2022 13:25:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/28/2022 13:25:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/28/2022 13:25:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/28/2022 13:25:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/28/2022 13:25:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
05/28/2022 13:25:13 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5549107142857143 on epoch=674
05/28/2022 13:25:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/28/2022 13:25:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/28/2022 13:25:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 13:25:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 13:25:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/28/2022 13:25:26 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6013517665130568 on epoch=687
05/28/2022 13:25:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 13:25:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
05/28/2022 13:25:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/28/2022 13:25:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 13:25:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 13:25:40 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.5660507264301852 on epoch=699
05/28/2022 13:25:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 13:25:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 13:25:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
05/28/2022 13:25:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/28/2022 13:25:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
05/28/2022 13:25:53 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6261904761904762 on epoch=712
05/28/2022 13:25:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/28/2022 13:25:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 13:26:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/28/2022 13:26:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/28/2022 13:26:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/28/2022 13:26:07 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5832364622687204 on epoch=724
05/28/2022 13:26:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 13:26:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/28/2022 13:26:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/28/2022 13:26:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 13:26:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 13:26:20 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.5846809440559441 on epoch=737
05/28/2022 13:26:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/28/2022 13:26:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/28/2022 13:26:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/28/2022 13:26:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 13:26:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/28/2022 13:26:34 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6398367863885104 on epoch=749
05/28/2022 13:26:34 - INFO - __main__ - save last model!
05/28/2022 13:26:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 13:26:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:26:34 - INFO - __main__ - Printing 3 examples
05/28/2022 13:26:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:26:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 13:26:34 - INFO - __main__ - Printing 3 examples
05/28/2022 13:26:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:26:34 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:26:34 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:26:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:26:34 - INFO - __main__ - Printing 3 examples
05/28/2022 13:26:34 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:26:34 - INFO - __main__ - ['others']
05/28/2022 13:26:34 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:26:34 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:26:34 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:26:36 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:26:42 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 13:26:53 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:26:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:26:54 - INFO - __main__ - Starting training!
05/28/2022 13:28:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/28/2022 13:28:21 - INFO - __main__ - Classification-F1 on test data: 0.2146
05/28/2022 13:28:22 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.6547619047619048, test_performance=0.21461194940523567
05/28/2022 13:28:22 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/28/2022 13:28:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:28:23 - INFO - __main__ - Printing 3 examples
05/28/2022 13:28:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:28:23 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:28:23 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:28:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:28:23 - INFO - __main__ - Printing 3 examples
05/28/2022 13:28:23 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:28:23 - INFO - __main__ - ['others']
05/28/2022 13:28:23 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:28:23 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:28:23 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:28:42 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:28:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:28:43 - INFO - __main__ - Starting training!
05/28/2022 13:28:46 - INFO - __main__ - Step 10 Global step 10 Train loss 4.28 on epoch=2
05/28/2022 13:28:48 - INFO - __main__ - Step 20 Global step 20 Train loss 1.91 on epoch=4
05/28/2022 13:28:50 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=7
05/28/2022 13:28:53 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
05/28/2022 13:28:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=12
05/28/2022 13:28:56 - INFO - __main__ - Global step 50 Train loss 1.87 Classification-F1 0.1 on epoch=12
05/28/2022 13:28:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 13:28:59 - INFO - __main__ - Step 60 Global step 60 Train loss 1.03 on epoch=14
05/28/2022 13:29:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=17
05/28/2022 13:29:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=19
05/28/2022 13:29:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=22
05/28/2022 13:29:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=24
05/28/2022 13:29:09 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.09493670886075949 on epoch=24
05/28/2022 13:29:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=27
05/28/2022 13:29:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=29
05/28/2022 13:29:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=32
05/28/2022 13:29:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=34
05/28/2022 13:29:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=37
05/28/2022 13:29:22 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.3125 on epoch=37
05/28/2022 13:29:22 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.3125 on epoch=37, global_step=150
05/28/2022 13:29:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=39
05/28/2022 13:29:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=42
05/28/2022 13:29:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=44
05/28/2022 13:29:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=47
05/28/2022 13:29:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=49
05/28/2022 13:29:35 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.21908315565031983 on epoch=49
05/28/2022 13:29:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=52
05/28/2022 13:29:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=54
05/28/2022 13:29:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=57
05/28/2022 13:29:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=59
05/28/2022 13:29:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.71 on epoch=62
05/28/2022 13:29:48 - INFO - __main__ - Global step 250 Train loss 0.76 Classification-F1 0.2455540355677154 on epoch=62
05/28/2022 13:29:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=64
05/28/2022 13:29:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.71 on epoch=67
05/28/2022 13:29:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=69
05/28/2022 13:29:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=72
05/28/2022 13:30:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=74
05/28/2022 13:30:01 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.44668775889624573 on epoch=74
05/28/2022 13:30:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3125 -> 0.44668775889624573 on epoch=74, global_step=300
05/28/2022 13:30:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=77
05/28/2022 13:30:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=79
05/28/2022 13:30:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=82
05/28/2022 13:30:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=84
05/28/2022 13:30:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=87
05/28/2022 13:30:14 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.49993524993524996 on epoch=87
05/28/2022 13:30:14 - INFO - __main__ - Saving model with best Classification-F1: 0.44668775889624573 -> 0.49993524993524996 on epoch=87, global_step=350
05/28/2022 13:30:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=89
05/28/2022 13:30:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=92
05/28/2022 13:30:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=94
05/28/2022 13:30:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=97
05/28/2022 13:30:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=99
05/28/2022 13:30:27 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.5129464285714286 on epoch=99
05/28/2022 13:30:27 - INFO - __main__ - Saving model with best Classification-F1: 0.49993524993524996 -> 0.5129464285714286 on epoch=99, global_step=400
05/28/2022 13:30:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=102
05/28/2022 13:30:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
05/28/2022 13:30:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=107
05/28/2022 13:30:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
05/28/2022 13:30:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=112
05/28/2022 13:30:40 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.6014957264957266 on epoch=112
05/28/2022 13:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5129464285714286 -> 0.6014957264957266 on epoch=112, global_step=450
05/28/2022 13:30:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=114
05/28/2022 13:30:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=117
05/28/2022 13:30:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=119
05/28/2022 13:30:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=122
05/28/2022 13:30:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=124
05/28/2022 13:30:53 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.6081587707546787 on epoch=124
05/28/2022 13:30:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6014957264957266 -> 0.6081587707546787 on epoch=124, global_step=500
05/28/2022 13:30:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=127
05/28/2022 13:30:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=129
05/28/2022 13:31:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=132
05/28/2022 13:31:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=134
05/28/2022 13:31:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=137
05/28/2022 13:31:06 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.6013071895424837 on epoch=137
05/28/2022 13:31:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=139
05/28/2022 13:31:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=142
05/28/2022 13:31:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
05/28/2022 13:31:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
05/28/2022 13:31:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=149
05/28/2022 13:31:19 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.6390955890955891 on epoch=149
05/28/2022 13:31:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6081587707546787 -> 0.6390955890955891 on epoch=149, global_step=600
05/28/2022 13:31:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/28/2022 13:31:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
05/28/2022 13:31:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/28/2022 13:31:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=159
05/28/2022 13:31:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.08 on epoch=162
05/28/2022 13:31:32 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.6161490683229813 on epoch=162
05/28/2022 13:31:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=164
05/28/2022 13:31:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=167
05/28/2022 13:31:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
05/28/2022 13:31:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=172
05/28/2022 13:31:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=174
05/28/2022 13:31:45 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.6337074303405573 on epoch=174
05/28/2022 13:31:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
05/28/2022 13:31:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/28/2022 13:31:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
05/28/2022 13:31:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=184
05/28/2022 13:31:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=187
05/28/2022 13:31:58 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.6255970686383564 on epoch=187
05/28/2022 13:32:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=189
05/28/2022 13:32:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=192
05/28/2022 13:32:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
05/28/2022 13:32:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=197
05/28/2022 13:32:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
05/28/2022 13:32:11 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6786890645586298 on epoch=199
05/28/2022 13:32:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6390955890955891 -> 0.6786890645586298 on epoch=199, global_step=800
05/28/2022 13:32:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=202
05/28/2022 13:32:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
05/28/2022 13:32:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=207
05/28/2022 13:32:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/28/2022 13:32:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/28/2022 13:32:25 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.7130508510279744 on epoch=212
05/28/2022 13:32:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6786890645586298 -> 0.7130508510279744 on epoch=212, global_step=850
05/28/2022 13:32:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=214
05/28/2022 13:32:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=217
05/28/2022 13:32:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/28/2022 13:32:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
05/28/2022 13:32:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
05/28/2022 13:32:38 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.586027823054471 on epoch=224
05/28/2022 13:32:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
05/28/2022 13:32:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/28/2022 13:32:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/28/2022 13:32:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
05/28/2022 13:32:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/28/2022 13:32:51 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.6837218450121675 on epoch=237
05/28/2022 13:32:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/28/2022 13:32:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=242
05/28/2022 13:32:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/28/2022 13:33:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/28/2022 13:33:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/28/2022 13:33:04 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.5523268398268398 on epoch=249
05/28/2022 13:33:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/28/2022 13:33:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
05/28/2022 13:33:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/28/2022 13:33:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=259
05/28/2022 13:33:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/28/2022 13:33:17 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6661704766967925 on epoch=262
05/28/2022 13:33:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/28/2022 13:33:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/28/2022 13:33:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/28/2022 13:33:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/28/2022 13:33:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/28/2022 13:33:30 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6543870073281838 on epoch=274
05/28/2022 13:33:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/28/2022 13:33:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/28/2022 13:33:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=282
05/28/2022 13:33:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/28/2022 13:33:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/28/2022 13:33:43 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.653557343657455 on epoch=287
05/28/2022 13:33:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/28/2022 13:33:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 13:33:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=294
05/28/2022 13:33:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=297
05/28/2022 13:33:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=299
05/28/2022 13:33:57 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.6794842412489472 on epoch=299
05/28/2022 13:33:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/28/2022 13:34:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/28/2022 13:34:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/28/2022 13:34:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
05/28/2022 13:34:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
05/28/2022 13:34:09 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6700009102386012 on epoch=312
05/28/2022 13:34:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/28/2022 13:34:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/28/2022 13:34:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/28/2022 13:34:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/28/2022 13:34:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/28/2022 13:34:23 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6568383167220376 on epoch=324
05/28/2022 13:34:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
05/28/2022 13:34:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
05/28/2022 13:34:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
05/28/2022 13:34:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/28/2022 13:34:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/28/2022 13:34:36 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7153044871794872 on epoch=337
05/28/2022 13:34:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7130508510279744 -> 0.7153044871794872 on epoch=337, global_step=1350
05/28/2022 13:34:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/28/2022 13:34:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/28/2022 13:34:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/28/2022 13:34:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
05/28/2022 13:34:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
05/28/2022 13:34:49 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6905303030303029 on epoch=349
05/28/2022 13:34:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=352
05/28/2022 13:34:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/28/2022 13:34:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/28/2022 13:34:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/28/2022 13:35:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/28/2022 13:35:02 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6522129186602871 on epoch=362
05/28/2022 13:35:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/28/2022 13:35:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 13:35:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 13:35:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/28/2022 13:35:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/28/2022 13:35:15 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.71431879748412 on epoch=374
05/28/2022 13:35:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/28/2022 13:35:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/28/2022 13:35:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/28/2022 13:35:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 13:35:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/28/2022 13:35:29 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6955059523809524 on epoch=387
05/28/2022 13:35:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/28/2022 13:35:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/28/2022 13:35:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/28/2022 13:35:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/28/2022 13:35:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/28/2022 13:35:42 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6566155266345021 on epoch=399
05/28/2022 13:35:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/28/2022 13:35:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/28/2022 13:35:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/28/2022 13:35:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/28/2022 13:35:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/28/2022 13:35:55 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.7010687229437229 on epoch=412
05/28/2022 13:35:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
05/28/2022 13:36:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/28/2022 13:36:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/28/2022 13:36:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 13:36:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 13:36:08 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.717086038961039 on epoch=424
05/28/2022 13:36:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7153044871794872 -> 0.717086038961039 on epoch=424, global_step=1700
05/28/2022 13:36:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/28/2022 13:36:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 13:36:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/28/2022 13:36:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
05/28/2022 13:36:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/28/2022 13:36:21 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6982959569166466 on epoch=437
05/28/2022 13:36:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/28/2022 13:36:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/28/2022 13:36:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/28/2022 13:36:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/28/2022 13:36:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/28/2022 13:36:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6970669227928756 on epoch=449
05/28/2022 13:36:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/28/2022 13:36:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 13:36:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/28/2022 13:36:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/28/2022 13:36:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/28/2022 13:36:48 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6800606460532931 on epoch=462
05/28/2022 13:36:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/28/2022 13:36:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/28/2022 13:36:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/28/2022 13:36:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/28/2022 13:37:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/28/2022 13:37:01 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.714032180818193 on epoch=474
05/28/2022 13:37:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 13:37:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 13:37:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 13:37:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 13:37:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 13:37:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6972260773637948 on epoch=487
05/28/2022 13:37:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 13:37:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 13:37:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 13:37:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 13:37:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 13:37:27 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.700189393939394 on epoch=499
05/28/2022 13:37:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 13:37:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/28/2022 13:37:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/28/2022 13:37:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 13:37:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 13:37:41 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.6788073038073039 on epoch=512
05/28/2022 13:37:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 13:37:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/28/2022 13:37:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/28/2022 13:37:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 13:37:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/28/2022 13:37:54 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7023259342922122 on epoch=524
05/28/2022 13:37:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
05/28/2022 13:37:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/28/2022 13:38:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/28/2022 13:38:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 13:38:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 13:38:08 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6970669227928756 on epoch=537
05/28/2022 13:38:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/28/2022 13:38:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 13:38:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/28/2022 13:38:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/28/2022 13:38:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 13:38:21 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6658205619412516 on epoch=549
05/28/2022 13:38:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 13:38:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 13:38:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 13:38:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 13:38:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/28/2022 13:38:34 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6658205619412516 on epoch=562
05/28/2022 13:38:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 13:38:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 13:38:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
05/28/2022 13:38:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/28/2022 13:38:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 13:38:48 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6449403815580286 on epoch=574
05/28/2022 13:38:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 13:38:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 13:38:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/28/2022 13:38:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 13:39:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/28/2022 13:39:01 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6445098039215686 on epoch=587
05/28/2022 13:39:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 13:39:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 13:39:08 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 13:39:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 13:39:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 13:39:15 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.6667054383788256 on epoch=599
05/28/2022 13:39:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/28/2022 13:39:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/28/2022 13:39:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/28/2022 13:39:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 13:39:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 13:39:28 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6550099206349207 on epoch=612
05/28/2022 13:39:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 13:39:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/28/2022 13:39:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 13:39:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 13:39:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 13:39:41 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6850198412698413 on epoch=624
05/28/2022 13:39:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/28/2022 13:39:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/28/2022 13:39:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 13:39:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/28/2022 13:39:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 13:39:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7021618324749255 on epoch=637
05/28/2022 13:39:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 13:39:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 13:40:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/28/2022 13:40:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 13:40:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 13:40:08 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6695988072648414 on epoch=649
05/28/2022 13:40:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/28/2022 13:40:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 13:40:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/28/2022 13:40:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/28/2022 13:40:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 13:40:21 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6370357820547574 on epoch=662
05/28/2022 13:40:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/28/2022 13:40:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 13:40:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 13:40:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 13:40:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 13:40:34 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6951447245564892 on epoch=674
05/28/2022 13:40:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/28/2022 13:40:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 13:40:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 13:40:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 13:40:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 13:40:48 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.688340053763441 on epoch=687
05/28/2022 13:40:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 13:40:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
05/28/2022 13:40:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 13:40:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 13:41:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 13:41:01 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6796613190730837 on epoch=699
05/28/2022 13:41:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 13:41:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/28/2022 13:41:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/28/2022 13:41:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 13:41:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 13:41:15 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6553094708406166 on epoch=712
05/28/2022 13:41:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
05/28/2022 13:41:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/28/2022 13:41:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 13:41:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/28/2022 13:41:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/28/2022 13:41:28 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6841201345658052 on epoch=724
05/28/2022 13:41:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 13:41:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 13:41:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 13:41:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 13:41:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 13:41:42 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7025573961057833 on epoch=737
05/28/2022 13:41:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 13:41:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 13:41:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 13:41:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 13:41:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 13:41:55 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6974697191431063 on epoch=749
05/28/2022 13:41:55 - INFO - __main__ - save last model!
05/28/2022 13:41:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 13:41:55 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 13:41:55 - INFO - __main__ - Printing 3 examples
05/28/2022 13:41:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:41:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:41:55 - INFO - __main__ - Printing 3 examples
05/28/2022 13:41:55 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:41:55 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:41:55 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:41:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:41:55 - INFO - __main__ - Printing 3 examples
05/28/2022 13:41:55 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:41:55 - INFO - __main__ - ['others']
05/28/2022 13:41:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:41:55 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:41:55 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:41:57 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:42:03 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 13:42:11 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:42:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:42:11 - INFO - __main__ - Starting training!
05/28/2022 13:43:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/28/2022 13:43:41 - INFO - __main__ - Classification-F1 on test data: 0.3005
05/28/2022 13:43:41 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.717086038961039, test_performance=0.3005326234283392
05/28/2022 13:43:41 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/28/2022 13:43:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:43:42 - INFO - __main__ - Printing 3 examples
05/28/2022 13:43:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:43:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:43:42 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:43:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:43:42 - INFO - __main__ - Printing 3 examples
05/28/2022 13:43:42 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:43:42 - INFO - __main__ - ['others']
05/28/2022 13:43:42 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:43:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:43:42 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:44:01 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:44:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:44:02 - INFO - __main__ - Starting training!
05/28/2022 13:44:05 - INFO - __main__ - Step 10 Global step 10 Train loss 4.48 on epoch=2
05/28/2022 13:44:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.00 on epoch=4
05/28/2022 13:44:10 - INFO - __main__ - Step 30 Global step 30 Train loss 1.25 on epoch=7
05/28/2022 13:44:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=9
05/28/2022 13:44:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=12
05/28/2022 13:44:15 - INFO - __main__ - Global step 50 Train loss 1.94 Classification-F1 0.1 on epoch=12
05/28/2022 13:44:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/28/2022 13:44:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=14
05/28/2022 13:44:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.98 on epoch=17
05/28/2022 13:44:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.02 on epoch=19
05/28/2022 13:44:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=22
05/28/2022 13:44:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=24
05/28/2022 13:44:28 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.1 on epoch=24
05/28/2022 13:44:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=27
05/28/2022 13:44:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/28/2022 13:44:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=32
05/28/2022 13:44:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 13:44:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=37
05/28/2022 13:44:41 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.28095238095238095 on epoch=37
05/28/2022 13:44:41 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.28095238095238095 on epoch=37, global_step=150
05/28/2022 13:44:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=39
05/28/2022 13:44:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
05/28/2022 13:44:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=44
05/28/2022 13:44:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.90 on epoch=47
05/28/2022 13:44:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/28/2022 13:44:54 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.1 on epoch=49
05/28/2022 13:44:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.86 on epoch=52
05/28/2022 13:44:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=54
05/28/2022 13:45:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=57
05/28/2022 13:45:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=59
05/28/2022 13:45:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=62
05/28/2022 13:45:07 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.2303030303030303 on epoch=62
05/28/2022 13:45:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=64
05/28/2022 13:45:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.72 on epoch=67
05/28/2022 13:45:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.68 on epoch=69
05/28/2022 13:45:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=72
05/28/2022 13:45:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=74
05/28/2022 13:45:20 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.3751885369532429 on epoch=74
05/28/2022 13:45:20 - INFO - __main__ - Saving model with best Classification-F1: 0.28095238095238095 -> 0.3751885369532429 on epoch=74, global_step=300
05/28/2022 13:45:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.73 on epoch=77
05/28/2022 13:45:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.76 on epoch=79
05/28/2022 13:45:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.68 on epoch=82
05/28/2022 13:45:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=84
05/28/2022 13:45:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=87
05/28/2022 13:45:33 - INFO - __main__ - Global step 350 Train loss 0.70 Classification-F1 0.40604288499025337 on epoch=87
05/28/2022 13:45:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3751885369532429 -> 0.40604288499025337 on epoch=87, global_step=350
05/28/2022 13:45:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=89
05/28/2022 13:45:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.64 on epoch=92
05/28/2022 13:45:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=94
05/28/2022 13:45:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=97
05/28/2022 13:45:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=99
05/28/2022 13:45:46 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.43080994963511154 on epoch=99
05/28/2022 13:45:46 - INFO - __main__ - Saving model with best Classification-F1: 0.40604288499025337 -> 0.43080994963511154 on epoch=99, global_step=400
05/28/2022 13:45:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.67 on epoch=102
05/28/2022 13:45:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=104
05/28/2022 13:45:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=107
05/28/2022 13:45:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=109
05/28/2022 13:45:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=112
05/28/2022 13:45:59 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.6120742947060807 on epoch=112
05/28/2022 13:45:59 - INFO - __main__ - Saving model with best Classification-F1: 0.43080994963511154 -> 0.6120742947060807 on epoch=112, global_step=450
05/28/2022 13:46:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=114
05/28/2022 13:46:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=117
05/28/2022 13:46:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=119
05/28/2022 13:46:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=122
05/28/2022 13:46:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=124
05/28/2022 13:46:12 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.43541666666666656 on epoch=124
05/28/2022 13:46:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=127
05/28/2022 13:46:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=129
05/28/2022 13:46:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=132
05/28/2022 13:46:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=134
05/28/2022 13:46:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=137
05/28/2022 13:46:25 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.5764509880363539 on epoch=137
05/28/2022 13:46:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=139
05/28/2022 13:46:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
05/28/2022 13:46:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=144
05/28/2022 13:46:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=147
05/28/2022 13:46:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=149
05/28/2022 13:46:38 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.6266672851341043 on epoch=149
05/28/2022 13:46:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6120742947060807 -> 0.6266672851341043 on epoch=149, global_step=600
05/28/2022 13:46:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=152
05/28/2022 13:46:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=154
05/28/2022 13:46:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/28/2022 13:46:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=159
05/28/2022 13:46:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/28/2022 13:46:51 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.5931085043988269 on epoch=162
05/28/2022 13:46:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=164
05/28/2022 13:46:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
05/28/2022 13:46:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=169
05/28/2022 13:47:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=172
05/28/2022 13:47:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=174
05/28/2022 13:47:04 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.6198054479304479 on epoch=174
05/28/2022 13:47:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/28/2022 13:47:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=179
05/28/2022 13:47:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/28/2022 13:47:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=184
05/28/2022 13:47:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=187
05/28/2022 13:47:17 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.5589301777682587 on epoch=187
05/28/2022 13:47:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=189
05/28/2022 13:47:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/28/2022 13:47:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/28/2022 13:47:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=197
05/28/2022 13:47:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
05/28/2022 13:47:30 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.6472498090145149 on epoch=199
05/28/2022 13:47:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6266672851341043 -> 0.6472498090145149 on epoch=199, global_step=800
05/28/2022 13:47:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
05/28/2022 13:47:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=204
05/28/2022 13:47:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
05/28/2022 13:47:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=209
05/28/2022 13:47:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=212
05/28/2022 13:47:43 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.6451223689126915 on epoch=212
05/28/2022 13:47:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/28/2022 13:47:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=217
05/28/2022 13:47:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=219
05/28/2022 13:47:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/28/2022 13:47:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/28/2022 13:47:56 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.6596753003003002 on epoch=224
05/28/2022 13:47:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6472498090145149 -> 0.6596753003003002 on epoch=224, global_step=900
05/28/2022 13:47:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/28/2022 13:48:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=229
05/28/2022 13:48:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/28/2022 13:48:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
05/28/2022 13:48:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
05/28/2022 13:48:09 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.6330552232854865 on epoch=237
05/28/2022 13:48:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/28/2022 13:48:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/28/2022 13:48:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
05/28/2022 13:48:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
05/28/2022 13:48:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/28/2022 13:48:22 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.6669465421303656 on epoch=249
05/28/2022 13:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6596753003003002 -> 0.6669465421303656 on epoch=249, global_step=1000
05/28/2022 13:48:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=252
05/28/2022 13:48:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/28/2022 13:48:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=257
05/28/2022 13:48:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/28/2022 13:48:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/28/2022 13:48:36 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6330552232854865 on epoch=262
05/28/2022 13:48:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
05/28/2022 13:48:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/28/2022 13:48:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
05/28/2022 13:48:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
05/28/2022 13:48:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/28/2022 13:48:49 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6522827899488242 on epoch=274
05/28/2022 13:48:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/28/2022 13:48:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/28/2022 13:48:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
05/28/2022 13:48:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/28/2022 13:49:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
05/28/2022 13:49:02 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6541877323127323 on epoch=287
05/28/2022 13:49:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=289
05/28/2022 13:49:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/28/2022 13:49:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/28/2022 13:49:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/28/2022 13:49:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/28/2022 13:49:15 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6501084782334783 on epoch=299
05/28/2022 13:49:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/28/2022 13:49:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/28/2022 13:49:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 13:49:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/28/2022 13:49:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/28/2022 13:49:28 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6661764705882353 on epoch=312
05/28/2022 13:49:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/28/2022 13:49:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/28/2022 13:49:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/28/2022 13:49:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/28/2022 13:49:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/28/2022 13:49:42 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6586832368082368 on epoch=324
05/28/2022 13:49:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/28/2022 13:49:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/28/2022 13:49:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/28/2022 13:49:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/28/2022 13:49:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/28/2022 13:49:55 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.6562289562289562 on epoch=337
05/28/2022 13:49:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/28/2022 13:50:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/28/2022 13:50:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/28/2022 13:50:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/28/2022 13:50:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/28/2022 13:50:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6442448680351907 on epoch=349
05/28/2022 13:50:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/28/2022 13:50:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/28/2022 13:50:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/28/2022 13:50:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/28/2022 13:50:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/28/2022 13:50:22 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6675122340956756 on epoch=362
05/28/2022 13:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6669465421303656 -> 0.6675122340956756 on epoch=362, global_step=1450
05/28/2022 13:50:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/28/2022 13:50:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/28/2022 13:50:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 13:50:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/28/2022 13:50:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/28/2022 13:50:35 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6298807448000996 on epoch=374
05/28/2022 13:50:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/28/2022 13:50:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/28/2022 13:50:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/28/2022 13:50:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/28/2022 13:50:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/28/2022 13:50:49 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6724068235103888 on epoch=387
05/28/2022 13:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6675122340956756 -> 0.6724068235103888 on epoch=387, global_step=1550
05/28/2022 13:50:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/28/2022 13:50:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/28/2022 13:50:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=394
05/28/2022 13:50:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/28/2022 13:51:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/28/2022 13:51:02 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.6469934640522876 on epoch=399
05/28/2022 13:51:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/28/2022 13:51:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/28/2022 13:51:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/28/2022 13:51:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/28/2022 13:51:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/28/2022 13:51:16 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6543308013896248 on epoch=412
05/28/2022 13:51:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 13:51:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/28/2022 13:51:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/28/2022 13:51:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
05/28/2022 13:51:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 13:51:29 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6492914979757085 on epoch=424
05/28/2022 13:51:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/28/2022 13:51:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/28/2022 13:51:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
05/28/2022 13:51:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 13:51:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/28/2022 13:51:43 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6193241167434715 on epoch=437
05/28/2022 13:51:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/28/2022 13:51:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/28/2022 13:51:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/28/2022 13:51:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/28/2022 13:51:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/28/2022 13:51:56 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6206322272498743 on epoch=449
05/28/2022 13:51:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/28/2022 13:52:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/28/2022 13:52:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/28/2022 13:52:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/28/2022 13:52:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
05/28/2022 13:52:10 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6328255528255529 on epoch=462
05/28/2022 13:52:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=464
05/28/2022 13:52:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/28/2022 13:52:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/28/2022 13:52:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/28/2022 13:52:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/28/2022 13:52:23 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6330552232854865 on epoch=474
05/28/2022 13:52:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/28/2022 13:52:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 13:52:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/28/2022 13:52:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 13:52:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 13:52:37 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6210407239819005 on epoch=487
05/28/2022 13:52:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/28/2022 13:52:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
05/28/2022 13:52:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/28/2022 13:52:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/28/2022 13:52:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/28/2022 13:52:51 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.635003885003885 on epoch=499
05/28/2022 13:52:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/28/2022 13:52:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/28/2022 13:52:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/28/2022 13:53:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 13:53:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/28/2022 13:53:04 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6613646331738438 on epoch=512
05/28/2022 13:53:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/28/2022 13:53:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 13:53:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/28/2022 13:53:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 13:53:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/28/2022 13:53:18 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.6687028657616892 on epoch=524
05/28/2022 13:53:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/28/2022 13:53:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/28/2022 13:53:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/28/2022 13:53:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/28/2022 13:53:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 13:53:31 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6573277623406342 on epoch=537
05/28/2022 13:53:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/28/2022 13:53:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 13:53:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/28/2022 13:53:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/28/2022 13:53:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 13:53:45 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.682852760977761 on epoch=549
05/28/2022 13:53:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6724068235103888 -> 0.682852760977761 on epoch=549, global_step=2200
05/28/2022 13:53:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 13:53:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/28/2022 13:53:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 13:53:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/28/2022 13:53:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 13:53:58 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6562289562289562 on epoch=562
05/28/2022 13:54:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/28/2022 13:54:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 13:54:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/28/2022 13:54:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 13:54:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 13:54:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.655881734006734 on epoch=574
05/28/2022 13:54:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 13:54:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/28/2022 13:54:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/28/2022 13:54:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 13:54:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/28/2022 13:54:25 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6440985485103132 on epoch=587
05/28/2022 13:54:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 13:54:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/28/2022 13:54:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 13:54:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 13:54:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/28/2022 13:54:38 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6634694434200012 on epoch=599
05/28/2022 13:54:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 13:54:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/28/2022 13:54:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/28/2022 13:54:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/28/2022 13:54:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 13:54:52 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6440985485103132 on epoch=612
05/28/2022 13:54:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=614
05/28/2022 13:54:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=617
05/28/2022 13:54:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/28/2022 13:55:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/28/2022 13:55:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 13:55:05 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.6587301587301587 on epoch=624
05/28/2022 13:55:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/28/2022 13:55:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
05/28/2022 13:55:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/28/2022 13:55:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 13:55:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 13:55:19 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6542346542346542 on epoch=637
05/28/2022 13:55:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 13:55:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/28/2022 13:55:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/28/2022 13:55:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 13:55:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 13:55:32 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6532961460446248 on epoch=649
05/28/2022 13:55:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 13:55:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 13:55:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/28/2022 13:55:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/28/2022 13:55:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 13:55:45 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6330552232854865 on epoch=662
05/28/2022 13:55:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 13:55:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/28/2022 13:55:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 13:55:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 13:55:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 13:55:58 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6214434495684495 on epoch=674
05/28/2022 13:56:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 13:56:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/28/2022 13:56:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 13:56:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
05/28/2022 13:56:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 13:56:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6330552232854865 on epoch=687
05/28/2022 13:56:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/28/2022 13:56:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 13:56:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/28/2022 13:56:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/28/2022 13:56:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/28/2022 13:56:25 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6234743265993267 on epoch=699
05/28/2022 13:56:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 13:56:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/28/2022 13:56:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 13:56:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/28/2022 13:56:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 13:56:39 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6431198965154822 on epoch=712
05/28/2022 13:56:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/28/2022 13:56:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 13:56:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 13:56:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/28/2022 13:56:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 13:56:52 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6586832368082368 on epoch=724
05/28/2022 13:56:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/28/2022 13:56:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 13:56:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 13:57:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/28/2022 13:57:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 13:57:05 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6586832368082368 on epoch=737
05/28/2022 13:57:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 13:57:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 13:57:13 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=744
05/28/2022 13:57:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 13:57:18 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 13:57:19 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6586832368082368 on epoch=749
05/28/2022 13:57:19 - INFO - __main__ - save last model!
05/28/2022 13:57:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 13:57:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 13:57:19 - INFO - __main__ - Printing 3 examples
05/28/2022 13:57:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:57:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:57:19 - INFO - __main__ - Printing 3 examples
05/28/2022 13:57:19 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:57:19 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:57:19 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:57:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:57:19 - INFO - __main__ - Printing 3 examples
05/28/2022 13:57:19 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:57:19 - INFO - __main__ - ['others']
05/28/2022 13:57:19 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:57:19 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:57:19 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:57:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:57:27 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 13:57:34 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:57:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:57:35 - INFO - __main__ - Starting training!
05/28/2022 13:59:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/28/2022 13:59:05 - INFO - __main__ - Classification-F1 on test data: 0.3808
05/28/2022 13:59:05 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.682852760977761, test_performance=0.3807936334106027
05/28/2022 13:59:05 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/28/2022 13:59:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:59:06 - INFO - __main__ - Printing 3 examples
05/28/2022 13:59:06 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:59:06 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:59:06 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 13:59:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 13:59:06 - INFO - __main__ - Printing 3 examples
05/28/2022 13:59:06 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 13:59:06 - INFO - __main__ - ['others']
05/28/2022 13:59:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 13:59:06 - INFO - __main__ - Tokenizing Output ...
05/28/2022 13:59:06 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 13:59:22 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 13:59:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 13:59:23 - INFO - __main__ - Starting training!
05/28/2022 13:59:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.89 on epoch=2
05/28/2022 13:59:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=4
05/28/2022 13:59:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.62 on epoch=7
05/28/2022 13:59:33 - INFO - __main__ - Step 40 Global step 40 Train loss 1.27 on epoch=9
05/28/2022 13:59:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=12
05/28/2022 13:59:36 - INFO - __main__ - Global step 50 Train loss 2.34 Classification-F1 0.10126582278481013 on epoch=12
05/28/2022 13:59:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10126582278481013 on epoch=12, global_step=50
05/28/2022 13:59:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=14
05/28/2022 13:59:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=17
05/28/2022 13:59:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=19
05/28/2022 13:59:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=22
05/28/2022 13:59:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=24
05/28/2022 13:59:50 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.09615384615384615 on epoch=24
05/28/2022 13:59:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=27
05/28/2022 13:59:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=29
05/28/2022 13:59:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=32
05/28/2022 13:59:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/28/2022 14:00:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
05/28/2022 14:00:03 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.18705547652916074 on epoch=37
05/28/2022 14:00:03 - INFO - __main__ - Saving model with best Classification-F1: 0.10126582278481013 -> 0.18705547652916074 on epoch=37, global_step=150
05/28/2022 14:00:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=39
05/28/2022 14:00:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=42
05/28/2022 14:00:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.82 on epoch=44
05/28/2022 14:00:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=47
05/28/2022 14:00:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.91 on epoch=49
05/28/2022 14:00:16 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.17635135135135135 on epoch=49
05/28/2022 14:00:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=52
05/28/2022 14:00:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=54
05/28/2022 14:00:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=57
05/28/2022 14:00:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=59
05/28/2022 14:00:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=62
05/28/2022 14:00:29 - INFO - __main__ - Global step 250 Train loss 0.79 Classification-F1 0.39480641309909603 on epoch=62
05/28/2022 14:00:29 - INFO - __main__ - Saving model with best Classification-F1: 0.18705547652916074 -> 0.39480641309909603 on epoch=62, global_step=250
05/28/2022 14:00:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=64
05/28/2022 14:00:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=67
05/28/2022 14:00:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/28/2022 14:00:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
05/28/2022 14:00:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=74
05/28/2022 14:00:42 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.22680995475113122 on epoch=74
05/28/2022 14:00:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=77
05/28/2022 14:00:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=79
05/28/2022 14:00:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.68 on epoch=82
05/28/2022 14:00:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=84
05/28/2022 14:00:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=87
05/28/2022 14:00:55 - INFO - __main__ - Global step 350 Train loss 0.76 Classification-F1 0.5454423177801391 on epoch=87
05/28/2022 14:00:55 - INFO - __main__ - Saving model with best Classification-F1: 0.39480641309909603 -> 0.5454423177801391 on epoch=87, global_step=350
05/28/2022 14:00:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
05/28/2022 14:01:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=92
05/28/2022 14:01:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=94
05/28/2022 14:01:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=97
05/28/2022 14:01:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.65 on epoch=99
05/28/2022 14:01:08 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.37373737373737376 on epoch=99
05/28/2022 14:01:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=102
05/28/2022 14:01:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=104
05/28/2022 14:01:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=107
05/28/2022 14:01:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.64 on epoch=109
05/28/2022 14:01:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=112
05/28/2022 14:01:21 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.5144927536231884 on epoch=112
05/28/2022 14:01:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=114
05/28/2022 14:01:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=117
05/28/2022 14:01:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=119
05/28/2022 14:01:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=122
05/28/2022 14:01:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=124
05/28/2022 14:01:34 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.5742518351214003 on epoch=124
05/28/2022 14:01:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5454423177801391 -> 0.5742518351214003 on epoch=124, global_step=500
05/28/2022 14:01:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=127
05/28/2022 14:01:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=129
05/28/2022 14:01:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=132
05/28/2022 14:01:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
05/28/2022 14:01:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=137
05/28/2022 14:01:47 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.5992724867724868 on epoch=137
05/28/2022 14:01:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5742518351214003 -> 0.5992724867724868 on epoch=137, global_step=550
05/28/2022 14:01:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=139
05/28/2022 14:01:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=142
05/28/2022 14:01:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=144
05/28/2022 14:01:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=147
05/28/2022 14:02:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=149
05/28/2022 14:02:00 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.661074378071642 on epoch=149
05/28/2022 14:02:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5992724867724868 -> 0.661074378071642 on epoch=149, global_step=600
05/28/2022 14:02:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=152
05/28/2022 14:02:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=154
05/28/2022 14:02:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=157
05/28/2022 14:02:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=159
05/28/2022 14:02:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
05/28/2022 14:02:14 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.646125059116238 on epoch=162
05/28/2022 14:02:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
05/28/2022 14:02:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
05/28/2022 14:02:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=169
05/28/2022 14:02:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=172
05/28/2022 14:02:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/28/2022 14:02:27 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6301313628899835 on epoch=174
05/28/2022 14:02:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/28/2022 14:02:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=179
05/28/2022 14:02:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=182
05/28/2022 14:02:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
05/28/2022 14:02:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/28/2022 14:02:40 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.6262082027168234 on epoch=187
05/28/2022 14:02:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/28/2022 14:02:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/28/2022 14:02:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/28/2022 14:02:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=197
05/28/2022 14:02:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
05/28/2022 14:02:53 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.6837670552091805 on epoch=199
05/28/2022 14:02:53 - INFO - __main__ - Saving model with best Classification-F1: 0.661074378071642 -> 0.6837670552091805 on epoch=199, global_step=800
05/28/2022 14:02:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
05/28/2022 14:02:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
05/28/2022 14:03:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
05/28/2022 14:03:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=209
05/28/2022 14:03:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=212
05/28/2022 14:03:06 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.6698103097202872 on epoch=212
05/28/2022 14:03:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/28/2022 14:03:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
05/28/2022 14:03:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=219
05/28/2022 14:03:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
05/28/2022 14:03:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=224
05/28/2022 14:03:19 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.6959907834101382 on epoch=224
05/28/2022 14:03:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6837670552091805 -> 0.6959907834101382 on epoch=224, global_step=900
05/28/2022 14:03:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.06 on epoch=227
05/28/2022 14:03:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/28/2022 14:03:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/28/2022 14:03:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
05/28/2022 14:03:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/28/2022 14:03:33 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.6690476190476191 on epoch=237
05/28/2022 14:03:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/28/2022 14:03:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/28/2022 14:03:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=244
05/28/2022 14:03:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/28/2022 14:03:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/28/2022 14:03:46 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.6820489844683393 on epoch=249
05/28/2022 14:03:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=252
05/28/2022 14:03:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=254
05/28/2022 14:03:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/28/2022 14:03:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/28/2022 14:03:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
05/28/2022 14:03:59 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6697232947232947 on epoch=262
05/28/2022 14:04:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
05/28/2022 14:04:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/28/2022 14:04:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/28/2022 14:04:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/28/2022 14:04:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/28/2022 14:04:12 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6974170828477304 on epoch=274
05/28/2022 14:04:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6959907834101382 -> 0.6974170828477304 on epoch=274, global_step=1100
05/28/2022 14:04:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/28/2022 14:04:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=279
05/28/2022 14:04:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/28/2022 14:04:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
05/28/2022 14:04:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/28/2022 14:04:26 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6984342189934296 on epoch=287
05/28/2022 14:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6974170828477304 -> 0.6984342189934296 on epoch=287, global_step=1150
05/28/2022 14:04:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
05/28/2022 14:04:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/28/2022 14:04:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=294
05/28/2022 14:04:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/28/2022 14:04:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/28/2022 14:04:39 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.7036764705882352 on epoch=299
05/28/2022 14:04:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6984342189934296 -> 0.7036764705882352 on epoch=299, global_step=1200
05/28/2022 14:04:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=302
05/28/2022 14:04:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/28/2022 14:04:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/28/2022 14:04:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
05/28/2022 14:04:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/28/2022 14:04:52 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.7516541353383459 on epoch=312
05/28/2022 14:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7036764705882352 -> 0.7516541353383459 on epoch=312, global_step=1250
05/28/2022 14:04:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/28/2022 14:04:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/28/2022 14:05:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/28/2022 14:05:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/28/2022 14:05:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/28/2022 14:05:06 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7104268292682927 on epoch=324
05/28/2022 14:05:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/28/2022 14:05:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/28/2022 14:05:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=332
05/28/2022 14:05:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/28/2022 14:05:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/28/2022 14:05:19 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7238927738927738 on epoch=337
05/28/2022 14:05:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/28/2022 14:05:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/28/2022 14:05:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/28/2022 14:05:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/28/2022 14:05:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/28/2022 14:05:33 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7192553414327608 on epoch=349
05/28/2022 14:05:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
05/28/2022 14:05:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/28/2022 14:05:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/28/2022 14:05:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/28/2022 14:05:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/28/2022 14:05:46 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.7576810576810578 on epoch=362
05/28/2022 14:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7516541353383459 -> 0.7576810576810578 on epoch=362, global_step=1450
05/28/2022 14:05:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/28/2022 14:05:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
05/28/2022 14:05:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/28/2022 14:05:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/28/2022 14:05:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=374
05/28/2022 14:05:59 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7045380824652234 on epoch=374
05/28/2022 14:06:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/28/2022 14:06:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/28/2022 14:06:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/28/2022 14:06:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/28/2022 14:06:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/28/2022 14:06:13 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7245098039215686 on epoch=387
05/28/2022 14:06:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/28/2022 14:06:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/28/2022 14:06:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/28/2022 14:06:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/28/2022 14:06:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/28/2022 14:06:26 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7551319648093842 on epoch=399
05/28/2022 14:06:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/28/2022 14:06:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/28/2022 14:06:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/28/2022 14:06:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/28/2022 14:06:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/28/2022 14:06:39 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7350101626016261 on epoch=412
05/28/2022 14:06:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/28/2022 14:06:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/28/2022 14:06:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/28/2022 14:06:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/28/2022 14:06:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/28/2022 14:06:53 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6996212121212121 on epoch=424
05/28/2022 14:06:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/28/2022 14:06:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/28/2022 14:07:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=432
05/28/2022 14:07:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/28/2022 14:07:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/28/2022 14:07:06 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6971460910340367 on epoch=437
05/28/2022 14:07:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/28/2022 14:07:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/28/2022 14:07:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/28/2022 14:07:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/28/2022 14:07:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/28/2022 14:07:19 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7187886725517696 on epoch=449
05/28/2022 14:07:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/28/2022 14:07:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/28/2022 14:07:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/28/2022 14:07:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/28/2022 14:07:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/28/2022 14:07:33 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.724469696969697 on epoch=462
05/28/2022 14:07:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/28/2022 14:07:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/28/2022 14:07:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/28/2022 14:07:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=472
05/28/2022 14:07:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/28/2022 14:07:46 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7262237762237762 on epoch=474
05/28/2022 14:07:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/28/2022 14:07:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/28/2022 14:07:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/28/2022 14:07:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/28/2022 14:07:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/28/2022 14:07:59 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.7637164416919701 on epoch=487
05/28/2022 14:07:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7576810576810578 -> 0.7637164416919701 on epoch=487, global_step=1950
05/28/2022 14:08:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/28/2022 14:08:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/28/2022 14:08:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/28/2022 14:08:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/28/2022 14:08:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/28/2022 14:08:13 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7173076923076922 on epoch=499
05/28/2022 14:08:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/28/2022 14:08:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/28/2022 14:08:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=507
05/28/2022 14:08:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/28/2022 14:08:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/28/2022 14:08:26 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6910714285714286 on epoch=512
05/28/2022 14:08:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/28/2022 14:08:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/28/2022 14:08:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/28/2022 14:08:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/28/2022 14:08:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/28/2022 14:08:40 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7229113384484228 on epoch=524
05/28/2022 14:08:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=527
05/28/2022 14:08:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/28/2022 14:08:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/28/2022 14:08:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/28/2022 14:08:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/28/2022 14:08:53 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6827989520304409 on epoch=537
05/28/2022 14:08:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 14:08:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/28/2022 14:09:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/28/2022 14:09:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/28/2022 14:09:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/28/2022 14:09:06 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7307017543859649 on epoch=549
05/28/2022 14:09:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/28/2022 14:09:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/28/2022 14:09:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/28/2022 14:09:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/28/2022 14:09:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/28/2022 14:09:20 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.6596218814968815 on epoch=562
05/28/2022 14:09:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/28/2022 14:09:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/28/2022 14:09:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/28/2022 14:09:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/28/2022 14:09:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/28/2022 14:09:33 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6851152319902319 on epoch=574
05/28/2022 14:09:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/28/2022 14:09:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/28/2022 14:09:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/28/2022 14:09:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/28/2022 14:09:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/28/2022 14:09:46 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6591241360978204 on epoch=587
05/28/2022 14:09:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/28/2022 14:09:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 14:09:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 14:09:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 14:09:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/28/2022 14:10:00 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7167384323456429 on epoch=599
05/28/2022 14:10:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/28/2022 14:10:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/28/2022 14:10:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/28/2022 14:10:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/28/2022 14:10:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/28/2022 14:10:13 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.72243265993266 on epoch=612
05/28/2022 14:10:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/28/2022 14:10:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 14:10:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/28/2022 14:10:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/28/2022 14:10:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/28/2022 14:10:27 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.645066605720256 on epoch=624
05/28/2022 14:10:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/28/2022 14:10:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/28/2022 14:10:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/28/2022 14:10:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/28/2022 14:10:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/28/2022 14:10:40 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6306365576102418 on epoch=637
05/28/2022 14:10:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/28/2022 14:10:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/28/2022 14:10:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
05/28/2022 14:10:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/28/2022 14:10:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/28/2022 14:10:53 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.655952380952381 on epoch=649
05/28/2022 14:10:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/28/2022 14:10:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/28/2022 14:11:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 14:11:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/28/2022 14:11:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/28/2022 14:11:06 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7169896331738437 on epoch=662
05/28/2022 14:11:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/28/2022 14:11:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/28/2022 14:11:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/28/2022 14:11:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/28/2022 14:11:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/28/2022 14:11:20 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.7169289044289044 on epoch=674
05/28/2022 14:11:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/28/2022 14:11:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/28/2022 14:11:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/28/2022 14:11:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 14:11:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 14:11:33 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7183823529411765 on epoch=687
05/28/2022 14:11:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/28/2022 14:11:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/28/2022 14:11:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/28/2022 14:11:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/28/2022 14:11:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/28/2022 14:11:47 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6815589000371609 on epoch=699
05/28/2022 14:11:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/28/2022 14:11:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/28/2022 14:11:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 14:11:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 14:11:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/28/2022 14:12:00 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7282488479262673 on epoch=712
05/28/2022 14:12:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 14:12:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/28/2022 14:12:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 14:12:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/28/2022 14:12:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/28/2022 14:12:13 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7032114127702364 on epoch=724
05/28/2022 14:12:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 14:12:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/28/2022 14:12:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/28/2022 14:12:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=734
05/28/2022 14:12:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/28/2022 14:12:27 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7107759511993383 on epoch=737
05/28/2022 14:12:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/28/2022 14:12:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 14:12:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/28/2022 14:12:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/28/2022 14:12:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 14:12:40 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.7032114127702364 on epoch=749
05/28/2022 14:12:40 - INFO - __main__ - save last model!
05/28/2022 14:12:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 14:12:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 14:12:40 - INFO - __main__ - Printing 3 examples
05/28/2022 14:12:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:12:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 14:12:40 - INFO - __main__ - Printing 3 examples
05/28/2022 14:12:40 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:12:40 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:12:40 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 14:12:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 14:12:40 - INFO - __main__ - Printing 3 examples
05/28/2022 14:12:40 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 14:12:40 - INFO - __main__ - ['others']
05/28/2022 14:12:40 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:12:41 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:12:41 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 14:12:42 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:12:48 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 14:12:56 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 14:12:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 14:12:56 - INFO - __main__ - Starting training!
05/28/2022 14:14:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/28/2022 14:14:21 - INFO - __main__ - Classification-F1 on test data: 0.3466
05/28/2022 14:14:21 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7637164416919701, test_performance=0.3465515813303618
05/28/2022 14:14:21 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/28/2022 14:14:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 14:14:22 - INFO - __main__ - Printing 3 examples
05/28/2022 14:14:22 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:14:22 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:14:22 - INFO - __main__ - Loaded 64 examples from train data
05/28/2022 14:14:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/28/2022 14:14:22 - INFO - __main__ - Printing 3 examples
05/28/2022 14:14:22 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/28/2022 14:14:22 - INFO - __main__ - ['others']
05/28/2022 14:14:22 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:14:22 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:14:22 - INFO - __main__ - Loaded 64 examples from dev data
05/28/2022 14:14:41 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 14:14:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 14:14:41 - INFO - __main__ - Starting training!
05/28/2022 14:14:44 - INFO - __main__ - Step 10 Global step 10 Train loss 4.98 on epoch=2
05/28/2022 14:14:47 - INFO - __main__ - Step 20 Global step 20 Train loss 3.55 on epoch=4
05/28/2022 14:14:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.53 on epoch=7
05/28/2022 14:14:52 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=9
05/28/2022 14:14:54 - INFO - __main__ - Step 50 Global step 50 Train loss 1.43 on epoch=12
05/28/2022 14:14:55 - INFO - __main__ - Global step 50 Train loss 2.81 Classification-F1 0.09868421052631579 on epoch=12
05/28/2022 14:14:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09868421052631579 on epoch=12, global_step=50
05/28/2022 14:14:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=14
05/28/2022 14:15:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=17
05/28/2022 14:15:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=19
05/28/2022 14:15:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
05/28/2022 14:15:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=24
05/28/2022 14:15:08 - INFO - __main__ - Global step 100 Train loss 1.02 Classification-F1 0.1 on epoch=24
05/28/2022 14:15:08 - INFO - __main__ - Saving model with best Classification-F1: 0.09868421052631579 -> 0.1 on epoch=24, global_step=100
05/28/2022 14:15:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=27
05/28/2022 14:15:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=29
05/28/2022 14:15:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=32
05/28/2022 14:15:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=34
05/28/2022 14:15:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=37
05/28/2022 14:15:21 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.1357498223169865 on epoch=37
05/28/2022 14:15:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1357498223169865 on epoch=37, global_step=150
05/28/2022 14:15:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=39
05/28/2022 14:15:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.99 on epoch=42
05/28/2022 14:15:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=44
05/28/2022 14:15:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=47
05/28/2022 14:15:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.85 on epoch=49
05/28/2022 14:15:34 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.1 on epoch=49
05/28/2022 14:15:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/28/2022 14:15:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=54
05/28/2022 14:15:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/28/2022 14:15:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.90 on epoch=59
05/28/2022 14:15:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=62
05/28/2022 14:15:47 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.21828469326980343 on epoch=62
05/28/2022 14:15:47 - INFO - __main__ - Saving model with best Classification-F1: 0.1357498223169865 -> 0.21828469326980343 on epoch=62, global_step=250
05/28/2022 14:15:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=64
05/28/2022 14:15:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.79 on epoch=67
05/28/2022 14:15:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=69
05/28/2022 14:15:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=72
05/28/2022 14:15:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=74
05/28/2022 14:16:00 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.1 on epoch=74
05/28/2022 14:16:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=77
05/28/2022 14:16:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=79
05/28/2022 14:16:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.80 on epoch=82
05/28/2022 14:16:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=84
05/28/2022 14:16:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=87
05/28/2022 14:16:13 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.2740046838407494 on epoch=87
05/28/2022 14:16:13 - INFO - __main__ - Saving model with best Classification-F1: 0.21828469326980343 -> 0.2740046838407494 on epoch=87, global_step=350
05/28/2022 14:16:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/28/2022 14:16:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=92
05/28/2022 14:16:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=94
05/28/2022 14:16:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=97
05/28/2022 14:16:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=99
05/28/2022 14:16:26 - INFO - __main__ - Global step 400 Train loss 0.78 Classification-F1 0.2678571428571429 on epoch=99
05/28/2022 14:16:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.76 on epoch=102
05/28/2022 14:16:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=104
05/28/2022 14:16:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.81 on epoch=107
05/28/2022 14:16:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=109
05/28/2022 14:16:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/28/2022 14:16:39 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.4508281573498965 on epoch=112
05/28/2022 14:16:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2740046838407494 -> 0.4508281573498965 on epoch=112, global_step=450
05/28/2022 14:16:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.82 on epoch=114
05/28/2022 14:16:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
05/28/2022 14:16:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.71 on epoch=119
05/28/2022 14:16:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=122
05/28/2022 14:16:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.76 on epoch=124
05/28/2022 14:16:52 - INFO - __main__ - Global step 500 Train loss 0.77 Classification-F1 0.32645502645502644 on epoch=124
05/28/2022 14:16:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=127
05/28/2022 14:16:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
05/28/2022 14:17:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.78 on epoch=132
05/28/2022 14:17:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=134
05/28/2022 14:17:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=137
05/28/2022 14:17:05 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.5346842812360054 on epoch=137
05/28/2022 14:17:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4508281573498965 -> 0.5346842812360054 on epoch=137, global_step=550
05/28/2022 14:17:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=139
05/28/2022 14:17:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=142
05/28/2022 14:17:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.67 on epoch=144
05/28/2022 14:17:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.69 on epoch=147
05/28/2022 14:17:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
05/28/2022 14:17:18 - INFO - __main__ - Global step 600 Train loss 0.69 Classification-F1 0.3858013428696541 on epoch=149
05/28/2022 14:17:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=152
05/28/2022 14:17:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=154
05/28/2022 14:17:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=157
05/28/2022 14:17:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=159
05/28/2022 14:17:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=162
05/28/2022 14:17:31 - INFO - __main__ - Global step 650 Train loss 0.60 Classification-F1 0.625010675001708 on epoch=162
05/28/2022 14:17:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5346842812360054 -> 0.625010675001708 on epoch=162, global_step=650
05/28/2022 14:17:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=164
05/28/2022 14:17:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=167
05/28/2022 14:17:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=169
05/28/2022 14:17:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.63 on epoch=172
05/28/2022 14:17:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=174
05/28/2022 14:17:44 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.6618348231251456 on epoch=174
05/28/2022 14:17:44 - INFO - __main__ - Saving model with best Classification-F1: 0.625010675001708 -> 0.6618348231251456 on epoch=174, global_step=700
05/28/2022 14:17:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=177
05/28/2022 14:17:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=179
05/28/2022 14:17:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=182
05/28/2022 14:17:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=184
05/28/2022 14:17:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=187
05/28/2022 14:17:57 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.7258670383670384 on epoch=187
05/28/2022 14:17:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6618348231251456 -> 0.7258670383670384 on epoch=187, global_step=750
05/28/2022 14:18:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=189
05/28/2022 14:18:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=192
05/28/2022 14:18:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=194
05/28/2022 14:18:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=197
05/28/2022 14:18:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=199
05/28/2022 14:18:10 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.592905194689477 on epoch=199
05/28/2022 14:18:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=202
05/28/2022 14:18:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=204
05/28/2022 14:18:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=207
05/28/2022 14:18:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=209
05/28/2022 14:18:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=212
05/28/2022 14:18:23 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.691126511126511 on epoch=212
05/28/2022 14:18:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=214
05/28/2022 14:18:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=217
05/28/2022 14:18:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=219
05/28/2022 14:18:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=222
05/28/2022 14:18:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=224
05/28/2022 14:18:36 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.6460190296397192 on epoch=224
05/28/2022 14:18:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=227
05/28/2022 14:18:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
05/28/2022 14:18:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=232
05/28/2022 14:18:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=234
05/28/2022 14:18:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=237
05/28/2022 14:18:49 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.6712723234462364 on epoch=237
05/28/2022 14:18:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=239
05/28/2022 14:18:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=242
05/28/2022 14:18:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=244
05/28/2022 14:18:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=247
05/28/2022 14:19:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=249
05/28/2022 14:19:02 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.6358036635706914 on epoch=249
05/28/2022 14:19:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
05/28/2022 14:19:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=254
05/28/2022 14:19:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=257
05/28/2022 14:19:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=259
05/28/2022 14:19:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=262
05/28/2022 14:19:15 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.6360347985347986 on epoch=262
05/28/2022 14:19:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=264
05/28/2022 14:19:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=267
05/28/2022 14:19:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=269
05/28/2022 14:19:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=272
05/28/2022 14:19:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=274
05/28/2022 14:19:28 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.6774304196371027 on epoch=274
05/28/2022 14:19:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=277
05/28/2022 14:19:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=279
05/28/2022 14:19:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=282
05/28/2022 14:19:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=284
05/28/2022 14:19:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=287
05/28/2022 14:19:41 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.7155180840664712 on epoch=287
05/28/2022 14:19:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=289
05/28/2022 14:19:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
05/28/2022 14:19:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=294
05/28/2022 14:19:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/28/2022 14:19:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=299
05/28/2022 14:19:55 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.7011562998405103 on epoch=299
05/28/2022 14:19:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=302
05/28/2022 14:19:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=304
05/28/2022 14:20:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=307
05/28/2022 14:20:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=309
05/28/2022 14:20:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=312
05/28/2022 14:20:08 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7090172558922558 on epoch=312
05/28/2022 14:20:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/28/2022 14:20:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=317
05/28/2022 14:20:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.09 on epoch=319
05/28/2022 14:20:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=322
05/28/2022 14:20:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/28/2022 14:20:21 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.6930139318701134 on epoch=324
05/28/2022 14:20:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/28/2022 14:20:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/28/2022 14:20:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/28/2022 14:20:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
05/28/2022 14:20:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/28/2022 14:20:34 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.6714346764346765 on epoch=337
05/28/2022 14:20:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/28/2022 14:20:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=342
05/28/2022 14:20:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/28/2022 14:20:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=347
05/28/2022 14:20:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/28/2022 14:20:47 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.7183080808080807 on epoch=349
05/28/2022 14:20:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=352
05/28/2022 14:20:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=354
05/28/2022 14:20:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
05/28/2022 14:20:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=359
05/28/2022 14:20:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=362
05/28/2022 14:21:00 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.6865306080356405 on epoch=362
05/28/2022 14:21:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/28/2022 14:21:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/28/2022 14:21:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=369
05/28/2022 14:21:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/28/2022 14:21:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
05/28/2022 14:21:13 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.6567867867867867 on epoch=374
05/28/2022 14:21:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=377
05/28/2022 14:21:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/28/2022 14:21:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/28/2022 14:21:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/28/2022 14:21:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/28/2022 14:21:26 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.7013496307613954 on epoch=387
05/28/2022 14:21:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/28/2022 14:21:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
05/28/2022 14:21:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/28/2022 14:21:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/28/2022 14:21:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/28/2022 14:21:39 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.713267125031831 on epoch=399
05/28/2022 14:21:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/28/2022 14:21:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=404
05/28/2022 14:21:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/28/2022 14:21:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=409
05/28/2022 14:21:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/28/2022 14:21:52 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7011429879076938 on epoch=412
05/28/2022 14:21:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/28/2022 14:21:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=417
05/28/2022 14:21:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
05/28/2022 14:22:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/28/2022 14:22:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/28/2022 14:22:05 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7309729831468962 on epoch=424
05/28/2022 14:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7258670383670384 -> 0.7309729831468962 on epoch=424, global_step=1700
05/28/2022 14:22:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/28/2022 14:22:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=429
05/28/2022 14:22:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/28/2022 14:22:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/28/2022 14:22:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/28/2022 14:22:18 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6998041519780651 on epoch=437
05/28/2022 14:22:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/28/2022 14:22:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/28/2022 14:22:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/28/2022 14:22:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/28/2022 14:22:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/28/2022 14:22:31 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7350040950040949 on epoch=449
05/28/2022 14:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7309729831468962 -> 0.7350040950040949 on epoch=449, global_step=1800
05/28/2022 14:22:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/28/2022 14:22:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/28/2022 14:22:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=457
05/28/2022 14:22:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/28/2022 14:22:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/28/2022 14:22:44 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7543951920474398 on epoch=462
05/28/2022 14:22:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7350040950040949 -> 0.7543951920474398 on epoch=462, global_step=1850
05/28/2022 14:22:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/28/2022 14:22:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/28/2022 14:22:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/28/2022 14:22:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
05/28/2022 14:22:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/28/2022 14:22:57 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.6927576927576927 on epoch=474
05/28/2022 14:23:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=477
05/28/2022 14:23:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/28/2022 14:23:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/28/2022 14:23:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/28/2022 14:23:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/28/2022 14:23:11 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.6888997826497826 on epoch=487
05/28/2022 14:23:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/28/2022 14:23:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
05/28/2022 14:23:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/28/2022 14:23:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/28/2022 14:23:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/28/2022 14:23:24 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7444593965975544 on epoch=499
05/28/2022 14:23:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
05/28/2022 14:23:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
05/28/2022 14:23:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/28/2022 14:23:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/28/2022 14:23:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/28/2022 14:23:37 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7313805282555282 on epoch=512
05/28/2022 14:23:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/28/2022 14:23:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/28/2022 14:23:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/28/2022 14:23:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
05/28/2022 14:23:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/28/2022 14:23:50 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6919476722108301 on epoch=524
05/28/2022 14:23:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/28/2022 14:23:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/28/2022 14:23:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
05/28/2022 14:24:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=534
05/28/2022 14:24:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/28/2022 14:24:04 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7209880762512342 on epoch=537
05/28/2022 14:24:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/28/2022 14:24:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/28/2022 14:24:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/28/2022 14:24:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
05/28/2022 14:24:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/28/2022 14:24:17 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6787764655411715 on epoch=549
05/28/2022 14:24:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/28/2022 14:24:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/28/2022 14:24:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/28/2022 14:24:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/28/2022 14:24:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/28/2022 14:24:30 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7675530867842819 on epoch=562
05/28/2022 14:24:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7543951920474398 -> 0.7675530867842819 on epoch=562, global_step=2250
05/28/2022 14:24:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/28/2022 14:24:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/28/2022 14:24:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/28/2022 14:24:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/28/2022 14:24:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/28/2022 14:24:43 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7383771929824562 on epoch=574
05/28/2022 14:24:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/28/2022 14:24:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/28/2022 14:24:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
05/28/2022 14:24:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/28/2022 14:24:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=587
05/28/2022 14:24:56 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7295023816762947 on epoch=587
05/28/2022 14:24:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
05/28/2022 14:25:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/28/2022 14:25:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/28/2022 14:25:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/28/2022 14:25:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/28/2022 14:25:10 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7150650410504991 on epoch=599
05/28/2022 14:25:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/28/2022 14:25:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/28/2022 14:25:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/28/2022 14:25:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/28/2022 14:25:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/28/2022 14:25:23 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7165915915915917 on epoch=612
05/28/2022 14:25:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/28/2022 14:25:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/28/2022 14:25:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=619
05/28/2022 14:25:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=622
05/28/2022 14:25:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/28/2022 14:25:37 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7179347826086956 on epoch=624
05/28/2022 14:25:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/28/2022 14:25:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/28/2022 14:25:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=632
05/28/2022 14:25:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/28/2022 14:25:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/28/2022 14:25:51 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7329542847369673 on epoch=637
05/28/2022 14:25:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/28/2022 14:25:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/28/2022 14:25:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
05/28/2022 14:26:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/28/2022 14:26:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/28/2022 14:26:04 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7085611756664388 on epoch=649
05/28/2022 14:26:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/28/2022 14:26:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/28/2022 14:26:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/28/2022 14:26:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=659
05/28/2022 14:26:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/28/2022 14:26:17 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7221667690417691 on epoch=662
05/28/2022 14:26:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/28/2022 14:26:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/28/2022 14:26:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/28/2022 14:26:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/28/2022 14:26:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/28/2022 14:26:31 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7209880762512342 on epoch=674
05/28/2022 14:26:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/28/2022 14:26:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/28/2022 14:26:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
05/28/2022 14:26:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/28/2022 14:26:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/28/2022 14:26:44 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7447916666666667 on epoch=687
05/28/2022 14:26:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/28/2022 14:26:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/28/2022 14:26:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/28/2022 14:26:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/28/2022 14:26:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/28/2022 14:26:58 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7507430704361651 on epoch=699
05/28/2022 14:27:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/28/2022 14:27:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=704
05/28/2022 14:27:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/28/2022 14:27:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/28/2022 14:27:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/28/2022 14:27:11 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7461121794738095 on epoch=712
05/28/2022 14:27:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/28/2022 14:27:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=717
05/28/2022 14:27:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/28/2022 14:27:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/28/2022 14:27:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/28/2022 14:27:25 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6780459212547497 on epoch=724
05/28/2022 14:27:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/28/2022 14:27:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/28/2022 14:27:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/28/2022 14:27:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/28/2022 14:27:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/28/2022 14:27:38 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7367698726394378 on epoch=737
05/28/2022 14:27:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/28/2022 14:27:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/28/2022 14:27:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/28/2022 14:27:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/28/2022 14:27:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/28/2022 14:27:51 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7281167108753315 on epoch=749
05/28/2022 14:27:51 - INFO - __main__ - save last model!
05/28/2022 14:27:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 14:27:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/28/2022 14:27:51 - INFO - __main__ - Printing 3 examples
05/28/2022 14:27:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/28/2022 14:27:51 - INFO - __main__ - ['others']
05/28/2022 14:27:51 - INFO - __main__ -  [emo] what you like very little things ok
05/28/2022 14:27:51 - INFO - __main__ - ['others']
05/28/2022 14:27:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/28/2022 14:27:51 - INFO - __main__ - ['others']
05/28/2022 14:27:51 - INFO - __main__ - Tokenizing Input ...
05/28/2022 14:27:53 - INFO - __main__ - Tokenizing Output ...
05/28/2022 14:27:59 - INFO - __main__ - Loaded 5509 examples from test data
05/28/2022 14:29:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/28/2022 14:29:33 - INFO - __main__ - Classification-F1 on test data: 0.4205
05/28/2022 14:29:33 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7675530867842819, test_performance=0.4205328656911712
