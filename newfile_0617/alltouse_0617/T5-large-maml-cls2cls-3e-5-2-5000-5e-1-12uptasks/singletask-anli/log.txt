05/29/2022 13:01:09 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/29/2022 13:01:09 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli
05/29/2022 13:01:09 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/29/2022 13:01:09 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli
05/29/2022 13:01:10 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/29/2022 13:01:10 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/29/2022 13:01:10 - INFO - __main__ - args.device: cuda:0
05/29/2022 13:01:10 - INFO - __main__ - Using 2 gpus
05/29/2022 13:01:10 - INFO - __main__ - args.device: cuda:1
05/29/2022 13:01:10 - INFO - __main__ - Using 2 gpus
05/29/2022 13:01:10 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/29/2022 13:01:10 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/29/2022 13:01:15 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
05/29/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:01:16 - INFO - __main__ - Printing 3 examples
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:01:16 - INFO - __main__ - Printing 3 examples
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:01:16 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:01:16 - INFO - __main__ - Printing 3 examples
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:01:16 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:01:16 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:01:16 - INFO - __main__ - Printing 3 examples
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:01:16 - INFO - __main__ - ['neutral']
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:01:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:01:16 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:01:16 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:01:34 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:01:34 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:01:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:01:35 - INFO - __main__ - Starting training!
05/29/2022 13:01:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:01:40 - INFO - __main__ - Starting training!
05/29/2022 13:01:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=3
05/29/2022 13:01:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=6
05/29/2022 13:01:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
05/29/2022 13:01:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=13
05/29/2022 13:01:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
05/29/2022 13:01:55 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=16
05/29/2022 13:01:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2085278555866791 on epoch=16, global_step=50
05/29/2022 13:01:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=19
05/29/2022 13:02:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=23
05/29/2022 13:02:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
05/29/2022 13:02:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
05/29/2022 13:02:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=33
05/29/2022 13:02:09 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.26508667983322365 on epoch=33
05/29/2022 13:02:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.26508667983322365 on epoch=33, global_step=100
05/29/2022 13:02:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
05/29/2022 13:02:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
05/29/2022 13:02:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
05/29/2022 13:02:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
05/29/2022 13:02:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
05/29/2022 13:02:23 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3353083168751372 on epoch=49
05/29/2022 13:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.26508667983322365 -> 0.3353083168751372 on epoch=49, global_step=150
05/29/2022 13:02:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
05/29/2022 13:02:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/29/2022 13:02:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
05/29/2022 13:02:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=63
05/29/2022 13:02:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=66
05/29/2022 13:02:38 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.28299319727891153 on epoch=66
05/29/2022 13:02:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 13:02:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
05/29/2022 13:02:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
05/29/2022 13:02:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
05/29/2022 13:02:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/29/2022 13:02:52 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.27611749680715203 on epoch=83
05/29/2022 13:02:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=86
05/29/2022 13:02:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
05/29/2022 13:02:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
05/29/2022 13:03:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
05/29/2022 13:03:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
05/29/2022 13:03:06 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.2333333333333333 on epoch=99
05/29/2022 13:03:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/29/2022 13:03:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
05/29/2022 13:03:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=109
05/29/2022 13:03:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/29/2022 13:03:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=116
05/29/2022 13:03:20 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.36079398637538174 on epoch=116
05/29/2022 13:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3353083168751372 -> 0.36079398637538174 on epoch=116, global_step=350
05/29/2022 13:03:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
05/29/2022 13:03:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
05/29/2022 13:03:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=126
05/29/2022 13:03:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
05/29/2022 13:03:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
05/29/2022 13:03:34 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.29560053981106615 on epoch=133
05/29/2022 13:03:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
05/29/2022 13:03:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=139
05/29/2022 13:03:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=143
05/29/2022 13:03:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/29/2022 13:03:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
05/29/2022 13:03:48 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.2409700722394221 on epoch=149
05/29/2022 13:03:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
05/29/2022 13:03:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
05/29/2022 13:03:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=159
05/29/2022 13:03:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
05/29/2022 13:04:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
05/29/2022 13:04:02 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.31990796745653355 on epoch=166
05/29/2022 13:04:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
05/29/2022 13:04:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=173
05/29/2022 13:04:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=176
05/29/2022 13:04:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=179
05/29/2022 13:04:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
05/29/2022 13:04:16 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.25678650036683787 on epoch=183
05/29/2022 13:04:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
05/29/2022 13:04:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=189
05/29/2022 13:04:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=193
05/29/2022 13:04:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=196
05/29/2022 13:04:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=199
05/29/2022 13:04:31 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.28218243819266836 on epoch=199
05/29/2022 13:04:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=203
05/29/2022 13:04:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=206
05/29/2022 13:04:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
05/29/2022 13:04:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=213
05/29/2022 13:04:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=216
05/29/2022 13:04:45 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.34602836879432625 on epoch=216
05/29/2022 13:04:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=219
05/29/2022 13:04:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=223
05/29/2022 13:04:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
05/29/2022 13:04:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
05/29/2022 13:04:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
05/29/2022 13:04:59 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.3160460992907801 on epoch=233
05/29/2022 13:05:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
05/29/2022 13:05:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=239
05/29/2022 13:05:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=243
05/29/2022 13:05:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=246
05/29/2022 13:05:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=249
05/29/2022 13:05:13 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.4113547858167658 on epoch=249
05/29/2022 13:05:13 - INFO - __main__ - Saving model with best Classification-F1: 0.36079398637538174 -> 0.4113547858167658 on epoch=249, global_step=750
05/29/2022 13:05:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
05/29/2022 13:05:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=256
05/29/2022 13:05:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
05/29/2022 13:05:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=263
05/29/2022 13:05:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
05/29/2022 13:05:27 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.36402922569722884 on epoch=266
05/29/2022 13:05:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
05/29/2022 13:05:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
05/29/2022 13:05:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
05/29/2022 13:05:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
05/29/2022 13:05:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=283
05/29/2022 13:05:42 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.320086563053315 on epoch=283
05/29/2022 13:05:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
05/29/2022 13:05:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
05/29/2022 13:05:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
05/29/2022 13:05:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/29/2022 13:05:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
05/29/2022 13:05:56 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.19406604747162023 on epoch=299
05/29/2022 13:05:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
05/29/2022 13:06:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
05/29/2022 13:06:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=309
05/29/2022 13:06:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
05/29/2022 13:06:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
05/29/2022 13:06:11 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.3833147528799703 on epoch=316
05/29/2022 13:06:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
05/29/2022 13:06:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
05/29/2022 13:06:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
05/29/2022 13:06:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
05/29/2022 13:06:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
05/29/2022 13:06:26 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.30617283950617286 on epoch=333
05/29/2022 13:06:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
05/29/2022 13:06:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
05/29/2022 13:06:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
05/29/2022 13:06:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
05/29/2022 13:06:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
05/29/2022 13:06:40 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.24270833333333333 on epoch=349
05/29/2022 13:06:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=353
05/29/2022 13:06:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
05/29/2022 13:06:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/29/2022 13:06:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
05/29/2022 13:06:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
05/29/2022 13:06:55 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.3087894248608534 on epoch=366
05/29/2022 13:06:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
05/29/2022 13:07:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
05/29/2022 13:07:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
05/29/2022 13:07:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
05/29/2022 13:07:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
05/29/2022 13:07:09 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.36501969208736124 on epoch=383
05/29/2022 13:07:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/29/2022 13:07:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
05/29/2022 13:07:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
05/29/2022 13:07:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
05/29/2022 13:07:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
05/29/2022 13:07:24 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.31734394637620444 on epoch=399
05/29/2022 13:07:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/29/2022 13:07:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
05/29/2022 13:07:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
05/29/2022 13:07:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
05/29/2022 13:07:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/29/2022 13:07:38 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.3121907332433648 on epoch=416
05/29/2022 13:07:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
05/29/2022 13:07:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
05/29/2022 13:07:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=426
05/29/2022 13:07:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/29/2022 13:07:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
05/29/2022 13:07:53 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.3849303849303849 on epoch=433
05/29/2022 13:07:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/29/2022 13:07:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
05/29/2022 13:08:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=443
05/29/2022 13:08:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/29/2022 13:08:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
05/29/2022 13:08:08 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.20460358056265981 on epoch=449
05/29/2022 13:08:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 13:08:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 13:08:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
05/29/2022 13:08:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
05/29/2022 13:08:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
05/29/2022 13:08:22 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.23103197674418602 on epoch=466
05/29/2022 13:08:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
05/29/2022 13:08:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
05/29/2022 13:08:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/29/2022 13:08:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
05/29/2022 13:08:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/29/2022 13:08:37 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.3374204665959703 on epoch=483
05/29/2022 13:08:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
05/29/2022 13:08:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
05/29/2022 13:08:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 13:08:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/29/2022 13:08:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/29/2022 13:08:51 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.12840475631173304 on epoch=499
05/29/2022 13:08:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
05/29/2022 13:08:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/29/2022 13:08:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
05/29/2022 13:09:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 13:09:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/29/2022 13:09:06 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.18118326118326117 on epoch=516
05/29/2022 13:09:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
05/29/2022 13:09:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 13:09:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 13:09:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
05/29/2022 13:09:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
05/29/2022 13:09:20 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.1586021505376344 on epoch=533
05/29/2022 13:09:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/29/2022 13:09:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/29/2022 13:09:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
05/29/2022 13:09:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
05/29/2022 13:09:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/29/2022 13:09:35 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.19838677152651743 on epoch=549
05/29/2022 13:09:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/29/2022 13:09:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/29/2022 13:09:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
05/29/2022 13:09:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/29/2022 13:09:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/29/2022 13:09:49 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.21106449106449107 on epoch=566
05/29/2022 13:09:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/29/2022 13:09:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 13:09:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/29/2022 13:10:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
05/29/2022 13:10:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 13:10:04 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.3530128205128205 on epoch=583
05/29/2022 13:10:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 13:10:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
05/29/2022 13:10:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/29/2022 13:10:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 13:10:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 13:10:18 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.24318435188000406 on epoch=599
05/29/2022 13:10:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 13:10:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/29/2022 13:10:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 13:10:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 13:10:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 13:10:33 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3373737373737374 on epoch=616
05/29/2022 13:10:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 13:10:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 13:10:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 13:10:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
05/29/2022 13:10:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 13:10:47 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.35185185185185186 on epoch=633
05/29/2022 13:10:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 13:10:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 13:10:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 13:10:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/29/2022 13:11:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 13:11:02 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4067609275942609 on epoch=649
05/29/2022 13:11:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/29/2022 13:11:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 13:11:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 13:11:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 13:11:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 13:11:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3598014888337469 on epoch=666
05/29/2022 13:11:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/29/2022 13:11:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 13:11:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 13:11:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 13:11:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 13:11:31 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.36232612703200945 on epoch=683
05/29/2022 13:11:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 13:11:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
05/29/2022 13:11:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 13:11:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 13:11:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 13:11:46 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.3991452991452991 on epoch=699
05/29/2022 13:11:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
05/29/2022 13:11:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 13:11:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 13:11:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/29/2022 13:11:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 13:12:00 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3270060207991243 on epoch=716
05/29/2022 13:12:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 13:12:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 13:12:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 13:12:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 13:12:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 13:12:14 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.3422739541160594 on epoch=733
05/29/2022 13:12:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 13:12:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 13:12:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 13:12:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 13:12:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 13:12:29 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.38052910052910055 on epoch=749
05/29/2022 13:12:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 13:12:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 13:12:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/29/2022 13:12:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 13:12:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 13:12:43 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.5009978820462692 on epoch=766
05/29/2022 13:12:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4113547858167658 -> 0.5009978820462692 on epoch=766, global_step=2300
05/29/2022 13:12:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/29/2022 13:12:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 13:12:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 13:12:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/29/2022 13:12:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 13:12:57 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.37941706412294646 on epoch=783
05/29/2022 13:13:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 13:13:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 13:13:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 13:13:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 13:13:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 13:13:12 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.39904761904761904 on epoch=799
05/29/2022 13:13:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 13:13:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 13:13:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 13:13:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=813
05/29/2022 13:13:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 13:13:26 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3682813248030639 on epoch=816
05/29/2022 13:13:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 13:13:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 13:13:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 13:13:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 13:13:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 13:13:41 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.4689852454080096 on epoch=833
05/29/2022 13:13:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 13:13:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 13:13:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 13:13:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 13:13:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=849
05/29/2022 13:13:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.38827838827838823 on epoch=849
05/29/2022 13:13:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/29/2022 13:14:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 13:14:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 13:14:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 13:14:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 13:14:10 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.38516746411483255 on epoch=866
05/29/2022 13:14:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/29/2022 13:14:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 13:14:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 13:14:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 13:14:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 13:14:24 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.35903225806451616 on epoch=883
05/29/2022 13:14:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 13:14:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 13:14:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/29/2022 13:14:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 13:14:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 13:14:38 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.38812238812238814 on epoch=899
05/29/2022 13:14:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
05/29/2022 13:14:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 13:14:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 13:14:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 13:14:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 13:14:53 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.43270048760244834 on epoch=916
05/29/2022 13:14:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 13:14:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
05/29/2022 13:15:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 13:15:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 13:15:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 13:15:07 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.4047619047619048 on epoch=933
05/29/2022 13:15:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 13:15:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 13:15:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 13:15:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 13:15:20 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 13:15:22 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.3926009342676009 on epoch=949
05/29/2022 13:15:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 13:15:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 13:15:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 13:15:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 13:15:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 13:15:36 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.3327184892402284 on epoch=966
05/29/2022 13:15:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 13:15:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 13:15:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 13:15:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 13:15:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 13:15:51 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.45519293924466336 on epoch=983
05/29/2022 13:15:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 13:15:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 13:15:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 13:16:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 13:16:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 13:16:05 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:16:05 - INFO - __main__ - Printing 3 examples
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:16:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:16:05 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:16:05 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:16:05 - INFO - __main__ - Printing 3 examples
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:16:05 - INFO - __main__ - ['neutral']
05/29/2022 13:16:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:16:05 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.3058201058201058 on epoch=999
05/29/2022 13:16:05 - INFO - __main__ - save last model!
05/29/2022 13:16:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:16:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 13:16:05 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 13:16:05 - INFO - __main__ - Printing 3 examples
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 13:16:05 - INFO - __main__ - ['contradiction']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 13:16:05 - INFO - __main__ - ['entailment']
05/29/2022 13:16:05 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 13:16:05 - INFO - __main__ - ['contradiction']
05/29/2022 13:16:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:16:05 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:16:06 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:16:07 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 13:16:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:16:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:16:21 - INFO - __main__ - Starting training!
05/29/2022 13:16:38 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_100_0.5_8_predictions.txt
05/29/2022 13:16:38 - INFO - __main__ - Classification-F1 on test data: 0.3239
05/29/2022 13:16:38 - INFO - __main__ - prefix=anli_16_100, lr=0.5, bsz=8, dev_performance=0.5009978820462692, test_performance=0.32393742135306075
05/29/2022 13:16:38 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.4, bsz=8 ...
05/29/2022 13:16:39 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:16:39 - INFO - __main__ - Printing 3 examples
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:16:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:16:39 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:16:39 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:16:39 - INFO - __main__ - Printing 3 examples
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:16:39 - INFO - __main__ - ['neutral']
05/29/2022 13:16:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:16:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:16:39 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:16:58 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:16:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:16:59 - INFO - __main__ - Starting training!
05/29/2022 13:17:02 - INFO - __main__ - Step 10 Global step 10 Train loss 0.58 on epoch=3
05/29/2022 13:17:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
05/29/2022 13:17:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=9
05/29/2022 13:17:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
05/29/2022 13:17:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
05/29/2022 13:17:14 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.32234432234432236 on epoch=16
05/29/2022 13:17:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.32234432234432236 on epoch=16, global_step=50
05/29/2022 13:17:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=19
05/29/2022 13:17:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
05/29/2022 13:17:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=26
05/29/2022 13:17:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
05/29/2022 13:17:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
05/29/2022 13:17:28 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.4188617886178862 on epoch=33
05/29/2022 13:17:28 - INFO - __main__ - Saving model with best Classification-F1: 0.32234432234432236 -> 0.4188617886178862 on epoch=33, global_step=100
05/29/2022 13:17:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
05/29/2022 13:17:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
05/29/2022 13:17:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=43
05/29/2022 13:17:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
05/29/2022 13:17:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/29/2022 13:17:42 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.40979039891818797 on epoch=49
05/29/2022 13:17:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
05/29/2022 13:17:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=56
05/29/2022 13:17:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
05/29/2022 13:17:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
05/29/2022 13:17:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=66
05/29/2022 13:17:57 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.2085278555866791 on epoch=66
05/29/2022 13:17:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 13:18:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=73
05/29/2022 13:18:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=76
05/29/2022 13:18:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=79
05/29/2022 13:18:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=83
05/29/2022 13:18:11 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.36287115588547186 on epoch=83
05/29/2022 13:18:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=86
05/29/2022 13:18:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
05/29/2022 13:18:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
05/29/2022 13:18:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
05/29/2022 13:18:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
05/29/2022 13:18:25 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.36383442265795213 on epoch=99
05/29/2022 13:18:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=103
05/29/2022 13:18:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
05/29/2022 13:18:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=109
05/29/2022 13:18:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=113
05/29/2022 13:18:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.33 on epoch=116
05/29/2022 13:18:39 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.4349206349206349 on epoch=116
05/29/2022 13:18:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4188617886178862 -> 0.4349206349206349 on epoch=116, global_step=350
05/29/2022 13:18:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
05/29/2022 13:18:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
05/29/2022 13:18:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/29/2022 13:18:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=129
05/29/2022 13:18:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=133
05/29/2022 13:18:53 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.35326953748006384 on epoch=133
05/29/2022 13:18:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=136
05/29/2022 13:18:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=139
05/29/2022 13:19:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=143
05/29/2022 13:19:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=146
05/29/2022 13:19:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=149
05/29/2022 13:19:08 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.30716430716430715 on epoch=149
05/29/2022 13:19:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=153
05/29/2022 13:19:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=156
05/29/2022 13:19:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=159
05/29/2022 13:19:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/29/2022 13:19:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=166
05/29/2022 13:19:22 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.38827838827838823 on epoch=166
05/29/2022 13:19:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=169
05/29/2022 13:19:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
05/29/2022 13:19:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=176
05/29/2022 13:19:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=179
05/29/2022 13:19:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=183
05/29/2022 13:19:37 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.31330005120327703 on epoch=183
05/29/2022 13:19:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
05/29/2022 13:19:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=189
05/29/2022 13:19:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
05/29/2022 13:19:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=196
05/29/2022 13:19:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
05/29/2022 13:19:51 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.26004273504273506 on epoch=199
05/29/2022 13:19:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=203
05/29/2022 13:19:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
05/29/2022 13:19:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=209
05/29/2022 13:20:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
05/29/2022 13:20:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
05/29/2022 13:20:07 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.28902691511387163 on epoch=216
05/29/2022 13:20:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
05/29/2022 13:20:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=223
05/29/2022 13:20:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
05/29/2022 13:20:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=229
05/29/2022 13:20:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=233
05/29/2022 13:20:21 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.42010760760760757 on epoch=233
05/29/2022 13:20:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=236
05/29/2022 13:20:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
05/29/2022 13:20:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
05/29/2022 13:20:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=246
05/29/2022 13:20:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
05/29/2022 13:20:36 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.28146853146853146 on epoch=249
05/29/2022 13:20:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
05/29/2022 13:20:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=256
05/29/2022 13:20:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
05/29/2022 13:20:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=263
05/29/2022 13:20:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=266
05/29/2022 13:20:50 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.2641173641173641 on epoch=266
05/29/2022 13:20:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
05/29/2022 13:20:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=273
05/29/2022 13:20:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
05/29/2022 13:21:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=279
05/29/2022 13:21:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=283
05/29/2022 13:21:05 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.23853310696095079 on epoch=283
05/29/2022 13:21:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=286
05/29/2022 13:21:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
05/29/2022 13:21:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=293
05/29/2022 13:21:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=296
05/29/2022 13:21:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
05/29/2022 13:21:19 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.1713083213083213 on epoch=299
05/29/2022 13:21:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
05/29/2022 13:21:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
05/29/2022 13:21:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
05/29/2022 13:21:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/29/2022 13:21:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
05/29/2022 13:21:34 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.20238095238095238 on epoch=316
05/29/2022 13:21:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=319
05/29/2022 13:21:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=323
05/29/2022 13:21:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
05/29/2022 13:21:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
05/29/2022 13:21:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
05/29/2022 13:21:48 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.26544286221705576 on epoch=333
05/29/2022 13:21:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
05/29/2022 13:21:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=339
05/29/2022 13:21:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=343
05/29/2022 13:21:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
05/29/2022 13:22:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
05/29/2022 13:22:02 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.33484320557491287 on epoch=349
05/29/2022 13:22:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
05/29/2022 13:22:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
05/29/2022 13:22:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
05/29/2022 13:22:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
05/29/2022 13:22:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
05/29/2022 13:22:17 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.21699346405228756 on epoch=366
05/29/2022 13:22:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
05/29/2022 13:22:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
05/29/2022 13:22:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
05/29/2022 13:22:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
05/29/2022 13:22:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
05/29/2022 13:22:31 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.18360708534621578 on epoch=383
05/29/2022 13:22:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
05/29/2022 13:22:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
05/29/2022 13:22:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/29/2022 13:22:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
05/29/2022 13:22:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/29/2022 13:22:45 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.2633101851851852 on epoch=399
05/29/2022 13:22:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
05/29/2022 13:22:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
05/29/2022 13:22:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
05/29/2022 13:22:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
05/29/2022 13:22:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/29/2022 13:23:00 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.18579807289484707 on epoch=416
05/29/2022 13:23:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
05/29/2022 13:23:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
05/29/2022 13:23:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
05/29/2022 13:23:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
05/29/2022 13:23:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
05/29/2022 13:23:14 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.36943150046598316 on epoch=433
05/29/2022 13:23:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/29/2022 13:23:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
05/29/2022 13:23:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/29/2022 13:23:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/29/2022 13:23:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
05/29/2022 13:23:29 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.3434572093315727 on epoch=449
05/29/2022 13:23:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/29/2022 13:23:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 13:23:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
05/29/2022 13:23:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
05/29/2022 13:23:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 13:23:43 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.2972459639126306 on epoch=466
05/29/2022 13:23:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
05/29/2022 13:23:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 13:23:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/29/2022 13:23:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
05/29/2022 13:23:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/29/2022 13:23:58 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.30843168527379056 on epoch=483
05/29/2022 13:24:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
05/29/2022 13:24:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
05/29/2022 13:24:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/29/2022 13:24:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/29/2022 13:24:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
05/29/2022 13:24:12 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.30000000000000004 on epoch=499
05/29/2022 13:24:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
05/29/2022 13:24:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
05/29/2022 13:24:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 13:24:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
05/29/2022 13:24:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 13:24:27 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.20306038047973535 on epoch=516
05/29/2022 13:24:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
05/29/2022 13:24:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 13:24:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
05/29/2022 13:24:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 13:24:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
05/29/2022 13:24:42 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.19935649935649935 on epoch=533
05/29/2022 13:24:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 13:24:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
05/29/2022 13:24:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 13:24:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
05/29/2022 13:24:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/29/2022 13:24:56 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.1915584415584416 on epoch=549
05/29/2022 13:24:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/29/2022 13:25:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 13:25:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/29/2022 13:25:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/29/2022 13:25:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/29/2022 13:25:11 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2336174242424242 on epoch=566
05/29/2022 13:25:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/29/2022 13:25:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 13:25:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/29/2022 13:25:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/29/2022 13:25:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 13:25:25 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.21096374889478337 on epoch=583
05/29/2022 13:25:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 13:25:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 13:25:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/29/2022 13:25:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 13:25:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/29/2022 13:25:40 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.15692307692307692 on epoch=599
05/29/2022 13:25:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 13:25:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 13:25:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/29/2022 13:25:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 13:25:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 13:25:55 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.24032738095238096 on epoch=616
05/29/2022 13:25:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/29/2022 13:26:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/29/2022 13:26:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 13:26:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/29/2022 13:26:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 13:26:09 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2515863689776733 on epoch=633
05/29/2022 13:26:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/29/2022 13:26:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 13:26:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 13:26:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 13:26:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 13:26:24 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.2279461279461279 on epoch=649
05/29/2022 13:26:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/29/2022 13:26:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/29/2022 13:26:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 13:26:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
05/29/2022 13:26:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/29/2022 13:26:38 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.25000855197892796 on epoch=666
05/29/2022 13:26:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 13:26:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 13:26:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 13:26:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 13:26:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/29/2022 13:26:53 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.35114379084967323 on epoch=683
05/29/2022 13:26:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 13:26:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 13:27:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 13:27:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 13:27:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 13:27:08 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.3060068471833178 on epoch=699
05/29/2022 13:27:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/29/2022 13:27:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 13:27:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 13:27:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 13:27:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 13:27:22 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.3308763308763309 on epoch=716
05/29/2022 13:27:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 13:27:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/29/2022 13:27:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 13:27:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 13:27:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 13:27:37 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.3753968253968254 on epoch=733
05/29/2022 13:27:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
05/29/2022 13:27:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 13:27:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 13:27:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 13:27:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 13:27:52 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2768847795163585 on epoch=749
05/29/2022 13:27:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
05/29/2022 13:27:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 13:28:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 13:28:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 13:28:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/29/2022 13:28:06 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.20897435897435895 on epoch=766
05/29/2022 13:28:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 13:28:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/29/2022 13:28:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 13:28:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/29/2022 13:28:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/29/2022 13:28:21 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.17192353643966546 on epoch=783
05/29/2022 13:28:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 13:28:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 13:28:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/29/2022 13:28:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=796
05/29/2022 13:28:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 13:28:36 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.3162962962962963 on epoch=799
05/29/2022 13:28:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 13:28:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 13:28:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 13:28:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 13:28:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 13:28:50 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.3271284271284271 on epoch=816
05/29/2022 13:28:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 13:28:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/29/2022 13:28:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 13:29:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 13:29:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 13:29:05 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.2806022806022806 on epoch=833
05/29/2022 13:29:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 13:29:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 13:29:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 13:29:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 13:29:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 13:29:19 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.3451178451178451 on epoch=849
05/29/2022 13:29:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/29/2022 13:29:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 13:29:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 13:29:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 13:29:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 13:29:34 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.3444633444633445 on epoch=866
05/29/2022 13:29:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 13:29:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 13:29:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 13:29:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 13:29:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 13:29:49 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.34358974358974353 on epoch=883
05/29/2022 13:29:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 13:29:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 13:29:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 13:29:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 13:30:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 13:30:03 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.3451178451178451 on epoch=899
05/29/2022 13:30:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 13:30:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 13:30:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 13:30:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 13:30:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 13:30:18 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3757115749525617 on epoch=916
05/29/2022 13:30:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 13:30:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 13:30:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 13:30:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/29/2022 13:30:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 13:30:33 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.36232612703200945 on epoch=933
05/29/2022 13:30:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 13:30:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 13:30:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 13:30:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 13:30:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 13:30:48 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.34380760851349085 on epoch=949
05/29/2022 13:30:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/29/2022 13:30:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 13:30:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 13:30:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=963
05/29/2022 13:31:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 13:31:03 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.3608108108108108 on epoch=966
05/29/2022 13:31:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 13:31:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 13:31:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 13:31:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 13:31:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=983
05/29/2022 13:31:18 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2511817226890756 on epoch=983
05/29/2022 13:31:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 13:31:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 13:31:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 13:31:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 13:31:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/29/2022 13:31:33 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:31:33 - INFO - __main__ - Printing 3 examples
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:31:33 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.3825064453247815 on epoch=999
05/29/2022 13:31:33 - INFO - __main__ - save last model!
05/29/2022 13:31:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 13:31:33 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:31:33 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:31:33 - INFO - __main__ - Printing 3 examples
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:31:33 - INFO - __main__ - ['neutral']
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:31:33 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 13:31:33 - INFO - __main__ - Printing 3 examples
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 13:31:33 - INFO - __main__ - ['contradiction']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 13:31:33 - INFO - __main__ - ['entailment']
05/29/2022 13:31:33 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 13:31:33 - INFO - __main__ - ['contradiction']
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:31:33 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:31:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:31:34 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 13:31:52 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:31:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:31:52 - INFO - __main__ - Starting training!
05/29/2022 13:32:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_100_0.4_8_predictions.txt
05/29/2022 13:32:05 - INFO - __main__ - Classification-F1 on test data: 0.1381
05/29/2022 13:32:06 - INFO - __main__ - prefix=anli_16_100, lr=0.4, bsz=8, dev_performance=0.4349206349206349, test_performance=0.13813334308821154
05/29/2022 13:32:06 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.3, bsz=8 ...
05/29/2022 13:32:07 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:32:07 - INFO - __main__ - Printing 3 examples
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:32:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:32:07 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:32:07 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:32:07 - INFO - __main__ - Printing 3 examples
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:32:07 - INFO - __main__ - ['neutral']
05/29/2022 13:32:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:32:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:32:07 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:32:22 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:32:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:32:23 - INFO - __main__ - Starting training!
05/29/2022 13:32:26 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=3
05/29/2022 13:32:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
05/29/2022 13:32:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
05/29/2022 13:32:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=13
05/29/2022 13:32:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
05/29/2022 13:32:38 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.24611708482676223 on epoch=16
05/29/2022 13:32:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24611708482676223 on epoch=16, global_step=50
05/29/2022 13:32:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=19
05/29/2022 13:32:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
05/29/2022 13:32:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
05/29/2022 13:32:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
05/29/2022 13:32:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
05/29/2022 13:32:52 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.31858076563958915 on epoch=33
05/29/2022 13:32:52 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.31858076563958915 on epoch=33, global_step=100
05/29/2022 13:32:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=36
05/29/2022 13:32:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=39
05/29/2022 13:33:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=43
05/29/2022 13:33:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
05/29/2022 13:33:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
05/29/2022 13:33:07 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.2760577915376677 on epoch=49
05/29/2022 13:33:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=53
05/29/2022 13:33:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=56
05/29/2022 13:33:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
05/29/2022 13:33:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
05/29/2022 13:33:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=66
05/29/2022 13:33:21 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.3418181818181818 on epoch=66
05/29/2022 13:33:21 - INFO - __main__ - Saving model with best Classification-F1: 0.31858076563958915 -> 0.3418181818181818 on epoch=66, global_step=200
05/29/2022 13:33:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/29/2022 13:33:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/29/2022 13:33:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
05/29/2022 13:33:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
05/29/2022 13:33:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/29/2022 13:33:36 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.23966696402022006 on epoch=83
05/29/2022 13:33:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
05/29/2022 13:33:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
05/29/2022 13:33:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
05/29/2022 13:33:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
05/29/2022 13:33:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=99
05/29/2022 13:33:50 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.34518518518518526 on epoch=99
05/29/2022 13:33:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3418181818181818 -> 0.34518518518518526 on epoch=99, global_step=300
05/29/2022 13:33:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/29/2022 13:33:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
05/29/2022 13:33:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
05/29/2022 13:34:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/29/2022 13:34:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=116
05/29/2022 13:34:05 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.28634167764602547 on epoch=116
05/29/2022 13:34:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=119
05/29/2022 13:34:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
05/29/2022 13:34:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
05/29/2022 13:34:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
05/29/2022 13:34:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
05/29/2022 13:34:20 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3605128205128205 on epoch=133
05/29/2022 13:34:20 - INFO - __main__ - Saving model with best Classification-F1: 0.34518518518518526 -> 0.3605128205128205 on epoch=133, global_step=400
05/29/2022 13:34:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
05/29/2022 13:34:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
05/29/2022 13:34:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
05/29/2022 13:34:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=146
05/29/2022 13:34:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=149
05/29/2022 13:34:35 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.2926652142338417 on epoch=149
05/29/2022 13:34:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=153
05/29/2022 13:34:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=156
05/29/2022 13:34:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=159
05/29/2022 13:34:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
05/29/2022 13:34:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
05/29/2022 13:34:49 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.303688141923436 on epoch=166
05/29/2022 13:34:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
05/29/2022 13:34:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=173
05/29/2022 13:34:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=176
05/29/2022 13:35:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=179
05/29/2022 13:35:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=183
05/29/2022 13:35:04 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.32049910873440285 on epoch=183
05/29/2022 13:35:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
05/29/2022 13:35:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
05/29/2022 13:35:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
05/29/2022 13:35:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=196
05/29/2022 13:35:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=199
05/29/2022 13:35:18 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.21053391053391055 on epoch=199
05/29/2022 13:35:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=203
05/29/2022 13:35:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=206
05/29/2022 13:35:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
05/29/2022 13:35:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=213
05/29/2022 13:35:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=216
05/29/2022 13:35:34 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.23143881208397335 on epoch=216
05/29/2022 13:35:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=219
05/29/2022 13:35:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=223
05/29/2022 13:35:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=226
05/29/2022 13:35:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
05/29/2022 13:35:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
05/29/2022 13:35:48 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.26647727272727273 on epoch=233
05/29/2022 13:35:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=236
05/29/2022 13:35:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=239
05/29/2022 13:35:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=243
05/29/2022 13:35:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=246
05/29/2022 13:36:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=249
05/29/2022 13:36:03 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.2982905982905983 on epoch=249
05/29/2022 13:36:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
05/29/2022 13:36:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=256
05/29/2022 13:36:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=259
05/29/2022 13:36:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=263
05/29/2022 13:36:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=266
05/29/2022 13:36:18 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.17 on epoch=266
05/29/2022 13:36:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=269
05/29/2022 13:36:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=273
05/29/2022 13:36:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=276
05/29/2022 13:36:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=279
05/29/2022 13:36:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=283
05/29/2022 13:36:33 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.3244778613199666 on epoch=283
05/29/2022 13:36:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=286
05/29/2022 13:36:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=289
05/29/2022 13:36:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=293
05/29/2022 13:36:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=296
05/29/2022 13:36:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=299
05/29/2022 13:36:48 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.2575569358178054 on epoch=299
05/29/2022 13:36:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=303
05/29/2022 13:36:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=306
05/29/2022 13:36:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
05/29/2022 13:36:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=313
05/29/2022 13:37:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=316
05/29/2022 13:37:02 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.20961386918833724 on epoch=316
05/29/2022 13:37:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=319
05/29/2022 13:37:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=323
05/29/2022 13:37:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.08 on epoch=326
05/29/2022 13:37:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
05/29/2022 13:37:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=333
05/29/2022 13:37:17 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.3252002002002002 on epoch=333
05/29/2022 13:37:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
05/29/2022 13:37:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=339
05/29/2022 13:37:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
05/29/2022 13:37:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=346
05/29/2022 13:37:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
05/29/2022 13:37:31 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.12262548262548263 on epoch=349
05/29/2022 13:37:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
05/29/2022 13:37:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
05/29/2022 13:37:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
05/29/2022 13:37:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=363
05/29/2022 13:37:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
05/29/2022 13:37:46 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.11035848682907505 on epoch=366
05/29/2022 13:37:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=369
05/29/2022 13:37:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=373
05/29/2022 13:37:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
05/29/2022 13:37:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
05/29/2022 13:37:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
05/29/2022 13:38:01 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.23194444444444445 on epoch=383
05/29/2022 13:38:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
05/29/2022 13:38:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=389
05/29/2022 13:38:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=393
05/29/2022 13:38:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=396
05/29/2022 13:38:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=399
05/29/2022 13:38:15 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.1421375921375921 on epoch=399
05/29/2022 13:38:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
05/29/2022 13:38:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
05/29/2022 13:38:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=409
05/29/2022 13:38:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
05/29/2022 13:38:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/29/2022 13:38:30 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.15271132376395535 on epoch=416
05/29/2022 13:38:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
05/29/2022 13:38:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
05/29/2022 13:38:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
05/29/2022 13:38:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
05/29/2022 13:38:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
05/29/2022 13:38:44 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.17086826565575564 on epoch=433
05/29/2022 13:38:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
05/29/2022 13:38:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
05/29/2022 13:38:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
05/29/2022 13:38:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
05/29/2022 13:38:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
05/29/2022 13:38:59 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.24430882495398626 on epoch=449
05/29/2022 13:39:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
05/29/2022 13:39:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
05/29/2022 13:39:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
05/29/2022 13:39:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=463
05/29/2022 13:39:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
05/29/2022 13:39:14 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.1262071262071262 on epoch=466
05/29/2022 13:39:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/29/2022 13:39:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
05/29/2022 13:39:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
05/29/2022 13:39:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
05/29/2022 13:39:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
05/29/2022 13:39:28 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.1539021164021164 on epoch=483
05/29/2022 13:39:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/29/2022 13:39:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/29/2022 13:39:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
05/29/2022 13:39:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 13:39:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
05/29/2022 13:39:43 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.18609822428411454 on epoch=499
05/29/2022 13:39:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
05/29/2022 13:39:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
05/29/2022 13:39:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
05/29/2022 13:39:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
05/29/2022 13:39:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
05/29/2022 13:39:58 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.16207258723304935 on epoch=516
05/29/2022 13:40:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
05/29/2022 13:40:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
05/29/2022 13:40:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=526
05/29/2022 13:40:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 13:40:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/29/2022 13:40:12 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.19852311939268458 on epoch=533
05/29/2022 13:40:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 13:40:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/29/2022 13:40:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 13:40:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
05/29/2022 13:40:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
05/29/2022 13:40:27 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.20469208211143694 on epoch=549
05/29/2022 13:40:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
05/29/2022 13:40:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/29/2022 13:40:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
05/29/2022 13:40:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/29/2022 13:40:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/29/2022 13:40:41 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.17305954639920484 on epoch=566
05/29/2022 13:40:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
05/29/2022 13:40:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 13:40:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 13:40:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 13:40:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/29/2022 13:40:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.22449655537890834 on epoch=583
05/29/2022 13:40:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 13:41:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 13:41:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
05/29/2022 13:41:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 13:41:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
05/29/2022 13:41:11 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.20070615658850954 on epoch=599
05/29/2022 13:41:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
05/29/2022 13:41:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
05/29/2022 13:41:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=609
05/29/2022 13:41:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 13:41:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/29/2022 13:41:25 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.19433137089991592 on epoch=616
05/29/2022 13:41:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
05/29/2022 13:41:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
05/29/2022 13:41:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 13:41:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
05/29/2022 13:41:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/29/2022 13:41:40 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.2155418017486983 on epoch=633
05/29/2022 13:41:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/29/2022 13:41:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 13:41:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 13:41:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/29/2022 13:41:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 13:41:54 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.23425971877584784 on epoch=649
05/29/2022 13:41:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/29/2022 13:42:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/29/2022 13:42:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 13:42:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 13:42:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/29/2022 13:42:09 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.251754158004158 on epoch=666
05/29/2022 13:42:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/29/2022 13:42:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/29/2022 13:42:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 13:42:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 13:42:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 13:42:24 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.30800915331807777 on epoch=683
05/29/2022 13:42:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/29/2022 13:42:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/29/2022 13:42:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 13:42:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/29/2022 13:42:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 13:42:39 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2635897435897436 on epoch=699
05/29/2022 13:42:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/29/2022 13:42:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/29/2022 13:42:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
05/29/2022 13:42:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/29/2022 13:42:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/29/2022 13:42:53 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.26672324498411454 on epoch=716
05/29/2022 13:42:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/29/2022 13:42:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
05/29/2022 13:43:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/29/2022 13:43:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/29/2022 13:43:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=733
05/29/2022 13:43:08 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.2327831810590431 on epoch=733
05/29/2022 13:43:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/29/2022 13:43:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
05/29/2022 13:43:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 13:43:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/29/2022 13:43:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/29/2022 13:43:22 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.24 on epoch=749
05/29/2022 13:43:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
05/29/2022 13:43:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
05/29/2022 13:43:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 13:43:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/29/2022 13:43:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 13:43:37 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.1871416871416871 on epoch=766
05/29/2022 13:43:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 13:43:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 13:43:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 13:43:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/29/2022 13:43:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 13:43:52 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.16848484848484852 on epoch=783
05/29/2022 13:43:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
05/29/2022 13:43:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 13:44:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/29/2022 13:44:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 13:44:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 13:44:06 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.20207968771032442 on epoch=799
05/29/2022 13:44:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 13:44:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 13:44:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 13:44:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 13:44:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 13:44:21 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.19954481792717088 on epoch=816
05/29/2022 13:44:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 13:44:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/29/2022 13:44:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/29/2022 13:44:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 13:44:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 13:44:36 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.18644688644688645 on epoch=833
05/29/2022 13:44:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 13:44:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 13:44:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 13:44:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
05/29/2022 13:44:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
05/29/2022 13:44:50 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.21678321678321677 on epoch=849
05/29/2022 13:44:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 13:44:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/29/2022 13:44:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 13:45:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 13:45:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 13:45:05 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.28718656544743504 on epoch=866
05/29/2022 13:45:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/29/2022 13:45:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 13:45:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/29/2022 13:45:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 13:45:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 13:45:19 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.22666416416416416 on epoch=883
05/29/2022 13:45:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 13:45:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 13:45:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 13:45:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 13:45:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 13:45:34 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.27820105820105817 on epoch=899
05/29/2022 13:45:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 13:45:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
05/29/2022 13:45:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 13:45:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 13:45:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/29/2022 13:45:49 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.32086021505376344 on epoch=916
05/29/2022 13:45:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 13:45:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/29/2022 13:45:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 13:45:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
05/29/2022 13:46:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 13:46:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2756683498371478 on epoch=933
05/29/2022 13:46:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 13:46:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 13:46:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
05/29/2022 13:46:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 13:46:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/29/2022 13:46:18 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3341503267973856 on epoch=949
05/29/2022 13:46:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 13:46:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/29/2022 13:46:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 13:46:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/29/2022 13:46:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
05/29/2022 13:46:33 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.179338535860275 on epoch=966
05/29/2022 13:46:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/29/2022 13:46:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 13:46:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 13:46:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=979
05/29/2022 13:46:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 13:46:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.16907748049052396 on epoch=983
05/29/2022 13:46:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/29/2022 13:46:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/29/2022 13:46:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 13:46:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 13:47:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 13:47:02 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:47:02 - INFO - __main__ - Printing 3 examples
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:47:02 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.31886039886039885 on epoch=999
05/29/2022 13:47:02 - INFO - __main__ - save last model!
05/29/2022 13:47:02 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:47:02 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:47:02 - INFO - __main__ - Printing 3 examples
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:47:02 - INFO - __main__ - ['neutral']
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:47:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 13:47:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 13:47:02 - INFO - __main__ - Printing 3 examples
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 13:47:02 - INFO - __main__ - ['contradiction']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 13:47:02 - INFO - __main__ - ['entailment']
05/29/2022 13:47:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 13:47:02 - INFO - __main__ - ['contradiction']
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:47:02 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:47:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:47:03 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 13:47:17 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:47:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:47:18 - INFO - __main__ - Starting training!
05/29/2022 13:47:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_100_0.3_8_predictions.txt
05/29/2022 13:47:33 - INFO - __main__ - Classification-F1 on test data: 0.1919
05/29/2022 13:47:33 - INFO - __main__ - prefix=anli_16_100, lr=0.3, bsz=8, dev_performance=0.3605128205128205, test_performance=0.19194805072476256
05/29/2022 13:47:33 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.2, bsz=8 ...
05/29/2022 13:47:34 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:47:34 - INFO - __main__ - Printing 3 examples
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:47:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:47:34 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 13:47:34 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 13:47:34 - INFO - __main__ - Printing 3 examples
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/29/2022 13:47:34 - INFO - __main__ - ['neutral']
05/29/2022 13:47:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 13:47:35 - INFO - __main__ - Tokenizing Output ...
05/29/2022 13:47:35 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 13:47:49 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 13:47:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 13:47:50 - INFO - __main__ - Starting training!
05/29/2022 13:47:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.59 on epoch=3
05/29/2022 13:47:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=6
05/29/2022 13:47:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=9
05/29/2022 13:48:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=13
05/29/2022 13:48:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
05/29/2022 13:48:06 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 13:48:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 13:48:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
05/29/2022 13:48:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=23
05/29/2022 13:48:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/29/2022 13:48:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/29/2022 13:48:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
05/29/2022 13:48:21 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 13:48:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
05/29/2022 13:48:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
05/29/2022 13:48:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
05/29/2022 13:48:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
05/29/2022 13:48:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
05/29/2022 13:48:35 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.26666666666666666 on epoch=49
05/29/2022 13:48:35 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26666666666666666 on epoch=49, global_step=150
05/29/2022 13:48:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
05/29/2022 13:48:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=56
05/29/2022 13:48:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/29/2022 13:48:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/29/2022 13:48:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
05/29/2022 13:48:50 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.20050600885515493 on epoch=66
05/29/2022 13:48:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
05/29/2022 13:48:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/29/2022 13:48:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
05/29/2022 13:49:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/29/2022 13:49:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
05/29/2022 13:49:04 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.3799864773495605 on epoch=83
05/29/2022 13:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.26666666666666666 -> 0.3799864773495605 on epoch=83, global_step=250
05/29/2022 13:49:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
05/29/2022 13:49:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=89
05/29/2022 13:49:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
05/29/2022 13:49:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
05/29/2022 13:49:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
05/29/2022 13:49:19 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.37724987430869783 on epoch=99
05/29/2022 13:49:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
05/29/2022 13:49:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
05/29/2022 13:49:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
05/29/2022 13:49:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
05/29/2022 13:49:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/29/2022 13:49:33 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.3358664039778591 on epoch=116
05/29/2022 13:49:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=119
05/29/2022 13:49:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
05/29/2022 13:49:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
05/29/2022 13:49:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
05/29/2022 13:49:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
05/29/2022 13:49:48 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.31746031746031744 on epoch=133
05/29/2022 13:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/29/2022 13:49:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
05/29/2022 13:49:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
05/29/2022 13:49:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=146
05/29/2022 13:50:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=149
05/29/2022 13:50:02 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.38027102732985085 on epoch=149
05/29/2022 13:50:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3799864773495605 -> 0.38027102732985085 on epoch=149, global_step=450
05/29/2022 13:50:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=153
05/29/2022 13:50:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=156
05/29/2022 13:50:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=159
05/29/2022 13:50:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
05/29/2022 13:50:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=166
05/29/2022 13:50:17 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.32941176470588235 on epoch=166
05/29/2022 13:50:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=169
05/29/2022 13:50:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
05/29/2022 13:50:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=176
05/29/2022 13:50:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=179
05/29/2022 13:50:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=183
05/29/2022 13:50:32 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.34863123993558776 on epoch=183
05/29/2022 13:50:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
05/29/2022 13:50:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=189
05/29/2022 13:50:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=193
05/29/2022 13:50:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=196
05/29/2022 13:50:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=199
05/29/2022 13:50:46 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.336214933468938 on epoch=199
05/29/2022 13:50:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=203
05/29/2022 13:50:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=206
05/29/2022 13:50:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=209
05/29/2022 13:50:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=213
05/29/2022 13:50:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=216
05/29/2022 13:51:01 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.28729752770673483 on epoch=216
05/29/2022 13:51:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=219
05/29/2022 13:51:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=223
05/29/2022 13:51:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=226
05/29/2022 13:51:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=229
05/29/2022 13:51:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=233
05/29/2022 13:51:16 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.3478015448603684 on epoch=233
05/29/2022 13:51:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=236
05/29/2022 13:51:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=239
05/29/2022 13:51:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=243
05/29/2022 13:51:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=246
05/29/2022 13:51:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=249
05/29/2022 13:51:31 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=249
05/29/2022 13:51:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=253
05/29/2022 13:51:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=256
05/29/2022 13:51:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=259
05/29/2022 13:51:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=263
05/29/2022 13:51:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=266
05/29/2022 13:51:46 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.44477085781433606 on epoch=266
05/29/2022 13:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.38027102732985085 -> 0.44477085781433606 on epoch=266, global_step=800
05/29/2022 13:51:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=269
05/29/2022 13:51:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=273
05/29/2022 13:51:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=276
05/29/2022 13:51:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=279
05/29/2022 13:51:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
05/29/2022 13:52:01 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.4144958355484671 on epoch=283
05/29/2022 13:52:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=286
05/29/2022 13:52:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=289
05/29/2022 13:52:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=293
05/29/2022 13:52:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=296
05/29/2022 13:52:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=299
05/29/2022 13:52:15 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.2739854562514661 on epoch=299
05/29/2022 13:52:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
05/29/2022 13:52:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=306
05/29/2022 13:52:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=309
05/29/2022 13:52:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=313
05/29/2022 13:52:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=316
05/29/2022 13:52:30 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.23571411514673923 on epoch=316
05/29/2022 13:52:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=319
05/29/2022 13:52:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=323
05/29/2022 13:52:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=326
05/29/2022 13:52:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
05/29/2022 13:52:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=333
05/29/2022 13:52:45 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.37012987012987014 on epoch=333
05/29/2022 13:52:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=336
05/29/2022 13:52:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=339
05/29/2022 13:52:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=343
05/29/2022 13:52:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=346
05/29/2022 13:52:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=349
05/29/2022 13:52:59 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.29311313335580275 on epoch=349
05/29/2022 13:53:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=353
05/29/2022 13:53:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=356
05/29/2022 13:53:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=359
05/29/2022 13:53:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=363
05/29/2022 13:53:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=366
05/29/2022 13:53:14 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.2225135975135975 on epoch=366
05/29/2022 13:53:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=369
05/29/2022 13:53:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
05/29/2022 13:53:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=376
05/29/2022 13:53:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=379
05/29/2022 13:53:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=383
05/29/2022 13:53:29 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.2279891304347826 on epoch=383
05/29/2022 13:53:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=386
05/29/2022 13:53:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=389
05/29/2022 13:53:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=393
05/29/2022 13:53:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=396
05/29/2022 13:53:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=399
05/29/2022 13:53:43 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.18998648648648647 on epoch=399
05/29/2022 13:53:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
05/29/2022 13:53:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=406
05/29/2022 13:53:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=409
05/29/2022 13:53:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=413
05/29/2022 13:53:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=416
05/29/2022 13:53:58 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.23448648648648648 on epoch=416
05/29/2022 13:54:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=419
05/29/2022 13:54:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=423
05/29/2022 13:54:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=426
05/29/2022 13:54:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
05/29/2022 13:54:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=433
05/29/2022 13:54:13 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.16280487804878047 on epoch=433
05/29/2022 13:54:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=436
05/29/2022 13:54:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=439
05/29/2022 13:54:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
05/29/2022 13:54:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
05/29/2022 13:54:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
05/29/2022 13:54:28 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.2360248447204969 on epoch=449
05/29/2022 13:54:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=453
05/29/2022 13:54:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=456
05/29/2022 13:54:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
05/29/2022 13:54:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=463
05/29/2022 13:54:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
05/29/2022 13:54:42 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.25168259523489034 on epoch=466
05/29/2022 13:54:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=469
05/29/2022 13:54:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=473
05/29/2022 13:54:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
05/29/2022 13:54:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=479
05/29/2022 13:54:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
05/29/2022 13:54:57 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.2086247086247086 on epoch=483
05/29/2022 13:55:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
05/29/2022 13:55:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/29/2022 13:55:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=493
05/29/2022 13:55:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=496
05/29/2022 13:55:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=499
05/29/2022 13:55:12 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.2886889293139293 on epoch=499
05/29/2022 13:55:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
05/29/2022 13:55:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
05/29/2022 13:55:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
05/29/2022 13:55:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=513
05/29/2022 13:55:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
05/29/2022 13:55:27 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.21541125541125544 on epoch=516
05/29/2022 13:55:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
05/29/2022 13:55:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
05/29/2022 13:55:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
05/29/2022 13:55:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=529
05/29/2022 13:55:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
05/29/2022 13:55:41 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.22435897435897434 on epoch=533
05/29/2022 13:55:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
05/29/2022 13:55:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
05/29/2022 13:55:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
05/29/2022 13:55:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=546
05/29/2022 13:55:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
05/29/2022 13:55:56 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.2632540374475858 on epoch=549
05/29/2022 13:55:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=553
05/29/2022 13:56:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/29/2022 13:56:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=559
05/29/2022 13:56:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=563
05/29/2022 13:56:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
05/29/2022 13:56:11 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.2008050089445438 on epoch=566
05/29/2022 13:56:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=569
05/29/2022 13:56:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
05/29/2022 13:56:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
05/29/2022 13:56:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
05/29/2022 13:56:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
05/29/2022 13:56:25 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.25050151365940837 on epoch=583
05/29/2022 13:56:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=586
05/29/2022 13:56:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
05/29/2022 13:56:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
05/29/2022 13:56:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
05/29/2022 13:56:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/29/2022 13:56:40 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.3188020556441609 on epoch=599
05/29/2022 13:56:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
05/29/2022 13:56:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
05/29/2022 13:56:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
05/29/2022 13:56:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 13:56:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/29/2022 13:56:55 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.318742985409652 on epoch=616
05/29/2022 13:56:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
05/29/2022 13:57:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
05/29/2022 13:57:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 13:57:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/29/2022 13:57:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
05/29/2022 13:57:09 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.14634920634920634 on epoch=633
05/29/2022 13:57:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/29/2022 13:57:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/29/2022 13:57:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
05/29/2022 13:57:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/29/2022 13:57:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 13:57:24 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.18020671834625324 on epoch=649
05/29/2022 13:57:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=653
05/29/2022 13:57:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
05/29/2022 13:57:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/29/2022 13:57:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
05/29/2022 13:57:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
05/29/2022 13:57:39 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.33572535991140645 on epoch=666
05/29/2022 13:57:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
05/29/2022 13:57:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/29/2022 13:57:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 13:57:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 13:57:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 13:57:54 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.283384702822043 on epoch=683
05/29/2022 13:57:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
05/29/2022 13:57:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
05/29/2022 13:58:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
05/29/2022 13:58:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
05/29/2022 13:58:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
05/29/2022 13:58:08 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.32193732193732194 on epoch=699
05/29/2022 13:58:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
05/29/2022 13:58:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/29/2022 13:58:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
05/29/2022 13:58:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/29/2022 13:58:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
05/29/2022 13:58:23 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.1556349873843566 on epoch=716
05/29/2022 13:58:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/29/2022 13:58:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=723
05/29/2022 13:58:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
05/29/2022 13:58:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/29/2022 13:58:36 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/29/2022 13:58:37 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.2163337250293772 on epoch=733
05/29/2022 13:58:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/29/2022 13:58:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 13:58:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 13:58:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=746
05/29/2022 13:58:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/29/2022 13:58:52 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.18267567567567566 on epoch=749
05/29/2022 13:58:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/29/2022 13:58:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 13:59:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
05/29/2022 13:59:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/29/2022 13:59:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/29/2022 13:59:07 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.09874835309617919 on epoch=766
05/29/2022 13:59:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
05/29/2022 13:59:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/29/2022 13:59:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/29/2022 13:59:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 13:59:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 13:59:21 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.24185837028824833 on epoch=783
05/29/2022 13:59:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 13:59:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
05/29/2022 13:59:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/29/2022 13:59:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
05/29/2022 13:59:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 13:59:36 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.1829243697478992 on epoch=799
05/29/2022 13:59:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 13:59:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/29/2022 13:59:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/29/2022 13:59:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
05/29/2022 13:59:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 13:59:50 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2536057692307692 on epoch=816
05/29/2022 13:59:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 13:59:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
05/29/2022 13:59:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 14:00:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
05/29/2022 14:00:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/29/2022 14:00:05 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.10393512243607121 on epoch=833
05/29/2022 14:00:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 14:00:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 14:00:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/29/2022 14:00:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 14:00:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/29/2022 14:00:20 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.19728506787330316 on epoch=849
05/29/2022 14:00:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/29/2022 14:00:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 14:00:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
05/29/2022 14:00:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
05/29/2022 14:00:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 14:00:35 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.33316133470932235 on epoch=866
05/29/2022 14:00:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 14:00:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 14:00:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 14:00:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/29/2022 14:00:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 14:00:49 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.22435897435897434 on epoch=883
05/29/2022 14:00:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 14:00:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 14:00:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 14:01:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=896
05/29/2022 14:01:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
05/29/2022 14:01:04 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.1565610859728507 on epoch=899
05/29/2022 14:01:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 14:01:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 14:01:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 14:01:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 14:01:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 14:01:19 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.2901709401709402 on epoch=916
05/29/2022 14:01:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 14:01:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/29/2022 14:01:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/29/2022 14:01:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 14:01:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
05/29/2022 14:01:33 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.25982042648709314 on epoch=933
05/29/2022 14:01:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 14:01:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
05/29/2022 14:01:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
05/29/2022 14:01:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
05/29/2022 14:01:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
05/29/2022 14:01:48 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.18181818181818182 on epoch=949
05/29/2022 14:01:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 14:01:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=956
05/29/2022 14:01:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 14:01:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/29/2022 14:02:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 14:02:03 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2812998405103668 on epoch=966
05/29/2022 14:02:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 14:02:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 14:02:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
05/29/2022 14:02:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 14:02:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/29/2022 14:02:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.21358533206969454 on epoch=983
05/29/2022 14:02:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/29/2022 14:02:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 14:02:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 14:02:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
05/29/2022 14:02:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
05/29/2022 14:02:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:02:32 - INFO - __main__ - Printing 3 examples
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:02:32 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.20366322204557497 on epoch=999
05/29/2022 14:02:32 - INFO - __main__ - save last model!
05/29/2022 14:02:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:02:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:02:32 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:02:32 - INFO - __main__ - Printing 3 examples
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:02:32 - INFO - __main__ - ['entailment']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:02:32 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:02:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:02:32 - INFO - __main__ - Printing 3 examples
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:02:32 - INFO - __main__ - ['contradiction']
05/29/2022 14:02:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:02:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:02:32 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:02:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:02:34 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:02:47 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:02:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:02:48 - INFO - __main__ - Starting training!
05/29/2022 14:03:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_100_0.2_8_predictions.txt
05/29/2022 14:03:03 - INFO - __main__ - Classification-F1 on test data: 0.1087
05/29/2022 14:03:03 - INFO - __main__ - prefix=anli_16_100, lr=0.2, bsz=8, dev_performance=0.44477085781433606, test_performance=0.10874692291242082
05/29/2022 14:03:03 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.5, bsz=8 ...
05/29/2022 14:03:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:03:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:03:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:03:04 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:03:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:03:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:03:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:03:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:03:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:03:04 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:03:23 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:03:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:03:24 - INFO - __main__ - Starting training!
05/29/2022 14:03:27 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=3
05/29/2022 14:03:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=6
05/29/2022 14:03:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
05/29/2022 14:03:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=13
05/29/2022 14:03:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
05/29/2022 14:03:39 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 14:03:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 14:03:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
05/29/2022 14:03:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=23
05/29/2022 14:03:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=26
05/29/2022 14:03:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
05/29/2022 14:03:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
05/29/2022 14:03:54 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.1693121693121693 on epoch=33
05/29/2022 14:03:54 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1693121693121693 on epoch=33, global_step=100
05/29/2022 14:03:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/29/2022 14:03:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
05/29/2022 14:04:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 14:04:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
05/29/2022 14:04:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
05/29/2022 14:04:08 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 14:04:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
05/29/2022 14:04:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
05/29/2022 14:04:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=59
05/29/2022 14:04:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=63
05/29/2022 14:04:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
05/29/2022 14:04:23 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 14:04:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 14:04:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
05/29/2022 14:04:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
05/29/2022 14:04:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
05/29/2022 14:04:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=83
05/29/2022 14:04:38 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.34722222222222215 on epoch=83
05/29/2022 14:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.34722222222222215 on epoch=83, global_step=250
05/29/2022 14:04:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
05/29/2022 14:04:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
05/29/2022 14:04:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
05/29/2022 14:04:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
05/29/2022 14:04:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
05/29/2022 14:04:52 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.3511111111111111 on epoch=99
05/29/2022 14:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.34722222222222215 -> 0.3511111111111111 on epoch=99, global_step=300
05/29/2022 14:04:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=103
05/29/2022 14:04:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
05/29/2022 14:05:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
05/29/2022 14:05:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=113
05/29/2022 14:05:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
05/29/2022 14:05:07 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3148148148148148 on epoch=116
05/29/2022 14:05:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=119
05/29/2022 14:05:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
05/29/2022 14:05:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
05/29/2022 14:05:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
05/29/2022 14:05:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=133
05/29/2022 14:05:22 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.20793650793650795 on epoch=133
05/29/2022 14:05:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
05/29/2022 14:05:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
05/29/2022 14:05:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=143
05/29/2022 14:05:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=146
05/29/2022 14:05:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
05/29/2022 14:05:36 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3508343508343508 on epoch=149
05/29/2022 14:05:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=153
05/29/2022 14:05:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
05/29/2022 14:05:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=159
05/29/2022 14:05:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
05/29/2022 14:05:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
05/29/2022 14:05:51 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.3422045114922081 on epoch=166
05/29/2022 14:05:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
05/29/2022 14:05:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=173
05/29/2022 14:05:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
05/29/2022 14:06:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=179
05/29/2022 14:06:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=183
05/29/2022 14:06:05 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.2586527293844367 on epoch=183
05/29/2022 14:06:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=186
05/29/2022 14:06:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=189
05/29/2022 14:06:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
05/29/2022 14:06:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=196
05/29/2022 14:06:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
05/29/2022 14:06:20 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.33073593073593077 on epoch=199
05/29/2022 14:06:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=203
05/29/2022 14:06:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=206
05/29/2022 14:06:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=209
05/29/2022 14:06:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=213
05/29/2022 14:06:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=216
05/29/2022 14:06:35 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.25228126677402035 on epoch=216
05/29/2022 14:06:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=219
05/29/2022 14:06:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
05/29/2022 14:06:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=226
05/29/2022 14:06:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=229
05/29/2022 14:06:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=233
05/29/2022 14:06:49 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.27543982019685337 on epoch=233
05/29/2022 14:06:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=236
05/29/2022 14:06:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=239
05/29/2022 14:06:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
05/29/2022 14:07:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=246
05/29/2022 14:07:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=249
05/29/2022 14:07:04 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3361111111111111 on epoch=249
05/29/2022 14:07:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=253
05/29/2022 14:07:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=256
05/29/2022 14:07:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=259
05/29/2022 14:07:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=263
05/29/2022 14:07:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
05/29/2022 14:07:19 - INFO - __main__ - Global step 800 Train loss 0.18 Classification-F1 0.32513781466809194 on epoch=266
05/29/2022 14:07:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=269
05/29/2022 14:07:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=273
05/29/2022 14:07:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
05/29/2022 14:07:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=279
05/29/2022 14:07:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=283
05/29/2022 14:07:33 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.2763511861090731 on epoch=283
05/29/2022 14:07:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
05/29/2022 14:07:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=289
05/29/2022 14:07:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=293
05/29/2022 14:07:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
05/29/2022 14:07:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=299
05/29/2022 14:07:48 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.26188866351138157 on epoch=299
05/29/2022 14:07:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=303
05/29/2022 14:07:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=306
05/29/2022 14:07:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
05/29/2022 14:07:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=313
05/29/2022 14:08:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
05/29/2022 14:08:03 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.2693763065280093 on epoch=316
05/29/2022 14:08:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=319
05/29/2022 14:08:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=323
05/29/2022 14:08:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=326
05/29/2022 14:08:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=329
05/29/2022 14:08:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=333
05/29/2022 14:08:17 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.246763480806034 on epoch=333
05/29/2022 14:08:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=336
05/29/2022 14:08:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
05/29/2022 14:08:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
05/29/2022 14:08:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
05/29/2022 14:08:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
05/29/2022 14:08:32 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.3196965113934261 on epoch=349
05/29/2022 14:08:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
05/29/2022 14:08:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
05/29/2022 14:08:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
05/29/2022 14:08:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
05/29/2022 14:08:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=366
05/29/2022 14:08:47 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.2384801189149015 on epoch=366
05/29/2022 14:08:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
05/29/2022 14:08:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=373
05/29/2022 14:08:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=376
05/29/2022 14:08:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
05/29/2022 14:09:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
05/29/2022 14:09:01 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.18888888888888888 on epoch=383
05/29/2022 14:09:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
05/29/2022 14:09:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
05/29/2022 14:09:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
05/29/2022 14:09:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
05/29/2022 14:09:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/29/2022 14:09:16 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.2241215574548908 on epoch=399
05/29/2022 14:09:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
05/29/2022 14:09:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
05/29/2022 14:09:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
05/29/2022 14:09:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
05/29/2022 14:09:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=416
05/29/2022 14:09:30 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.2119724025974026 on epoch=416
05/29/2022 14:09:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
05/29/2022 14:09:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
05/29/2022 14:09:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
05/29/2022 14:09:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
05/29/2022 14:09:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=433
05/29/2022 14:09:45 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.2530875576036866 on epoch=433
05/29/2022 14:09:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/29/2022 14:09:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/29/2022 14:09:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/29/2022 14:09:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
05/29/2022 14:09:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
05/29/2022 14:10:00 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.20623100303951364 on epoch=449
05/29/2022 14:10:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/29/2022 14:10:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
05/29/2022 14:10:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
05/29/2022 14:10:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/29/2022 14:10:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 14:10:14 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.29128205128205126 on epoch=466
05/29/2022 14:10:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/29/2022 14:10:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 14:10:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
05/29/2022 14:10:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
05/29/2022 14:10:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
05/29/2022 14:10:29 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.30272267859925645 on epoch=483
05/29/2022 14:10:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/29/2022 14:10:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/29/2022 14:10:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 14:10:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 14:10:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/29/2022 14:10:44 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.18664477285166942 on epoch=499
05/29/2022 14:10:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
05/29/2022 14:10:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/29/2022 14:10:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=509
05/29/2022 14:10:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
05/29/2022 14:10:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 14:10:58 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.19411764705882353 on epoch=516
05/29/2022 14:11:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/29/2022 14:11:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
05/29/2022 14:11:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 14:11:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
05/29/2022 14:11:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/29/2022 14:11:13 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.1720754716981132 on epoch=533
05/29/2022 14:11:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 14:11:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/29/2022 14:11:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
05/29/2022 14:11:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
05/29/2022 14:11:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/29/2022 14:11:28 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.16666666666666666 on epoch=549
05/29/2022 14:11:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/29/2022 14:11:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
05/29/2022 14:11:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 14:11:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
05/29/2022 14:11:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/29/2022 14:11:42 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.16346153846153846 on epoch=566
05/29/2022 14:11:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/29/2022 14:11:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
05/29/2022 14:11:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/29/2022 14:11:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 14:11:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
05/29/2022 14:11:57 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.17613636363636365 on epoch=583
05/29/2022 14:11:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 14:12:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
05/29/2022 14:12:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/29/2022 14:12:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
05/29/2022 14:12:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 14:12:12 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.1868429189857761 on epoch=599
05/29/2022 14:12:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 14:12:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 14:12:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 14:12:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 14:12:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 14:12:26 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.17755376344086024 on epoch=616
05/29/2022 14:12:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 14:12:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 14:12:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/29/2022 14:12:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
05/29/2022 14:12:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 14:12:41 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.1967391304347826 on epoch=633
05/29/2022 14:12:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 14:12:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 14:12:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 14:12:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 14:12:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/29/2022 14:12:55 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.17952127659574466 on epoch=649
05/29/2022 14:12:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/29/2022 14:13:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/29/2022 14:13:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/29/2022 14:13:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 14:13:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/29/2022 14:13:10 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.09177489177489177 on epoch=666
05/29/2022 14:13:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 14:13:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/29/2022 14:13:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 14:13:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 14:13:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 14:13:25 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.22658294086865516 on epoch=683
05/29/2022 14:13:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/29/2022 14:13:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
05/29/2022 14:13:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 14:13:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
05/29/2022 14:13:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 14:13:39 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.24544179523141654 on epoch=699
05/29/2022 14:13:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/29/2022 14:13:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=706
05/29/2022 14:13:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/29/2022 14:13:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/29/2022 14:13:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 14:13:54 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.3210638297872341 on epoch=716
05/29/2022 14:13:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/29/2022 14:13:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 14:14:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 14:14:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 14:14:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 14:14:09 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.0767857142857143 on epoch=733
05/29/2022 14:14:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/29/2022 14:14:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 14:14:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 14:14:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 14:14:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 14:14:23 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.15302325581395346 on epoch=749
05/29/2022 14:14:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 14:14:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 14:14:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 14:14:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/29/2022 14:14:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 14:14:38 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.13115151515151519 on epoch=766
05/29/2022 14:14:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=769
05/29/2022 14:14:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 14:14:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/29/2022 14:14:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/29/2022 14:14:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 14:14:53 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.17129439188262718 on epoch=783
05/29/2022 14:14:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 14:14:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=789
05/29/2022 14:15:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
05/29/2022 14:15:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 14:15:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 14:15:08 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2522594364699628 on epoch=799
05/29/2022 14:15:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 14:15:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/29/2022 14:15:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 14:15:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/29/2022 14:15:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 14:15:22 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.3071864847303444 on epoch=816
05/29/2022 14:15:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 14:15:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 14:15:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 14:15:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 14:15:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 14:15:37 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.1609105180533752 on epoch=833
05/29/2022 14:15:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 14:15:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 14:15:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 14:15:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 14:15:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 14:15:52 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.1624489795918367 on epoch=849
05/29/2022 14:15:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 14:15:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 14:16:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 14:16:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 14:16:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 14:16:06 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2242063492063492 on epoch=866
05/29/2022 14:16:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/29/2022 14:16:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 14:16:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 14:16:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 14:16:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 14:16:21 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.23713947990543735 on epoch=883
05/29/2022 14:16:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
05/29/2022 14:16:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 14:16:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 14:16:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 14:16:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 14:16:36 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.26256564141035255 on epoch=899
05/29/2022 14:16:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 14:16:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 14:16:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 14:16:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 14:16:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 14:16:51 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.23192848020434229 on epoch=916
05/29/2022 14:16:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
05/29/2022 14:16:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 14:16:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/29/2022 14:17:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 14:17:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 14:17:05 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2940740740740741 on epoch=933
05/29/2022 14:17:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 14:17:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 14:17:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 14:17:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 14:17:19 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 14:17:20 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.27697262479871176 on epoch=949
05/29/2022 14:17:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 14:17:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 14:17:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 14:17:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 14:17:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 14:17:35 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.27697262479871176 on epoch=966
05/29/2022 14:17:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 14:17:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 14:17:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
05/29/2022 14:17:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 14:17:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 14:17:50 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2384801189149015 on epoch=983
05/29/2022 14:17:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 14:17:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 14:17:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 14:18:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 14:18:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/29/2022 14:18:04 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.28218694885361556 on epoch=999
05/29/2022 14:18:04 - INFO - __main__ - save last model!
05/29/2022 14:18:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:18:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:18:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:18:04 - INFO - __main__ - ['entailment']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:18:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:18:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:18:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:18:04 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:18:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:18:04 - INFO - __main__ - Printing 3 examples
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:18:04 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:18:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:18:05 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:18:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:18:06 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:18:23 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:18:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:18:24 - INFO - __main__ - Starting training!
05/29/2022 14:18:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_13_0.5_8_predictions.txt
05/29/2022 14:18:35 - INFO - __main__ - Classification-F1 on test data: 0.0940
05/29/2022 14:18:36 - INFO - __main__ - prefix=anli_16_13, lr=0.5, bsz=8, dev_performance=0.3511111111111111, test_performance=0.09399623781038074
05/29/2022 14:18:36 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.4, bsz=8 ...
05/29/2022 14:18:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:18:37 - INFO - __main__ - Printing 3 examples
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:18:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:18:37 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:18:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:18:37 - INFO - __main__ - Printing 3 examples
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:18:37 - INFO - __main__ - ['contradiction']
05/29/2022 14:18:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:18:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:18:37 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:18:52 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:18:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:18:53 - INFO - __main__ - Starting training!
05/29/2022 14:18:56 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=3
05/29/2022 14:18:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=6
05/29/2022 14:19:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=9
05/29/2022 14:19:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
05/29/2022 14:19:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
05/29/2022 14:19:08 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.2913806254767353 on epoch=16
05/29/2022 14:19:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2913806254767353 on epoch=16, global_step=50
05/29/2022 14:19:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
05/29/2022 14:19:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/29/2022 14:19:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
05/29/2022 14:19:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/29/2022 14:19:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
05/29/2022 14:19:23 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 14:19:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=36
05/29/2022 14:19:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
05/29/2022 14:19:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 14:19:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=46
05/29/2022 14:19:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=49
05/29/2022 14:19:37 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.1639344262295082 on epoch=49
05/29/2022 14:19:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
05/29/2022 14:19:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
05/29/2022 14:19:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
05/29/2022 14:19:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
05/29/2022 14:19:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
05/29/2022 14:19:52 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 14:19:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
05/29/2022 14:19:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
05/29/2022 14:20:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
05/29/2022 14:20:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
05/29/2022 14:20:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/29/2022 14:20:06 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.1983273596176822 on epoch=83
05/29/2022 14:20:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
05/29/2022 14:20:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
05/29/2022 14:20:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
05/29/2022 14:20:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=96
05/29/2022 14:20:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
05/29/2022 14:20:21 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.2450388265746333 on epoch=99
05/29/2022 14:20:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
05/29/2022 14:20:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=106
05/29/2022 14:20:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
05/29/2022 14:20:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=113
05/29/2022 14:20:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
05/29/2022 14:20:35 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.27636363636363637 on epoch=116
05/29/2022 14:20:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
05/29/2022 14:20:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
05/29/2022 14:20:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=126
05/29/2022 14:20:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
05/29/2022 14:20:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=133
05/29/2022 14:20:50 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.1726446676231468 on epoch=133
05/29/2022 14:20:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
05/29/2022 14:20:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
05/29/2022 14:20:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
05/29/2022 14:21:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/29/2022 14:21:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=149
05/29/2022 14:21:04 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.2692678604008653 on epoch=149
05/29/2022 14:21:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=153
05/29/2022 14:21:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
05/29/2022 14:21:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=159
05/29/2022 14:21:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=163
05/29/2022 14:21:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
05/29/2022 14:21:18 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.25069921179760996 on epoch=166
05/29/2022 14:21:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=169
05/29/2022 14:21:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
05/29/2022 14:21:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=176
05/29/2022 14:21:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
05/29/2022 14:21:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=183
05/29/2022 14:21:33 - INFO - __main__ - Global step 550 Train loss 0.29 Classification-F1 0.20908004778972522 on epoch=183
05/29/2022 14:21:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=186
05/29/2022 14:21:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=189
05/29/2022 14:21:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=193
05/29/2022 14:21:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=196
05/29/2022 14:21:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=199
05/29/2022 14:21:47 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.263111275903267 on epoch=199
05/29/2022 14:21:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
05/29/2022 14:21:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=206
05/29/2022 14:21:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
05/29/2022 14:21:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
05/29/2022 14:22:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=216
05/29/2022 14:22:02 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.2984126984126984 on epoch=216
05/29/2022 14:22:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2913806254767353 -> 0.2984126984126984 on epoch=216, global_step=650
05/29/2022 14:22:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=219
05/29/2022 14:22:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=223
05/29/2022 14:22:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=226
05/29/2022 14:22:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=229
05/29/2022 14:22:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=233
05/29/2022 14:22:16 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.2692307692307692 on epoch=233
05/29/2022 14:22:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=236
05/29/2022 14:22:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=239
05/29/2022 14:22:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=243
05/29/2022 14:22:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
05/29/2022 14:22:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=249
05/29/2022 14:22:31 - INFO - __main__ - Global step 750 Train loss 0.18 Classification-F1 0.3282548791874474 on epoch=249
05/29/2022 14:22:31 - INFO - __main__ - Saving model with best Classification-F1: 0.2984126984126984 -> 0.3282548791874474 on epoch=249, global_step=750
05/29/2022 14:22:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=253
05/29/2022 14:22:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=256
05/29/2022 14:22:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=259
05/29/2022 14:22:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=263
05/29/2022 14:22:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=266
05/29/2022 14:22:45 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.2928686717133829 on epoch=266
05/29/2022 14:22:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/29/2022 14:22:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=273
05/29/2022 14:22:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=276
05/29/2022 14:22:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
05/29/2022 14:22:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
05/29/2022 14:23:00 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.28571428571428575 on epoch=283
05/29/2022 14:23:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=286
05/29/2022 14:23:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=289
05/29/2022 14:23:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
05/29/2022 14:23:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/29/2022 14:23:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=299
05/29/2022 14:23:14 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.31881427707199034 on epoch=299
05/29/2022 14:23:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
05/29/2022 14:23:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=306
05/29/2022 14:23:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=309
05/29/2022 14:23:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=313
05/29/2022 14:23:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=316
05/29/2022 14:23:29 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.3484939557870834 on epoch=316
05/29/2022 14:23:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3282548791874474 -> 0.3484939557870834 on epoch=316, global_step=950
05/29/2022 14:23:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=319
05/29/2022 14:23:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
05/29/2022 14:23:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=326
05/29/2022 14:23:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=329
05/29/2022 14:23:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=333
05/29/2022 14:23:43 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.2716273442226255 on epoch=333
05/29/2022 14:23:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
05/29/2022 14:23:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=339
05/29/2022 14:23:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
05/29/2022 14:23:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=346
05/29/2022 14:23:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
05/29/2022 14:23:58 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.29425213675213674 on epoch=349
05/29/2022 14:24:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
05/29/2022 14:24:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/29/2022 14:24:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
05/29/2022 14:24:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
05/29/2022 14:24:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
05/29/2022 14:24:12 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.1259893309159374 on epoch=366
05/29/2022 14:24:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
05/29/2022 14:24:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=373
05/29/2022 14:24:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
05/29/2022 14:24:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=379
05/29/2022 14:24:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
05/29/2022 14:24:26 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.2777777777777778 on epoch=383
05/29/2022 14:24:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=386
05/29/2022 14:24:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
05/29/2022 14:24:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=393
05/29/2022 14:24:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
05/29/2022 14:24:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
05/29/2022 14:24:41 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.3074074074074074 on epoch=399
05/29/2022 14:24:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
05/29/2022 14:24:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
05/29/2022 14:24:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=409
05/29/2022 14:24:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
05/29/2022 14:24:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=416
05/29/2022 14:24:55 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.24305555555555555 on epoch=416
05/29/2022 14:24:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
05/29/2022 14:25:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
05/29/2022 14:25:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
05/29/2022 14:25:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
05/29/2022 14:25:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
05/29/2022 14:25:10 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.27132867132867133 on epoch=433
05/29/2022 14:25:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
05/29/2022 14:25:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=439
05/29/2022 14:25:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
05/29/2022 14:25:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
05/29/2022 14:25:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=449
05/29/2022 14:25:24 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.23301587301587298 on epoch=449
05/29/2022 14:25:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
05/29/2022 14:25:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
05/29/2022 14:25:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/29/2022 14:25:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=463
05/29/2022 14:25:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
05/29/2022 14:25:39 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.16747798349629014 on epoch=466
05/29/2022 14:25:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/29/2022 14:25:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
05/29/2022 14:25:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=476
05/29/2022 14:25:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
05/29/2022 14:25:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=483
05/29/2022 14:25:53 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.12625795257374203 on epoch=483
05/29/2022 14:25:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
05/29/2022 14:25:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
05/29/2022 14:26:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 14:26:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/29/2022 14:26:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
05/29/2022 14:26:08 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.13335704125177808 on epoch=499
05/29/2022 14:26:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
05/29/2022 14:26:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
05/29/2022 14:26:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
05/29/2022 14:26:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
05/29/2022 14:26:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/29/2022 14:26:22 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.17266281512605042 on epoch=516
05/29/2022 14:26:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
05/29/2022 14:26:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
05/29/2022 14:26:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
05/29/2022 14:26:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=529
05/29/2022 14:26:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
05/29/2022 14:26:37 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.1691118077324974 on epoch=533
05/29/2022 14:26:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 14:26:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
05/29/2022 14:26:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 14:26:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
05/29/2022 14:26:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
05/29/2022 14:26:51 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.1945970695970696 on epoch=549
05/29/2022 14:26:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=553
05/29/2022 14:26:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=556
05/29/2022 14:26:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
05/29/2022 14:27:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
05/29/2022 14:27:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
05/29/2022 14:27:06 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.1920585161964472 on epoch=566
05/29/2022 14:27:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/29/2022 14:27:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=573
05/29/2022 14:27:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=576
05/29/2022 14:27:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 14:27:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
05/29/2022 14:27:21 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.27166907166907167 on epoch=583
05/29/2022 14:27:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 14:27:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
05/29/2022 14:27:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/29/2022 14:27:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/29/2022 14:27:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/29/2022 14:27:35 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.245981355505165 on epoch=599
05/29/2022 14:27:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=603
05/29/2022 14:27:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
05/29/2022 14:27:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/29/2022 14:27:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 14:27:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/29/2022 14:27:50 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.22239346019833825 on epoch=616
05/29/2022 14:27:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/29/2022 14:27:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/29/2022 14:27:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 14:28:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 14:28:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
05/29/2022 14:28:04 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2521014492753624 on epoch=633
05/29/2022 14:28:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/29/2022 14:28:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 14:28:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
05/29/2022 14:28:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/29/2022 14:28:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 14:28:19 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.15980586450960566 on epoch=649
05/29/2022 14:28:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
05/29/2022 14:28:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 14:28:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 14:28:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 14:28:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 14:28:33 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2174239350912779 on epoch=666
05/29/2022 14:28:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/29/2022 14:28:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/29/2022 14:28:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 14:28:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
05/29/2022 14:28:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=683
05/29/2022 14:28:48 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.1556455240665767 on epoch=683
05/29/2022 14:28:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 14:28:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
05/29/2022 14:28:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/29/2022 14:28:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
05/29/2022 14:29:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 14:29:02 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.2381894089211162 on epoch=699
05/29/2022 14:29:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/29/2022 14:29:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
05/29/2022 14:29:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/29/2022 14:29:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 14:29:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 14:29:17 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.33055555555555555 on epoch=716
05/29/2022 14:29:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 14:29:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 14:29:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 14:29:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/29/2022 14:29:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 14:29:31 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.13375295043273014 on epoch=733
05/29/2022 14:29:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/29/2022 14:29:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 14:29:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 14:29:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=746
05/29/2022 14:29:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 14:29:46 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.2564814814814815 on epoch=749
05/29/2022 14:29:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/29/2022 14:29:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 14:29:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
05/29/2022 14:29:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 14:29:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
05/29/2022 14:30:00 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.2805788982259571 on epoch=766
05/29/2022 14:30:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/29/2022 14:30:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=773
05/29/2022 14:30:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/29/2022 14:30:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=779
05/29/2022 14:30:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/29/2022 14:30:15 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.2919187336156483 on epoch=783
05/29/2022 14:30:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 14:30:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 14:30:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 14:30:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 14:30:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 14:30:29 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.26421404682274247 on epoch=799
05/29/2022 14:30:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 14:30:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 14:30:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/29/2022 14:30:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 14:30:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 14:30:44 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3485714285714286 on epoch=816
05/29/2022 14:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3484939557870834 -> 0.3485714285714286 on epoch=816, global_step=2450
05/29/2022 14:30:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=819
05/29/2022 14:30:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 14:30:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 14:30:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 14:30:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/29/2022 14:30:59 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.2939022881880025 on epoch=833
05/29/2022 14:31:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 14:31:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
05/29/2022 14:31:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 14:31:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 14:31:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 14:31:13 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2299145299145299 on epoch=849
05/29/2022 14:31:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/29/2022 14:31:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 14:31:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
05/29/2022 14:31:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 14:31:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 14:31:28 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3438881757238625 on epoch=866
05/29/2022 14:31:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 14:31:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/29/2022 14:31:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 14:31:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 14:31:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 14:31:42 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.29971509971509974 on epoch=883
05/29/2022 14:31:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
05/29/2022 14:31:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 14:31:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 14:31:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=896
05/29/2022 14:31:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 14:31:57 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.3526881720430108 on epoch=899
05/29/2022 14:31:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3485714285714286 -> 0.3526881720430108 on epoch=899, global_step=2700
05/29/2022 14:32:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 14:32:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 14:32:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 14:32:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=913
05/29/2022 14:32:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 14:32:12 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3477183477183477 on epoch=916
05/29/2022 14:32:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 14:32:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 14:32:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 14:32:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 14:32:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=933
05/29/2022 14:32:26 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.3934218848829855 on epoch=933
05/29/2022 14:32:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3526881720430108 -> 0.3934218848829855 on epoch=933, global_step=2800
05/29/2022 14:32:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 14:32:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 14:32:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 14:32:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 14:32:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/29/2022 14:32:41 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.33473389355742295 on epoch=949
05/29/2022 14:32:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/29/2022 14:32:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/29/2022 14:32:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 14:32:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=963
05/29/2022 14:32:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 14:32:56 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.27070707070707073 on epoch=966
05/29/2022 14:32:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/29/2022 14:33:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 14:33:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=976
05/29/2022 14:33:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 14:33:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 14:33:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.32136752136752134 on epoch=983
05/29/2022 14:33:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 14:33:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
05/29/2022 14:33:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 14:33:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 14:33:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 14:33:25 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.34792368125701456 on epoch=999
05/29/2022 14:33:25 - INFO - __main__ - save last model!
05/29/2022 14:33:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:33:25 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:33:25 - INFO - __main__ - Printing 3 examples
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:33:25 - INFO - __main__ - ['entailment']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:33:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:33:25 - INFO - __main__ - Printing 3 examples
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:33:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:33:25 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:33:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:33:25 - INFO - __main__ - Printing 3 examples
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:33:25 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:33:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:33:25 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:33:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:33:27 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:33:44 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:33:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:33:45 - INFO - __main__ - Starting training!
05/29/2022 14:33:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_13_0.4_8_predictions.txt
05/29/2022 14:33:57 - INFO - __main__ - Classification-F1 on test data: 0.2605
05/29/2022 14:33:57 - INFO - __main__ - prefix=anli_16_13, lr=0.4, bsz=8, dev_performance=0.3934218848829855, test_performance=0.2604991185501032
05/29/2022 14:33:57 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.3, bsz=8 ...
05/29/2022 14:33:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:33:58 - INFO - __main__ - Printing 3 examples
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:33:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:33:58 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:33:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:33:58 - INFO - __main__ - Printing 3 examples
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:33:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:33:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:33:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:33:58 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:34:13 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:34:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:34:14 - INFO - __main__ - Starting training!
05/29/2022 14:34:17 - INFO - __main__ - Step 10 Global step 10 Train loss 0.63 on epoch=3
05/29/2022 14:34:20 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=6
05/29/2022 14:34:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
05/29/2022 14:34:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=13
05/29/2022 14:34:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
05/29/2022 14:34:29 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.1983273596176822 on epoch=16
05/29/2022 14:34:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1983273596176822 on epoch=16, global_step=50
05/29/2022 14:34:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
05/29/2022 14:34:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=23
05/29/2022 14:34:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
05/29/2022 14:34:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
05/29/2022 14:34:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=33
05/29/2022 14:34:44 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3638262322472848 on epoch=33
05/29/2022 14:34:44 - INFO - __main__ - Saving model with best Classification-F1: 0.1983273596176822 -> 0.3638262322472848 on epoch=33, global_step=100
05/29/2022 14:34:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/29/2022 14:34:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
05/29/2022 14:34:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=43
05/29/2022 14:34:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
05/29/2022 14:34:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/29/2022 14:34:58 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 14:35:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
05/29/2022 14:35:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/29/2022 14:35:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/29/2022 14:35:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
05/29/2022 14:35:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
05/29/2022 14:35:13 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 14:35:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
05/29/2022 14:35:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/29/2022 14:35:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
05/29/2022 14:35:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/29/2022 14:35:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/29/2022 14:35:28 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.32668566001899335 on epoch=83
05/29/2022 14:35:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
05/29/2022 14:35:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
05/29/2022 14:35:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
05/29/2022 14:35:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
05/29/2022 14:35:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
05/29/2022 14:35:42 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.1693121693121693 on epoch=99
05/29/2022 14:35:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
05/29/2022 14:35:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
05/29/2022 14:35:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
05/29/2022 14:35:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/29/2022 14:35:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=116
05/29/2022 14:35:57 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.1693121693121693 on epoch=116
05/29/2022 14:35:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
05/29/2022 14:36:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
05/29/2022 14:36:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
05/29/2022 14:36:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
05/29/2022 14:36:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=133
05/29/2022 14:36:11 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3121951219512195 on epoch=133
05/29/2022 14:36:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
05/29/2022 14:36:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
05/29/2022 14:36:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
05/29/2022 14:36:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
05/29/2022 14:36:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
05/29/2022 14:36:26 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.20908004778972522 on epoch=149
05/29/2022 14:36:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
05/29/2022 14:36:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=156
05/29/2022 14:36:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=159
05/29/2022 14:36:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
05/29/2022 14:36:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
05/29/2022 14:36:41 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.31344537815126056 on epoch=166
05/29/2022 14:36:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
05/29/2022 14:36:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=173
05/29/2022 14:36:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
05/29/2022 14:36:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=179
05/29/2022 14:36:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=183
05/29/2022 14:36:56 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.3701234567901235 on epoch=183
05/29/2022 14:36:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3638262322472848 -> 0.3701234567901235 on epoch=183, global_step=550
05/29/2022 14:36:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
05/29/2022 14:37:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=189
05/29/2022 14:37:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=193
05/29/2022 14:37:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=196
05/29/2022 14:37:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=199
05/29/2022 14:37:11 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.2866295609152752 on epoch=199
05/29/2022 14:37:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=203
05/29/2022 14:37:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=206
05/29/2022 14:37:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
05/29/2022 14:37:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=213
05/29/2022 14:37:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=216
05/29/2022 14:37:25 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.3171394799054373 on epoch=216
05/29/2022 14:37:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=219
05/29/2022 14:37:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=223
05/29/2022 14:37:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=226
05/29/2022 14:37:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=229
05/29/2022 14:37:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=233
05/29/2022 14:37:40 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.29875575158594025 on epoch=233
05/29/2022 14:37:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
05/29/2022 14:37:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=239
05/29/2022 14:37:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=243
05/29/2022 14:37:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=246
05/29/2022 14:37:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
05/29/2022 14:37:55 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.3078656750056345 on epoch=249
05/29/2022 14:37:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
05/29/2022 14:38:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=256
05/29/2022 14:38:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=259
05/29/2022 14:38:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=263
05/29/2022 14:38:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=266
05/29/2022 14:38:10 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.23647586980920313 on epoch=266
05/29/2022 14:38:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=269
05/29/2022 14:38:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
05/29/2022 14:38:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=276
05/29/2022 14:38:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=279
05/29/2022 14:38:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
05/29/2022 14:38:24 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.47113997113997114 on epoch=283
05/29/2022 14:38:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3701234567901235 -> 0.47113997113997114 on epoch=283, global_step=850
05/29/2022 14:38:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=286
05/29/2022 14:38:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=289
05/29/2022 14:38:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=293
05/29/2022 14:38:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=296
05/29/2022 14:38:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=299
05/29/2022 14:38:39 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.2442216652742968 on epoch=299
05/29/2022 14:38:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=303
05/29/2022 14:38:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
05/29/2022 14:38:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=309
05/29/2022 14:38:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=313
05/29/2022 14:38:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=316
05/29/2022 14:38:54 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.27079218784287906 on epoch=316
05/29/2022 14:38:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=319
05/29/2022 14:38:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=323
05/29/2022 14:39:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=326
05/29/2022 14:39:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=329
05/29/2022 14:39:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=333
05/29/2022 14:39:09 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.40080213903743306 on epoch=333
05/29/2022 14:39:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
05/29/2022 14:39:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=339
05/29/2022 14:39:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=343
05/29/2022 14:39:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=346
05/29/2022 14:39:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=349
05/29/2022 14:39:24 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.2929292929292929 on epoch=349
05/29/2022 14:39:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=353
05/29/2022 14:39:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=356
05/29/2022 14:39:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=359
05/29/2022 14:39:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=363
05/29/2022 14:39:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=366
05/29/2022 14:39:38 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.2526115859449193 on epoch=366
05/29/2022 14:39:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=369
05/29/2022 14:39:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=373
05/29/2022 14:39:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=376
05/29/2022 14:39:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=379
05/29/2022 14:39:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=383
05/29/2022 14:39:53 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.36648745519713266 on epoch=383
05/29/2022 14:39:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=386
05/29/2022 14:39:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=389
05/29/2022 14:40:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=393
05/29/2022 14:40:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=396
05/29/2022 14:40:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
05/29/2022 14:40:08 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.27777777777777773 on epoch=399
05/29/2022 14:40:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
05/29/2022 14:40:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
05/29/2022 14:40:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=409
05/29/2022 14:40:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=413
05/29/2022 14:40:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=416
05/29/2022 14:40:23 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.29223125564588975 on epoch=416
05/29/2022 14:40:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
05/29/2022 14:40:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
05/29/2022 14:40:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=426
05/29/2022 14:40:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=429
05/29/2022 14:40:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
05/29/2022 14:40:38 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.31644777986241407 on epoch=433
05/29/2022 14:40:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=436
05/29/2022 14:40:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=439
05/29/2022 14:40:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=443
05/29/2022 14:40:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
05/29/2022 14:40:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=449
05/29/2022 14:40:53 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.35038363171355497 on epoch=449
05/29/2022 14:40:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
05/29/2022 14:40:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
05/29/2022 14:41:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
05/29/2022 14:41:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
05/29/2022 14:41:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
05/29/2022 14:41:07 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.3483923483923484 on epoch=466
05/29/2022 14:41:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=469
05/29/2022 14:41:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=473
05/29/2022 14:41:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
05/29/2022 14:41:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
05/29/2022 14:41:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
05/29/2022 14:41:22 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.2622874446773818 on epoch=483
05/29/2022 14:41:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
05/29/2022 14:41:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
05/29/2022 14:41:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=493
05/29/2022 14:41:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=496
05/29/2022 14:41:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=499
05/29/2022 14:41:37 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.2974425102084676 on epoch=499
05/29/2022 14:41:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
05/29/2022 14:41:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/29/2022 14:41:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=509
05/29/2022 14:41:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
05/29/2022 14:41:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
05/29/2022 14:41:52 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.3510638297872341 on epoch=516
05/29/2022 14:41:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=519
05/29/2022 14:41:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
05/29/2022 14:42:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
05/29/2022 14:42:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=529
05/29/2022 14:42:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=533
05/29/2022 14:42:07 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.303921568627451 on epoch=533
05/29/2022 14:42:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=536
05/29/2022 14:42:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
05/29/2022 14:42:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
05/29/2022 14:42:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
05/29/2022 14:42:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
05/29/2022 14:42:22 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.3510638297872341 on epoch=549
05/29/2022 14:42:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=553
05/29/2022 14:42:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=556
05/29/2022 14:42:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=559
05/29/2022 14:42:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
05/29/2022 14:42:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
05/29/2022 14:42:36 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.25723684210526315 on epoch=566
05/29/2022 14:42:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/29/2022 14:42:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 14:42:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
05/29/2022 14:42:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=579
05/29/2022 14:42:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=583
05/29/2022 14:42:51 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.1985294117647059 on epoch=583
05/29/2022 14:42:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=586
05/29/2022 14:42:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=589
05/29/2022 14:42:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=593
05/29/2022 14:43:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
05/29/2022 14:43:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
05/29/2022 14:43:06 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.24066924066924067 on epoch=599
05/29/2022 14:43:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=603
05/29/2022 14:43:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=606
05/29/2022 14:43:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
05/29/2022 14:43:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/29/2022 14:43:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/29/2022 14:43:21 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.2626262626262626 on epoch=616
05/29/2022 14:43:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 14:43:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/29/2022 14:43:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/29/2022 14:43:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
05/29/2022 14:43:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
05/29/2022 14:43:35 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.20336605890603088 on epoch=633
05/29/2022 14:43:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/29/2022 14:43:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 14:43:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
05/29/2022 14:43:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
05/29/2022 14:43:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/29/2022 14:43:50 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.3292847503373819 on epoch=649
05/29/2022 14:43:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=653
05/29/2022 14:43:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
05/29/2022 14:43:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
05/29/2022 14:44:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=663
05/29/2022 14:44:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
05/29/2022 14:44:05 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.2130867709815078 on epoch=666
05/29/2022 14:44:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=669
05/29/2022 14:44:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
05/29/2022 14:44:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=676
05/29/2022 14:44:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
05/29/2022 14:44:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/29/2022 14:44:20 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.2484320557491289 on epoch=683
05/29/2022 14:44:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/29/2022 14:44:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/29/2022 14:44:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
05/29/2022 14:44:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/29/2022 14:44:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 14:44:35 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.33205574912891983 on epoch=699
05/29/2022 14:44:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/29/2022 14:44:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
05/29/2022 14:44:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/29/2022 14:44:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/29/2022 14:44:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
05/29/2022 14:44:49 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.1741978431633604 on epoch=716
05/29/2022 14:44:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/29/2022 14:44:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
05/29/2022 14:44:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/29/2022 14:45:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
05/29/2022 14:45:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 14:45:04 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.3152218152218152 on epoch=733
05/29/2022 14:45:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 14:45:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
05/29/2022 14:45:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 14:45:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/29/2022 14:45:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
05/29/2022 14:45:18 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.26916221033868093 on epoch=749
05/29/2022 14:45:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/29/2022 14:45:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 14:45:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/29/2022 14:45:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=763
05/29/2022 14:45:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/29/2022 14:45:33 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.2857757137942896 on epoch=766
05/29/2022 14:45:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/29/2022 14:45:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/29/2022 14:45:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 14:45:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/29/2022 14:45:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/29/2022 14:45:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3213675213675214 on epoch=783
05/29/2022 14:45:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
05/29/2022 14:45:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 14:45:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 14:45:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=796
05/29/2022 14:46:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
05/29/2022 14:46:02 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.26249999999999996 on epoch=799
05/29/2022 14:46:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=803
05/29/2022 14:46:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
05/29/2022 14:46:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/29/2022 14:46:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
05/29/2022 14:46:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
05/29/2022 14:46:17 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.3814664867296446 on epoch=816
05/29/2022 14:46:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 14:46:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
05/29/2022 14:46:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/29/2022 14:46:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/29/2022 14:46:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/29/2022 14:46:32 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.33516483516483514 on epoch=833
05/29/2022 14:46:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 14:46:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
05/29/2022 14:46:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/29/2022 14:46:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/29/2022 14:46:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/29/2022 14:46:46 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2971419261741843 on epoch=849
05/29/2022 14:46:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/29/2022 14:46:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 14:46:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 14:46:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
05/29/2022 14:46:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 14:47:01 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3698336017176597 on epoch=866
05/29/2022 14:47:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/29/2022 14:47:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/29/2022 14:47:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 14:47:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/29/2022 14:47:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 14:47:15 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.36586754241926656 on epoch=883
05/29/2022 14:47:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 14:47:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 14:47:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 14:47:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=896
05/29/2022 14:47:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
05/29/2022 14:47:30 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3245614035087719 on epoch=899
05/29/2022 14:47:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 14:47:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 14:47:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/29/2022 14:47:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 14:47:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/29/2022 14:47:45 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3636363636363636 on epoch=916
05/29/2022 14:47:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 14:47:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/29/2022 14:47:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/29/2022 14:47:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 14:47:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 14:47:59 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2987224157955865 on epoch=933
05/29/2022 14:48:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/29/2022 14:48:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/29/2022 14:48:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 14:48:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 14:48:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/29/2022 14:48:14 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.36735966735966735 on epoch=949
05/29/2022 14:48:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/29/2022 14:48:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 14:48:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
05/29/2022 14:48:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/29/2022 14:48:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 14:48:28 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.31394692264257484 on epoch=966
05/29/2022 14:48:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=969
05/29/2022 14:48:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 14:48:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
05/29/2022 14:48:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/29/2022 14:48:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 14:48:43 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.32926829268292684 on epoch=983
05/29/2022 14:48:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 14:48:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=989
05/29/2022 14:48:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/29/2022 14:48:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 14:48:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
05/29/2022 14:48:57 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.30912698412698414 on epoch=999
05/29/2022 14:48:58 - INFO - __main__ - save last model!
05/29/2022 14:48:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 14:48:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 14:48:58 - INFO - __main__ - Printing 3 examples
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 14:48:58 - INFO - __main__ - ['entailment']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:48:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:48:58 - INFO - __main__ - Printing 3 examples
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:48:58 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:48:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:48:58 - INFO - __main__ - Printing 3 examples
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:48:58 - INFO - __main__ - ['contradiction']
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:48:58 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:48:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:48:59 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 14:49:13 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:49:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:49:14 - INFO - __main__ - Starting training!
05/29/2022 14:49:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_13_0.3_8_predictions.txt
05/29/2022 14:49:28 - INFO - __main__ - Classification-F1 on test data: 0.2948
05/29/2022 14:49:28 - INFO - __main__ - prefix=anli_16_13, lr=0.3, bsz=8, dev_performance=0.47113997113997114, test_performance=0.2947621295632772
05/29/2022 14:49:28 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.2, bsz=8 ...
05/29/2022 14:49:29 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:49:29 - INFO - __main__ - Printing 3 examples
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:49:29 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:49:29 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 14:49:29 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 14:49:29 - INFO - __main__ - Printing 3 examples
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/29/2022 14:49:29 - INFO - __main__ - ['contradiction']
05/29/2022 14:49:29 - INFO - __main__ - Tokenizing Input ...
05/29/2022 14:49:29 - INFO - __main__ - Tokenizing Output ...
05/29/2022 14:49:29 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 14:49:44 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 14:49:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 14:49:45 - INFO - __main__ - Starting training!
05/29/2022 14:49:49 - INFO - __main__ - Step 10 Global step 10 Train loss 0.71 on epoch=3
05/29/2022 14:49:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=6
05/29/2022 14:49:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
05/29/2022 14:49:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=13
05/29/2022 14:49:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
05/29/2022 14:50:01 - INFO - __main__ - Global step 50 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 14:50:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 14:50:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
05/29/2022 14:50:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/29/2022 14:50:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
05/29/2022 14:50:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
05/29/2022 14:50:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/29/2022 14:50:15 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 14:50:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
05/29/2022 14:50:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
05/29/2022 14:50:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
05/29/2022 14:50:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
05/29/2022 14:50:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
05/29/2022 14:50:30 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 14:50:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=53
05/29/2022 14:50:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
05/29/2022 14:50:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/29/2022 14:50:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/29/2022 14:50:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
05/29/2022 14:50:44 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 14:50:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
05/29/2022 14:50:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=73
05/29/2022 14:50:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
05/29/2022 14:50:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=79
05/29/2022 14:50:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=83
05/29/2022 14:50:59 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.4833081335349317 on epoch=83
05/29/2022 14:50:59 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.4833081335349317 on epoch=83, global_step=250
05/29/2022 14:51:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=86
05/29/2022 14:51:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
05/29/2022 14:51:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
05/29/2022 14:51:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
05/29/2022 14:51:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
05/29/2022 14:51:13 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 14:51:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
05/29/2022 14:51:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
05/29/2022 14:51:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
05/29/2022 14:51:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
05/29/2022 14:51:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=116
05/29/2022 14:51:27 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 14:51:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
05/29/2022 14:51:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
05/29/2022 14:51:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
05/29/2022 14:51:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=129
05/29/2022 14:51:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=133
05/29/2022 14:51:41 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.2085278555866791 on epoch=133
05/29/2022 14:51:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
05/29/2022 14:51:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=139
05/29/2022 14:51:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=143
05/29/2022 14:51:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=146
05/29/2022 14:51:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=149
05/29/2022 14:51:56 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.3646723646723647 on epoch=149
05/29/2022 14:51:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
05/29/2022 14:52:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
05/29/2022 14:52:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/29/2022 14:52:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=163
05/29/2022 14:52:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=166
05/29/2022 14:52:10 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3367003367003367 on epoch=166
05/29/2022 14:52:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
05/29/2022 14:52:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
05/29/2022 14:52:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=176
05/29/2022 14:52:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=179
05/29/2022 14:52:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=183
05/29/2022 14:52:24 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.27409102466310703 on epoch=183
05/29/2022 14:52:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=186
05/29/2022 14:52:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=189
05/29/2022 14:52:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=193
05/29/2022 14:52:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=196
05/29/2022 14:52:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=199
05/29/2022 14:52:39 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.24444444444444446 on epoch=199
05/29/2022 14:52:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=203
05/29/2022 14:52:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=206
05/29/2022 14:52:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=209
05/29/2022 14:52:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=213
05/29/2022 14:52:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=216
05/29/2022 14:52:53 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.23301985370950887 on epoch=216
05/29/2022 14:52:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=219
05/29/2022 14:52:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
05/29/2022 14:53:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=226
05/29/2022 14:53:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=229
05/29/2022 14:53:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=233
05/29/2022 14:53:08 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.29785661492978566 on epoch=233
05/29/2022 14:53:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=236
05/29/2022 14:53:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=239
05/29/2022 14:53:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=243
05/29/2022 14:53:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=246
05/29/2022 14:53:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=249
05/29/2022 14:53:22 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.26560232220609575 on epoch=249
05/29/2022 14:53:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=253
05/29/2022 14:53:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=256
05/29/2022 14:53:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=259
05/29/2022 14:53:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=263
05/29/2022 14:53:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
05/29/2022 14:53:37 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.26870007262164125 on epoch=266
05/29/2022 14:53:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=269
05/29/2022 14:53:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
05/29/2022 14:53:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=276
05/29/2022 14:53:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=279
05/29/2022 14:53:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=283
05/29/2022 14:53:51 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.25268817204301075 on epoch=283
05/29/2022 14:53:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=286
05/29/2022 14:53:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=289
05/29/2022 14:53:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=293
05/29/2022 14:54:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=296
05/29/2022 14:54:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=299
05/29/2022 14:54:05 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.27079218784287906 on epoch=299
05/29/2022 14:54:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
05/29/2022 14:54:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=306
05/29/2022 14:54:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=309
05/29/2022 14:54:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=313
05/29/2022 14:54:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=316
05/29/2022 14:54:20 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.1871604938271605 on epoch=316
05/29/2022 14:54:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=319
05/29/2022 14:54:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=323
05/29/2022 14:54:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=326
05/29/2022 14:54:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=329
05/29/2022 14:54:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=333
05/29/2022 14:54:34 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.243859649122807 on epoch=333
05/29/2022 14:54:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=336
05/29/2022 14:54:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=339
05/29/2022 14:54:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=343
05/29/2022 14:54:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=346
05/29/2022 14:54:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=349
05/29/2022 14:54:49 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.27380952380952384 on epoch=349
05/29/2022 14:54:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=353
05/29/2022 14:54:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=356
05/29/2022 14:54:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=359
05/29/2022 14:54:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=363
05/29/2022 14:55:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=366
05/29/2022 14:55:03 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3568253968253969 on epoch=366
05/29/2022 14:55:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=369
05/29/2022 14:55:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=373
05/29/2022 14:55:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=376
05/29/2022 14:55:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=379
05/29/2022 14:55:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=383
05/29/2022 14:55:18 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.30493224932249324 on epoch=383
05/29/2022 14:55:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=386
05/29/2022 14:55:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=389
05/29/2022 14:55:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=393
05/29/2022 14:55:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=396
05/29/2022 14:55:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=399
05/29/2022 14:55:32 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.27794871794871795 on epoch=399
05/29/2022 14:55:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=403
05/29/2022 14:55:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=406
05/29/2022 14:55:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=409
05/29/2022 14:55:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=413
05/29/2022 14:55:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=416
05/29/2022 14:55:47 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.2603030303030303 on epoch=416
05/29/2022 14:55:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=419
05/29/2022 14:55:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=423
05/29/2022 14:55:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=426
05/29/2022 14:55:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=429
05/29/2022 14:56:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=433
05/29/2022 14:56:01 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.2844312692138779 on epoch=433
05/29/2022 14:56:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=436
05/29/2022 14:56:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=439
05/29/2022 14:56:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=443
05/29/2022 14:56:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=446
05/29/2022 14:56:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=449
05/29/2022 14:56:16 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.3074371904956612 on epoch=449
05/29/2022 14:56:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=453
05/29/2022 14:56:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=456
05/29/2022 14:56:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
05/29/2022 14:56:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=463
05/29/2022 14:56:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
05/29/2022 14:56:31 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.3010287203835591 on epoch=466
05/29/2022 14:56:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=469
05/29/2022 14:56:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=473
05/29/2022 14:56:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=476
05/29/2022 14:56:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=479
05/29/2022 14:56:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=483
05/29/2022 14:56:45 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.3192912545363979 on epoch=483
05/29/2022 14:56:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=486
05/29/2022 14:56:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
05/29/2022 14:56:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=493
05/29/2022 14:56:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=496
05/29/2022 14:56:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=499
05/29/2022 14:56:59 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.215878679750223 on epoch=499
05/29/2022 14:57:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=503
05/29/2022 14:57:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=506
05/29/2022 14:57:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=509
05/29/2022 14:57:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=513
05/29/2022 14:57:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
05/29/2022 14:57:14 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.34268167431721475 on epoch=516
05/29/2022 14:57:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=519
05/29/2022 14:57:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=523
05/29/2022 14:57:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=526
05/29/2022 14:57:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
05/29/2022 14:57:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=533
05/29/2022 14:57:29 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.30522486772486773 on epoch=533
05/29/2022 14:57:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=536
05/29/2022 14:57:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=539
05/29/2022 14:57:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=543
05/29/2022 14:57:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=546
05/29/2022 14:57:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=549
05/29/2022 14:57:43 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.3324154589371981 on epoch=549
05/29/2022 14:57:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=553
05/29/2022 14:57:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=556
05/29/2022 14:57:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=559
05/29/2022 14:57:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=563
05/29/2022 14:57:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
05/29/2022 14:57:57 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.23500000000000001 on epoch=566
05/29/2022 14:58:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=569
05/29/2022 14:58:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=573
05/29/2022 14:58:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=576
05/29/2022 14:58:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=579
05/29/2022 14:58:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=583
05/29/2022 14:58:12 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.20415982484948003 on epoch=583
05/29/2022 14:58:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
05/29/2022 14:58:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
05/29/2022 14:58:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
05/29/2022 14:58:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
05/29/2022 14:58:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=599
05/29/2022 14:58:27 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.2910731244064577 on epoch=599
05/29/2022 14:58:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=603
05/29/2022 14:58:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=606
05/29/2022 14:58:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=609
05/29/2022 14:58:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/29/2022 14:58:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=616
05/29/2022 14:58:41 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.3468947344509563 on epoch=616
05/29/2022 14:58:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=619
05/29/2022 14:58:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=623
05/29/2022 14:58:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
05/29/2022 14:58:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=629
05/29/2022 14:58:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=633
05/29/2022 14:58:56 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.2508925882909622 on epoch=633
05/29/2022 14:58:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
05/29/2022 14:59:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=639
05/29/2022 14:59:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
05/29/2022 14:59:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=646
05/29/2022 14:59:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/29/2022 14:59:10 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.2798006644518272 on epoch=649
05/29/2022 14:59:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=653
05/29/2022 14:59:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=656
05/29/2022 14:59:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
05/29/2022 14:59:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
05/29/2022 14:59:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
05/29/2022 14:59:25 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.2803948850460478 on epoch=666
05/29/2022 14:59:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
05/29/2022 14:59:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
05/29/2022 14:59:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
05/29/2022 14:59:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=679
05/29/2022 14:59:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=683
05/29/2022 14:59:40 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.2582162582162582 on epoch=683
05/29/2022 14:59:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=686
05/29/2022 14:59:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=689
05/29/2022 14:59:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
05/29/2022 14:59:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
05/29/2022 14:59:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
05/29/2022 14:59:54 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.1994514106583072 on epoch=699
05/29/2022 14:59:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
05/29/2022 14:59:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
05/29/2022 15:00:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/29/2022 15:00:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=713
05/29/2022 15:00:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
05/29/2022 15:00:09 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.17922077922077922 on epoch=716
05/29/2022 15:00:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=719
05/29/2022 15:00:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
05/29/2022 15:00:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
05/29/2022 15:00:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=729
05/29/2022 15:00:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 15:00:23 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.17012987012987013 on epoch=733
05/29/2022 15:00:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
05/29/2022 15:00:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
05/29/2022 15:00:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=743
05/29/2022 15:00:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=746
05/29/2022 15:00:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=749
05/29/2022 15:00:38 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.1775974025974026 on epoch=749
05/29/2022 15:00:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/29/2022 15:00:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 15:00:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
05/29/2022 15:00:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
05/29/2022 15:00:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
05/29/2022 15:00:53 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.20610276679841893 on epoch=766
05/29/2022 15:00:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/29/2022 15:00:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/29/2022 15:01:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/29/2022 15:01:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/29/2022 15:01:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=783
05/29/2022 15:01:07 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.20415982484948003 on epoch=783
05/29/2022 15:01:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
05/29/2022 15:01:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
05/29/2022 15:01:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/29/2022 15:01:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/29/2022 15:01:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
05/29/2022 15:01:22 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.11 on epoch=799
05/29/2022 15:01:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=803
05/29/2022 15:01:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=806
05/29/2022 15:01:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
05/29/2022 15:01:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
05/29/2022 15:01:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=816
05/29/2022 15:01:36 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.08319649698960044 on epoch=816
05/29/2022 15:01:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 15:01:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/29/2022 15:01:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
05/29/2022 15:01:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
05/29/2022 15:01:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=833
05/29/2022 15:01:51 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.16874541452677916 on epoch=833
05/29/2022 15:01:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
05/29/2022 15:01:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/29/2022 15:01:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=843
05/29/2022 15:02:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
05/29/2022 15:02:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/29/2022 15:02:06 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.14029669517474397 on epoch=849
05/29/2022 15:02:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/29/2022 15:02:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 15:02:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
05/29/2022 15:02:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=863
05/29/2022 15:02:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=866
05/29/2022 15:02:20 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.15677655677655677 on epoch=866
05/29/2022 15:02:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
05/29/2022 15:02:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/29/2022 15:02:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 15:02:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/29/2022 15:02:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 15:02:35 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.15467650789989623 on epoch=883
05/29/2022 15:02:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 15:02:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 15:02:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 15:02:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 15:02:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/29/2022 15:02:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.17308755760368663 on epoch=899
05/29/2022 15:02:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 15:02:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/29/2022 15:02:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/29/2022 15:03:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/29/2022 15:03:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
05/29/2022 15:03:04 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.1580578096707129 on epoch=916
05/29/2022 15:03:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=919
05/29/2022 15:03:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 15:03:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 15:03:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/29/2022 15:03:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/29/2022 15:03:19 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.1498076923076923 on epoch=933
05/29/2022 15:03:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/29/2022 15:03:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
05/29/2022 15:03:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/29/2022 15:03:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
05/29/2022 15:03:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/29/2022 15:03:33 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.1258036832257066 on epoch=949
05/29/2022 15:03:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=953
05/29/2022 15:03:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/29/2022 15:03:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
05/29/2022 15:03:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/29/2022 15:03:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 15:03:48 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.1662990533958276 on epoch=966
05/29/2022 15:03:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/29/2022 15:03:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
05/29/2022 15:03:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 15:03:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/29/2022 15:04:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/29/2022 15:04:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.10953859804791484 on epoch=983
05/29/2022 15:04:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=986
05/29/2022 15:04:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=989
05/29/2022 15:04:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 15:04:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
05/29/2022 15:04:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 15:04:17 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.13289151220185702 on epoch=999
05/29/2022 15:04:17 - INFO - __main__ - save last model!
05/29/2022 15:04:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:04:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:04:17 - INFO - __main__ - Printing 3 examples
05/29/2022 15:04:17 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:04:17 - INFO - __main__ - ['contradiction']
05/29/2022 15:04:17 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:04:17 - INFO - __main__ - ['entailment']
05/29/2022 15:04:17 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:04:17 - INFO - __main__ - ['contradiction']
05/29/2022 15:04:17 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:04:18 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:04:18 - INFO - __main__ - Printing 3 examples
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:04:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:04:18 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:04:18 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:04:18 - INFO - __main__ - Printing 3 examples
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:04:18 - INFO - __main__ - ['entailment']
05/29/2022 15:04:18 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:04:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:04:18 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:04:18 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:04:19 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:04:33 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:04:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:04:33 - INFO - __main__ - Starting training!
05/29/2022 15:04:49 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_13_0.2_8_predictions.txt
05/29/2022 15:04:49 - INFO - __main__ - Classification-F1 on test data: 0.0751
05/29/2022 15:04:49 - INFO - __main__ - prefix=anli_16_13, lr=0.2, bsz=8, dev_performance=0.4833081335349317, test_performance=0.0750691750762831
05/29/2022 15:04:49 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.5, bsz=8 ...
05/29/2022 15:04:50 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:04:50 - INFO - __main__ - Printing 3 examples
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:04:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:04:50 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:04:50 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:04:50 - INFO - __main__ - Printing 3 examples
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:04:50 - INFO - __main__ - ['entailment']
05/29/2022 15:04:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:04:50 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:04:50 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:05:05 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:05:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:05:06 - INFO - __main__ - Starting training!
05/29/2022 15:05:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.69 on epoch=3
05/29/2022 15:05:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=6
05/29/2022 15:05:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=9
05/29/2022 15:05:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
05/29/2022 15:05:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
05/29/2022 15:05:22 - INFO - __main__ - Global step 50 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 15:05:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 15:05:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
05/29/2022 15:05:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
05/29/2022 15:05:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
05/29/2022 15:05:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
05/29/2022 15:05:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=33
05/29/2022 15:05:36 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 15:05:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=36
05/29/2022 15:05:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=39
05/29/2022 15:05:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
05/29/2022 15:05:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
05/29/2022 15:05:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
05/29/2022 15:05:51 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.18993812214151196 on epoch=49
05/29/2022 15:05:51 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18993812214151196 on epoch=49, global_step=150
05/29/2022 15:05:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
05/29/2022 15:05:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
05/29/2022 15:05:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=59
05/29/2022 15:06:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/29/2022 15:06:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
05/29/2022 15:06:06 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.20908004778972522 on epoch=66
05/29/2022 15:06:06 - INFO - __main__ - Saving model with best Classification-F1: 0.18993812214151196 -> 0.20908004778972522 on epoch=66, global_step=200
05/29/2022 15:06:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=69
05/29/2022 15:06:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
05/29/2022 15:06:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=76
05/29/2022 15:06:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=79
05/29/2022 15:06:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
05/29/2022 15:06:20 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.32002376708259056 on epoch=83
05/29/2022 15:06:20 - INFO - __main__ - Saving model with best Classification-F1: 0.20908004778972522 -> 0.32002376708259056 on epoch=83, global_step=250
05/29/2022 15:06:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
05/29/2022 15:06:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/29/2022 15:06:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
05/29/2022 15:06:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
05/29/2022 15:06:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=99
05/29/2022 15:06:35 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.26837606837606837 on epoch=99
05/29/2022 15:06:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
05/29/2022 15:06:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
05/29/2022 15:06:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=109
05/29/2022 15:06:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=113
05/29/2022 15:06:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
05/29/2022 15:06:49 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.2791245791245791 on epoch=116
05/29/2022 15:06:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=119
05/29/2022 15:06:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=123
05/29/2022 15:06:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/29/2022 15:07:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
05/29/2022 15:07:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
05/29/2022 15:07:04 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.24453138456238777 on epoch=133
05/29/2022 15:07:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
05/29/2022 15:07:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=139
05/29/2022 15:07:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=143
05/29/2022 15:07:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=146
05/29/2022 15:07:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=149
05/29/2022 15:07:18 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.3050108932461873 on epoch=149
05/29/2022 15:07:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=153
05/29/2022 15:07:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=156
05/29/2022 15:07:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=159
05/29/2022 15:07:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=163
05/29/2022 15:07:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=166
05/29/2022 15:07:33 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.2082706766917293 on epoch=166
05/29/2022 15:07:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=169
05/29/2022 15:07:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=173
05/29/2022 15:07:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
05/29/2022 15:07:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
05/29/2022 15:07:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=183
05/29/2022 15:07:48 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.2178279974890144 on epoch=183
05/29/2022 15:07:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
05/29/2022 15:07:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=189
05/29/2022 15:07:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=193
05/29/2022 15:07:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=196
05/29/2022 15:08:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=199
05/29/2022 15:08:02 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.33903133903133903 on epoch=199
05/29/2022 15:08:02 - INFO - __main__ - Saving model with best Classification-F1: 0.32002376708259056 -> 0.33903133903133903 on epoch=199, global_step=600
05/29/2022 15:08:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
05/29/2022 15:08:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
05/29/2022 15:08:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
05/29/2022 15:08:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
05/29/2022 15:08:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=216
05/29/2022 15:08:17 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.23714285714285716 on epoch=216
05/29/2022 15:08:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
05/29/2022 15:08:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
05/29/2022 15:08:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
05/29/2022 15:08:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=229
05/29/2022 15:08:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=233
05/29/2022 15:08:31 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.28442028985507245 on epoch=233
05/29/2022 15:08:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=236
05/29/2022 15:08:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
05/29/2022 15:08:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=243
05/29/2022 15:08:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.11 on epoch=246
05/29/2022 15:08:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=249
05/29/2022 15:08:46 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.2599437061646364 on epoch=249
05/29/2022 15:08:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=253
05/29/2022 15:08:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=256
05/29/2022 15:08:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=259
05/29/2022 15:08:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
05/29/2022 15:08:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=266
05/29/2022 15:09:00 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.19729286300246102 on epoch=266
05/29/2022 15:09:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
05/29/2022 15:09:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
05/29/2022 15:09:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
05/29/2022 15:09:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
05/29/2022 15:09:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
05/29/2022 15:09:15 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.20675675675675675 on epoch=283
05/29/2022 15:09:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=286
05/29/2022 15:09:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
05/29/2022 15:09:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
05/29/2022 15:09:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
05/29/2022 15:09:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
05/29/2022 15:09:29 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.17533936651583706 on epoch=299
05/29/2022 15:09:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=303
05/29/2022 15:09:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=306
05/29/2022 15:09:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=309
05/29/2022 15:09:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=313
05/29/2022 15:09:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
05/29/2022 15:09:44 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.12368839427662956 on epoch=316
05/29/2022 15:09:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
05/29/2022 15:09:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
05/29/2022 15:09:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
05/29/2022 15:09:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
05/29/2022 15:09:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
05/29/2022 15:09:58 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.14523809523809522 on epoch=333
05/29/2022 15:10:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
05/29/2022 15:10:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
05/29/2022 15:10:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
05/29/2022 15:10:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/29/2022 15:10:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
05/29/2022 15:10:13 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.12123256860098965 on epoch=349
05/29/2022 15:10:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
05/29/2022 15:10:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
05/29/2022 15:10:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
05/29/2022 15:10:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
05/29/2022 15:10:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
05/29/2022 15:10:27 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.10026164311878596 on epoch=366
05/29/2022 15:10:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
05/29/2022 15:10:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
05/29/2022 15:10:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
05/29/2022 15:10:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
05/29/2022 15:10:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
05/29/2022 15:10:42 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.11826769721506562 on epoch=383
05/29/2022 15:10:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/29/2022 15:10:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
05/29/2022 15:10:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=393
05/29/2022 15:10:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
05/29/2022 15:10:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=399
05/29/2022 15:10:57 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.08203169213355971 on epoch=399
05/29/2022 15:11:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/29/2022 15:11:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
05/29/2022 15:11:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
05/29/2022 15:11:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/29/2022 15:11:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
05/29/2022 15:11:12 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.1125 on epoch=416
05/29/2022 15:11:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
05/29/2022 15:11:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
05/29/2022 15:11:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
05/29/2022 15:11:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/29/2022 15:11:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
05/29/2022 15:11:26 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.10443984357027834 on epoch=433
05/29/2022 15:11:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
05/29/2022 15:11:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
05/29/2022 15:11:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
05/29/2022 15:11:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
05/29/2022 15:11:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
05/29/2022 15:11:41 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.07009760425909495 on epoch=449
05/29/2022 15:11:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 15:11:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 15:11:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
05/29/2022 15:11:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/29/2022 15:11:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
05/29/2022 15:11:56 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.07363636363636364 on epoch=466
05/29/2022 15:11:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=469
05/29/2022 15:12:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 15:12:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
05/29/2022 15:12:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
05/29/2022 15:12:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
05/29/2022 15:12:11 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.13495934959349593 on epoch=483
05/29/2022 15:12:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
05/29/2022 15:12:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
05/29/2022 15:12:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/29/2022 15:12:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 15:12:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/29/2022 15:12:25 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.12431679348408531 on epoch=499
05/29/2022 15:12:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
05/29/2022 15:12:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
05/29/2022 15:12:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 15:12:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 15:12:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 15:12:40 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.14814814814814814 on epoch=516
05/29/2022 15:12:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
05/29/2022 15:12:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 15:12:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 15:12:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 15:12:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
05/29/2022 15:12:55 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.08189401151457454 on epoch=533
05/29/2022 15:12:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 15:13:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/29/2022 15:13:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
05/29/2022 15:13:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
05/29/2022 15:13:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
05/29/2022 15:13:09 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.11715403444357025 on epoch=549
05/29/2022 15:13:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/29/2022 15:13:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 15:13:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 15:13:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
05/29/2022 15:13:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/29/2022 15:13:24 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.09542902967121092 on epoch=566
05/29/2022 15:13:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/29/2022 15:13:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 15:13:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/29/2022 15:13:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
05/29/2022 15:13:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 15:13:39 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.10501021266370299 on epoch=583
05/29/2022 15:13:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 15:13:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
05/29/2022 15:13:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
05/29/2022 15:13:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
05/29/2022 15:13:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/29/2022 15:13:53 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.08393269252313414 on epoch=599
05/29/2022 15:13:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 15:13:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 15:14:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 15:14:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 15:14:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
05/29/2022 15:14:08 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.08517608517608517 on epoch=616
05/29/2022 15:14:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 15:14:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/29/2022 15:14:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 15:14:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 15:14:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 15:14:22 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.09660107334525939 on epoch=633
05/29/2022 15:14:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 15:14:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 15:14:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 15:14:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 15:14:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 15:14:37 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.1086626139817629 on epoch=649
05/29/2022 15:14:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/29/2022 15:14:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 15:14:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 15:14:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 15:14:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 15:14:52 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.08974358974358976 on epoch=666
05/29/2022 15:14:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 15:14:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 15:15:00 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 15:15:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 15:15:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
05/29/2022 15:15:06 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.12173458725182862 on epoch=683
05/29/2022 15:15:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
05/29/2022 15:15:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/29/2022 15:15:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 15:15:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 15:15:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 15:15:21 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.12703962703962704 on epoch=699
05/29/2022 15:15:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 15:15:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 15:15:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 15:15:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/29/2022 15:15:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 15:15:36 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.11359250068927489 on epoch=716
05/29/2022 15:15:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 15:15:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 15:15:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 15:15:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 15:15:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 15:15:51 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.1728494623655914 on epoch=733
05/29/2022 15:15:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 15:15:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 15:15:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 15:16:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 15:16:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 15:16:05 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.18768253968253967 on epoch=749
05/29/2022 15:16:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/29/2022 15:16:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 15:16:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/29/2022 15:16:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/29/2022 15:16:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 15:16:20 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.17953296703296703 on epoch=766
05/29/2022 15:16:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 15:16:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 15:16:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 15:16:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
05/29/2022 15:16:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 15:16:35 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.15644444444444444 on epoch=783
05/29/2022 15:16:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 15:16:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 15:16:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 15:16:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 15:16:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 15:16:50 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.22572227399813605 on epoch=799
05/29/2022 15:16:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 15:16:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 15:16:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 15:17:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 15:17:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 15:17:05 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.21176046176046182 on epoch=816
05/29/2022 15:17:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 15:17:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 15:17:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 15:17:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 15:17:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 15:17:20 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.16014309764309767 on epoch=833
05/29/2022 15:17:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 15:17:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 15:17:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 15:17:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 15:17:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/29/2022 15:17:35 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.137854609929078 on epoch=849
05/29/2022 15:17:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 15:17:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 15:17:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 15:17:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/29/2022 15:17:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 15:17:50 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.14407407407407408 on epoch=866
05/29/2022 15:17:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 15:17:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 15:17:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 15:18:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 15:18:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 15:18:05 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.1454212454212454 on epoch=883
05/29/2022 15:18:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 15:18:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 15:18:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 15:18:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=896
05/29/2022 15:18:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 15:18:19 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.13960113960113962 on epoch=899
05/29/2022 15:18:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 15:18:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 15:18:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 15:18:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 15:18:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 15:18:34 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.16014309764309767 on epoch=916
05/29/2022 15:18:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 15:18:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 15:18:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 15:18:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 15:18:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 15:18:49 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.1684599619382228 on epoch=933
05/29/2022 15:18:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=936
05/29/2022 15:18:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 15:18:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 15:19:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
05/29/2022 15:19:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 15:19:04 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.2479444962203583 on epoch=949
05/29/2022 15:19:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
05/29/2022 15:19:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 15:19:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 15:19:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=963
05/29/2022 15:19:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 15:19:19 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.138996138996139 on epoch=966
05/29/2022 15:19:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 15:19:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 15:19:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 15:19:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 15:19:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 15:19:34 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2127379053694843 on epoch=983
05/29/2022 15:19:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
05/29/2022 15:19:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 15:19:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 15:19:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 15:19:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 15:19:49 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.24753694581280786 on epoch=999
05/29/2022 15:19:49 - INFO - __main__ - save last model!
05/29/2022 15:19:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:19:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:19:49 - INFO - __main__ - Printing 3 examples
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:19:49 - INFO - __main__ - ['contradiction']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:19:49 - INFO - __main__ - ['contradiction']
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:19:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:19:49 - INFO - __main__ - Printing 3 examples
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:19:49 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:19:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:19:49 - INFO - __main__ - Printing 3 examples
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:19:49 - INFO - __main__ - ['entailment']
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:19:49 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:19:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:19:50 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:20:04 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:20:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:20:05 - INFO - __main__ - Starting training!
05/29/2022 15:20:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_21_0.5_8_predictions.txt
05/29/2022 15:20:20 - INFO - __main__ - Classification-F1 on test data: 0.2256
05/29/2022 15:20:20 - INFO - __main__ - prefix=anli_16_21, lr=0.5, bsz=8, dev_performance=0.33903133903133903, test_performance=0.2255720521667514
05/29/2022 15:20:20 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.4, bsz=8 ...
05/29/2022 15:20:21 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:20:21 - INFO - __main__ - Printing 3 examples
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:20:21 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:20:21 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:20:21 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:20:21 - INFO - __main__ - Printing 3 examples
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:20:21 - INFO - __main__ - ['entailment']
05/29/2022 15:20:21 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:20:21 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:20:21 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:20:36 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:20:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:20:37 - INFO - __main__ - Starting training!
05/29/2022 15:20:41 - INFO - __main__ - Step 10 Global step 10 Train loss 0.65 on epoch=3
05/29/2022 15:20:43 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
05/29/2022 15:20:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=9
05/29/2022 15:20:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=13
05/29/2022 15:20:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=16
05/29/2022 15:20:53 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 15:20:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 15:20:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/29/2022 15:20:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=23
05/29/2022 15:21:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/29/2022 15:21:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
05/29/2022 15:21:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
05/29/2022 15:21:08 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.28571428571428575 on epoch=33
05/29/2022 15:21:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.28571428571428575 on epoch=33, global_step=100
05/29/2022 15:21:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
05/29/2022 15:21:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
05/29/2022 15:21:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=43
05/29/2022 15:21:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
05/29/2022 15:21:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=49
05/29/2022 15:21:23 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 15:21:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=53
05/29/2022 15:21:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
05/29/2022 15:21:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
05/29/2022 15:21:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
05/29/2022 15:21:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
05/29/2022 15:21:37 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/29/2022 15:21:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 15:21:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
05/29/2022 15:21:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
05/29/2022 15:21:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/29/2022 15:21:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
05/29/2022 15:21:52 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.2696969696969697 on epoch=83
05/29/2022 15:21:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
05/29/2022 15:21:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
05/29/2022 15:22:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
05/29/2022 15:22:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
05/29/2022 15:22:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
05/29/2022 15:22:07 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.2128654970760234 on epoch=99
05/29/2022 15:22:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
05/29/2022 15:22:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
05/29/2022 15:22:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=109
05/29/2022 15:22:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=113
05/29/2022 15:22:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
05/29/2022 15:22:22 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.31319966583124476 on epoch=116
05/29/2022 15:22:22 - INFO - __main__ - Saving model with best Classification-F1: 0.28571428571428575 -> 0.31319966583124476 on epoch=116, global_step=350
05/29/2022 15:22:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
05/29/2022 15:22:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
05/29/2022 15:22:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=126
05/29/2022 15:22:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=129
05/29/2022 15:22:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=133
05/29/2022 15:22:37 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.2425892316999395 on epoch=133
05/29/2022 15:22:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=136
05/29/2022 15:22:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=139
05/29/2022 15:22:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
05/29/2022 15:22:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=146
05/29/2022 15:22:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
05/29/2022 15:22:51 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.16949152542372883 on epoch=149
05/29/2022 15:22:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=153
05/29/2022 15:22:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
05/29/2022 15:22:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
05/29/2022 15:23:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/29/2022 15:23:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=166
05/29/2022 15:23:06 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.2178279974890144 on epoch=166
05/29/2022 15:23:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
05/29/2022 15:23:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=173
05/29/2022 15:23:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
05/29/2022 15:23:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=179
05/29/2022 15:23:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=183
05/29/2022 15:23:21 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.3274083827079219 on epoch=183
05/29/2022 15:23:21 - INFO - __main__ - Saving model with best Classification-F1: 0.31319966583124476 -> 0.3274083827079219 on epoch=183, global_step=550
05/29/2022 15:23:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=186
05/29/2022 15:23:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=189
05/29/2022 15:23:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
05/29/2022 15:23:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=196
05/29/2022 15:23:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
05/29/2022 15:23:36 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.29660130718954253 on epoch=199
05/29/2022 15:23:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=203
05/29/2022 15:23:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=206
05/29/2022 15:23:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
05/29/2022 15:23:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=213
05/29/2022 15:23:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=216
05/29/2022 15:23:50 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.3408248816768087 on epoch=216
05/29/2022 15:23:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3274083827079219 -> 0.3408248816768087 on epoch=216, global_step=650
05/29/2022 15:23:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=219
05/29/2022 15:23:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=223
05/29/2022 15:23:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=226
05/29/2022 15:24:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=229
05/29/2022 15:24:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=233
05/29/2022 15:24:05 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.3618233618233619 on epoch=233
05/29/2022 15:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3408248816768087 -> 0.3618233618233619 on epoch=233, global_step=700
05/29/2022 15:24:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=236
05/29/2022 15:24:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=239
05/29/2022 15:24:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
05/29/2022 15:24:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=246
05/29/2022 15:24:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
05/29/2022 15:24:20 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.34543209876543207 on epoch=249
05/29/2022 15:24:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
05/29/2022 15:24:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=256
05/29/2022 15:24:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=259
05/29/2022 15:24:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=263
05/29/2022 15:24:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=266
05/29/2022 15:24:34 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.20638297872340425 on epoch=266
05/29/2022 15:24:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/29/2022 15:24:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
05/29/2022 15:24:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
05/29/2022 15:24:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=279
05/29/2022 15:24:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
05/29/2022 15:24:49 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.34047619047619043 on epoch=283
05/29/2022 15:24:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
05/29/2022 15:24:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
05/29/2022 15:24:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=293
05/29/2022 15:25:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/29/2022 15:25:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/29/2022 15:25:04 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.38164924506387926 on epoch=299
05/29/2022 15:25:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3618233618233619 -> 0.38164924506387926 on epoch=299, global_step=900
05/29/2022 15:25:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
05/29/2022 15:25:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=306
05/29/2022 15:25:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=309
05/29/2022 15:25:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/29/2022 15:25:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
05/29/2022 15:25:19 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.20912698412698413 on epoch=316
05/29/2022 15:25:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=319
05/29/2022 15:25:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
05/29/2022 15:25:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
05/29/2022 15:25:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
05/29/2022 15:25:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
05/29/2022 15:25:33 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.22638750965463972 on epoch=333
05/29/2022 15:25:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
05/29/2022 15:25:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
05/29/2022 15:25:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=343
05/29/2022 15:25:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=346
05/29/2022 15:25:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=349
05/29/2022 15:25:48 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.13952465834818775 on epoch=349
05/29/2022 15:25:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
05/29/2022 15:25:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
05/29/2022 15:25:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.09 on epoch=359
05/29/2022 15:25:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
05/29/2022 15:26:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
05/29/2022 15:26:03 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.3148148148148148 on epoch=366
05/29/2022 15:26:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
05/29/2022 15:26:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
05/29/2022 15:26:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
05/29/2022 15:26:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
05/29/2022 15:26:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
05/29/2022 15:26:18 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.2227106227106227 on epoch=383
05/29/2022 15:26:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=386
05/29/2022 15:26:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
05/29/2022 15:26:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=393
05/29/2022 15:26:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=396
05/29/2022 15:26:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
05/29/2022 15:26:33 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.2795598324868573 on epoch=399
05/29/2022 15:26:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
05/29/2022 15:26:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
05/29/2022 15:26:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
05/29/2022 15:26:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/29/2022 15:26:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
05/29/2022 15:26:47 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.22101983382471185 on epoch=416
05/29/2022 15:26:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
05/29/2022 15:26:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
05/29/2022 15:26:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
05/29/2022 15:26:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/29/2022 15:27:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
05/29/2022 15:27:02 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.33174603174603173 on epoch=433
05/29/2022 15:27:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/29/2022 15:27:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
05/29/2022 15:27:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/29/2022 15:27:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/29/2022 15:27:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
05/29/2022 15:27:17 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.16245791245791244 on epoch=449
05/29/2022 15:27:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
05/29/2022 15:27:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=456
05/29/2022 15:27:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
05/29/2022 15:27:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=463
05/29/2022 15:27:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 15:27:31 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.33174478826652737 on epoch=466
05/29/2022 15:27:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/29/2022 15:27:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 15:27:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/29/2022 15:27:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
05/29/2022 15:27:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
05/29/2022 15:27:46 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.09426356589147285 on epoch=483
05/29/2022 15:27:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
05/29/2022 15:27:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/29/2022 15:27:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=493
05/29/2022 15:27:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 15:28:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/29/2022 15:28:01 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.21025641025641023 on epoch=499
05/29/2022 15:28:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
05/29/2022 15:28:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
05/29/2022 15:28:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
05/29/2022 15:28:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
05/29/2022 15:28:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 15:28:16 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.18686530860443903 on epoch=516
05/29/2022 15:28:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
05/29/2022 15:28:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
05/29/2022 15:28:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
05/29/2022 15:28:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
05/29/2022 15:28:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
05/29/2022 15:28:31 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.26021331609566906 on epoch=533
05/29/2022 15:28:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
05/29/2022 15:28:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/29/2022 15:28:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
05/29/2022 15:28:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/29/2022 15:28:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/29/2022 15:28:45 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.1524193548387097 on epoch=549
05/29/2022 15:28:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
05/29/2022 15:28:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 15:28:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/29/2022 15:28:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
05/29/2022 15:28:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/29/2022 15:29:00 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.17926490434230372 on epoch=566
05/29/2022 15:29:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/29/2022 15:29:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 15:29:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 15:29:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
05/29/2022 15:29:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 15:29:15 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.16062191223481545 on epoch=583
05/29/2022 15:29:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 15:29:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
05/29/2022 15:29:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/29/2022 15:29:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/29/2022 15:29:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 15:29:30 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.1513947102182396 on epoch=599
05/29/2022 15:29:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 15:29:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/29/2022 15:29:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/29/2022 15:29:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/29/2022 15:29:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 15:29:44 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.09214426877470355 on epoch=616
05/29/2022 15:29:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 15:29:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
05/29/2022 15:29:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=626
05/29/2022 15:29:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/29/2022 15:29:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/29/2022 15:29:59 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.12764336372847013 on epoch=633
05/29/2022 15:30:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 15:30:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/29/2022 15:30:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 15:30:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 15:30:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 15:30:14 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.15909645909645911 on epoch=649
05/29/2022 15:30:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/29/2022 15:30:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
05/29/2022 15:30:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 15:30:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 15:30:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 15:30:29 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.14292417417417416 on epoch=666
05/29/2022 15:30:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 15:30:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=673
05/29/2022 15:30:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 15:30:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 15:30:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/29/2022 15:30:43 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.11113140908537328 on epoch=683
05/29/2022 15:30:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 15:30:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 15:30:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 15:30:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 15:30:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=699
05/29/2022 15:30:58 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.15086629617522065 on epoch=699
05/29/2022 15:31:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 15:31:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
05/29/2022 15:31:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 15:31:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 15:31:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 15:31:13 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.17038383838383836 on epoch=716
05/29/2022 15:31:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 15:31:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
05/29/2022 15:31:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/29/2022 15:31:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/29/2022 15:31:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 15:31:28 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.14199134199134197 on epoch=733
05/29/2022 15:31:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 15:31:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 15:31:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 15:31:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 15:31:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/29/2022 15:31:43 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.15476190476190477 on epoch=749
05/29/2022 15:31:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 15:31:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 15:31:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/29/2022 15:31:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 15:31:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 15:31:57 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.1399138673557278 on epoch=766
05/29/2022 15:32:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 15:32:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 15:32:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 15:32:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/29/2022 15:32:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 15:32:12 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.1921407624633431 on epoch=783
05/29/2022 15:32:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 15:32:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 15:32:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 15:32:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
05/29/2022 15:32:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 15:32:27 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.18644490644490644 on epoch=799
05/29/2022 15:32:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 15:32:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 15:32:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 15:32:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 15:32:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 15:32:42 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.15392156862745096 on epoch=816
05/29/2022 15:32:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 15:32:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 15:32:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 15:32:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 15:32:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 15:32:56 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.08616551459293396 on epoch=833
05/29/2022 15:32:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 15:33:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
05/29/2022 15:33:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 15:33:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 15:33:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 15:33:11 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.176023976023976 on epoch=849
05/29/2022 15:33:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 15:33:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/29/2022 15:33:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 15:33:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
05/29/2022 15:33:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 15:33:26 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.13634782608695653 on epoch=866
05/29/2022 15:33:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 15:33:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 15:33:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 15:33:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 15:33:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 15:33:41 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.18190476190476187 on epoch=883
05/29/2022 15:33:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 15:33:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 15:33:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=893
05/29/2022 15:33:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 15:33:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 15:33:56 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.17206793206793206 on epoch=899
05/29/2022 15:33:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 15:34:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 15:34:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 15:34:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 15:34:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 15:34:10 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.153125 on epoch=916
05/29/2022 15:34:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 15:34:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 15:34:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 15:34:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 15:34:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 15:34:25 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.16317016317016314 on epoch=933
05/29/2022 15:34:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 15:34:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 15:34:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
05/29/2022 15:34:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 15:34:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 15:34:40 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.1874525474525474 on epoch=949
05/29/2022 15:34:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 15:34:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 15:34:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 15:34:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 15:34:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 15:34:55 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.12937409812409814 on epoch=966
05/29/2022 15:34:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 15:35:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 15:35:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 15:35:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 15:35:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 15:35:10 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.1709610354771645 on epoch=983
05/29/2022 15:35:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 15:35:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 15:35:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 15:35:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/29/2022 15:35:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 15:35:25 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.16380090497737557 on epoch=999
05/29/2022 15:35:25 - INFO - __main__ - save last model!
05/29/2022 15:35:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:35:25 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:25 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:35:25 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:35:25 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:35:25 - INFO - __main__ - ['contradiction']
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:25 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:35:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:35:25 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:35:25 - INFO - __main__ - ['entailment']
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:25 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:35:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:26 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:35:43 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:35:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:35:44 - INFO - __main__ - Starting training!
05/29/2022 15:35:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_21_0.4_8_predictions.txt
05/29/2022 15:35:55 - INFO - __main__ - Classification-F1 on test data: 0.1273
05/29/2022 15:35:56 - INFO - __main__ - prefix=anli_16_21, lr=0.4, bsz=8, dev_performance=0.38164924506387926, test_performance=0.12731232369163403
05/29/2022 15:35:56 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.3, bsz=8 ...
05/29/2022 15:35:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:35:57 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:57 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:57 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:35:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:35:57 - INFO - __main__ - Printing 3 examples
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:35:57 - INFO - __main__ - ['entailment']
05/29/2022 15:35:57 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:35:57 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:35:57 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:36:12 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:36:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:36:13 - INFO - __main__ - Starting training!
05/29/2022 15:36:16 - INFO - __main__ - Step 10 Global step 10 Train loss 0.78 on epoch=3
05/29/2022 15:36:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=6
05/29/2022 15:36:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
05/29/2022 15:36:24 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
05/29/2022 15:36:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
05/29/2022 15:36:28 - INFO - __main__ - Global step 50 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 15:36:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 15:36:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=19
05/29/2022 15:36:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
05/29/2022 15:36:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
05/29/2022 15:36:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
05/29/2022 15:36:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/29/2022 15:36:43 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.2085278555866791 on epoch=33
05/29/2022 15:36:43 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2085278555866791 on epoch=33, global_step=100
05/29/2022 15:36:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
05/29/2022 15:36:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
05/29/2022 15:36:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 15:36:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
05/29/2022 15:36:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
05/29/2022 15:36:57 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.36657306741546064 on epoch=49
05/29/2022 15:36:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.36657306741546064 on epoch=49, global_step=150
05/29/2022 15:37:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
05/29/2022 15:37:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/29/2022 15:37:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
05/29/2022 15:37:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
05/29/2022 15:37:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
05/29/2022 15:37:11 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.28439763001974977 on epoch=66
05/29/2022 15:37:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 15:37:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/29/2022 15:37:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
05/29/2022 15:37:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
05/29/2022 15:37:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/29/2022 15:37:26 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
05/29/2022 15:37:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
05/29/2022 15:37:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=89
05/29/2022 15:37:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
05/29/2022 15:37:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
05/29/2022 15:37:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
05/29/2022 15:37:40 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.21212121212121215 on epoch=99
05/29/2022 15:37:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/29/2022 15:37:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=106
05/29/2022 15:37:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=109
05/29/2022 15:37:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
05/29/2022 15:37:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=116
05/29/2022 15:37:54 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.30696457326892107 on epoch=116
05/29/2022 15:37:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=119
05/29/2022 15:37:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
05/29/2022 15:38:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/29/2022 15:38:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
05/29/2022 15:38:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
05/29/2022 15:38:09 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.26916179337231966 on epoch=133
05/29/2022 15:38:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
05/29/2022 15:38:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=139
05/29/2022 15:38:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
05/29/2022 15:38:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=146
05/29/2022 15:38:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=149
05/29/2022 15:38:23 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.2099511072763877 on epoch=149
05/29/2022 15:38:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=153
05/29/2022 15:38:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=156
05/29/2022 15:38:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=159
05/29/2022 15:38:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/29/2022 15:38:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=166
05/29/2022 15:38:37 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.3022173489278752 on epoch=166
05/29/2022 15:38:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
05/29/2022 15:38:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=173
05/29/2022 15:38:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=176
05/29/2022 15:38:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=179
05/29/2022 15:38:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=183
05/29/2022 15:38:51 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.31711525189786055 on epoch=183
05/29/2022 15:38:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=186
05/29/2022 15:38:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=189
05/29/2022 15:38:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
05/29/2022 15:39:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=196
05/29/2022 15:39:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=199
05/29/2022 15:39:06 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.2931376659678547 on epoch=199
05/29/2022 15:39:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=203
05/29/2022 15:39:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=206
05/29/2022 15:39:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=209
05/29/2022 15:39:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=213
05/29/2022 15:39:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
05/29/2022 15:39:20 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.24352941176470586 on epoch=216
05/29/2022 15:39:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
05/29/2022 15:39:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=223
05/29/2022 15:39:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=226
05/29/2022 15:39:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=229
05/29/2022 15:39:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=233
05/29/2022 15:39:34 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.2592029719689294 on epoch=233
05/29/2022 15:39:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=236
05/29/2022 15:39:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=239
05/29/2022 15:39:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=243
05/29/2022 15:39:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=246
05/29/2022 15:39:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=249
05/29/2022 15:39:49 - INFO - __main__ - Global step 750 Train loss 0.12 Classification-F1 0.18937062937062937 on epoch=249
05/29/2022 15:39:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=253
05/29/2022 15:39:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
05/29/2022 15:39:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=259
05/29/2022 15:39:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=263
05/29/2022 15:40:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=266
05/29/2022 15:40:03 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.3125651967757231 on epoch=266
05/29/2022 15:40:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/29/2022 15:40:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
05/29/2022 15:40:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=276
05/29/2022 15:40:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
05/29/2022 15:40:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
05/29/2022 15:40:17 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.19369918699186994 on epoch=283
05/29/2022 15:40:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=286
05/29/2022 15:40:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
05/29/2022 15:40:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=293
05/29/2022 15:40:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/29/2022 15:40:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=299
05/29/2022 15:40:32 - INFO - __main__ - Global step 900 Train loss 0.09 Classification-F1 0.2510327417380661 on epoch=299
05/29/2022 15:40:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
05/29/2022 15:40:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
05/29/2022 15:40:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
05/29/2022 15:40:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
05/29/2022 15:40:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
05/29/2022 15:40:46 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.21000863204761255 on epoch=316
05/29/2022 15:40:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
05/29/2022 15:40:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=323
05/29/2022 15:40:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
05/29/2022 15:40:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
05/29/2022 15:40:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=333
05/29/2022 15:41:00 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.27315452379391 on epoch=333
05/29/2022 15:41:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=336
05/29/2022 15:41:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
05/29/2022 15:41:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
05/29/2022 15:41:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
05/29/2022 15:41:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
05/29/2022 15:41:15 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.29259259259259257 on epoch=349
05/29/2022 15:41:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
05/29/2022 15:41:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=356
05/29/2022 15:41:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
05/29/2022 15:41:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
05/29/2022 15:41:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
05/29/2022 15:41:29 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.24119241192411922 on epoch=366
05/29/2022 15:41:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
05/29/2022 15:41:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
05/29/2022 15:41:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
05/29/2022 15:41:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
05/29/2022 15:41:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
05/29/2022 15:41:43 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.2568310143174027 on epoch=383
05/29/2022 15:41:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=386
05/29/2022 15:41:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
05/29/2022 15:41:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/29/2022 15:41:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=396
05/29/2022 15:41:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/29/2022 15:41:57 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.1661967418546366 on epoch=399
05/29/2022 15:42:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/29/2022 15:42:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
05/29/2022 15:42:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
05/29/2022 15:42:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
05/29/2022 15:42:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/29/2022 15:42:12 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.26535873734515497 on epoch=416
05/29/2022 15:42:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
05/29/2022 15:42:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
05/29/2022 15:42:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
05/29/2022 15:42:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/29/2022 15:42:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
05/29/2022 15:42:26 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.10538548752834469 on epoch=433
05/29/2022 15:42:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
05/29/2022 15:42:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
05/29/2022 15:42:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
05/29/2022 15:42:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/29/2022 15:42:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
05/29/2022 15:42:40 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.13783993783993784 on epoch=449
05/29/2022 15:42:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 15:42:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
05/29/2022 15:42:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
05/29/2022 15:42:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
05/29/2022 15:42:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
05/29/2022 15:42:54 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.14154761904761903 on epoch=466
05/29/2022 15:42:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/29/2022 15:43:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
05/29/2022 15:43:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
05/29/2022 15:43:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
05/29/2022 15:43:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
05/29/2022 15:43:09 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.13920162107396147 on epoch=483
05/29/2022 15:43:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
05/29/2022 15:43:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
05/29/2022 15:43:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/29/2022 15:43:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
05/29/2022 15:43:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
05/29/2022 15:43:23 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.14203042328042328 on epoch=499
05/29/2022 15:43:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
05/29/2022 15:43:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/29/2022 15:43:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 15:43:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
05/29/2022 15:43:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
05/29/2022 15:43:37 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.1748148148148148 on epoch=516
05/29/2022 15:43:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
05/29/2022 15:43:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
05/29/2022 15:43:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 15:43:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 15:43:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/29/2022 15:43:51 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.1523846960167715 on epoch=533
05/29/2022 15:43:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 15:43:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/29/2022 15:43:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 15:44:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
05/29/2022 15:44:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/29/2022 15:44:06 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.12701293463543917 on epoch=549
05/29/2022 15:44:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
05/29/2022 15:44:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 15:44:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
05/29/2022 15:44:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/29/2022 15:44:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
05/29/2022 15:44:20 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.2808080808080808 on epoch=566
05/29/2022 15:44:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/29/2022 15:44:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/29/2022 15:44:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/29/2022 15:44:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/29/2022 15:44:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 15:44:34 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.15739463601532566 on epoch=583
05/29/2022 15:44:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 15:44:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
05/29/2022 15:44:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
05/29/2022 15:44:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 15:44:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/29/2022 15:44:48 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.1152492668621701 on epoch=599
05/29/2022 15:44:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 15:44:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 15:44:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/29/2022 15:44:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 15:45:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 15:45:02 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.1916995741385985 on epoch=616
05/29/2022 15:45:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 15:45:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 15:45:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 15:45:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 15:45:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/29/2022 15:45:17 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.23634485924112605 on epoch=633
05/29/2022 15:45:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/29/2022 15:45:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 15:45:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
05/29/2022 15:45:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 15:45:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 15:45:31 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.18164086687306505 on epoch=649
05/29/2022 15:45:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/29/2022 15:45:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 15:45:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 15:45:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 15:45:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 15:45:45 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.20136363636363636 on epoch=666
05/29/2022 15:45:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
05/29/2022 15:45:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 15:45:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 15:45:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 15:45:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/29/2022 15:45:59 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.17225030895405008 on epoch=683
05/29/2022 15:46:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 15:46:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/29/2022 15:46:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 15:46:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 15:46:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 15:46:14 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.15464285714285717 on epoch=699
05/29/2022 15:46:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
05/29/2022 15:46:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
05/29/2022 15:46:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 15:46:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 15:46:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 15:46:28 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.12357509157509158 on epoch=716
05/29/2022 15:46:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 15:46:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
05/29/2022 15:46:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 15:46:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 15:46:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 15:46:42 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.1109253065774805 on epoch=733
05/29/2022 15:46:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 15:46:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 15:46:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 15:46:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 15:46:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 15:46:57 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.20611849390919157 on epoch=749
05/29/2022 15:46:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 15:47:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 15:47:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 15:47:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
05/29/2022 15:47:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 15:47:11 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.31569892473118283 on epoch=766
05/29/2022 15:47:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 15:47:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 15:47:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 15:47:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 15:47:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 15:47:25 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.2604054731714306 on epoch=783
05/29/2022 15:47:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
05/29/2022 15:47:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 15:47:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 15:47:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 15:47:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 15:47:40 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.15865942028985508 on epoch=799
05/29/2022 15:47:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 15:47:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 15:47:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 15:47:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 15:47:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 15:47:54 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.16911025145067698 on epoch=816
05/29/2022 15:47:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 15:47:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 15:48:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=826
05/29/2022 15:48:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 15:48:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 15:48:08 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.17725752508361203 on epoch=833
05/29/2022 15:48:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 15:48:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
05/29/2022 15:48:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 15:48:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/29/2022 15:48:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 15:48:22 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.25602175602175603 on epoch=849
05/29/2022 15:48:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 15:48:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 15:48:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 15:48:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 15:48:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 15:48:37 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2039337474120083 on epoch=866
05/29/2022 15:48:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 15:48:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 15:48:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 15:48:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 15:48:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 15:48:51 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.2638888888888889 on epoch=883
05/29/2022 15:48:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 15:48:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 15:48:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 15:49:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
05/29/2022 15:49:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 15:49:05 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.23727238944630247 on epoch=899
05/29/2022 15:49:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 15:49:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 15:49:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 15:49:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 15:49:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/29/2022 15:49:20 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3483616654348362 on epoch=916
05/29/2022 15:49:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/29/2022 15:49:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 15:49:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
05/29/2022 15:49:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 15:49:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 15:49:34 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.1294894130259984 on epoch=933
05/29/2022 15:49:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 15:49:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 15:49:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 15:49:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
05/29/2022 15:49:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 15:49:48 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.20923520923520925 on epoch=949
05/29/2022 15:49:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 15:49:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 15:49:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 15:49:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 15:50:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 15:50:03 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2517651430694909 on epoch=966
05/29/2022 15:50:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 15:50:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 15:50:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 15:50:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/29/2022 15:50:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/29/2022 15:50:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.20197280613947277 on epoch=983
05/29/2022 15:50:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 15:50:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 15:50:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 15:50:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 15:50:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 15:50:31 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.23359825463612283 on epoch=999
05/29/2022 15:50:31 - INFO - __main__ - save last model!
05/29/2022 15:50:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 15:50:31 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:50:31 - INFO - __main__ - Printing 3 examples
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:50:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 15:50:31 - INFO - __main__ - Printing 3 examples
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 15:50:31 - INFO - __main__ - ['contradiction']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 15:50:31 - INFO - __main__ - ['contradiction']
05/29/2022 15:50:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:50:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:50:31 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:50:31 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:50:31 - INFO - __main__ - Printing 3 examples
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:50:31 - INFO - __main__ - ['entailment']
05/29/2022 15:50:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:50:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:50:31 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:50:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:50:33 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 15:50:47 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:50:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:50:47 - INFO - __main__ - Starting training!
05/29/2022 15:51:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_21_0.3_8_predictions.txt
05/29/2022 15:51:01 - INFO - __main__ - Classification-F1 on test data: 0.1497
05/29/2022 15:51:02 - INFO - __main__ - prefix=anli_16_21, lr=0.3, bsz=8, dev_performance=0.36657306741546064, test_performance=0.14970621494563205
05/29/2022 15:51:02 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.2, bsz=8 ...
05/29/2022 15:51:03 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:51:03 - INFO - __main__ - Printing 3 examples
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:51:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:51:03 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 15:51:03 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 15:51:03 - INFO - __main__ - Printing 3 examples
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/29/2022 15:51:03 - INFO - __main__ - ['entailment']
05/29/2022 15:51:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 15:51:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 15:51:03 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 15:51:18 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 15:51:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 15:51:19 - INFO - __main__ - Starting training!
05/29/2022 15:51:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.73 on epoch=3
05/29/2022 15:51:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=6
05/29/2022 15:51:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
05/29/2022 15:51:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
05/29/2022 15:51:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
05/29/2022 15:51:34 - INFO - __main__ - Global step 50 Train loss 0.60 Classification-F1 0.3146867798030588 on epoch=16
05/29/2022 15:51:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3146867798030588 on epoch=16, global_step=50
05/29/2022 15:51:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=19
05/29/2022 15:51:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
05/29/2022 15:51:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
05/29/2022 15:51:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
05/29/2022 15:51:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
05/29/2022 15:51:48 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 15:51:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=36
05/29/2022 15:51:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=39
05/29/2022 15:51:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
05/29/2022 15:51:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
05/29/2022 15:52:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
05/29/2022 15:52:03 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/29/2022 15:52:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/29/2022 15:52:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/29/2022 15:52:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/29/2022 15:52:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
05/29/2022 15:52:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
05/29/2022 15:52:17 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.2995574190542744 on epoch=66
05/29/2022 15:52:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
05/29/2022 15:52:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=73
05/29/2022 15:52:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
05/29/2022 15:52:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
05/29/2022 15:52:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=83
05/29/2022 15:52:32 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.20427672955974843 on epoch=83
05/29/2022 15:52:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=86
05/29/2022 15:52:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
05/29/2022 15:52:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
05/29/2022 15:52:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
05/29/2022 15:52:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
05/29/2022 15:52:46 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=99
05/29/2022 15:52:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=103
05/29/2022 15:52:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
05/29/2022 15:52:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=109
05/29/2022 15:52:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
05/29/2022 15:52:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
05/29/2022 15:53:01 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.2321637426900585 on epoch=116
05/29/2022 15:53:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
05/29/2022 15:53:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=123
05/29/2022 15:53:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
05/29/2022 15:53:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
05/29/2022 15:53:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
05/29/2022 15:53:15 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.2436063856436884 on epoch=133
05/29/2022 15:53:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
05/29/2022 15:53:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
05/29/2022 15:53:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
05/29/2022 15:53:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
05/29/2022 15:53:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
05/29/2022 15:53:30 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.29952034577271763 on epoch=149
05/29/2022 15:53:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=153
05/29/2022 15:53:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
05/29/2022 15:53:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=159
05/29/2022 15:53:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=163
05/29/2022 15:53:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
05/29/2022 15:53:44 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.276765785873713 on epoch=166
05/29/2022 15:53:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=169
05/29/2022 15:53:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=173
05/29/2022 15:53:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
05/29/2022 15:53:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
05/29/2022 15:53:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=183
05/29/2022 15:53:59 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.29266347687400324 on epoch=183
05/29/2022 15:54:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=186
05/29/2022 15:54:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=189
05/29/2022 15:54:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=193
05/29/2022 15:54:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=196
05/29/2022 15:54:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=199
05/29/2022 15:54:16 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.28740253411306044 on epoch=199
05/29/2022 15:54:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=203
05/29/2022 15:54:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=206
05/29/2022 15:54:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
05/29/2022 15:54:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=213
05/29/2022 15:54:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=216
05/29/2022 15:54:30 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.2909919028340081 on epoch=216
05/29/2022 15:54:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=219
05/29/2022 15:54:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=223
05/29/2022 15:54:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=226
05/29/2022 15:54:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=229
05/29/2022 15:54:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=233
05/29/2022 15:54:45 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.3501078592356483 on epoch=233
05/29/2022 15:54:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3146867798030588 -> 0.3501078592356483 on epoch=233, global_step=700
05/29/2022 15:54:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=236
05/29/2022 15:54:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=239
05/29/2022 15:54:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=243
05/29/2022 15:54:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=246
05/29/2022 15:54:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=249
05/29/2022 15:54:59 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.3119047619047619 on epoch=249
05/29/2022 15:55:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=253
05/29/2022 15:55:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
05/29/2022 15:55:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=259
05/29/2022 15:55:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
05/29/2022 15:55:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=266
05/29/2022 15:55:14 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.3778653407136379 on epoch=266
05/29/2022 15:55:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3501078592356483 -> 0.3778653407136379 on epoch=266, global_step=800
05/29/2022 15:55:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=269
05/29/2022 15:55:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=273
05/29/2022 15:55:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=276
05/29/2022 15:55:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=279
05/29/2022 15:55:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=283
05/29/2022 15:55:28 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.2794743913825693 on epoch=283
05/29/2022 15:55:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=286
05/29/2022 15:55:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=289
05/29/2022 15:55:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=293
05/29/2022 15:55:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=296
05/29/2022 15:55:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=299
05/29/2022 15:55:43 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.30127041742286753 on epoch=299
05/29/2022 15:55:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
05/29/2022 15:55:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=306
05/29/2022 15:55:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=309
05/29/2022 15:55:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
05/29/2022 15:55:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=316
05/29/2022 15:55:59 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.21201298701298704 on epoch=316
05/29/2022 15:56:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=319
05/29/2022 15:56:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
05/29/2022 15:56:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=326
05/29/2022 15:56:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
05/29/2022 15:56:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=333
05/29/2022 15:56:13 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.3014141414141414 on epoch=333
05/29/2022 15:56:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=336
05/29/2022 15:56:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
05/29/2022 15:56:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=343
05/29/2022 15:56:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=346
05/29/2022 15:56:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=349
05/29/2022 15:56:28 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.20839874411302983 on epoch=349
05/29/2022 15:56:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=353
05/29/2022 15:56:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
05/29/2022 15:56:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=359
05/29/2022 15:56:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
05/29/2022 15:56:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=366
05/29/2022 15:56:42 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.1498949579831933 on epoch=366
05/29/2022 15:56:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
05/29/2022 15:56:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=373
05/29/2022 15:56:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.12 on epoch=376
05/29/2022 15:56:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=379
05/29/2022 15:56:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
05/29/2022 15:56:57 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.3417521367521368 on epoch=383
05/29/2022 15:56:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
05/29/2022 15:57:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=389
05/29/2022 15:57:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=393
05/29/2022 15:57:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=396
05/29/2022 15:57:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=399
05/29/2022 15:57:11 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.2 on epoch=399
05/29/2022 15:57:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=403
05/29/2022 15:57:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=406
05/29/2022 15:57:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=409
05/29/2022 15:57:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=413
05/29/2022 15:57:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=416
05/29/2022 15:57:25 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.18466830466830464 on epoch=416
05/29/2022 15:57:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
05/29/2022 15:57:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=423
05/29/2022 15:57:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=426
05/29/2022 15:57:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=429
05/29/2022 15:57:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=433
05/29/2022 15:57:40 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.2779905437352246 on epoch=433
05/29/2022 15:57:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=436
05/29/2022 15:57:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
05/29/2022 15:57:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
05/29/2022 15:57:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
05/29/2022 15:57:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=449
05/29/2022 15:57:54 - INFO - __main__ - Global step 1350 Train loss 0.07 Classification-F1 0.22914653784219002 on epoch=449
05/29/2022 15:57:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=453
05/29/2022 15:57:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
05/29/2022 15:58:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/29/2022 15:58:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=463
05/29/2022 15:58:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=466
05/29/2022 15:58:08 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.1970695970695971 on epoch=466
05/29/2022 15:58:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=469
05/29/2022 15:58:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
05/29/2022 15:58:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
05/29/2022 15:58:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
05/29/2022 15:58:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=483
05/29/2022 15:58:23 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.13215488215488216 on epoch=483
05/29/2022 15:58:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
05/29/2022 15:58:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=489
05/29/2022 15:58:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=493
05/29/2022 15:58:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
05/29/2022 15:58:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=499
05/29/2022 15:58:37 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.16338591837684396 on epoch=499
05/29/2022 15:58:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
05/29/2022 15:58:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=506
05/29/2022 15:58:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
05/29/2022 15:58:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=513
05/29/2022 15:58:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=516
05/29/2022 15:58:51 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.22489239598278338 on epoch=516
05/29/2022 15:58:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
05/29/2022 15:58:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=523
05/29/2022 15:58:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
05/29/2022 15:59:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
05/29/2022 15:59:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=533
05/29/2022 15:59:06 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.21921146953405019 on epoch=533
05/29/2022 15:59:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
05/29/2022 15:59:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=539
05/29/2022 15:59:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
05/29/2022 15:59:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=546
05/29/2022 15:59:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
05/29/2022 15:59:21 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.13192518031227707 on epoch=549
05/29/2022 15:59:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=553
05/29/2022 15:59:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
05/29/2022 15:59:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 15:59:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/29/2022 15:59:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
05/29/2022 15:59:35 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.13386613386613386 on epoch=566
05/29/2022 15:59:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/29/2022 15:59:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 15:59:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/29/2022 15:59:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
05/29/2022 15:59:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=583
05/29/2022 15:59:50 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.20122887864823352 on epoch=583
05/29/2022 15:59:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=586
05/29/2022 15:59:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 15:59:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/29/2022 16:00:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=596
05/29/2022 16:00:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
05/29/2022 16:00:04 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.26679841897233203 on epoch=599
05/29/2022 16:00:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 16:00:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
05/29/2022 16:00:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
05/29/2022 16:00:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
05/29/2022 16:00:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/29/2022 16:00:18 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.16190476190476188 on epoch=616
05/29/2022 16:00:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 16:00:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=623
05/29/2022 16:00:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=626
05/29/2022 16:00:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/29/2022 16:00:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/29/2022 16:00:33 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.21702741702741704 on epoch=633
05/29/2022 16:00:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/29/2022 16:00:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/29/2022 16:00:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
05/29/2022 16:00:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
05/29/2022 16:00:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 16:00:47 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2565546218487395 on epoch=649
05/29/2022 16:00:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/29/2022 16:00:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/29/2022 16:00:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 16:00:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
05/29/2022 16:01:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/29/2022 16:01:01 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.22657004830917876 on epoch=666
05/29/2022 16:01:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
05/29/2022 16:01:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
05/29/2022 16:01:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 16:01:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 16:01:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=683
05/29/2022 16:01:16 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.21543859649122807 on epoch=683
05/29/2022 16:01:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/29/2022 16:01:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
05/29/2022 16:01:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=693
05/29/2022 16:01:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
05/29/2022 16:01:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 16:01:31 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.15421052631578946 on epoch=699
05/29/2022 16:01:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/29/2022 16:01:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=706
05/29/2022 16:01:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/29/2022 16:01:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=713
05/29/2022 16:01:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/29/2022 16:01:45 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.13939758544742636 on epoch=716
05/29/2022 16:01:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/29/2022 16:01:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/29/2022 16:01:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/29/2022 16:01:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
05/29/2022 16:01:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/29/2022 16:02:00 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.15880508725229223 on epoch=733
05/29/2022 16:02:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
05/29/2022 16:02:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 16:02:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 16:02:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=746
05/29/2022 16:02:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
05/29/2022 16:02:14 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.14736212297187906 on epoch=749
05/29/2022 16:02:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/29/2022 16:02:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
05/29/2022 16:02:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 16:02:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=763
05/29/2022 16:02:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
05/29/2022 16:02:29 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.15341626794258373 on epoch=766
05/29/2022 16:02:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
05/29/2022 16:02:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
05/29/2022 16:02:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/29/2022 16:02:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 16:02:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/29/2022 16:02:43 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.36122188338937106 on epoch=783
05/29/2022 16:02:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
05/29/2022 16:02:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 16:02:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 16:02:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/29/2022 16:02:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
05/29/2022 16:02:58 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.15438419786245872 on epoch=799
05/29/2022 16:03:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 16:03:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=806
05/29/2022 16:03:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
05/29/2022 16:03:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/29/2022 16:03:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
05/29/2022 16:03:13 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.13992052486220036 on epoch=816
05/29/2022 16:03:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 16:03:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=823
05/29/2022 16:03:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 16:03:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 16:03:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
05/29/2022 16:03:27 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.21809523809523806 on epoch=833
05/29/2022 16:03:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 16:03:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
05/29/2022 16:03:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
05/29/2022 16:03:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/29/2022 16:03:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
05/29/2022 16:03:42 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.158223889931207 on epoch=849
05/29/2022 16:03:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/29/2022 16:03:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/29/2022 16:03:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/29/2022 16:03:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=863
05/29/2022 16:03:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 16:03:56 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.19842190016103062 on epoch=866
05/29/2022 16:03:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
05/29/2022 16:04:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/29/2022 16:04:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 16:04:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/29/2022 16:04:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 16:04:11 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.16303232998885173 on epoch=883
05/29/2022 16:04:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 16:04:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 16:04:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 16:04:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 16:04:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 16:04:26 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.1908932806324111 on epoch=899
05/29/2022 16:04:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 16:04:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/29/2022 16:04:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 16:04:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/29/2022 16:04:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 16:04:40 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.1881592662080467 on epoch=916
05/29/2022 16:04:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 16:04:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 16:04:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=926
05/29/2022 16:04:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 16:04:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 16:04:55 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.13140826873385014 on epoch=933
05/29/2022 16:04:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 16:05:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 16:05:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 16:05:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=946
05/29/2022 16:05:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 16:05:09 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.1885057471264368 on epoch=949
05/29/2022 16:05:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/29/2022 16:05:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 16:05:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 16:05:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=963
05/29/2022 16:05:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 16:05:24 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.18840579710144928 on epoch=966
05/29/2022 16:05:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/29/2022 16:05:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 16:05:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
05/29/2022 16:05:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/29/2022 16:05:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 16:05:39 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.1741616996379859 on epoch=983
05/29/2022 16:05:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 16:05:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 16:05:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
05/29/2022 16:05:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/29/2022 16:05:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 16:05:53 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2222222222222222 on epoch=999
05/29/2022 16:05:53 - INFO - __main__ - save last model!
05/29/2022 16:05:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:05:53 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:05:53 - INFO - __main__ - Printing 3 examples
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:05:53 - INFO - __main__ - ['contradiction']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:05:53 - INFO - __main__ - ['entailment']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:05:53 - INFO - __main__ - ['contradiction']
05/29/2022 16:05:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:05:53 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:05:53 - INFO - __main__ - Printing 3 examples
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:05:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:05:53 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:05:53 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:05:53 - INFO - __main__ - Printing 3 examples
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:05:53 - INFO - __main__ - ['neutral']
05/29/2022 16:05:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:05:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:05:53 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:05:54 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:05:55 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:06:09 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:06:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:06:09 - INFO - __main__ - Starting training!
05/29/2022 16:06:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_21_0.2_8_predictions.txt
05/29/2022 16:06:24 - INFO - __main__ - Classification-F1 on test data: 0.1529
05/29/2022 16:06:25 - INFO - __main__ - prefix=anli_16_21, lr=0.2, bsz=8, dev_performance=0.3778653407136379, test_performance=0.15292579966707534
05/29/2022 16:06:25 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.5, bsz=8 ...
05/29/2022 16:06:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:06:25 - INFO - __main__ - Printing 3 examples
05/29/2022 16:06:25 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:06:25 - INFO - __main__ - ['neutral']
05/29/2022 16:06:25 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:06:25 - INFO - __main__ - ['neutral']
05/29/2022 16:06:25 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:06:25 - INFO - __main__ - ['neutral']
05/29/2022 16:06:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:06:25 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:06:26 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:06:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:06:26 - INFO - __main__ - Printing 3 examples
05/29/2022 16:06:26 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:06:26 - INFO - __main__ - ['neutral']
05/29/2022 16:06:26 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:06:26 - INFO - __main__ - ['neutral']
05/29/2022 16:06:26 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:06:26 - INFO - __main__ - ['neutral']
05/29/2022 16:06:26 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:06:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:06:26 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:06:41 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:06:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:06:42 - INFO - __main__ - Starting training!
05/29/2022 16:06:45 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=3
05/29/2022 16:06:48 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
05/29/2022 16:06:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=9
05/29/2022 16:06:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
05/29/2022 16:06:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=16
05/29/2022 16:06:57 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.22501747030048916 on epoch=16
05/29/2022 16:06:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22501747030048916 on epoch=16, global_step=50
05/29/2022 16:07:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/29/2022 16:07:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
05/29/2022 16:07:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=26
05/29/2022 16:07:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=29
05/29/2022 16:07:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
05/29/2022 16:07:11 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 16:07:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/29/2022 16:07:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
05/29/2022 16:07:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=43
05/29/2022 16:07:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
05/29/2022 16:07:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
05/29/2022 16:07:26 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.22905323653962492 on epoch=49
05/29/2022 16:07:26 - INFO - __main__ - Saving model with best Classification-F1: 0.22501747030048916 -> 0.22905323653962492 on epoch=49, global_step=150
05/29/2022 16:07:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/29/2022 16:07:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/29/2022 16:07:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
05/29/2022 16:07:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/29/2022 16:07:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
05/29/2022 16:07:41 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.20634920634920637 on epoch=66
05/29/2022 16:07:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/29/2022 16:07:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/29/2022 16:07:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
05/29/2022 16:07:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
05/29/2022 16:07:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=83
05/29/2022 16:07:55 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.23410986482599946 on epoch=83
05/29/2022 16:07:55 - INFO - __main__ - Saving model with best Classification-F1: 0.22905323653962492 -> 0.23410986482599946 on epoch=83, global_step=250
05/29/2022 16:07:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
05/29/2022 16:08:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
05/29/2022 16:08:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=93
05/29/2022 16:08:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=96
05/29/2022 16:08:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
05/29/2022 16:08:10 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.17863868711326336 on epoch=99
05/29/2022 16:08:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
05/29/2022 16:08:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=106
05/29/2022 16:08:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=109
05/29/2022 16:08:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=113
05/29/2022 16:08:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
05/29/2022 16:08:24 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.16666666666666666 on epoch=116
05/29/2022 16:08:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=119
05/29/2022 16:08:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=123
05/29/2022 16:08:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
05/29/2022 16:08:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=129
05/29/2022 16:08:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=133
05/29/2022 16:08:39 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.2494172494172494 on epoch=133
05/29/2022 16:08:39 - INFO - __main__ - Saving model with best Classification-F1: 0.23410986482599946 -> 0.2494172494172494 on epoch=133, global_step=400
05/29/2022 16:08:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=136
05/29/2022 16:08:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=139
05/29/2022 16:08:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=143
05/29/2022 16:08:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=146
05/29/2022 16:08:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=149
05/29/2022 16:08:53 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.2127563669145819 on epoch=149
05/29/2022 16:08:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=153
05/29/2022 16:08:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=156
05/29/2022 16:09:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=159
05/29/2022 16:09:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=163
05/29/2022 16:09:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=166
05/29/2022 16:09:08 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.2411483253588517 on epoch=166
05/29/2022 16:09:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=169
05/29/2022 16:09:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=173
05/29/2022 16:09:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=176
05/29/2022 16:09:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=179
05/29/2022 16:09:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
05/29/2022 16:09:22 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.30358974358974355 on epoch=183
05/29/2022 16:09:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2494172494172494 -> 0.30358974358974355 on epoch=183, global_step=550
05/29/2022 16:09:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=186
05/29/2022 16:09:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=189
05/29/2022 16:09:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.11 on epoch=193
05/29/2022 16:09:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=196
05/29/2022 16:09:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=199
05/29/2022 16:09:37 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.32196870355800333 on epoch=199
05/29/2022 16:09:37 - INFO - __main__ - Saving model with best Classification-F1: 0.30358974358974355 -> 0.32196870355800333 on epoch=199, global_step=600
05/29/2022 16:09:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=203
05/29/2022 16:09:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=206
05/29/2022 16:09:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
05/29/2022 16:09:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=213
05/29/2022 16:09:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=216
05/29/2022 16:09:51 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.20877541998231652 on epoch=216
05/29/2022 16:09:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=219
05/29/2022 16:09:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=223
05/29/2022 16:09:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=226
05/29/2022 16:10:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=229
05/29/2022 16:10:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=233
05/29/2022 16:10:06 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.20501993901008678 on epoch=233
05/29/2022 16:10:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=236
05/29/2022 16:10:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=239
05/29/2022 16:10:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=243
05/29/2022 16:10:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=246
05/29/2022 16:10:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=249
05/29/2022 16:10:21 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.30802275629861836 on epoch=249
05/29/2022 16:10:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=253
05/29/2022 16:10:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=256
05/29/2022 16:10:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=259
05/29/2022 16:10:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=263
05/29/2022 16:10:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=266
05/29/2022 16:10:35 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.34708994708994706 on epoch=266
05/29/2022 16:10:35 - INFO - __main__ - Saving model with best Classification-F1: 0.32196870355800333 -> 0.34708994708994706 on epoch=266, global_step=800
05/29/2022 16:10:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=269
05/29/2022 16:10:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
05/29/2022 16:10:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=276
05/29/2022 16:10:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=279
05/29/2022 16:10:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=283
05/29/2022 16:10:50 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.3313131313131313 on epoch=283
05/29/2022 16:10:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=286
05/29/2022 16:10:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=289
05/29/2022 16:10:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=293
05/29/2022 16:11:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=296
05/29/2022 16:11:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=299
05/29/2022 16:11:04 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.25210084033613445 on epoch=299
05/29/2022 16:11:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=303
05/29/2022 16:11:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=306
05/29/2022 16:11:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
05/29/2022 16:11:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
05/29/2022 16:11:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=316
05/29/2022 16:11:19 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.2686275909664216 on epoch=316
05/29/2022 16:11:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
05/29/2022 16:11:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
05/29/2022 16:11:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
05/29/2022 16:11:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=329
05/29/2022 16:11:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=333
05/29/2022 16:11:33 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.2805295145296622 on epoch=333
05/29/2022 16:11:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=336
05/29/2022 16:11:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
05/29/2022 16:11:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
05/29/2022 16:11:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=346
05/29/2022 16:11:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
05/29/2022 16:11:48 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.2688280986153327 on epoch=349
05/29/2022 16:11:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=353
05/29/2022 16:11:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
05/29/2022 16:11:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/29/2022 16:11:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
05/29/2022 16:12:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
05/29/2022 16:12:02 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.29897018970189704 on epoch=366
05/29/2022 16:12:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
05/29/2022 16:12:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
05/29/2022 16:12:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
05/29/2022 16:12:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=379
05/29/2022 16:12:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
05/29/2022 16:12:17 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.19274891774891778 on epoch=383
05/29/2022 16:12:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/29/2022 16:12:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=389
05/29/2022 16:12:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/29/2022 16:12:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=396
05/29/2022 16:12:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/29/2022 16:12:32 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2558594194003046 on epoch=399
05/29/2022 16:12:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
05/29/2022 16:12:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
05/29/2022 16:12:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
05/29/2022 16:12:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=413
05/29/2022 16:12:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/29/2022 16:12:46 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.25523809523809526 on epoch=416
05/29/2022 16:12:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=419
05/29/2022 16:12:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
05/29/2022 16:12:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
05/29/2022 16:12:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
05/29/2022 16:12:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
05/29/2022 16:13:01 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.24190476190476193 on epoch=433
05/29/2022 16:13:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=436
05/29/2022 16:13:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
05/29/2022 16:13:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/29/2022 16:13:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
05/29/2022 16:13:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
05/29/2022 16:13:15 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.21969696969696972 on epoch=449
05/29/2022 16:13:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
05/29/2022 16:13:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 16:13:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
05/29/2022 16:13:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
05/29/2022 16:13:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=466
05/29/2022 16:13:30 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.31526444569922835 on epoch=466
05/29/2022 16:13:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=469
05/29/2022 16:13:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 16:13:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
05/29/2022 16:13:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
05/29/2022 16:13:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/29/2022 16:13:44 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3013572578789971 on epoch=483
05/29/2022 16:13:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
05/29/2022 16:13:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
05/29/2022 16:13:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/29/2022 16:13:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
05/29/2022 16:13:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
05/29/2022 16:13:59 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.28628867384489576 on epoch=499
05/29/2022 16:14:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
05/29/2022 16:14:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
05/29/2022 16:14:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
05/29/2022 16:14:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
05/29/2022 16:14:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=516
05/29/2022 16:14:13 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.2813157251161653 on epoch=516
05/29/2022 16:14:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
05/29/2022 16:14:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
05/29/2022 16:14:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
05/29/2022 16:14:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 16:14:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
05/29/2022 16:14:28 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.28628867384489576 on epoch=533
05/29/2022 16:14:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/29/2022 16:14:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/29/2022 16:14:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
05/29/2022 16:14:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=546
05/29/2022 16:14:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/29/2022 16:14:43 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.2692826886375273 on epoch=549
05/29/2022 16:14:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/29/2022 16:14:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 16:14:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/29/2022 16:14:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=563
05/29/2022 16:14:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/29/2022 16:14:57 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.3249870560215387 on epoch=566
05/29/2022 16:15:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=569
05/29/2022 16:15:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 16:15:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/29/2022 16:15:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=579
05/29/2022 16:15:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 16:15:12 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.2558594194003046 on epoch=583
05/29/2022 16:15:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 16:15:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
05/29/2022 16:15:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
05/29/2022 16:15:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 16:15:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 16:15:26 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.29536407155454775 on epoch=599
05/29/2022 16:15:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 16:15:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/29/2022 16:15:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 16:15:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 16:15:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 16:15:41 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2813492063492064 on epoch=616
05/29/2022 16:15:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/29/2022 16:15:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/29/2022 16:15:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 16:15:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 16:15:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 16:15:55 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.3156536327268035 on epoch=633
05/29/2022 16:15:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 16:16:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 16:16:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 16:16:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 16:16:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 16:16:10 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.23703703703703705 on epoch=649
05/29/2022 16:16:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/29/2022 16:16:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 16:16:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 16:16:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
05/29/2022 16:16:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/29/2022 16:16:24 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.301010101010101 on epoch=666
05/29/2022 16:16:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 16:16:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 16:16:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 16:16:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 16:16:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/29/2022 16:16:39 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.3023088023088023 on epoch=683
05/29/2022 16:16:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 16:16:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 16:16:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 16:16:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/29/2022 16:16:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 16:16:53 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.25210084033613445 on epoch=699
05/29/2022 16:16:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=703
05/29/2022 16:16:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 16:17:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/29/2022 16:17:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/29/2022 16:17:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
05/29/2022 16:17:08 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.28764581124072114 on epoch=716
05/29/2022 16:17:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 16:17:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 16:17:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/29/2022 16:17:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 16:17:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 16:17:22 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.30037362271245327 on epoch=733
05/29/2022 16:17:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 16:17:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 16:17:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 16:17:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 16:17:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 16:17:37 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.28587720523204396 on epoch=749
05/29/2022 16:17:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 16:17:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
05/29/2022 16:17:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 16:17:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 16:17:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 16:17:51 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.34047619047619054 on epoch=766
05/29/2022 16:17:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
05/29/2022 16:17:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
05/29/2022 16:17:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 16:18:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/29/2022 16:18:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 16:18:06 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3302768549280177 on epoch=783
05/29/2022 16:18:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 16:18:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
05/29/2022 16:18:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 16:18:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 16:18:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 16:18:21 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2687830687830688 on epoch=799
05/29/2022 16:18:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 16:18:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 16:18:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 16:18:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 16:18:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 16:18:35 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.2681531872008062 on epoch=816
05/29/2022 16:18:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 16:18:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 16:18:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 16:18:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 16:18:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 16:18:50 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.33034276512537386 on epoch=833
05/29/2022 16:18:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 16:18:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 16:18:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 16:19:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 16:19:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 16:19:04 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.33034276512537386 on epoch=849
05/29/2022 16:19:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 16:19:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/29/2022 16:19:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 16:19:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 16:19:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 16:19:19 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.3566051533531209 on epoch=866
05/29/2022 16:19:19 - INFO - __main__ - Saving model with best Classification-F1: 0.34708994708994706 -> 0.3566051533531209 on epoch=866, global_step=2600
05/29/2022 16:19:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 16:19:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/29/2022 16:19:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 16:19:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 16:19:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 16:19:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.23412698412698416 on epoch=883
05/29/2022 16:19:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=886
05/29/2022 16:19:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 16:19:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 16:19:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
05/29/2022 16:19:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
05/29/2022 16:19:48 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.35711975156886844 on epoch=899
05/29/2022 16:19:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3566051533531209 -> 0.35711975156886844 on epoch=899, global_step=2700
05/29/2022 16:19:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 16:19:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 16:19:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 16:19:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 16:20:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 16:20:02 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.28684966272624063 on epoch=916
05/29/2022 16:20:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 16:20:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 16:20:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 16:20:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 16:20:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 16:20:17 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3007082317427145 on epoch=933
05/29/2022 16:20:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 16:20:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 16:20:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 16:20:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 16:20:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 16:20:31 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.3007082317427145 on epoch=949
05/29/2022 16:20:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 16:20:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 16:20:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 16:20:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 16:20:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 16:20:46 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2546142488002953 on epoch=966
05/29/2022 16:20:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 16:20:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 16:20:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 16:20:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 16:20:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 16:21:00 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.286162449703335 on epoch=983
05/29/2022 16:21:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 16:21:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 16:21:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 16:21:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 16:21:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 16:21:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:21:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:21:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:21:15 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.36737303234190616 on epoch=999
05/29/2022 16:21:15 - INFO - __main__ - Saving model with best Classification-F1: 0.35711975156886844 -> 0.36737303234190616 on epoch=999, global_step=3000
05/29/2022 16:21:15 - INFO - __main__ - save last model!
05/29/2022 16:21:15 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:21:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:21:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:21:15 - INFO - __main__ - ['neutral']
05/29/2022 16:21:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:21:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:21:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:21:15 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:21:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:21:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:21:15 - INFO - __main__ - ['entailment']
05/29/2022 16:21:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:21:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:21:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:21:15 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:21:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:21:17 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:21:30 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:21:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:21:31 - INFO - __main__ - Starting training!
05/29/2022 16:21:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_42_0.5_8_predictions.txt
05/29/2022 16:21:47 - INFO - __main__ - Classification-F1 on test data: 0.2515
05/29/2022 16:21:47 - INFO - __main__ - prefix=anli_16_42, lr=0.5, bsz=8, dev_performance=0.36737303234190616, test_performance=0.251541653707008
05/29/2022 16:21:47 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.4, bsz=8 ...
05/29/2022 16:21:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:21:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:21:48 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:21:48 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:21:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:21:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:21:48 - INFO - __main__ - ['neutral']
05/29/2022 16:21:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:21:48 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:21:48 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:22:03 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:22:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:22:04 - INFO - __main__ - Starting training!
05/29/2022 16:22:07 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=3
05/29/2022 16:22:10 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=6
05/29/2022 16:22:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=9
05/29/2022 16:22:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
05/29/2022 16:22:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=16
05/29/2022 16:22:19 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.2690058479532164 on epoch=16
05/29/2022 16:22:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2690058479532164 on epoch=16, global_step=50
05/29/2022 16:22:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
05/29/2022 16:22:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
05/29/2022 16:22:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=26
05/29/2022 16:22:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/29/2022 16:22:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
05/29/2022 16:22:34 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.36680134680134674 on epoch=33
05/29/2022 16:22:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2690058479532164 -> 0.36680134680134674 on epoch=33, global_step=100
05/29/2022 16:22:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=36
05/29/2022 16:22:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=39
05/29/2022 16:22:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
05/29/2022 16:22:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
05/29/2022 16:22:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=49
05/29/2022 16:22:49 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.28787878787878785 on epoch=49
05/29/2022 16:22:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/29/2022 16:22:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
05/29/2022 16:22:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=59
05/29/2022 16:22:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/29/2022 16:23:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
05/29/2022 16:23:03 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.20148148148148146 on epoch=66
05/29/2022 16:23:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=69
05/29/2022 16:23:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
05/29/2022 16:23:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
05/29/2022 16:23:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/29/2022 16:23:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
05/29/2022 16:23:18 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.20097146326654525 on epoch=83
05/29/2022 16:23:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=86
05/29/2022 16:23:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
05/29/2022 16:23:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
05/29/2022 16:23:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
05/29/2022 16:23:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=99
05/29/2022 16:23:32 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.291299117882919 on epoch=99
05/29/2022 16:23:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=103
05/29/2022 16:23:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
05/29/2022 16:23:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=109
05/29/2022 16:23:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=113
05/29/2022 16:23:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
05/29/2022 16:23:47 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.24257071249552453 on epoch=116
05/29/2022 16:23:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=119
05/29/2022 16:23:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=123
05/29/2022 16:23:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
05/29/2022 16:23:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=129
05/29/2022 16:24:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
05/29/2022 16:24:01 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.23741690408357075 on epoch=133
05/29/2022 16:24:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
05/29/2022 16:24:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=139
05/29/2022 16:24:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=143
05/29/2022 16:24:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/29/2022 16:24:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=149
05/29/2022 16:24:16 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.22360649686231082 on epoch=149
05/29/2022 16:24:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
05/29/2022 16:24:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=156
05/29/2022 16:24:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=159
05/29/2022 16:24:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=163
05/29/2022 16:24:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=166
05/29/2022 16:24:31 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.30718623481781376 on epoch=166
05/29/2022 16:24:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=169
05/29/2022 16:24:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
05/29/2022 16:24:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
05/29/2022 16:24:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=179
05/29/2022 16:24:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=183
05/29/2022 16:24:45 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.37149014778325123 on epoch=183
05/29/2022 16:24:45 - INFO - __main__ - Saving model with best Classification-F1: 0.36680134680134674 -> 0.37149014778325123 on epoch=183, global_step=550
05/29/2022 16:24:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=186
05/29/2022 16:24:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=189
05/29/2022 16:24:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=193
05/29/2022 16:24:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=196
05/29/2022 16:24:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=199
05/29/2022 16:25:00 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.3075396825396825 on epoch=199
05/29/2022 16:25:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=203
05/29/2022 16:25:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
05/29/2022 16:25:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
05/29/2022 16:25:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.11 on epoch=213
05/29/2022 16:25:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
05/29/2022 16:25:14 - INFO - __main__ - Global step 650 Train loss 0.14 Classification-F1 0.29785661492978566 on epoch=216
05/29/2022 16:25:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=219
05/29/2022 16:25:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
05/29/2022 16:25:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=226
05/29/2022 16:25:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=229
05/29/2022 16:25:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=233
05/29/2022 16:25:29 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.3094239006906198 on epoch=233
05/29/2022 16:25:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=236
05/29/2022 16:25:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=239
05/29/2022 16:25:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=243
05/29/2022 16:25:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=246
05/29/2022 16:25:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=249
05/29/2022 16:25:43 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.32317170201641315 on epoch=249
05/29/2022 16:25:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=253
05/29/2022 16:25:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=256
05/29/2022 16:25:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=259
05/29/2022 16:25:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=263
05/29/2022 16:25:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=266
05/29/2022 16:25:58 - INFO - __main__ - Global step 800 Train loss 0.09 Classification-F1 0.13907785336356765 on epoch=266
05/29/2022 16:26:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=269
05/29/2022 16:26:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=273
05/29/2022 16:26:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=276
05/29/2022 16:26:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=279
05/29/2022 16:26:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=283
05/29/2022 16:26:13 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.31351981351981345 on epoch=283
05/29/2022 16:26:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=286
05/29/2022 16:26:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=289
05/29/2022 16:26:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=293
05/29/2022 16:26:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=296
05/29/2022 16:26:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=299
05/29/2022 16:26:27 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.31086021505376343 on epoch=299
05/29/2022 16:26:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
05/29/2022 16:26:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
05/29/2022 16:26:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
05/29/2022 16:26:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/29/2022 16:26:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=316
05/29/2022 16:26:42 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.25753968253968257 on epoch=316
05/29/2022 16:26:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=319
05/29/2022 16:26:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
05/29/2022 16:26:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=326
05/29/2022 16:26:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=329
05/29/2022 16:26:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
05/29/2022 16:26:56 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.26349651595093393 on epoch=333
05/29/2022 16:26:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
05/29/2022 16:27:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=339
05/29/2022 16:27:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=343
05/29/2022 16:27:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=346
05/29/2022 16:27:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
05/29/2022 16:27:11 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.28353096179183135 on epoch=349
05/29/2022 16:27:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=353
05/29/2022 16:27:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
05/29/2022 16:27:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/29/2022 16:27:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
05/29/2022 16:27:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
05/29/2022 16:27:25 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.24053030303030307 on epoch=366
05/29/2022 16:27:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
05/29/2022 16:27:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
05/29/2022 16:27:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
05/29/2022 16:27:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
05/29/2022 16:27:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
05/29/2022 16:27:40 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.16678885630498536 on epoch=383
05/29/2022 16:27:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/29/2022 16:27:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
05/29/2022 16:27:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/29/2022 16:27:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
05/29/2022 16:27:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/29/2022 16:27:54 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.1917562724014337 on epoch=399
05/29/2022 16:27:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/29/2022 16:28:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
05/29/2022 16:28:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
05/29/2022 16:28:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
05/29/2022 16:28:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/29/2022 16:28:09 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.1753205128205128 on epoch=416
05/29/2022 16:28:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
05/29/2022 16:28:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
05/29/2022 16:28:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
05/29/2022 16:28:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
05/29/2022 16:28:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
05/29/2022 16:28:23 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.23381642512077294 on epoch=433
05/29/2022 16:28:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/29/2022 16:28:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=439
05/29/2022 16:28:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
05/29/2022 16:28:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/29/2022 16:28:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
05/29/2022 16:28:38 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.20388615216201425 on epoch=449
05/29/2022 16:28:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 16:28:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=456
05/29/2022 16:28:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
05/29/2022 16:28:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
05/29/2022 16:28:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 16:28:52 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.2095298318702574 on epoch=466
05/29/2022 16:28:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
05/29/2022 16:28:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 16:29:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
05/29/2022 16:29:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
05/29/2022 16:29:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/29/2022 16:29:07 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.1942815249266862 on epoch=483
05/29/2022 16:29:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
05/29/2022 16:29:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
05/29/2022 16:29:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=493
05/29/2022 16:29:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
05/29/2022 16:29:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
05/29/2022 16:29:21 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.329405162738496 on epoch=499
05/29/2022 16:29:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
05/29/2022 16:29:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/29/2022 16:29:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 16:29:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 16:29:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 16:29:36 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.32317170201641315 on epoch=516
05/29/2022 16:29:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
05/29/2022 16:29:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 16:29:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
05/29/2022 16:29:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 16:29:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/29/2022 16:29:50 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.22777777777777777 on epoch=533
05/29/2022 16:29:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/29/2022 16:29:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/29/2022 16:29:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
05/29/2022 16:30:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
05/29/2022 16:30:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/29/2022 16:30:05 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.27754773406947325 on epoch=549
05/29/2022 16:30:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/29/2022 16:30:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
05/29/2022 16:30:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
05/29/2022 16:30:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
05/29/2022 16:30:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/29/2022 16:30:19 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.3225340967276451 on epoch=566
05/29/2022 16:30:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/29/2022 16:30:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 16:30:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/29/2022 16:30:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 16:30:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 16:30:34 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.25970973237583833 on epoch=583
05/29/2022 16:30:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 16:30:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 16:30:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=593
05/29/2022 16:30:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
05/29/2022 16:30:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 16:30:48 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3117847914359542 on epoch=599
05/29/2022 16:30:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 16:30:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/29/2022 16:30:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 16:30:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 16:31:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/29/2022 16:31:03 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.32317170201641315 on epoch=616
05/29/2022 16:31:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 16:31:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 16:31:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 16:31:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 16:31:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 16:31:18 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2890083632019116 on epoch=633
05/29/2022 16:31:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/29/2022 16:31:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 16:31:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 16:31:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 16:31:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/29/2022 16:31:32 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.20277777777777778 on epoch=649
05/29/2022 16:31:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/29/2022 16:31:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
05/29/2022 16:31:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 16:31:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
05/29/2022 16:31:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 16:31:47 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2773647957443906 on epoch=666
05/29/2022 16:31:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 16:31:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 16:31:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=676
05/29/2022 16:31:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 16:32:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/29/2022 16:32:01 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2907880649816133 on epoch=683
05/29/2022 16:32:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 16:32:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 16:32:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
05/29/2022 16:32:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
05/29/2022 16:32:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 16:32:16 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.3020348908737482 on epoch=699
05/29/2022 16:32:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 16:32:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 16:32:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 16:32:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/29/2022 16:32:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 16:32:30 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.2907880649816133 on epoch=716
05/29/2022 16:32:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 16:32:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 16:32:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 16:32:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 16:32:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 16:32:45 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.32918313570487484 on epoch=733
05/29/2022 16:32:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 16:32:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 16:32:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 16:32:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
05/29/2022 16:32:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 16:33:00 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2890083632019116 on epoch=749
05/29/2022 16:33:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 16:33:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
05/29/2022 16:33:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
05/29/2022 16:33:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 16:33:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 16:33:14 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.27652410631134033 on epoch=766
05/29/2022 16:33:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 16:33:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/29/2022 16:33:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/29/2022 16:33:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/29/2022 16:33:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 16:33:29 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3532135621897192 on epoch=783
05/29/2022 16:33:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=786
05/29/2022 16:33:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 16:33:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 16:33:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 16:33:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 16:33:44 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.323502886002886 on epoch=799
05/29/2022 16:33:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/29/2022 16:33:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/29/2022 16:33:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 16:33:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/29/2022 16:33:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 16:33:58 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.29095145810262085 on epoch=816
05/29/2022 16:34:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 16:34:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 16:34:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/29/2022 16:34:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/29/2022 16:34:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 16:34:13 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.291005291005291 on epoch=833
05/29/2022 16:34:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 16:34:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/29/2022 16:34:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 16:34:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 16:34:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 16:34:27 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.3020348908737482 on epoch=849
05/29/2022 16:34:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 16:34:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 16:34:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/29/2022 16:34:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 16:34:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 16:34:42 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.3551068376068376 on epoch=866
05/29/2022 16:34:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 16:34:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 16:34:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 16:34:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 16:34:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 16:34:56 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.29095145810262085 on epoch=883
05/29/2022 16:34:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 16:35:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 16:35:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 16:35:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 16:35:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 16:35:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3537248144220573 on epoch=899
05/29/2022 16:35:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 16:35:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
05/29/2022 16:35:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 16:35:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/29/2022 16:35:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 16:35:25 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.23502906976744184 on epoch=916
05/29/2022 16:35:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 16:35:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 16:35:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 16:35:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 16:35:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/29/2022 16:35:40 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.3532135621897192 on epoch=933
05/29/2022 16:35:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/29/2022 16:35:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 16:35:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 16:35:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
05/29/2022 16:35:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 16:35:55 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3229665071770335 on epoch=949
05/29/2022 16:35:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/29/2022 16:36:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 16:36:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
05/29/2022 16:36:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/29/2022 16:36:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 16:36:09 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.29148629148629146 on epoch=966
05/29/2022 16:36:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 16:36:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 16:36:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 16:36:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 16:36:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 16:36:24 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.3117847914359542 on epoch=983
05/29/2022 16:36:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 16:36:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 16:36:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 16:36:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 16:36:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 16:36:38 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:36:38 - INFO - __main__ - Printing 3 examples
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:36:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:36:38 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2907880649816133 on epoch=999
05/29/2022 16:36:38 - INFO - __main__ - save last model!
05/29/2022 16:36:38 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:36:38 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:36:38 - INFO - __main__ - Printing 3 examples
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:36:38 - INFO - __main__ - ['neutral']
05/29/2022 16:36:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:36:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:36:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:36:38 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:36:38 - INFO - __main__ - Printing 3 examples
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:36:38 - INFO - __main__ - ['contradiction']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:36:38 - INFO - __main__ - ['entailment']
05/29/2022 16:36:38 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:36:38 - INFO - __main__ - ['contradiction']
05/29/2022 16:36:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:36:38 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:36:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:36:40 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:36:57 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:36:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:36:58 - INFO - __main__ - Starting training!
05/29/2022 16:37:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_42_0.4_8_predictions.txt
05/29/2022 16:37:10 - INFO - __main__ - Classification-F1 on test data: 0.3303
05/29/2022 16:37:10 - INFO - __main__ - prefix=anli_16_42, lr=0.4, bsz=8, dev_performance=0.37149014778325123, test_performance=0.3302981911677564
05/29/2022 16:37:10 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.3, bsz=8 ...
05/29/2022 16:37:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:37:11 - INFO - __main__ - Printing 3 examples
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:37:11 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:37:11 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:37:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:37:11 - INFO - __main__ - Printing 3 examples
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:37:11 - INFO - __main__ - ['neutral']
05/29/2022 16:37:11 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:37:11 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:37:11 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:37:26 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:37:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:37:27 - INFO - __main__ - Starting training!
05/29/2022 16:37:30 - INFO - __main__ - Step 10 Global step 10 Train loss 0.69 on epoch=3
05/29/2022 16:37:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
05/29/2022 16:37:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=9
05/29/2022 16:37:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
05/29/2022 16:37:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=16
05/29/2022 16:37:42 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.20461538461538464 on epoch=16
05/29/2022 16:37:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20461538461538464 on epoch=16, global_step=50
05/29/2022 16:37:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
05/29/2022 16:37:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/29/2022 16:37:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=26
05/29/2022 16:37:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
05/29/2022 16:37:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/29/2022 16:37:57 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 16:38:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/29/2022 16:38:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=39
05/29/2022 16:38:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
05/29/2022 16:38:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
05/29/2022 16:38:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
05/29/2022 16:38:12 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.1176470588235294 on epoch=49
05/29/2022 16:38:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
05/29/2022 16:38:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
05/29/2022 16:38:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=59
05/29/2022 16:38:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/29/2022 16:38:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=66
05/29/2022 16:38:26 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.21509971509971512 on epoch=66
05/29/2022 16:38:26 - INFO - __main__ - Saving model with best Classification-F1: 0.20461538461538464 -> 0.21509971509971512 on epoch=66, global_step=200
05/29/2022 16:38:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=69
05/29/2022 16:38:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=73
05/29/2022 16:38:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
05/29/2022 16:38:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
05/29/2022 16:38:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=83
05/29/2022 16:38:41 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.23757575757575758 on epoch=83
05/29/2022 16:38:41 - INFO - __main__ - Saving model with best Classification-F1: 0.21509971509971512 -> 0.23757575757575758 on epoch=83, global_step=250
05/29/2022 16:38:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=86
05/29/2022 16:38:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=89
05/29/2022 16:38:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
05/29/2022 16:38:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
05/29/2022 16:38:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
05/29/2022 16:38:55 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.2595242827800967 on epoch=99
05/29/2022 16:38:55 - INFO - __main__ - Saving model with best Classification-F1: 0.23757575757575758 -> 0.2595242827800967 on epoch=99, global_step=300
05/29/2022 16:38:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
05/29/2022 16:39:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
05/29/2022 16:39:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=109
05/29/2022 16:39:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
05/29/2022 16:39:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
05/29/2022 16:39:10 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.2450388265746333 on epoch=116
05/29/2022 16:39:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=119
05/29/2022 16:39:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=123
05/29/2022 16:39:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=126
05/29/2022 16:39:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=129
05/29/2022 16:39:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
05/29/2022 16:39:24 - INFO - __main__ - Global step 400 Train loss 0.34 Classification-F1 0.23809523809523814 on epoch=133
05/29/2022 16:39:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
05/29/2022 16:39:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=139
05/29/2022 16:39:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.30 on epoch=143
05/29/2022 16:39:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=146
05/29/2022 16:39:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=149
05/29/2022 16:39:39 - INFO - __main__ - Global step 450 Train loss 0.30 Classification-F1 0.20229885057471264 on epoch=149
05/29/2022 16:39:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=153
05/29/2022 16:39:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=156
05/29/2022 16:39:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=159
05/29/2022 16:39:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/29/2022 16:39:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=166
05/29/2022 16:39:53 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.19675010979358806 on epoch=166
05/29/2022 16:39:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=169
05/29/2022 16:39:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
05/29/2022 16:40:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
05/29/2022 16:40:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=179
05/29/2022 16:40:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=183
05/29/2022 16:40:08 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.21521942110177406 on epoch=183
05/29/2022 16:40:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=186
05/29/2022 16:40:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=189
05/29/2022 16:40:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=193
05/29/2022 16:40:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=196
05/29/2022 16:40:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=199
05/29/2022 16:40:22 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.22399854250643347 on epoch=199
05/29/2022 16:40:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
05/29/2022 16:40:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=206
05/29/2022 16:40:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=209
05/29/2022 16:40:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=213
05/29/2022 16:40:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=216
05/29/2022 16:40:37 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.275725900116144 on epoch=216
05/29/2022 16:40:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2595242827800967 -> 0.275725900116144 on epoch=216, global_step=650
05/29/2022 16:40:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=219
05/29/2022 16:40:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=223
05/29/2022 16:40:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=226
05/29/2022 16:40:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=229
05/29/2022 16:40:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=233
05/29/2022 16:40:51 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.22810522810522812 on epoch=233
05/29/2022 16:40:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
05/29/2022 16:40:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=239
05/29/2022 16:40:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=243
05/29/2022 16:41:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
05/29/2022 16:41:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
05/29/2022 16:41:06 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.18045815801776532 on epoch=249
05/29/2022 16:41:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
05/29/2022 16:41:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=256
05/29/2022 16:41:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
05/29/2022 16:41:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.13 on epoch=263
05/29/2022 16:41:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
05/29/2022 16:41:21 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.23216638434029738 on epoch=266
05/29/2022 16:41:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
05/29/2022 16:41:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
05/29/2022 16:41:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
05/29/2022 16:41:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=279
05/29/2022 16:41:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=283
05/29/2022 16:41:36 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.27622377622377625 on epoch=283
05/29/2022 16:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.275725900116144 -> 0.27622377622377625 on epoch=283, global_step=850
05/29/2022 16:41:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=286
05/29/2022 16:41:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
05/29/2022 16:41:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
05/29/2022 16:41:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=296
05/29/2022 16:41:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=299
05/29/2022 16:41:51 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.27719023371197277 on epoch=299
05/29/2022 16:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.27622377622377625 -> 0.27719023371197277 on epoch=299, global_step=900
05/29/2022 16:41:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=303
05/29/2022 16:41:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=306
05/29/2022 16:41:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
05/29/2022 16:42:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
05/29/2022 16:42:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=316
05/29/2022 16:42:06 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.23886532343584302 on epoch=316
05/29/2022 16:42:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
05/29/2022 16:42:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
05/29/2022 16:42:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=326
05/29/2022 16:42:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
05/29/2022 16:42:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=333
05/29/2022 16:42:21 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.21184668989547037 on epoch=333
05/29/2022 16:42:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
05/29/2022 16:42:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
05/29/2022 16:42:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
05/29/2022 16:42:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
05/29/2022 16:42:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=349
05/29/2022 16:42:36 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.28587720523204396 on epoch=349
05/29/2022 16:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.27719023371197277 -> 0.28587720523204396 on epoch=349, global_step=1050
05/29/2022 16:42:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=353
05/29/2022 16:42:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
05/29/2022 16:42:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/29/2022 16:42:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
05/29/2022 16:42:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=366
05/29/2022 16:42:51 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.24542124542124535 on epoch=366
05/29/2022 16:42:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
05/29/2022 16:42:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
05/29/2022 16:42:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=376
05/29/2022 16:43:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
05/29/2022 16:43:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
05/29/2022 16:43:06 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.2485343794268279 on epoch=383
05/29/2022 16:43:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/29/2022 16:43:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=389
05/29/2022 16:43:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
05/29/2022 16:43:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
05/29/2022 16:43:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
05/29/2022 16:43:22 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.22810522810522807 on epoch=399
05/29/2022 16:43:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/29/2022 16:43:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=406
05/29/2022 16:43:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=409
05/29/2022 16:43:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/29/2022 16:43:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/29/2022 16:43:37 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.2397831978319783 on epoch=416
05/29/2022 16:43:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
05/29/2022 16:43:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
05/29/2022 16:43:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
05/29/2022 16:43:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
05/29/2022 16:43:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
05/29/2022 16:43:52 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.2681818181818182 on epoch=433
05/29/2022 16:43:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/29/2022 16:43:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
05/29/2022 16:44:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
05/29/2022 16:44:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=446
05/29/2022 16:44:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
05/29/2022 16:44:07 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.23987854251012145 on epoch=449
05/29/2022 16:44:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 16:44:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 16:44:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
05/29/2022 16:44:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/29/2022 16:44:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 16:44:22 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.27154058629599925 on epoch=466
05/29/2022 16:44:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/29/2022 16:44:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 16:44:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=476
05/29/2022 16:44:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
05/29/2022 16:44:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/29/2022 16:44:37 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.209980982359499 on epoch=483
05/29/2022 16:44:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=486
05/29/2022 16:44:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/29/2022 16:44:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 16:44:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/29/2022 16:44:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
05/29/2022 16:44:52 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.22213675213675213 on epoch=499
05/29/2022 16:44:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
05/29/2022 16:44:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/29/2022 16:45:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 16:45:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
05/29/2022 16:45:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
05/29/2022 16:45:07 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.2832722832722833 on epoch=516
05/29/2022 16:45:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
05/29/2022 16:45:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
05/29/2022 16:45:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
05/29/2022 16:45:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
05/29/2022 16:45:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/29/2022 16:45:23 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.25555555555555554 on epoch=533
05/29/2022 16:45:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
05/29/2022 16:45:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
05/29/2022 16:45:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
05/29/2022 16:45:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/29/2022 16:45:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
05/29/2022 16:45:38 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.2595242827800967 on epoch=549
05/29/2022 16:45:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
05/29/2022 16:45:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/29/2022 16:45:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/29/2022 16:45:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
05/29/2022 16:45:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
05/29/2022 16:45:53 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.28585858585858587 on epoch=566
05/29/2022 16:45:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/29/2022 16:45:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/29/2022 16:46:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/29/2022 16:46:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 16:46:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/29/2022 16:46:09 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2397831978319783 on epoch=583
05/29/2022 16:46:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 16:46:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 16:46:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/29/2022 16:46:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=596
05/29/2022 16:46:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/29/2022 16:46:24 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.2658119658119658 on epoch=599
05/29/2022 16:46:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/29/2022 16:46:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 16:46:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 16:46:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=613
05/29/2022 16:46:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 16:46:38 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.22611812373383203 on epoch=616
05/29/2022 16:46:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 16:46:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 16:46:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 16:46:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 16:46:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=633
05/29/2022 16:46:53 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.21136025876387796 on epoch=633
05/29/2022 16:46:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/29/2022 16:46:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 16:47:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 16:47:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/29/2022 16:47:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 16:47:08 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.2119047619047619 on epoch=649
05/29/2022 16:47:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/29/2022 16:47:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 16:47:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 16:47:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/29/2022 16:47:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/29/2022 16:47:23 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.2560215053763441 on epoch=666
05/29/2022 16:47:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 16:47:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 16:47:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 16:47:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
05/29/2022 16:47:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/29/2022 16:47:38 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.22350427350427352 on epoch=683
05/29/2022 16:47:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/29/2022 16:47:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 16:47:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 16:47:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 16:47:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 16:47:52 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.2397831978319783 on epoch=699
05/29/2022 16:47:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 16:47:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
05/29/2022 16:48:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 16:48:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/29/2022 16:48:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/29/2022 16:48:07 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.2397831978319783 on epoch=716
05/29/2022 16:48:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 16:48:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 16:48:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/29/2022 16:48:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/29/2022 16:48:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 16:48:22 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.15268065268065267 on epoch=733
05/29/2022 16:48:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 16:48:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 16:48:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=743
05/29/2022 16:48:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 16:48:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 16:48:36 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.16678885630498536 on epoch=749
05/29/2022 16:48:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 16:48:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 16:48:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 16:48:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 16:48:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/29/2022 16:48:51 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.2403555434313124 on epoch=766
05/29/2022 16:48:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/29/2022 16:48:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/29/2022 16:48:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/29/2022 16:49:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 16:49:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 16:49:05 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.21136025876387796 on epoch=783
05/29/2022 16:49:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 16:49:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 16:49:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 16:49:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 16:49:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 16:49:20 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.21935483870967745 on epoch=799
05/29/2022 16:49:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/29/2022 16:49:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 16:49:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/29/2022 16:49:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/29/2022 16:49:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 16:49:34 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.13768686073957515 on epoch=816
05/29/2022 16:49:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 16:49:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 16:49:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 16:49:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 16:49:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 16:49:48 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.13768686073957515 on epoch=833
05/29/2022 16:49:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 16:49:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 16:49:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=843
05/29/2022 16:49:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 16:50:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 16:50:03 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.2658119658119658 on epoch=849
05/29/2022 16:50:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 16:50:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=856
05/29/2022 16:50:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/29/2022 16:50:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 16:50:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 16:50:17 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.22213675213675213 on epoch=866
05/29/2022 16:50:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 16:50:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 16:50:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/29/2022 16:50:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 16:50:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 16:50:32 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.24238224238224237 on epoch=883
05/29/2022 16:50:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 16:50:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 16:50:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 16:50:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 16:50:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 16:50:47 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.20987981618946625 on epoch=899
05/29/2022 16:50:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 16:50:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
05/29/2022 16:50:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 16:50:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 16:51:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 16:51:02 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.21425796425796426 on epoch=916
05/29/2022 16:51:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 16:51:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/29/2022 16:51:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 16:51:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 16:51:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 16:51:16 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2299956082564778 on epoch=933
05/29/2022 16:51:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 16:51:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/29/2022 16:51:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 16:51:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 16:51:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 16:51:31 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.2469733121244256 on epoch=949
05/29/2022 16:51:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 16:51:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
05/29/2022 16:51:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 16:51:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 16:51:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 16:51:46 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.2397831978319783 on epoch=966
05/29/2022 16:51:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 16:51:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 16:51:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 16:51:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 16:51:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 16:52:00 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2541311734860123 on epoch=983
05/29/2022 16:52:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/29/2022 16:52:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/29/2022 16:52:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/29/2022 16:52:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 16:52:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 16:52:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:52:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:52:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:52:15 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:52:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:52:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:52:15 - INFO - __main__ - ['neutral']
05/29/2022 16:52:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:52:15 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.28585858585858587 on epoch=999
05/29/2022 16:52:15 - INFO - __main__ - save last model!
05/29/2022 16:52:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:52:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 16:52:15 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 16:52:15 - INFO - __main__ - Printing 3 examples
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 16:52:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 16:52:15 - INFO - __main__ - ['entailment']
05/29/2022 16:52:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 16:52:15 - INFO - __main__ - ['contradiction']
05/29/2022 16:52:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:52:15 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:52:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:52:17 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 16:52:34 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:52:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:52:35 - INFO - __main__ - Starting training!
05/29/2022 16:52:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_42_0.3_8_predictions.txt
05/29/2022 16:52:47 - INFO - __main__ - Classification-F1 on test data: 0.2523
05/29/2022 16:52:47 - INFO - __main__ - prefix=anli_16_42, lr=0.3, bsz=8, dev_performance=0.28587720523204396, test_performance=0.2523417287922689
05/29/2022 16:52:47 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.2, bsz=8 ...
05/29/2022 16:52:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:52:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:52:48 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:52:48 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 16:52:48 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 16:52:48 - INFO - __main__ - Printing 3 examples
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/29/2022 16:52:48 - INFO - __main__ - ['neutral']
05/29/2022 16:52:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 16:52:48 - INFO - __main__ - Tokenizing Output ...
05/29/2022 16:52:48 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 16:53:07 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 16:53:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 16:53:08 - INFO - __main__ - Starting training!
05/29/2022 16:53:11 - INFO - __main__ - Step 10 Global step 10 Train loss 0.71 on epoch=3
05/29/2022 16:53:14 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=6
05/29/2022 16:53:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
05/29/2022 16:53:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=13
05/29/2022 16:53:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=16
05/29/2022 16:53:23 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 16:53:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 16:53:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/29/2022 16:53:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=23
05/29/2022 16:53:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
05/29/2022 16:53:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
05/29/2022 16:53:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/29/2022 16:53:38 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/29/2022 16:53:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/29/2022 16:53:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
05/29/2022 16:53:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
05/29/2022 16:53:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
05/29/2022 16:53:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
05/29/2022 16:53:53 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.2918735518040384 on epoch=49
05/29/2022 16:53:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2918735518040384 on epoch=49, global_step=150
05/29/2022 16:53:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=53
05/29/2022 16:53:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=56
05/29/2022 16:54:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
05/29/2022 16:54:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=63
05/29/2022 16:54:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
05/29/2022 16:54:08 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.29147422625683495 on epoch=66
05/29/2022 16:54:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/29/2022 16:54:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/29/2022 16:54:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=76
05/29/2022 16:54:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=79
05/29/2022 16:54:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=83
05/29/2022 16:54:23 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.2450388265746333 on epoch=83
05/29/2022 16:54:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
05/29/2022 16:54:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=89
05/29/2022 16:54:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=93
05/29/2022 16:54:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
05/29/2022 16:54:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
05/29/2022 16:54:37 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.26936444885799404 on epoch=99
05/29/2022 16:54:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/29/2022 16:54:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
05/29/2022 16:54:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
05/29/2022 16:54:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
05/29/2022 16:54:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=116
05/29/2022 16:54:52 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.22786647314949202 on epoch=116
05/29/2022 16:54:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
05/29/2022 16:54:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
05/29/2022 16:55:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
05/29/2022 16:55:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
05/29/2022 16:55:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=133
05/29/2022 16:55:07 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.22501747030048916 on epoch=133
05/29/2022 16:55:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
05/29/2022 16:55:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=139
05/29/2022 16:55:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
05/29/2022 16:55:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=146
05/29/2022 16:55:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=149
05/29/2022 16:55:21 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.23241106719367588 on epoch=149
05/29/2022 16:55:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=153
05/29/2022 16:55:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
05/29/2022 16:55:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
05/29/2022 16:55:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=163
05/29/2022 16:55:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=166
05/29/2022 16:55:36 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.23757575757575758 on epoch=166
05/29/2022 16:55:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=169
05/29/2022 16:55:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=173
05/29/2022 16:55:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=176
05/29/2022 16:55:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
05/29/2022 16:55:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=183
05/29/2022 16:55:50 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.25703846980442724 on epoch=183
05/29/2022 16:55:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=186
05/29/2022 16:55:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=189
05/29/2022 16:55:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
05/29/2022 16:56:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=196
05/29/2022 16:56:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
05/29/2022 16:56:05 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.21836477987421388 on epoch=199
05/29/2022 16:56:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=203
05/29/2022 16:56:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=206
05/29/2022 16:56:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=209
05/29/2022 16:56:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
05/29/2022 16:56:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=216
05/29/2022 16:56:20 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.2021164021164021 on epoch=216
05/29/2022 16:56:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=219
05/29/2022 16:56:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=223
05/29/2022 16:56:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
05/29/2022 16:56:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=229
05/29/2022 16:56:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=233
05/29/2022 16:56:34 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.22814814814814813 on epoch=233
05/29/2022 16:56:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.18 on epoch=236
05/29/2022 16:56:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=239
05/29/2022 16:56:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=243
05/29/2022 16:56:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=246
05/29/2022 16:56:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=249
05/29/2022 16:56:49 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.20476190476190478 on epoch=249
05/29/2022 16:56:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=253
05/29/2022 16:56:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=256
05/29/2022 16:56:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=259
05/29/2022 16:56:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=263
05/29/2022 16:57:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
05/29/2022 16:57:04 - INFO - __main__ - Global step 800 Train loss 0.17 Classification-F1 0.21376811594202896 on epoch=266
05/29/2022 16:57:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=269
05/29/2022 16:57:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=273
05/29/2022 16:57:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
05/29/2022 16:57:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=279
05/29/2022 16:57:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=283
05/29/2022 16:57:18 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.2074074074074074 on epoch=283
05/29/2022 16:57:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=286
05/29/2022 16:57:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
05/29/2022 16:57:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=293
05/29/2022 16:57:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
05/29/2022 16:57:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=299
05/29/2022 16:57:33 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.20501993901008678 on epoch=299
05/29/2022 16:57:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=303
05/29/2022 16:57:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=306
05/29/2022 16:57:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=309
05/29/2022 16:57:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
05/29/2022 16:57:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=316
05/29/2022 16:57:48 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.22995842504279776 on epoch=316
05/29/2022 16:57:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
05/29/2022 16:57:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=323
05/29/2022 16:57:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=326
05/29/2022 16:57:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
05/29/2022 16:58:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
05/29/2022 16:58:02 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.18490374873353596 on epoch=333
05/29/2022 16:58:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
05/29/2022 16:58:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=339
05/29/2022 16:58:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
05/29/2022 16:58:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=346
05/29/2022 16:58:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=349
05/29/2022 16:58:17 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.191414496833216 on epoch=349
05/29/2022 16:58:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
05/29/2022 16:58:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/29/2022 16:58:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
05/29/2022 16:58:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
05/29/2022 16:58:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=366
05/29/2022 16:58:32 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.1939655172413793 on epoch=366
05/29/2022 16:58:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
05/29/2022 16:58:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
05/29/2022 16:58:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
05/29/2022 16:58:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=379
05/29/2022 16:58:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
05/29/2022 16:58:47 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.22346891070593736 on epoch=383
05/29/2022 16:58:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
05/29/2022 16:58:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=389
05/29/2022 16:58:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
05/29/2022 16:58:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
05/29/2022 16:59:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
05/29/2022 16:59:01 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.21678885630498534 on epoch=399
05/29/2022 16:59:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
05/29/2022 16:59:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
05/29/2022 16:59:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
05/29/2022 16:59:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
05/29/2022 16:59:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=416
05/29/2022 16:59:16 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.18907563025210083 on epoch=416
05/29/2022 16:59:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
05/29/2022 16:59:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
05/29/2022 16:59:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=426
05/29/2022 16:59:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
05/29/2022 16:59:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
05/29/2022 16:59:31 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.24358130348913756 on epoch=433
05/29/2022 16:59:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/29/2022 16:59:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=439
05/29/2022 16:59:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/29/2022 16:59:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
05/29/2022 16:59:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
05/29/2022 16:59:45 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.31746031746031744 on epoch=449
05/29/2022 16:59:45 - INFO - __main__ - Saving model with best Classification-F1: 0.2918735518040384 -> 0.31746031746031744 on epoch=449, global_step=1350
05/29/2022 16:59:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
05/29/2022 16:59:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 16:59:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/29/2022 16:59:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=463
05/29/2022 16:59:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 17:00:00 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.2656084656084656 on epoch=466
05/29/2022 17:00:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/29/2022 17:00:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
05/29/2022 17:00:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
05/29/2022 17:00:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
05/29/2022 17:00:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
05/29/2022 17:00:15 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.2708994708994709 on epoch=483
05/29/2022 17:00:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
05/29/2022 17:00:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
05/29/2022 17:00:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 17:00:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 17:00:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
05/29/2022 17:00:30 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.32867883995703545 on epoch=499
05/29/2022 17:00:30 - INFO - __main__ - Saving model with best Classification-F1: 0.31746031746031744 -> 0.32867883995703545 on epoch=499, global_step=1500
05/29/2022 17:00:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
05/29/2022 17:00:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
05/29/2022 17:00:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
05/29/2022 17:00:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 17:00:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/29/2022 17:00:44 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2921639500784287 on epoch=516
05/29/2022 17:00:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/29/2022 17:00:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 17:00:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 17:00:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=529
05/29/2022 17:00:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/29/2022 17:00:59 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2830651340996169 on epoch=533
05/29/2022 17:01:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
05/29/2022 17:01:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
05/29/2022 17:01:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
05/29/2022 17:01:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
05/29/2022 17:01:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/29/2022 17:01:14 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.36630036630036633 on epoch=549
05/29/2022 17:01:14 - INFO - __main__ - Saving model with best Classification-F1: 0.32867883995703545 -> 0.36630036630036633 on epoch=549, global_step=1650
05/29/2022 17:01:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/29/2022 17:01:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
05/29/2022 17:01:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 17:01:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/29/2022 17:01:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/29/2022 17:01:30 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.34974529346622374 on epoch=566
05/29/2022 17:01:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/29/2022 17:01:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 17:01:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 17:01:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 17:01:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/29/2022 17:01:45 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3154320987654321 on epoch=583
05/29/2022 17:01:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 17:01:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
05/29/2022 17:01:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=593
05/29/2022 17:01:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 17:01:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
05/29/2022 17:02:00 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.22596075699523976 on epoch=599
05/29/2022 17:02:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
05/29/2022 17:02:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
05/29/2022 17:02:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 17:02:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 17:02:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/29/2022 17:02:15 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.28247863247863253 on epoch=616
05/29/2022 17:02:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 17:02:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 17:02:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/29/2022 17:02:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=629
05/29/2022 17:02:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/29/2022 17:02:30 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.2560433604336043 on epoch=633
05/29/2022 17:02:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
05/29/2022 17:02:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/29/2022 17:02:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 17:02:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 17:02:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/29/2022 17:02:45 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.27419354838709675 on epoch=649
05/29/2022 17:02:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/29/2022 17:02:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 17:02:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
05/29/2022 17:02:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
05/29/2022 17:02:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 17:03:00 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.25555555555555554 on epoch=666
05/29/2022 17:03:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 17:03:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 17:03:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 17:03:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=679
05/29/2022 17:03:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
05/29/2022 17:03:16 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.3041935483870968 on epoch=683
05/29/2022 17:03:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/29/2022 17:03:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 17:03:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/29/2022 17:03:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/29/2022 17:03:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 17:03:31 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2924820231973107 on epoch=699
05/29/2022 17:03:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 17:03:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/29/2022 17:03:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/29/2022 17:03:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
05/29/2022 17:03:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 17:03:46 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.20736434108527133 on epoch=716
05/29/2022 17:03:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 17:03:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 17:03:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 17:03:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/29/2022 17:04:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 17:04:01 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.22222222222222224 on epoch=733
05/29/2022 17:04:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 17:04:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 17:04:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 17:04:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 17:04:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/29/2022 17:04:16 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2708909111335805 on epoch=749
05/29/2022 17:04:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/29/2022 17:04:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 17:04:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 17:04:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
05/29/2022 17:04:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
05/29/2022 17:04:32 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.2170949409016422 on epoch=766
05/29/2022 17:04:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/29/2022 17:04:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/29/2022 17:04:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
05/29/2022 17:04:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 17:04:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/29/2022 17:04:47 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.19396551724137934 on epoch=783
05/29/2022 17:04:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/29/2022 17:04:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 17:04:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 17:04:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/29/2022 17:05:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 17:05:02 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.22231182795698923 on epoch=799
05/29/2022 17:05:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 17:05:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 17:05:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 17:05:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 17:05:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 17:05:17 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.22231182795698923 on epoch=816
05/29/2022 17:05:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 17:05:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/29/2022 17:05:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 17:05:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/29/2022 17:05:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 17:05:31 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3478943519593926 on epoch=833
05/29/2022 17:05:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 17:05:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 17:05:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 17:05:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 17:05:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 17:05:46 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.3041080783016267 on epoch=849
05/29/2022 17:05:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=853
05/29/2022 17:05:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 17:05:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/29/2022 17:05:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 17:05:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 17:06:01 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3041080783016267 on epoch=866
05/29/2022 17:06:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
05/29/2022 17:06:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 17:06:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 17:06:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 17:06:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 17:06:15 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.32086021505376344 on epoch=883
05/29/2022 17:06:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 17:06:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/29/2022 17:06:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 17:06:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 17:06:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 17:06:30 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.269431643625192 on epoch=899
05/29/2022 17:06:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 17:06:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 17:06:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/29/2022 17:06:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 17:06:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 17:06:45 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.2775268817204301 on epoch=916
05/29/2022 17:06:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 17:06:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 17:06:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=926
05/29/2022 17:06:56 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=929
05/29/2022 17:06:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 17:07:00 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.32086021505376344 on epoch=933
05/29/2022 17:07:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/29/2022 17:07:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/29/2022 17:07:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/29/2022 17:07:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 17:07:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 17:07:14 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.28654244306418225 on epoch=949
05/29/2022 17:07:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 17:07:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 17:07:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 17:07:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 17:07:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 17:07:29 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2708909111335805 on epoch=966
05/29/2022 17:07:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 17:07:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 17:07:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 17:07:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/29/2022 17:07:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/29/2022 17:07:44 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3041080783016267 on epoch=983
05/29/2022 17:07:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/29/2022 17:07:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 17:07:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 17:07:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 17:07:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 17:07:59 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:07:59 - INFO - __main__ - Printing 3 examples
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:07:59 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.32091880341880347 on epoch=999
05/29/2022 17:07:59 - INFO - __main__ - save last model!
05/29/2022 17:07:59 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:07:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:07:59 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:07:59 - INFO - __main__ - Printing 3 examples
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:07:59 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:07:59 - INFO - __main__ - Printing 3 examples
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:07:59 - INFO - __main__ - ['entailment']
05/29/2022 17:07:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:07:59 - INFO - __main__ - ['contradiction']
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:07:59 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:07:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:08:00 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:08:14 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:08:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:08:15 - INFO - __main__ - Starting training!
05/29/2022 17:08:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_42_0.2_8_predictions.txt
05/29/2022 17:08:30 - INFO - __main__ - Classification-F1 on test data: 0.3534
05/29/2022 17:08:31 - INFO - __main__ - prefix=anli_16_42, lr=0.2, bsz=8, dev_performance=0.36630036630036633, test_performance=0.3534487771764961
05/29/2022 17:08:31 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.5, bsz=8 ...
05/29/2022 17:08:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:08:32 - INFO - __main__ - Printing 3 examples
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:08:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:08:32 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:08:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:08:32 - INFO - __main__ - Printing 3 examples
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:08:32 - INFO - __main__ - ['contradiction']
05/29/2022 17:08:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:08:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:08:32 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:08:47 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:08:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:08:48 - INFO - __main__ - Starting training!
05/29/2022 17:08:51 - INFO - __main__ - Step 10 Global step 10 Train loss 0.56 on epoch=3
05/29/2022 17:08:54 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
05/29/2022 17:08:57 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=9
05/29/2022 17:08:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=13
05/29/2022 17:09:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
05/29/2022 17:09:04 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.27777777777777773 on epoch=16
05/29/2022 17:09:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27777777777777773 on epoch=16, global_step=50
05/29/2022 17:09:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=19
05/29/2022 17:09:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=23
05/29/2022 17:09:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
05/29/2022 17:09:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=29
05/29/2022 17:09:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
05/29/2022 17:09:18 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.21245421245421248 on epoch=33
05/29/2022 17:09:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
05/29/2022 17:09:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
05/29/2022 17:09:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 17:09:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=46
05/29/2022 17:09:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/29/2022 17:09:33 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.28684966272624063 on epoch=49
05/29/2022 17:09:33 - INFO - __main__ - Saving model with best Classification-F1: 0.27777777777777773 -> 0.28684966272624063 on epoch=49, global_step=150
05/29/2022 17:09:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=53
05/29/2022 17:09:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/29/2022 17:09:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/29/2022 17:09:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=63
05/29/2022 17:09:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=66
05/29/2022 17:09:48 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.17068093963722863 on epoch=66
05/29/2022 17:09:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 17:09:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
05/29/2022 17:09:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
05/29/2022 17:09:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=79
05/29/2022 17:10:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=83
05/29/2022 17:10:02 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.35399976779287123 on epoch=83
05/29/2022 17:10:02 - INFO - __main__ - Saving model with best Classification-F1: 0.28684966272624063 -> 0.35399976779287123 on epoch=83, global_step=250
05/29/2022 17:10:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
05/29/2022 17:10:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=89
05/29/2022 17:10:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=93
05/29/2022 17:10:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=96
05/29/2022 17:10:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=99
05/29/2022 17:10:18 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.22222222222222224 on epoch=99
05/29/2022 17:10:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
05/29/2022 17:10:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
05/29/2022 17:10:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=109
05/29/2022 17:10:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=113
05/29/2022 17:10:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=116
05/29/2022 17:10:32 - INFO - __main__ - Global step 350 Train loss 0.34 Classification-F1 0.22730739893211283 on epoch=116
05/29/2022 17:10:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=119
05/29/2022 17:10:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
05/29/2022 17:10:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=126
05/29/2022 17:10:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=129
05/29/2022 17:10:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=133
05/29/2022 17:10:47 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.20142309797482214 on epoch=133
05/29/2022 17:10:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=136
05/29/2022 17:10:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=139
05/29/2022 17:10:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=143
05/29/2022 17:10:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=146
05/29/2022 17:11:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=149
05/29/2022 17:11:02 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.29998666133119917 on epoch=149
05/29/2022 17:11:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=153
05/29/2022 17:11:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=156
05/29/2022 17:11:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=159
05/29/2022 17:11:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=163
05/29/2022 17:11:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=166
05/29/2022 17:11:16 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.2945326278659612 on epoch=166
05/29/2022 17:11:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=169
05/29/2022 17:11:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=173
05/29/2022 17:11:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=176
05/29/2022 17:11:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=179
05/29/2022 17:11:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=183
05/29/2022 17:11:31 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3099020899532408 on epoch=183
05/29/2022 17:11:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
05/29/2022 17:11:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=189
05/29/2022 17:11:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=193
05/29/2022 17:11:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=196
05/29/2022 17:11:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
05/29/2022 17:11:45 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.22222222222222224 on epoch=199
05/29/2022 17:11:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=203
05/29/2022 17:11:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=206
05/29/2022 17:11:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.10 on epoch=209
05/29/2022 17:11:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=213
05/29/2022 17:11:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
05/29/2022 17:12:00 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.2333822450476889 on epoch=216
05/29/2022 17:12:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=219
05/29/2022 17:12:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=223
05/29/2022 17:12:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=226
05/29/2022 17:12:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=229
05/29/2022 17:12:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
05/29/2022 17:12:14 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.3280201143290875 on epoch=233
05/29/2022 17:12:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=236
05/29/2022 17:12:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.12 on epoch=239
05/29/2022 17:12:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
05/29/2022 17:12:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=246
05/29/2022 17:12:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
05/29/2022 17:12:29 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.32452265930526797 on epoch=249
05/29/2022 17:12:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
05/29/2022 17:12:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
05/29/2022 17:12:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=259
05/29/2022 17:12:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=263
05/29/2022 17:12:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=266
05/29/2022 17:12:44 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.2845636102835705 on epoch=266
05/29/2022 17:12:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/29/2022 17:12:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=273
05/29/2022 17:12:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=276
05/29/2022 17:12:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.07 on epoch=279
05/29/2022 17:12:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=283
05/29/2022 17:12:58 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.19185185185185186 on epoch=283
05/29/2022 17:13:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
05/29/2022 17:13:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=289
05/29/2022 17:13:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
05/29/2022 17:13:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=296
05/29/2022 17:13:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/29/2022 17:13:13 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.18883723029701183 on epoch=299
05/29/2022 17:13:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
05/29/2022 17:13:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=306
05/29/2022 17:13:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=309
05/29/2022 17:13:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=313
05/29/2022 17:13:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=316
05/29/2022 17:13:27 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.2003968253968254 on epoch=316
05/29/2022 17:13:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
05/29/2022 17:13:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=323
05/29/2022 17:13:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
05/29/2022 17:13:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=329
05/29/2022 17:13:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=333
05/29/2022 17:13:42 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.1992063492063492 on epoch=333
05/29/2022 17:13:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
05/29/2022 17:13:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=339
05/29/2022 17:13:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
05/29/2022 17:13:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=346
05/29/2022 17:13:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
05/29/2022 17:13:57 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.29254079254079257 on epoch=349
05/29/2022 17:13:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=353
05/29/2022 17:14:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=356
05/29/2022 17:14:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=359
05/29/2022 17:14:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
05/29/2022 17:14:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
05/29/2022 17:14:11 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.281569316351925 on epoch=366
05/29/2022 17:14:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
05/29/2022 17:14:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
05/29/2022 17:14:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
05/29/2022 17:14:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
05/29/2022 17:14:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=383
05/29/2022 17:14:26 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.22493601869366858 on epoch=383
05/29/2022 17:14:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=386
05/29/2022 17:14:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
05/29/2022 17:14:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/29/2022 17:14:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
05/29/2022 17:14:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
05/29/2022 17:14:40 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.23095238095238094 on epoch=399
05/29/2022 17:14:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
05/29/2022 17:14:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
05/29/2022 17:14:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=409
05/29/2022 17:14:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
05/29/2022 17:14:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/29/2022 17:14:55 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.3809012106884447 on epoch=416
05/29/2022 17:14:55 - INFO - __main__ - Saving model with best Classification-F1: 0.35399976779287123 -> 0.3809012106884447 on epoch=416, global_step=1250
05/29/2022 17:14:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
05/29/2022 17:15:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
05/29/2022 17:15:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
05/29/2022 17:15:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
05/29/2022 17:15:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
05/29/2022 17:15:10 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.3232202580028667 on epoch=433
05/29/2022 17:15:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
05/29/2022 17:15:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/29/2022 17:15:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
05/29/2022 17:15:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
05/29/2022 17:15:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
05/29/2022 17:15:24 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.32284382284382285 on epoch=449
05/29/2022 17:15:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/29/2022 17:15:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
05/29/2022 17:15:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=459
05/29/2022 17:15:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=463
05/29/2022 17:15:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
05/29/2022 17:15:39 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.23524531024531026 on epoch=466
05/29/2022 17:15:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=469
05/29/2022 17:15:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=473
05/29/2022 17:15:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/29/2022 17:15:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
05/29/2022 17:15:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
05/29/2022 17:15:53 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.37646464646464645 on epoch=483
05/29/2022 17:15:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=486
05/29/2022 17:15:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/29/2022 17:16:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/29/2022 17:16:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
05/29/2022 17:16:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
05/29/2022 17:16:08 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.334548024470505 on epoch=499
05/29/2022 17:16:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
05/29/2022 17:16:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/29/2022 17:16:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/29/2022 17:16:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
05/29/2022 17:16:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=516
05/29/2022 17:16:23 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.2568027210884354 on epoch=516
05/29/2022 17:16:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/29/2022 17:16:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
05/29/2022 17:16:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/29/2022 17:16:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
05/29/2022 17:16:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=533
05/29/2022 17:16:38 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.32839506172839505 on epoch=533
05/29/2022 17:16:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/29/2022 17:16:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
05/29/2022 17:16:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 17:16:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
05/29/2022 17:16:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
05/29/2022 17:16:52 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.28432760364004045 on epoch=549
05/29/2022 17:16:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/29/2022 17:16:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/29/2022 17:17:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/29/2022 17:17:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
05/29/2022 17:17:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/29/2022 17:17:07 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.29276593427536823 on epoch=566
05/29/2022 17:17:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/29/2022 17:17:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 17:17:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 17:17:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
05/29/2022 17:17:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/29/2022 17:17:21 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3336301793444651 on epoch=583
05/29/2022 17:17:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/29/2022 17:17:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 17:17:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/29/2022 17:17:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/29/2022 17:17:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/29/2022 17:17:36 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.32634032634032634 on epoch=599
05/29/2022 17:17:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 17:17:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/29/2022 17:17:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/29/2022 17:17:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/29/2022 17:17:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
05/29/2022 17:17:51 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.3451422436929683 on epoch=616
05/29/2022 17:17:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/29/2022 17:17:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/29/2022 17:17:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 17:18:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/29/2022 17:18:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=633
05/29/2022 17:18:05 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.23281963669328126 on epoch=633
05/29/2022 17:18:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/29/2022 17:18:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/29/2022 17:18:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/29/2022 17:18:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
05/29/2022 17:18:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 17:18:20 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.3114973262032086 on epoch=649
05/29/2022 17:18:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=653
05/29/2022 17:18:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 17:18:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/29/2022 17:18:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
05/29/2022 17:18:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/29/2022 17:18:34 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.25865463494667756 on epoch=666
05/29/2022 17:18:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/29/2022 17:18:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/29/2022 17:18:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/29/2022 17:18:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 17:18:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/29/2022 17:18:49 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.243399209486166 on epoch=683
05/29/2022 17:18:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=686
05/29/2022 17:18:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=689
05/29/2022 17:18:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
05/29/2022 17:19:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 17:19:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 17:19:04 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.3085845668965873 on epoch=699
05/29/2022 17:19:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/29/2022 17:19:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 17:19:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 17:19:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
05/29/2022 17:19:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 17:19:18 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.29099290780141845 on epoch=716
05/29/2022 17:19:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 17:19:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 17:19:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 17:19:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 17:19:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/29/2022 17:19:33 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.3231111716868838 on epoch=733
05/29/2022 17:19:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 17:19:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/29/2022 17:19:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 17:19:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 17:19:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 17:19:47 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.3006172839506173 on epoch=749
05/29/2022 17:19:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/29/2022 17:19:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 17:19:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/29/2022 17:19:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
05/29/2022 17:20:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
05/29/2022 17:20:02 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.3278410583375122 on epoch=766
05/29/2022 17:20:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/29/2022 17:20:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/29/2022 17:20:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/29/2022 17:20:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/29/2022 17:20:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 17:20:17 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.35331088664422 on epoch=783
05/29/2022 17:20:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/29/2022 17:20:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/29/2022 17:20:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 17:20:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 17:20:30 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/29/2022 17:20:31 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.3451422436929683 on epoch=799
05/29/2022 17:20:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 17:20:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 17:20:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 17:20:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
05/29/2022 17:20:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/29/2022 17:20:46 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.34956521739130436 on epoch=816
05/29/2022 17:20:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 17:20:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 17:20:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=826
05/29/2022 17:20:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/29/2022 17:21:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
05/29/2022 17:21:01 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.2733395004625347 on epoch=833
05/29/2022 17:21:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 17:21:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 17:21:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 17:21:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/29/2022 17:21:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 17:21:16 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.35064196956780075 on epoch=849
05/29/2022 17:21:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/29/2022 17:21:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/29/2022 17:21:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/29/2022 17:21:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/29/2022 17:21:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 17:21:31 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.350140056022409 on epoch=866
05/29/2022 17:21:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 17:21:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 17:21:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 17:21:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=879
05/29/2022 17:21:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/29/2022 17:21:46 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.3441142191142191 on epoch=883
05/29/2022 17:21:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/29/2022 17:21:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 17:21:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/29/2022 17:21:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/29/2022 17:21:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 17:22:01 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.27680851063829787 on epoch=899
05/29/2022 17:22:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=903
05/29/2022 17:22:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 17:22:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/29/2022 17:22:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/29/2022 17:22:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 17:22:15 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.3428212244123438 on epoch=916
05/29/2022 17:22:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 17:22:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/29/2022 17:22:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 17:22:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/29/2022 17:22:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/29/2022 17:22:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2198639455782313 on epoch=933
05/29/2022 17:22:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/29/2022 17:22:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 17:22:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/29/2022 17:22:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 17:22:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/29/2022 17:22:45 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.22032608695652173 on epoch=949
05/29/2022 17:22:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 17:22:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
05/29/2022 17:22:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
05/29/2022 17:22:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 17:22:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/29/2022 17:23:00 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.2088888888888889 on epoch=966
05/29/2022 17:23:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 17:23:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 17:23:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 17:23:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/29/2022 17:23:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 17:23:15 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.23247559893522624 on epoch=983
05/29/2022 17:23:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 17:23:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 17:23:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 17:23:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 17:23:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/29/2022 17:23:30 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:23:30 - INFO - __main__ - Printing 3 examples
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:23:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:23:30 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:23:30 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:23:30 - INFO - __main__ - Printing 3 examples
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:23:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:23:30 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.3010826210826211 on epoch=999
05/29/2022 17:23:30 - INFO - __main__ - save last model!
05/29/2022 17:23:30 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:23:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:23:30 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:23:30 - INFO - __main__ - Printing 3 examples
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:23:30 - INFO - __main__ - ['entailment']
05/29/2022 17:23:30 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:23:30 - INFO - __main__ - ['contradiction']
05/29/2022 17:23:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:23:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:23:32 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:23:45 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:23:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:23:46 - INFO - __main__ - Starting training!
05/29/2022 17:24:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_87_0.5_8_predictions.txt
05/29/2022 17:24:01 - INFO - __main__ - Classification-F1 on test data: 0.1924
05/29/2022 17:24:02 - INFO - __main__ - prefix=anli_16_87, lr=0.5, bsz=8, dev_performance=0.3809012106884447, test_performance=0.19240955684932143
05/29/2022 17:24:02 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.4, bsz=8 ...
05/29/2022 17:24:03 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:24:03 - INFO - __main__ - Printing 3 examples
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:24:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:24:03 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:24:03 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:24:03 - INFO - __main__ - Printing 3 examples
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:24:03 - INFO - __main__ - ['contradiction']
05/29/2022 17:24:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:24:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:24:03 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:24:18 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:24:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:24:19 - INFO - __main__ - Starting training!
05/29/2022 17:24:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=3
05/29/2022 17:24:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
05/29/2022 17:24:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=9
05/29/2022 17:24:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=13
05/29/2022 17:24:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=16
05/29/2022 17:24:34 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.18253968253968256 on epoch=16
05/29/2022 17:24:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18253968253968256 on epoch=16, global_step=50
05/29/2022 17:24:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=19
05/29/2022 17:24:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=23
05/29/2022 17:24:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
05/29/2022 17:24:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/29/2022 17:24:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
05/29/2022 17:24:49 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.2569230769230769 on epoch=33
05/29/2022 17:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.18253968253968256 -> 0.2569230769230769 on epoch=33, global_step=100
05/29/2022 17:24:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
05/29/2022 17:24:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
05/29/2022 17:24:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 17:25:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
05/29/2022 17:25:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
05/29/2022 17:25:05 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.24158667108021625 on epoch=49
05/29/2022 17:25:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
05/29/2022 17:25:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/29/2022 17:25:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=59
05/29/2022 17:25:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/29/2022 17:25:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
05/29/2022 17:25:20 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.24296675191815856 on epoch=66
05/29/2022 17:25:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/29/2022 17:25:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/29/2022 17:25:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
05/29/2022 17:25:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
05/29/2022 17:25:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/29/2022 17:25:34 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.2222222222222222 on epoch=83
05/29/2022 17:25:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
05/29/2022 17:25:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
05/29/2022 17:25:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=93
05/29/2022 17:25:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=96
05/29/2022 17:25:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=99
05/29/2022 17:25:49 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.24593763724198506 on epoch=99
05/29/2022 17:25:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=103
05/29/2022 17:25:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.96 on epoch=106
05/29/2022 17:25:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=109
05/29/2022 17:26:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
05/29/2022 17:26:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=116
05/29/2022 17:26:04 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.2085278555866791 on epoch=116
05/29/2022 17:26:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=119
05/29/2022 17:26:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=123
05/29/2022 17:26:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
05/29/2022 17:26:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=129
05/29/2022 17:26:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=133
05/29/2022 17:26:19 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=133
05/29/2022 17:26:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/29/2022 17:26:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=139
05/29/2022 17:26:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
05/29/2022 17:26:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=146
05/29/2022 17:26:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=149
05/29/2022 17:26:34 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1693121693121693 on epoch=149
05/29/2022 17:26:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=153
05/29/2022 17:26:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=156
05/29/2022 17:26:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/29/2022 17:26:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=163
05/29/2022 17:26:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=166
05/29/2022 17:26:50 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=166
05/29/2022 17:26:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=169
05/29/2022 17:26:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=173
05/29/2022 17:26:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=176
05/29/2022 17:27:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=179
05/29/2022 17:27:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=183
05/29/2022 17:27:05 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.34683393070489843 on epoch=183
05/29/2022 17:27:05 - INFO - __main__ - Saving model with best Classification-F1: 0.2569230769230769 -> 0.34683393070489843 on epoch=183, global_step=550
05/29/2022 17:27:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=186
05/29/2022 17:27:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=189
05/29/2022 17:27:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=193
05/29/2022 17:27:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=196
05/29/2022 17:27:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=199
05/29/2022 17:27:20 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.18888888888888888 on epoch=199
05/29/2022 17:27:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=203
05/29/2022 17:27:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=206
05/29/2022 17:27:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=209
05/29/2022 17:27:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=213
05/29/2022 17:27:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=216
05/29/2022 17:27:35 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.30722571628232004 on epoch=216
05/29/2022 17:27:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=219
05/29/2022 17:27:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=223
05/29/2022 17:27:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=226
05/29/2022 17:27:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=229
05/29/2022 17:27:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=233
05/29/2022 17:27:51 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.27825311942959 on epoch=233
05/29/2022 17:27:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=236
05/29/2022 17:27:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=239
05/29/2022 17:27:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=243
05/29/2022 17:28:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=246
05/29/2022 17:28:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=249
05/29/2022 17:28:06 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.293853151397011 on epoch=249
05/29/2022 17:28:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=253
05/29/2022 17:28:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=256
05/29/2022 17:28:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=259
05/29/2022 17:28:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=263
05/29/2022 17:28:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=266
05/29/2022 17:28:21 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.240879977722083 on epoch=266
05/29/2022 17:28:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=269
05/29/2022 17:28:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=273
05/29/2022 17:28:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=276
05/29/2022 17:28:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=279
05/29/2022 17:28:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=283
05/29/2022 17:28:37 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.2631897203325775 on epoch=283
05/29/2022 17:28:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=286
05/29/2022 17:28:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=289
05/29/2022 17:28:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=293
05/29/2022 17:28:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=296
05/29/2022 17:28:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=299
05/29/2022 17:28:52 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.20370370370370372 on epoch=299
05/29/2022 17:28:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=303
05/29/2022 17:28:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=306
05/29/2022 17:29:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=309
05/29/2022 17:29:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=313
05/29/2022 17:29:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=316
05/29/2022 17:29:07 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.27175602175602176 on epoch=316
05/29/2022 17:29:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=319
05/29/2022 17:29:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=323
05/29/2022 17:29:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=326
05/29/2022 17:29:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=329
05/29/2022 17:29:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=333
05/29/2022 17:29:23 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.25 on epoch=333
05/29/2022 17:29:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=336
05/29/2022 17:29:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=339
05/29/2022 17:29:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=343
05/29/2022 17:29:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=346
05/29/2022 17:29:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=349
05/29/2022 17:29:38 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.2514619883040936 on epoch=349
05/29/2022 17:29:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=353
05/29/2022 17:29:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=356
05/29/2022 17:29:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=359
05/29/2022 17:29:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=363
05/29/2022 17:29:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=366
05/29/2022 17:29:53 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.2679738562091503 on epoch=366
05/29/2022 17:29:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=369
05/29/2022 17:29:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=373
05/29/2022 17:30:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=376
05/29/2022 17:30:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=379
05/29/2022 17:30:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=383
05/29/2022 17:30:08 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.2731829573934837 on epoch=383
05/29/2022 17:30:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=386
05/29/2022 17:30:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=389
05/29/2022 17:30:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=393
05/29/2022 17:30:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=396
05/29/2022 17:30:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=399
05/29/2022 17:30:23 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.2871794871794872 on epoch=399
05/29/2022 17:30:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=403
05/29/2022 17:30:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=406
05/29/2022 17:30:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=409
05/29/2022 17:30:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=413
05/29/2022 17:30:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=416
05/29/2022 17:30:39 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3097105508870215 on epoch=416
05/29/2022 17:30:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=419
05/29/2022 17:30:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=423
05/29/2022 17:30:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=426
05/29/2022 17:30:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=429
05/29/2022 17:30:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=433
05/29/2022 17:30:54 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.32982456140350874 on epoch=433
05/29/2022 17:30:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=436
05/29/2022 17:30:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=439
05/29/2022 17:31:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=443
05/29/2022 17:31:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=446
05/29/2022 17:31:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=449
05/29/2022 17:31:09 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.2791175791175791 on epoch=449
05/29/2022 17:31:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=453
05/29/2022 17:31:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=456
05/29/2022 17:31:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=459
05/29/2022 17:31:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=463
05/29/2022 17:31:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=466
05/29/2022 17:31:25 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.26251526251526247 on epoch=466
05/29/2022 17:31:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=469
05/29/2022 17:31:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=473
05/29/2022 17:31:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=476
05/29/2022 17:31:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=479
05/29/2022 17:31:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=483
05/29/2022 17:31:40 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.2705128205128205 on epoch=483
05/29/2022 17:31:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=486
05/29/2022 17:31:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=489
05/29/2022 17:31:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=493
05/29/2022 17:31:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=496
05/29/2022 17:31:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=499
05/29/2022 17:31:55 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.2692826886375273 on epoch=499
05/29/2022 17:31:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=503
05/29/2022 17:32:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=506
05/29/2022 17:32:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=509
05/29/2022 17:32:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=513
05/29/2022 17:32:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=516
05/29/2022 17:32:10 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.22287581699346407 on epoch=516
05/29/2022 17:32:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=519
05/29/2022 17:32:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=523
05/29/2022 17:32:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=526
05/29/2022 17:32:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=529
05/29/2022 17:32:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=533
05/29/2022 17:32:26 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.21194290073008606 on epoch=533
05/29/2022 17:32:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=536
05/29/2022 17:32:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=539
05/29/2022 17:32:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=543
05/29/2022 17:32:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=546
05/29/2022 17:32:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=549
05/29/2022 17:32:41 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.24400554400554397 on epoch=549
05/29/2022 17:32:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=553
05/29/2022 17:32:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=556
05/29/2022 17:32:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=559
05/29/2022 17:32:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=563
05/29/2022 17:32:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=566
05/29/2022 17:32:56 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.28508771929824567 on epoch=566
05/29/2022 17:32:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=569
05/29/2022 17:33:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=573
05/29/2022 17:33:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=576
05/29/2022 17:33:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=579
05/29/2022 17:33:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=583
05/29/2022 17:33:11 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.27936507936507937 on epoch=583
05/29/2022 17:33:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=586
05/29/2022 17:33:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=589
05/29/2022 17:33:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=593
05/29/2022 17:33:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=596
05/29/2022 17:33:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=599
05/29/2022 17:33:27 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.2712332512988788 on epoch=599
05/29/2022 17:33:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=603
05/29/2022 17:33:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=606
05/29/2022 17:33:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=609
05/29/2022 17:33:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=613
05/29/2022 17:33:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=616
05/29/2022 17:33:42 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.2837908496732026 on epoch=616
05/29/2022 17:33:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=619
05/29/2022 17:33:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=623
05/29/2022 17:33:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=626
05/29/2022 17:33:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=629
05/29/2022 17:33:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=633
05/29/2022 17:33:57 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.2928445452254976 on epoch=633
05/29/2022 17:34:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=636
05/29/2022 17:34:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=639
05/29/2022 17:34:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=643
05/29/2022 17:34:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=646
05/29/2022 17:34:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=649
05/29/2022 17:34:13 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.20306513409961688 on epoch=649
05/29/2022 17:34:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=653
05/29/2022 17:34:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=656
05/29/2022 17:34:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=659
05/29/2022 17:34:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=663
05/29/2022 17:34:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=666
05/29/2022 17:34:28 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.19997911445279865 on epoch=666
05/29/2022 17:34:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=669
05/29/2022 17:34:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=673
05/29/2022 17:34:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=676
05/29/2022 17:34:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=679
05/29/2022 17:34:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=683
05/29/2022 17:34:43 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.19681186868686867 on epoch=683
05/29/2022 17:34:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=686
05/29/2022 17:34:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=689
05/29/2022 17:34:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=693
05/29/2022 17:34:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=696
05/29/2022 17:34:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=699
05/29/2022 17:34:59 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.33968253968253964 on epoch=699
05/29/2022 17:35:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=703
05/29/2022 17:35:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=706
05/29/2022 17:35:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=709
05/29/2022 17:35:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=713
05/29/2022 17:35:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=716
05/29/2022 17:35:14 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.31129785247432307 on epoch=716
05/29/2022 17:35:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=719
05/29/2022 17:35:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=723
05/29/2022 17:35:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=726
05/29/2022 17:35:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=729
05/29/2022 17:35:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=733
05/29/2022 17:35:29 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.2626984126984127 on epoch=733
05/29/2022 17:35:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=736
05/29/2022 17:35:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=739
05/29/2022 17:35:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=743
05/29/2022 17:35:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=746
05/29/2022 17:35:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
05/29/2022 17:35:44 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.3335983568541708 on epoch=749
05/29/2022 17:35:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=753
05/29/2022 17:35:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
05/29/2022 17:35:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=759
05/29/2022 17:35:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=763
05/29/2022 17:35:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=766
05/29/2022 17:36:00 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.3130523153057618 on epoch=766
05/29/2022 17:36:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
05/29/2022 17:36:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=773
05/29/2022 17:36:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
05/29/2022 17:36:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
05/29/2022 17:36:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
05/29/2022 17:36:15 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.15306762203313926 on epoch=783
05/29/2022 17:36:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
05/29/2022 17:36:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=789
05/29/2022 17:36:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
05/29/2022 17:36:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/29/2022 17:36:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
05/29/2022 17:36:30 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.17360114777618366 on epoch=799
05/29/2022 17:36:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=803
05/29/2022 17:36:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
05/29/2022 17:36:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=809
05/29/2022 17:36:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=813
05/29/2022 17:36:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
05/29/2022 17:36:45 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.3319573443412453 on epoch=816
05/29/2022 17:36:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
05/29/2022 17:36:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=823
05/29/2022 17:36:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/29/2022 17:36:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=829
05/29/2022 17:36:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=833
05/29/2022 17:37:00 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.3799775533108867 on epoch=833
05/29/2022 17:37:00 - INFO - __main__ - Saving model with best Classification-F1: 0.34683393070489843 -> 0.3799775533108867 on epoch=833, global_step=2500
05/29/2022 17:37:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
05/29/2022 17:37:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=839
05/29/2022 17:37:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=843
05/29/2022 17:37:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=846
05/29/2022 17:37:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
05/29/2022 17:37:15 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.27269736842105263 on epoch=849
05/29/2022 17:37:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
05/29/2022 17:37:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=856
05/29/2022 17:37:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
05/29/2022 17:37:26 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=863
05/29/2022 17:37:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
05/29/2022 17:37:31 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.3055555555555556 on epoch=866
05/29/2022 17:37:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
05/29/2022 17:37:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=873
05/29/2022 17:37:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/29/2022 17:37:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
05/29/2022 17:37:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
05/29/2022 17:37:46 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.21678302343263983 on epoch=883
05/29/2022 17:37:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
05/29/2022 17:37:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
05/29/2022 17:37:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=893
05/29/2022 17:37:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
05/29/2022 17:37:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
05/29/2022 17:38:01 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.275 on epoch=899
05/29/2022 17:38:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
05/29/2022 17:38:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
05/29/2022 17:38:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=909
05/29/2022 17:38:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
05/29/2022 17:38:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/29/2022 17:38:16 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.18933164996449142 on epoch=916
05/29/2022 17:38:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
05/29/2022 17:38:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/29/2022 17:38:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/29/2022 17:38:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
05/29/2022 17:38:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/29/2022 17:38:31 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.18646878483835003 on epoch=933
05/29/2022 17:38:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/29/2022 17:38:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 17:38:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/29/2022 17:38:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 17:38:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
05/29/2022 17:38:46 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.29496026052697505 on epoch=949
05/29/2022 17:38:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=953
05/29/2022 17:38:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
05/29/2022 17:38:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
05/29/2022 17:38:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/29/2022 17:39:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
05/29/2022 17:39:01 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.33477633477633484 on epoch=966
05/29/2022 17:39:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/29/2022 17:39:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
05/29/2022 17:39:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=976
05/29/2022 17:39:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/29/2022 17:39:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/29/2022 17:39:16 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.2726495726495726 on epoch=983
05/29/2022 17:39:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 17:39:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 17:39:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 17:39:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=996
05/29/2022 17:39:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=999
05/29/2022 17:39:31 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:39:31 - INFO - __main__ - Printing 3 examples
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:39:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:39:31 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.30517730496453904 on epoch=999
05/29/2022 17:39:31 - INFO - __main__ - save last model!
05/29/2022 17:39:31 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:39:31 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:39:31 - INFO - __main__ - Printing 3 examples
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:39:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:39:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:39:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:39:31 - INFO - __main__ - Printing 3 examples
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:39:31 - INFO - __main__ - ['entailment']
05/29/2022 17:39:31 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:39:31 - INFO - __main__ - ['contradiction']
05/29/2022 17:39:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:39:31 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:39:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:39:33 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:39:47 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:39:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:39:47 - INFO - __main__ - Starting training!
05/29/2022 17:40:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_87_0.4_8_predictions.txt
05/29/2022 17:40:03 - INFO - __main__ - Classification-F1 on test data: 0.1344
05/29/2022 17:40:03 - INFO - __main__ - prefix=anli_16_87, lr=0.4, bsz=8, dev_performance=0.3799775533108867, test_performance=0.13440777337868373
05/29/2022 17:40:03 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.3, bsz=8 ...
05/29/2022 17:40:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:40:04 - INFO - __main__ - Printing 3 examples
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:40:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:40:04 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:40:04 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:40:04 - INFO - __main__ - Printing 3 examples
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:40:04 - INFO - __main__ - ['contradiction']
05/29/2022 17:40:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:40:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:40:05 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:40:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:40:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:40:20 - INFO - __main__ - Starting training!
05/29/2022 17:40:24 - INFO - __main__ - Step 10 Global step 10 Train loss 0.64 on epoch=3
05/29/2022 17:40:26 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=6
05/29/2022 17:40:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=9
05/29/2022 17:40:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=13
05/29/2022 17:40:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=16
05/29/2022 17:40:36 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.1990221455277538 on epoch=16
05/29/2022 17:40:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1990221455277538 on epoch=16, global_step=50
05/29/2022 17:40:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=19
05/29/2022 17:40:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/29/2022 17:40:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
05/29/2022 17:40:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
05/29/2022 17:40:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
05/29/2022 17:40:50 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.21747428199041105 on epoch=33
05/29/2022 17:40:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1990221455277538 -> 0.21747428199041105 on epoch=33, global_step=100
05/29/2022 17:40:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/29/2022 17:40:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=39
05/29/2022 17:40:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/29/2022 17:41:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
05/29/2022 17:41:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
05/29/2022 17:41:05 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.28888888888888886 on epoch=49
05/29/2022 17:41:05 - INFO - __main__ - Saving model with best Classification-F1: 0.21747428199041105 -> 0.28888888888888886 on epoch=49, global_step=150
05/29/2022 17:41:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/29/2022 17:41:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
05/29/2022 17:41:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
05/29/2022 17:41:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=63
05/29/2022 17:41:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=66
05/29/2022 17:41:20 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.24275362318840576 on epoch=66
05/29/2022 17:41:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=69
05/29/2022 17:41:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
05/29/2022 17:41:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=76
05/29/2022 17:41:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
05/29/2022 17:41:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/29/2022 17:41:34 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.25703846980442724 on epoch=83
05/29/2022 17:41:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
05/29/2022 17:41:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/29/2022 17:41:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=93
05/29/2022 17:41:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=96
05/29/2022 17:41:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
05/29/2022 17:41:49 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.24611708482676223 on epoch=99
05/29/2022 17:41:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
05/29/2022 17:41:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=106
05/29/2022 17:41:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
05/29/2022 17:42:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
05/29/2022 17:42:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
05/29/2022 17:42:04 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.27724867724867724 on epoch=116
05/29/2022 17:42:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
05/29/2022 17:42:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
05/29/2022 17:42:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=126
05/29/2022 17:42:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
05/29/2022 17:42:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=133
05/29/2022 17:42:19 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.34009121952384364 on epoch=133
05/29/2022 17:42:19 - INFO - __main__ - Saving model with best Classification-F1: 0.28888888888888886 -> 0.34009121952384364 on epoch=133, global_step=400
05/29/2022 17:42:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=136
05/29/2022 17:42:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
05/29/2022 17:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
05/29/2022 17:42:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=146
05/29/2022 17:42:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=149
05/29/2022 17:42:34 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.25580164589452514 on epoch=149
05/29/2022 17:42:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=153
05/29/2022 17:42:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=156
05/29/2022 17:42:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=159
05/29/2022 17:42:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=163
05/29/2022 17:42:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=166
05/29/2022 17:42:49 - INFO - __main__ - Global step 500 Train loss 0.28 Classification-F1 0.2947986497408834 on epoch=166
05/29/2022 17:42:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
05/29/2022 17:42:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
05/29/2022 17:42:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=176
05/29/2022 17:43:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=179
05/29/2022 17:43:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=183
05/29/2022 17:43:04 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.26936444885799404 on epoch=183
05/29/2022 17:43:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=186
05/29/2022 17:43:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=189
05/29/2022 17:43:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
05/29/2022 17:43:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=196
05/29/2022 17:43:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=199
05/29/2022 17:43:19 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.33082826626585593 on epoch=199
05/29/2022 17:43:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=203
05/29/2022 17:43:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=206
05/29/2022 17:43:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=209
05/29/2022 17:43:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=213
05/29/2022 17:43:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
05/29/2022 17:43:34 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.2703645560788418 on epoch=216
05/29/2022 17:43:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
05/29/2022 17:43:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=223
05/29/2022 17:43:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=226
05/29/2022 17:43:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=229
05/29/2022 17:43:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=233
05/29/2022 17:43:49 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.2835725677830941 on epoch=233
05/29/2022 17:43:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=236
05/29/2022 17:43:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=239
05/29/2022 17:43:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=243
05/29/2022 17:43:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=246
05/29/2022 17:44:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=249
05/29/2022 17:44:03 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.2442907334211682 on epoch=249
05/29/2022 17:44:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.12 on epoch=253
05/29/2022 17:44:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
05/29/2022 17:44:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=259
05/29/2022 17:44:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=263
05/29/2022 17:44:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=266
05/29/2022 17:44:18 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.2673832378801323 on epoch=266
05/29/2022 17:44:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/29/2022 17:44:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=273
05/29/2022 17:44:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
05/29/2022 17:44:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
05/29/2022 17:44:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
05/29/2022 17:44:33 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.23503039513677815 on epoch=283
05/29/2022 17:44:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=286
05/29/2022 17:44:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
05/29/2022 17:44:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=293
05/29/2022 17:44:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=296
05/29/2022 17:44:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/29/2022 17:44:48 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.21782407407407406 on epoch=299
05/29/2022 17:44:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=303
05/29/2022 17:44:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=306
05/29/2022 17:44:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=309
05/29/2022 17:44:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=313
05/29/2022 17:45:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
05/29/2022 17:45:03 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.37628384687208216 on epoch=316
05/29/2022 17:45:03 - INFO - __main__ - Saving model with best Classification-F1: 0.34009121952384364 -> 0.37628384687208216 on epoch=316, global_step=950
05/29/2022 17:45:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
05/29/2022 17:45:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
05/29/2022 17:45:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=326
05/29/2022 17:45:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=329
05/29/2022 17:45:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=333
05/29/2022 17:45:17 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.34772749679581977 on epoch=333
05/29/2022 17:45:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=336
05/29/2022 17:45:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=339
05/29/2022 17:45:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
05/29/2022 17:45:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=346
05/29/2022 17:45:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
05/29/2022 17:45:32 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.14592074592074591 on epoch=349
05/29/2022 17:45:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
05/29/2022 17:45:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=356
05/29/2022 17:45:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
05/29/2022 17:45:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=363
05/29/2022 17:45:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=366
05/29/2022 17:45:47 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.29971988795518206 on epoch=366
05/29/2022 17:45:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=369
05/29/2022 17:45:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=373
05/29/2022 17:45:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
05/29/2022 17:45:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
05/29/2022 17:46:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
05/29/2022 17:46:02 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.3092771795920222 on epoch=383
05/29/2022 17:46:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
05/29/2022 17:46:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
05/29/2022 17:46:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
05/29/2022 17:46:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
05/29/2022 17:46:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
05/29/2022 17:46:17 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.2502786377708978 on epoch=399
05/29/2022 17:46:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
05/29/2022 17:46:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
05/29/2022 17:46:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=409
05/29/2022 17:46:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=413
05/29/2022 17:46:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
05/29/2022 17:46:32 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.31805555555555554 on epoch=416
05/29/2022 17:46:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
05/29/2022 17:46:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=423
05/29/2022 17:46:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
05/29/2022 17:46:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/29/2022 17:46:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=433
05/29/2022 17:46:46 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.2586538461538461 on epoch=433
05/29/2022 17:46:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/29/2022 17:46:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/29/2022 17:46:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
05/29/2022 17:46:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
05/29/2022 17:47:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=449
05/29/2022 17:47:01 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.3679012345679012 on epoch=449
05/29/2022 17:47:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/29/2022 17:47:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/29/2022 17:47:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
05/29/2022 17:47:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/29/2022 17:47:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/29/2022 17:47:16 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.193703007518797 on epoch=466
05/29/2022 17:47:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/29/2022 17:47:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/29/2022 17:47:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
05/29/2022 17:47:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
05/29/2022 17:47:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=483
05/29/2022 17:47:31 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3911440940012368 on epoch=483
05/29/2022 17:47:31 - INFO - __main__ - Saving model with best Classification-F1: 0.37628384687208216 -> 0.3911440940012368 on epoch=483, global_step=1450
05/29/2022 17:47:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
05/29/2022 17:47:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
05/29/2022 17:47:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/29/2022 17:47:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/29/2022 17:47:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/29/2022 17:47:46 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.354884246188594 on epoch=499
05/29/2022 17:47:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=503
05/29/2022 17:47:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
05/29/2022 17:47:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=509
05/29/2022 17:47:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 17:47:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/29/2022 17:48:01 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.39365079365079364 on epoch=516
05/29/2022 17:48:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3911440940012368 -> 0.39365079365079364 on epoch=516, global_step=1550
05/29/2022 17:48:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
05/29/2022 17:48:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/29/2022 17:48:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
05/29/2022 17:48:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/29/2022 17:48:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
05/29/2022 17:48:15 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.34480856228621865 on epoch=533
05/29/2022 17:48:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/29/2022 17:48:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=539
05/29/2022 17:48:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/29/2022 17:48:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/29/2022 17:48:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/29/2022 17:48:30 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.37855072463768114 on epoch=549
05/29/2022 17:48:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/29/2022 17:48:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/29/2022 17:48:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 17:48:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/29/2022 17:48:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
05/29/2022 17:48:45 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.29102564102564105 on epoch=566
05/29/2022 17:48:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/29/2022 17:48:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 17:48:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 17:48:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 17:48:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
05/29/2022 17:49:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.3859911288218411 on epoch=583
05/29/2022 17:49:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/29/2022 17:49:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 17:49:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
05/29/2022 17:49:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/29/2022 17:49:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/29/2022 17:49:15 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.2822386679000925 on epoch=599
05/29/2022 17:49:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/29/2022 17:49:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/29/2022 17:49:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
05/29/2022 17:49:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 17:49:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/29/2022 17:49:29 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4158881600742066 on epoch=616
05/29/2022 17:49:29 - INFO - __main__ - Saving model with best Classification-F1: 0.39365079365079364 -> 0.4158881600742066 on epoch=616, global_step=1850
05/29/2022 17:49:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/29/2022 17:49:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/29/2022 17:49:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/29/2022 17:49:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/29/2022 17:49:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/29/2022 17:49:44 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.2603260869565217 on epoch=633
05/29/2022 17:49:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/29/2022 17:49:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=639
05/29/2022 17:49:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
05/29/2022 17:49:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/29/2022 17:49:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/29/2022 17:49:59 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.3504785343177468 on epoch=649
05/29/2022 17:50:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
05/29/2022 17:50:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/29/2022 17:50:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/29/2022 17:50:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
05/29/2022 17:50:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/29/2022 17:50:14 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.364474678760393 on epoch=666
05/29/2022 17:50:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
05/29/2022 17:50:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/29/2022 17:50:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/29/2022 17:50:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/29/2022 17:50:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 17:50:28 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.3701754385964912 on epoch=683
05/29/2022 17:50:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/29/2022 17:50:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/29/2022 17:50:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
05/29/2022 17:50:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/29/2022 17:50:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/29/2022 17:50:43 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.38857142857142857 on epoch=699
05/29/2022 17:50:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/29/2022 17:50:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/29/2022 17:50:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/29/2022 17:50:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 17:50:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 17:50:58 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3454373522458629 on epoch=716
05/29/2022 17:51:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/29/2022 17:51:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/29/2022 17:51:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/29/2022 17:51:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/29/2022 17:51:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
05/29/2022 17:51:12 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2496598639455782 on epoch=733
05/29/2022 17:51:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/29/2022 17:51:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
05/29/2022 17:51:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/29/2022 17:51:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/29/2022 17:51:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/29/2022 17:51:27 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.291482463896257 on epoch=749
05/29/2022 17:51:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/29/2022 17:51:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/29/2022 17:51:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
05/29/2022 17:51:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/29/2022 17:51:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
05/29/2022 17:51:42 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.27181372549019606 on epoch=766
05/29/2022 17:51:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/29/2022 17:51:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/29/2022 17:51:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=776
05/29/2022 17:51:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 17:51:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/29/2022 17:51:56 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.2583559168925022 on epoch=783
05/29/2022 17:51:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
05/29/2022 17:52:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/29/2022 17:52:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/29/2022 17:52:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/29/2022 17:52:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 17:52:11 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.3200903039612717 on epoch=799
05/29/2022 17:52:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/29/2022 17:52:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/29/2022 17:52:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
05/29/2022 17:52:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 17:52:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
05/29/2022 17:52:26 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2763285024154589 on epoch=816
05/29/2022 17:52:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/29/2022 17:52:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 17:52:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 17:52:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/29/2022 17:52:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/29/2022 17:52:40 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.36314699792960664 on epoch=833
05/29/2022 17:52:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/29/2022 17:52:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
05/29/2022 17:52:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/29/2022 17:52:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/29/2022 17:52:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/29/2022 17:52:55 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.26 on epoch=849
05/29/2022 17:52:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/29/2022 17:53:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/29/2022 17:53:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/29/2022 17:53:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
05/29/2022 17:53:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/29/2022 17:53:09 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3460996774340102 on epoch=866
05/29/2022 17:53:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/29/2022 17:53:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 17:53:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 17:53:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
05/29/2022 17:53:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
05/29/2022 17:53:24 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.3526570048309179 on epoch=883
05/29/2022 17:53:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 17:53:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=889
05/29/2022 17:53:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/29/2022 17:53:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 17:53:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/29/2022 17:53:39 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3022386679000925 on epoch=899
05/29/2022 17:53:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/29/2022 17:53:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 17:53:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/29/2022 17:53:49 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/29/2022 17:53:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/29/2022 17:53:53 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2427651515151515 on epoch=916
05/29/2022 17:53:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/29/2022 17:53:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/29/2022 17:54:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
05/29/2022 17:54:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
05/29/2022 17:54:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/29/2022 17:54:08 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.295 on epoch=933
05/29/2022 17:54:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 17:54:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/29/2022 17:54:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/29/2022 17:54:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
05/29/2022 17:54:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/29/2022 17:54:23 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.15864067026857723 on epoch=949
05/29/2022 17:54:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 17:54:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/29/2022 17:54:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 17:54:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 17:54:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 17:54:37 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.2921182266009852 on epoch=966
05/29/2022 17:54:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 17:54:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=973
05/29/2022 17:54:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 17:54:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 17:54:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/29/2022 17:54:52 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.20563323201621075 on epoch=983
05/29/2022 17:54:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/29/2022 17:54:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 17:55:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 17:55:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/29/2022 17:55:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/29/2022 17:55:06 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:55:06 - INFO - __main__ - Printing 3 examples
05/29/2022 17:55:06 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:55:06 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:06 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:55:06 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:06 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:55:06 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:55:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:55:07 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.18573913043478257 on epoch=999
05/29/2022 17:55:07 - INFO - __main__ - save last model!
05/29/2022 17:55:07 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:55:07 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:55:07 - INFO - __main__ - Printing 3 examples
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:55:07 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:55:07 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:55:07 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:55:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:55:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 17:55:07 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 17:55:07 - INFO - __main__ - Printing 3 examples
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 17:55:07 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 17:55:07 - INFO - __main__ - ['entailment']
05/29/2022 17:55:07 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 17:55:07 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:07 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:55:07 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:55:07 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:55:08 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 17:55:22 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:55:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:55:22 - INFO - __main__ - Starting training!
05/29/2022 17:55:38 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_87_0.3_8_predictions.txt
05/29/2022 17:55:38 - INFO - __main__ - Classification-F1 on test data: 0.1262
05/29/2022 17:55:38 - INFO - __main__ - prefix=anli_16_87, lr=0.3, bsz=8, dev_performance=0.4158881600742066, test_performance=0.12617141100657586
05/29/2022 17:55:38 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.2, bsz=8 ...
05/29/2022 17:55:39 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:55:39 - INFO - __main__ - Printing 3 examples
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:55:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:55:39 - INFO - __main__ - Loaded 48 examples from train data
05/29/2022 17:55:39 - INFO - __main__ - Start tokenizing ... 48 instances
05/29/2022 17:55:39 - INFO - __main__ - Printing 3 examples
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/29/2022 17:55:39 - INFO - __main__ - ['contradiction']
05/29/2022 17:55:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 17:55:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 17:55:39 - INFO - __main__ - Loaded 48 examples from dev data
05/29/2022 17:55:54 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 17:55:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 17:55:55 - INFO - __main__ - Starting training!
05/29/2022 17:55:59 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=3
05/29/2022 17:56:01 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=6
05/29/2022 17:56:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=9
05/29/2022 17:56:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=13
05/29/2022 17:56:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
05/29/2022 17:56:11 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=16
05/29/2022 17:56:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/29/2022 17:56:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
05/29/2022 17:56:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/29/2022 17:56:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=26
05/29/2022 17:56:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
05/29/2022 17:56:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=33
05/29/2022 17:56:25 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.15873015873015875 on epoch=33
05/29/2022 17:56:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
05/29/2022 17:56:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
05/29/2022 17:56:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=43
05/29/2022 17:56:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
05/29/2022 17:56:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=49
05/29/2022 17:56:40 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.2109090909090909 on epoch=49
05/29/2022 17:56:40 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2109090909090909 on epoch=49, global_step=150
05/29/2022 17:56:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
05/29/2022 17:56:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
05/29/2022 17:56:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=59
05/29/2022 17:56:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
05/29/2022 17:56:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=66
05/29/2022 17:56:54 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.21804511278195485 on epoch=66
05/29/2022 17:56:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2109090909090909 -> 0.21804511278195485 on epoch=66, global_step=200
05/29/2022 17:56:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=69
05/29/2022 17:57:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=73
05/29/2022 17:57:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
05/29/2022 17:57:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/29/2022 17:57:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
05/29/2022 17:57:09 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.25098039215686274 on epoch=83
05/29/2022 17:57:09 - INFO - __main__ - Saving model with best Classification-F1: 0.21804511278195485 -> 0.25098039215686274 on epoch=83, global_step=250
05/29/2022 17:57:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
05/29/2022 17:57:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
05/29/2022 17:57:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=93
05/29/2022 17:57:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=96
05/29/2022 17:57:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
05/29/2022 17:57:24 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.2480818414322251 on epoch=99
05/29/2022 17:57:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/29/2022 17:57:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=106
05/29/2022 17:57:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
05/29/2022 17:57:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=113
05/29/2022 17:57:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=116
05/29/2022 17:57:38 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.24881676808654493 on epoch=116
05/29/2022 17:57:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=119
05/29/2022 17:57:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=123
05/29/2022 17:57:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/29/2022 17:57:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
05/29/2022 17:57:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=133
05/29/2022 17:57:53 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3156075373619233 on epoch=133
05/29/2022 17:57:53 - INFO - __main__ - Saving model with best Classification-F1: 0.25098039215686274 -> 0.3156075373619233 on epoch=133, global_step=400
05/29/2022 17:57:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=136
05/29/2022 17:57:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
05/29/2022 17:58:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=143
05/29/2022 17:58:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=146
05/29/2022 17:58:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=149
05/29/2022 17:58:07 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.2996632996632997 on epoch=149
05/29/2022 17:58:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=153
05/29/2022 17:58:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=156
05/29/2022 17:58:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=159
05/29/2022 17:58:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=163
05/29/2022 17:58:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=166
05/29/2022 17:58:22 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.28008254465238736 on epoch=166
05/29/2022 17:58:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
05/29/2022 17:58:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=173
05/29/2022 17:58:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=176
05/29/2022 17:58:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=179
05/29/2022 17:58:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=183
05/29/2022 17:58:37 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.23142857142857143 on epoch=183
05/29/2022 17:58:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=186
05/29/2022 17:58:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=189
05/29/2022 17:58:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
05/29/2022 17:58:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=196
05/29/2022 17:58:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=199
05/29/2022 17:58:51 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.3286974048878811 on epoch=199
05/29/2022 17:58:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3156075373619233 -> 0.3286974048878811 on epoch=199, global_step=600
05/29/2022 17:58:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=203
05/29/2022 17:58:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=206
05/29/2022 17:58:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=209
05/29/2022 17:59:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=213
05/29/2022 17:59:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=216
05/29/2022 17:59:06 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.29998666133119917 on epoch=216
05/29/2022 17:59:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
05/29/2022 17:59:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=223
05/29/2022 17:59:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=226
05/29/2022 17:59:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=229
05/29/2022 17:59:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=233
05/29/2022 17:59:20 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.27735876959479444 on epoch=233
05/29/2022 17:59:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=236
05/29/2022 17:59:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=239
05/29/2022 17:59:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=243
05/29/2022 17:59:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=246
05/29/2022 17:59:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=249
05/29/2022 17:59:35 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=249
05/29/2022 17:59:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3286974048878811 -> 0.3333333333333333 on epoch=249, global_step=750
05/29/2022 17:59:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=253
05/29/2022 17:59:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
05/29/2022 17:59:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=259
05/29/2022 17:59:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=263
05/29/2022 17:59:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=266
05/29/2022 17:59:50 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.31662781662781664 on epoch=266
05/29/2022 17:59:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=269
05/29/2022 17:59:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=273
05/29/2022 17:59:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=276
05/29/2022 18:00:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=279
05/29/2022 18:00:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=283
05/29/2022 18:00:04 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.2981143836683809 on epoch=283
05/29/2022 18:00:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=286
05/29/2022 18:00:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=289
05/29/2022 18:00:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=293
05/29/2022 18:00:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=296
05/29/2022 18:00:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=299
05/29/2022 18:00:19 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.3925894195759263 on epoch=299
05/29/2022 18:00:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3925894195759263 on epoch=299, global_step=900
05/29/2022 18:00:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
05/29/2022 18:00:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=306
05/29/2022 18:00:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=309
05/29/2022 18:00:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=313
05/29/2022 18:00:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
05/29/2022 18:00:34 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.23461538461538461 on epoch=316
05/29/2022 18:00:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=319
05/29/2022 18:00:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=323
05/29/2022 18:00:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=326
05/29/2022 18:00:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
05/29/2022 18:00:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=333
05/29/2022 18:00:48 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.3795995670995671 on epoch=333
05/29/2022 18:00:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
05/29/2022 18:00:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=339
05/29/2022 18:00:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=343
05/29/2022 18:00:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/29/2022 18:01:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=349
05/29/2022 18:01:03 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.37147266313932975 on epoch=349
05/29/2022 18:01:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
05/29/2022 18:01:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/29/2022 18:01:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
05/29/2022 18:01:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=363
05/29/2022 18:01:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=366
05/29/2022 18:01:18 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.24044817927170867 on epoch=366
05/29/2022 18:01:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
05/29/2022 18:01:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
05/29/2022 18:01:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=376
05/29/2022 18:01:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=379
05/29/2022 18:01:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
05/29/2022 18:01:32 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.25949279379157425 on epoch=383
05/29/2022 18:01:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
05/29/2022 18:01:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=389
05/29/2022 18:01:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=393
05/29/2022 18:01:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=396
05/29/2022 18:01:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=399
05/29/2022 18:01:47 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.2542147293700089 on epoch=399
05/29/2022 18:01:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
05/29/2022 18:01:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
05/29/2022 18:01:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=409
05/29/2022 18:01:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=413
05/29/2022 18:02:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=416
05/29/2022 18:02:01 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.2730496453900709 on epoch=416
05/29/2022 18:02:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=419
05/29/2022 18:02:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=423
05/29/2022 18:02:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
05/29/2022 18:02:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
05/29/2022 18:02:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
05/29/2022 18:02:16 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.2692654918387982 on epoch=433
05/29/2022 18:02:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/29/2022 18:02:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
05/29/2022 18:02:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=443
05/29/2022 18:02:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=446
05/29/2022 18:02:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
05/29/2022 18:02:31 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.16545454545454547 on epoch=449
05/29/2022 18:02:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=453
05/29/2022 18:02:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=456
05/29/2022 18:02:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=459
05/29/2022 18:02:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=463
05/29/2022 18:02:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=466
05/29/2022 18:02:45 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.1421437090041741 on epoch=466
05/29/2022 18:02:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/29/2022 18:02:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=473
05/29/2022 18:02:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
05/29/2022 18:02:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=479
05/29/2022 18:02:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
05/29/2022 18:03:00 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.17014042867701404 on epoch=483
05/29/2022 18:03:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
05/29/2022 18:03:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/29/2022 18:03:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=493
05/29/2022 18:03:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=496
05/29/2022 18:03:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
05/29/2022 18:03:14 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.162015503875969 on epoch=499
05/29/2022 18:03:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
05/29/2022 18:03:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/29/2022 18:03:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=509
05/29/2022 18:03:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/29/2022 18:03:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/29/2022 18:03:29 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.16156462585034012 on epoch=516
05/29/2022 18:03:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
05/29/2022 18:03:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
05/29/2022 18:03:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=526
05/29/2022 18:03:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
05/29/2022 18:03:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
05/29/2022 18:03:44 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.14947089947089948 on epoch=533
05/29/2022 18:03:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=536
05/29/2022 18:03:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
05/29/2022 18:03:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
05/29/2022 18:03:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/29/2022 18:03:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=549
05/29/2022 18:03:58 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.2610092514718251 on epoch=549
05/29/2022 18:04:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
05/29/2022 18:04:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=556
05/29/2022 18:04:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/29/2022 18:04:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/29/2022 18:04:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
05/29/2022 18:04:13 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.2642276422764227 on epoch=566
05/29/2022 18:04:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=569
05/29/2022 18:04:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=573
05/29/2022 18:04:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/29/2022 18:04:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/29/2022 18:04:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=583
05/29/2022 18:04:27 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.28654244306418225 on epoch=583
05/29/2022 18:04:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
05/29/2022 18:04:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/29/2022 18:04:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
05/29/2022 18:04:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/29/2022 18:04:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/29/2022 18:04:42 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.2560754506467696 on epoch=599
05/29/2022 18:04:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=603
05/29/2022 18:04:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=606
05/29/2022 18:04:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/29/2022 18:04:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/29/2022 18:04:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/29/2022 18:04:57 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.3725857099840839 on epoch=616
05/29/2022 18:04:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/29/2022 18:05:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/29/2022 18:05:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/29/2022 18:05:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/29/2022 18:05:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=633
05/29/2022 18:05:11 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.19827670071572506 on epoch=633
05/29/2022 18:05:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
05/29/2022 18:05:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/29/2022 18:05:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/29/2022 18:05:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
05/29/2022 18:05:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/29/2022 18:05:26 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2457875457875458 on epoch=649
05/29/2022 18:05:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/29/2022 18:05:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/29/2022 18:05:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/29/2022 18:05:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
05/29/2022 18:05:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=666
05/29/2022 18:05:41 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.14349206349206353 on epoch=666
05/29/2022 18:05:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
05/29/2022 18:05:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
05/29/2022 18:05:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=676
05/29/2022 18:05:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/29/2022 18:05:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/29/2022 18:05:55 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.24221059003667697 on epoch=683
05/29/2022 18:05:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/29/2022 18:06:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/29/2022 18:06:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=693
05/29/2022 18:06:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
05/29/2022 18:06:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/29/2022 18:06:10 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.18485861964122835 on epoch=699
05/29/2022 18:06:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/29/2022 18:06:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/29/2022 18:06:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/29/2022 18:06:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/29/2022 18:06:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/29/2022 18:06:25 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.25849731663685155 on epoch=716
05/29/2022 18:06:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/29/2022 18:06:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
05/29/2022 18:06:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=726
05/29/2022 18:06:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/29/2022 18:06:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/29/2022 18:06:39 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.2733168299571145 on epoch=733
05/29/2022 18:06:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/29/2022 18:06:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/29/2022 18:06:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/29/2022 18:06:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/29/2022 18:06:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/29/2022 18:06:55 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.2425053207661903 on epoch=749
05/29/2022 18:06:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/29/2022 18:07:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/29/2022 18:07:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
05/29/2022 18:07:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/29/2022 18:07:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=766
05/29/2022 18:07:10 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.28113553113553114 on epoch=766
05/29/2022 18:07:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/29/2022 18:07:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/29/2022 18:07:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
05/29/2022 18:07:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/29/2022 18:07:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/29/2022 18:07:25 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.16599999999999998 on epoch=783
05/29/2022 18:07:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=786
05/29/2022 18:07:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
05/29/2022 18:07:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/29/2022 18:07:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
05/29/2022 18:07:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/29/2022 18:07:40 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.2697902097902098 on epoch=799
05/29/2022 18:07:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/29/2022 18:07:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/29/2022 18:07:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/29/2022 18:07:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/29/2022 18:07:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
05/29/2022 18:07:55 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.23725274725274725 on epoch=816
05/29/2022 18:07:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/29/2022 18:08:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/29/2022 18:08:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/29/2022 18:08:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
05/29/2022 18:08:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/29/2022 18:08:10 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.256043956043956 on epoch=833
05/29/2022 18:08:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/29/2022 18:08:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/29/2022 18:08:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/29/2022 18:08:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
05/29/2022 18:08:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/29/2022 18:08:26 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.35534084809447125 on epoch=849
05/29/2022 18:08:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=853
05/29/2022 18:08:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
05/29/2022 18:08:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
05/29/2022 18:08:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=863
05/29/2022 18:08:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/29/2022 18:08:41 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.31026315789473685 on epoch=866
05/29/2022 18:08:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
05/29/2022 18:08:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/29/2022 18:08:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/29/2022 18:08:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/29/2022 18:08:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/29/2022 18:08:56 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2730911330049261 on epoch=883
05/29/2022 18:08:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/29/2022 18:09:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/29/2022 18:09:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
05/29/2022 18:09:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/29/2022 18:09:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/29/2022 18:09:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.22417582417582418 on epoch=899
05/29/2022 18:09:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/29/2022 18:09:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/29/2022 18:09:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/29/2022 18:09:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/29/2022 18:09:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/29/2022 18:09:26 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2893559928443649 on epoch=916
05/29/2022 18:09:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/29/2022 18:09:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=923
05/29/2022 18:09:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/29/2022 18:09:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/29/2022 18:09:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/29/2022 18:09:41 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.22776306620209058 on epoch=933
05/29/2022 18:09:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/29/2022 18:09:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/29/2022 18:09:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
05/29/2022 18:09:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/29/2022 18:09:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=949
05/29/2022 18:09:56 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3554778554778555 on epoch=949
05/29/2022 18:09:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/29/2022 18:10:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/29/2022 18:10:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/29/2022 18:10:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/29/2022 18:10:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/29/2022 18:10:12 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.27933829611248967 on epoch=966
05/29/2022 18:10:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/29/2022 18:10:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/29/2022 18:10:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/29/2022 18:10:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/29/2022 18:10:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/29/2022 18:10:27 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2506211180124224 on epoch=983
05/29/2022 18:10:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/29/2022 18:10:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/29/2022 18:10:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/29/2022 18:10:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/29/2022 18:10:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/29/2022 18:10:42 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.32369673748984096 on epoch=999
05/29/2022 18:10:42 - INFO - __main__ - save last model!
05/29/2022 18:10:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 18:10:42 - INFO - __main__ - Start tokenizing ... 1000 instances
05/29/2022 18:10:42 - INFO - __main__ - Printing 3 examples
05/29/2022 18:10:42 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/29/2022 18:10:42 - INFO - __main__ - ['contradiction']
05/29/2022 18:10:42 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/29/2022 18:10:42 - INFO - __main__ - ['entailment']
05/29/2022 18:10:42 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/29/2022 18:10:42 - INFO - __main__ - ['contradiction']
05/29/2022 18:10:42 - INFO - __main__ - Tokenizing Input ...
05/29/2022 18:10:43 - INFO - __main__ - Tokenizing Output ...
05/29/2022 18:10:44 - INFO - __main__ - Loaded 1000 examples from test data
05/29/2022 18:11:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-12uptasks/singletask-anli/anli_16_87_0.2_8_predictions.txt
05/29/2022 18:11:14 - INFO - __main__ - Classification-F1 on test data: 0.1392
05/29/2022 18:11:15 - INFO - __main__ - prefix=anli_16_87, lr=0.2, bsz=8, dev_performance=0.3925894195759263, test_performance=0.13922316223126247
