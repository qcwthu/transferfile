05/30/2022 08:43:12 - INFO - __main__ - Namespace(task_dir='data_128/tab_fact/', task_name='tab_fact', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/30/2022 08:43:12 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact
05/30/2022 08:43:12 - INFO - __main__ - Namespace(task_dir='data_128/tab_fact/', task_name='tab_fact', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/30/2022 08:43:12 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact
05/30/2022 08:43:13 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/30/2022 08:43:13 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/30/2022 08:43:13 - INFO - __main__ - args.device: cuda:0
05/30/2022 08:43:13 - INFO - __main__ - Using 2 gpus
05/30/2022 08:43:13 - INFO - __main__ - args.device: cuda:1
05/30/2022 08:43:13 - INFO - __main__ - Using 2 gpus
05/30/2022 08:43:13 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_128_100', 'tab_fact_128_13', 'tab_fact_128_21', 'tab_fact_128_42', 'tab_fact_128_87']
05/30/2022 08:43:13 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_128_100', 'tab_fact_128_13', 'tab_fact_128_21', 'tab_fact_128_42', 'tab_fact_128_87']
05/30/2022 08:43:18 - INFO - __main__ - Running ... prefix=tab_fact_128_100, lr=0.5, bsz=8 ...
05/30/2022 08:43:19 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 08:43:19 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 08:43:19 - INFO - __main__ - Printing 3 examples
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 08:43:19 - INFO - __main__ - Printing 3 examples
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 08:43:19 - INFO - __main__ - Tokenizing Input ...
05/30/2022 08:43:19 - INFO - __main__ - ['refuted']
05/30/2022 08:43:19 - INFO - __main__ - Tokenizing Input ...
05/30/2022 08:43:19 - INFO - __main__ - Tokenizing Output ...
05/30/2022 08:43:19 - INFO - __main__ - Tokenizing Output ...
05/30/2022 08:43:20 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 08:43:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 08:43:20 - INFO - __main__ - Printing 3 examples
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 08:43:20 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 08:43:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 08:43:20 - INFO - __main__ - Printing 3 examples
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 08:43:20 - INFO - __main__ - ['refuted']
05/30/2022 08:43:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 08:43:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 08:43:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 08:43:20 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 08:43:20 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 08:43:38 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 08:43:38 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 08:43:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 08:43:39 - INFO - __main__ - Starting training!
05/30/2022 08:43:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 08:43:44 - INFO - __main__ - Starting training!
05/30/2022 08:43:50 - INFO - __main__ - Step 10 Global step 10 Train loss 4.27 on epoch=0
05/30/2022 08:43:54 - INFO - __main__ - Step 20 Global step 20 Train loss 2.69 on epoch=1
05/30/2022 08:43:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.86 on epoch=1
05/30/2022 08:44:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=2
05/30/2022 08:44:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.73 on epoch=3
05/30/2022 08:44:18 - INFO - __main__ - Global step 50 Train loss 2.14 Classification-F1 0.4267300234857174 on epoch=3
05/30/2022 08:44:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4267300234857174 on epoch=3, global_step=50
05/30/2022 08:44:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=3
05/30/2022 08:44:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=4
05/30/2022 08:44:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.33 on epoch=4
05/30/2022 08:44:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=5
05/30/2022 08:44:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=6
05/30/2022 08:44:50 - INFO - __main__ - Global step 100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 08:44:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=6
05/30/2022 08:44:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=7
05/30/2022 08:45:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=8
05/30/2022 08:45:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=8
05/30/2022 08:45:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=9
05/30/2022 08:45:24 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 08:45:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=9
05/30/2022 08:45:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=10
05/30/2022 08:45:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=11
05/30/2022 08:45:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=11
05/30/2022 08:45:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=12
05/30/2022 08:45:57 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 08:46:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=13
05/30/2022 08:46:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=13
05/30/2022 08:46:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=14
05/30/2022 08:46:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=14
05/30/2022 08:46:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=15
05/30/2022 08:46:31 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 08:46:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=16
05/30/2022 08:46:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=16
05/30/2022 08:46:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=17
05/30/2022 08:46:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=18
05/30/2022 08:46:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=18
05/30/2022 08:47:05 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 08:47:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=19
05/30/2022 08:47:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=19
05/30/2022 08:47:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=20
05/30/2022 08:47:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=21
05/30/2022 08:47:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=21
05/30/2022 08:47:39 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 08:47:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=22
05/30/2022 08:47:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 08:47:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=23
05/30/2022 08:47:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=24
05/30/2022 08:48:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=24
05/30/2022 08:48:12 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 08:48:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=25
05/30/2022 08:48:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 08:48:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=26
05/30/2022 08:48:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=27
05/30/2022 08:48:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=28
05/30/2022 08:48:46 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 08:48:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=28
05/30/2022 08:48:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=29
05/30/2022 08:48:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=29
05/30/2022 08:49:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=30
05/30/2022 08:49:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=31
05/30/2022 08:49:20 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 08:49:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=31
05/30/2022 08:49:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=32
05/30/2022 08:49:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=33
05/30/2022 08:49:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=33
05/30/2022 08:49:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 08:49:53 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 08:49:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=34
05/30/2022 08:50:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=35
05/30/2022 08:50:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/30/2022 08:50:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=36
05/30/2022 08:50:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=37
05/30/2022 08:50:27 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 08:50:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=38
05/30/2022 08:50:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=38
05/30/2022 08:50:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=39
05/30/2022 08:50:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=39
05/30/2022 08:50:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=40
05/30/2022 08:51:00 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 08:51:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=41
05/30/2022 08:51:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=41
05/30/2022 08:51:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=42
05/30/2022 08:51:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/30/2022 08:51:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=43
05/30/2022 08:51:34 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 08:51:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 08:51:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=44
05/30/2022 08:51:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 08:51:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=46
05/30/2022 08:51:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=46
05/30/2022 08:52:08 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 08:52:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=47
05/30/2022 08:52:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=48
05/30/2022 08:52:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/30/2022 08:52:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=49
05/30/2022 08:52:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=49
05/30/2022 08:52:41 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 08:52:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=50
05/30/2022 08:52:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=51
05/30/2022 08:52:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/30/2022 08:52:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=52
05/30/2022 08:53:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=53
05/30/2022 08:53:15 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 08:53:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 08:53:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/30/2022 08:53:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/30/2022 08:53:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=55
05/30/2022 08:53:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 08:53:49 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 08:53:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/30/2022 08:53:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/30/2022 08:54:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 08:54:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=58
05/30/2022 08:54:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/30/2022 08:54:22 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 08:54:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=59
05/30/2022 08:54:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/30/2022 08:54:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=61
05/30/2022 08:54:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=61
05/30/2022 08:54:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=62
05/30/2022 08:54:56 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 08:55:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=63
05/30/2022 08:55:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=63
05/30/2022 08:55:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=64
05/30/2022 08:55:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 08:55:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 08:55:30 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.341074761764417 on epoch=65
05/30/2022 08:55:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/30/2022 08:55:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/30/2022 08:55:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 08:55:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 08:55:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/30/2022 08:56:03 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.4988989478835331 on epoch=68
05/30/2022 08:56:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4267300234857174 -> 0.4988989478835331 on epoch=68, global_step=1100
05/30/2022 08:56:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/30/2022 08:56:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=69
05/30/2022 08:56:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=70
05/30/2022 08:56:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=71
05/30/2022 08:56:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/30/2022 08:56:37 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.38245614035087716 on epoch=71
05/30/2022 08:56:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=72
05/30/2022 08:56:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 08:56:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 08:56:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=74
05/30/2022 08:56:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/30/2022 08:57:10 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.2502283105022831 on epoch=74
05/30/2022 08:57:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=75
05/30/2022 08:57:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/30/2022 08:57:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=76
05/30/2022 08:57:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/30/2022 08:57:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/30/2022 08:57:44 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.17231680236861582 on epoch=78
05/30/2022 08:57:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/30/2022 08:57:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 08:57:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/30/2022 08:58:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=80
05/30/2022 08:58:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=81
05/30/2022 08:58:18 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.2564874911846619 on epoch=81
05/30/2022 08:58:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=81
05/30/2022 08:58:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=82
05/30/2022 08:58:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/30/2022 08:58:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=83
05/30/2022 08:58:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/30/2022 08:58:52 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.11423466407488672 on epoch=84
05/30/2022 08:58:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/30/2022 08:59:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=85
05/30/2022 08:59:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=86
05/30/2022 08:59:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=86
05/30/2022 08:59:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=87
05/30/2022 08:59:25 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.10785777070169328 on epoch=87
05/30/2022 08:59:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/30/2022 08:59:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/30/2022 08:59:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/30/2022 08:59:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/30/2022 08:59:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
05/30/2022 08:59:59 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.09937685310819638 on epoch=90
05/30/2022 09:00:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
05/30/2022 09:00:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/30/2022 09:00:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=92
05/30/2022 09:00:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/30/2022 09:00:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/30/2022 09:00:33 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.10185775902024613 on epoch=93
05/30/2022 09:00:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
05/30/2022 09:00:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
05/30/2022 09:00:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=95
05/30/2022 09:00:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
05/30/2022 09:00:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
05/30/2022 09:01:07 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.10158305746541041 on epoch=96
05/30/2022 09:01:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=97
05/30/2022 09:01:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=98
05/30/2022 09:01:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/30/2022 09:01:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=99
05/30/2022 09:01:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/30/2022 09:01:40 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.12907883687069158 on epoch=99
05/30/2022 09:01:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=100
05/30/2022 09:01:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=101
05/30/2022 09:01:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/30/2022 09:01:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
05/30/2022 09:02:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/30/2022 09:02:14 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.12153825600402364 on epoch=103
05/30/2022 09:02:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
05/30/2022 09:02:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=104
05/30/2022 09:02:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/30/2022 09:02:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=105
05/30/2022 09:02:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/30/2022 09:02:48 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.10076941080100638 on epoch=106
05/30/2022 09:02:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=106
05/30/2022 09:02:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=107
05/30/2022 09:03:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=108
05/30/2022 09:03:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=108
05/30/2022 09:03:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/30/2022 09:03:22 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.07806538229580534 on epoch=109
05/30/2022 09:03:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/30/2022 09:03:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=110
05/30/2022 09:03:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=111
05/30/2022 09:03:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/30/2022 09:03:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=112
05/30/2022 09:03:56 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.10517161410018552 on epoch=112
05/30/2022 09:04:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=113
05/30/2022 09:04:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/30/2022 09:04:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/30/2022 09:04:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 09:04:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/30/2022 09:04:29 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.11764093529788597 on epoch=115
05/30/2022 09:04:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/30/2022 09:04:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=116
05/30/2022 09:04:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/30/2022 09:04:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
05/30/2022 09:04:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
05/30/2022 09:05:03 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.1391229894744392 on epoch=118
05/30/2022 09:05:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
05/30/2022 09:05:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
05/30/2022 09:05:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=120
05/30/2022 09:05:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=121
05/30/2022 09:05:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/30/2022 09:05:36 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.08249554252325407 on epoch=121
05/30/2022 09:05:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=122
05/30/2022 09:05:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=123
05/30/2022 09:05:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=123
05/30/2022 09:05:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/30/2022 09:05:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=124
05/30/2022 09:06:10 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.09515820456217806 on epoch=124
05/30/2022 09:06:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
05/30/2022 09:06:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
05/30/2022 09:06:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=126
05/30/2022 09:06:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=127
05/30/2022 09:06:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/30/2022 09:06:43 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.10057271557271556 on epoch=128
05/30/2022 09:06:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
05/30/2022 09:06:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
05/30/2022 09:06:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=129
05/30/2022 09:07:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=130
05/30/2022 09:07:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/30/2022 09:07:16 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.10794820745259175 on epoch=131
05/30/2022 09:07:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=131
05/30/2022 09:07:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/30/2022 09:07:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=133
05/30/2022 09:07:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
05/30/2022 09:07:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=134
05/30/2022 09:07:50 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.07014759120022278 on epoch=134
05/30/2022 09:07:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/30/2022 09:07:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
05/30/2022 09:08:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=136
05/30/2022 09:08:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=136
05/30/2022 09:08:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/30/2022 09:08:23 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.13764189886480907 on epoch=137
05/30/2022 09:08:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=138
05/30/2022 09:08:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/30/2022 09:08:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=139
05/30/2022 09:08:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/30/2022 09:08:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=140
05/30/2022 09:08:57 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.13216127297459745 on epoch=140
05/30/2022 09:09:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
05/30/2022 09:09:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/30/2022 09:09:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=142
05/30/2022 09:09:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=143
05/30/2022 09:09:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=143
05/30/2022 09:09:30 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.09292505409481347 on epoch=143
05/30/2022 09:09:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/30/2022 09:09:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=144
05/30/2022 09:09:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/30/2022 09:09:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
05/30/2022 09:09:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
05/30/2022 09:10:04 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.05644555694618273 on epoch=146
05/30/2022 09:10:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
05/30/2022 09:10:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
05/30/2022 09:10:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=148
05/30/2022 09:10:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
05/30/2022 09:10:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=149
05/30/2022 09:10:37 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.1340871062592394 on epoch=149
05/30/2022 09:10:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=150
05/30/2022 09:10:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=151
05/30/2022 09:10:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/30/2022 09:10:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
05/30/2022 09:10:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=153
05/30/2022 09:11:11 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.07929004828440797 on epoch=153
05/30/2022 09:11:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=153
05/30/2022 09:11:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
05/30/2022 09:11:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
05/30/2022 09:11:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=155
05/30/2022 09:11:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=156
05/30/2022 09:11:44 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.07416815277715337 on epoch=156
05/30/2022 09:11:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/30/2022 09:11:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=157
05/30/2022 09:11:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=158
05/30/2022 09:12:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=158
05/30/2022 09:12:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/30/2022 09:12:17 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.16187712119770356 on epoch=159
05/30/2022 09:12:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
05/30/2022 09:12:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
05/30/2022 09:12:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/30/2022 09:12:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
05/30/2022 09:12:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
05/30/2022 09:12:51 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.14680327868852458 on epoch=162
05/30/2022 09:12:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=163
05/30/2022 09:13:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/30/2022 09:13:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/30/2022 09:13:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/30/2022 09:13:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
05/30/2022 09:13:24 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.08123168099977453 on epoch=165
05/30/2022 09:13:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
05/30/2022 09:13:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/30/2022 09:13:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=167
05/30/2022 09:13:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/30/2022 09:13:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=168
05/30/2022 09:13:58 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.15059952038369304 on epoch=168
05/30/2022 09:14:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/30/2022 09:14:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
05/30/2022 09:14:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/30/2022 09:14:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
05/30/2022 09:14:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/30/2022 09:14:31 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.2381646655231561 on epoch=171
05/30/2022 09:14:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/30/2022 09:14:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/30/2022 09:14:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/30/2022 09:14:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
05/30/2022 09:14:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/30/2022 09:15:05 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.16561578518100256 on epoch=174
05/30/2022 09:15:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/30/2022 09:15:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/30/2022 09:15:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/30/2022 09:15:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
05/30/2022 09:15:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/30/2022 09:15:39 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.11462118011547669 on epoch=178
05/30/2022 09:15:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/30/2022 09:15:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/30/2022 09:15:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/30/2022 09:15:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/30/2022 09:16:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
05/30/2022 09:16:13 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.09072942867409271 on epoch=181
05/30/2022 09:16:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/30/2022 09:16:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
05/30/2022 09:16:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/30/2022 09:16:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/30/2022 09:16:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
05/30/2022 09:16:47 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.09065527065527065 on epoch=184
05/30/2022 09:16:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
05/30/2022 09:16:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/30/2022 09:17:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=186
05/30/2022 09:17:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/30/2022 09:17:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/30/2022 09:17:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 09:17:11 - INFO - __main__ - Printing 3 examples
05/30/2022 09:17:11 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 09:17:11 - INFO - __main__ - ['refuted']
05/30/2022 09:17:11 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 09:17:11 - INFO - __main__ - ['refuted']
05/30/2022 09:17:11 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 09:17:11 - INFO - __main__ - ['refuted']
05/30/2022 09:17:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 09:17:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 09:17:12 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 09:17:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 09:17:12 - INFO - __main__ - Printing 3 examples
05/30/2022 09:17:12 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 09:17:12 - INFO - __main__ - ['refuted']
05/30/2022 09:17:12 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 09:17:12 - INFO - __main__ - ['refuted']
05/30/2022 09:17:12 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 09:17:12 - INFO - __main__ - ['refuted']
05/30/2022 09:17:12 - INFO - __main__ - Tokenizing Input ...
05/30/2022 09:17:12 - INFO - __main__ - Tokenizing Output ...
05/30/2022 09:17:12 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 09:17:21 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.10976725567564476 on epoch=187
05/30/2022 09:17:21 - INFO - __main__ - save last model!
05/30/2022 09:17:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 09:17:21 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 09:17:21 - INFO - __main__ - Printing 3 examples
05/30/2022 09:17:21 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 09:17:21 - INFO - __main__ - ['entailed']
05/30/2022 09:17:21 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 09:17:21 - INFO - __main__ - ['entailed']
05/30/2022 09:17:21 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 09:17:21 - INFO - __main__ - ['entailed']
05/30/2022 09:17:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 09:17:29 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 09:17:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 09:17:30 - INFO - __main__ - Starting training!
05/30/2022 09:17:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 09:17:58 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 09:27:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_100_0.5_8_predictions.txt
05/30/2022 09:27:08 - INFO - __main__ - Classification-F1 on test data: 0.0096
05/30/2022 09:27:09 - INFO - __main__ - prefix=tab_fact_128_100, lr=0.5, bsz=8, dev_performance=0.4988989478835331, test_performance=0.009574732329502851
05/30/2022 09:27:09 - INFO - __main__ - Running ... prefix=tab_fact_128_100, lr=0.4, bsz=8 ...
05/30/2022 09:27:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 09:27:10 - INFO - __main__ - Printing 3 examples
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 09:27:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 09:27:10 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 09:27:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 09:27:10 - INFO - __main__ - Printing 3 examples
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 09:27:10 - INFO - __main__ - ['refuted']
05/30/2022 09:27:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 09:27:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 09:27:11 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 09:27:30 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 09:27:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 09:27:31 - INFO - __main__ - Starting training!
05/30/2022 09:27:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.72 on epoch=0
05/30/2022 09:27:40 - INFO - __main__ - Step 20 Global step 20 Train loss 2.89 on epoch=1
05/30/2022 09:27:45 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=1
05/30/2022 09:27:49 - INFO - __main__ - Step 40 Global step 40 Train loss 1.56 on epoch=2
05/30/2022 09:27:54 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=3
05/30/2022 09:28:05 - INFO - __main__ - Global step 50 Train loss 2.48 Classification-F1 0.3333333333333333 on epoch=3
05/30/2022 09:28:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/30/2022 09:28:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.73 on epoch=3
05/30/2022 09:28:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=4
05/30/2022 09:28:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.35 on epoch=4
05/30/2022 09:28:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
05/30/2022 09:28:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=6
05/30/2022 09:28:39 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 09:28:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=6
05/30/2022 09:28:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=7
05/30/2022 09:28:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=8
05/30/2022 09:28:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=8
05/30/2022 09:29:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=9
05/30/2022 09:29:12 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 09:29:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=9
05/30/2022 09:29:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=10
05/30/2022 09:29:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=11
05/30/2022 09:29:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=11
05/30/2022 09:29:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=12
05/30/2022 09:29:46 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 09:29:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=13
05/30/2022 09:29:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=13
05/30/2022 09:29:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=14
05/30/2022 09:30:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=14
05/30/2022 09:30:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=15
05/30/2022 09:30:20 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 09:30:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=16
05/30/2022 09:30:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=16
05/30/2022 09:30:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=17
05/30/2022 09:30:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=18
05/30/2022 09:30:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=18
05/30/2022 09:30:52 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 09:30:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=19
05/30/2022 09:31:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=19
05/30/2022 09:31:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=20
05/30/2022 09:31:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=21
05/30/2022 09:31:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=21
05/30/2022 09:31:26 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 09:31:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 09:31:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=23
05/30/2022 09:31:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=23
05/30/2022 09:31:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=24
05/30/2022 09:31:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=24
05/30/2022 09:31:59 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 09:32:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=25
05/30/2022 09:32:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=26
05/30/2022 09:32:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=26
05/30/2022 09:32:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=27
05/30/2022 09:32:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=28
05/30/2022 09:32:33 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 09:32:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=28
05/30/2022 09:32:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=29
05/30/2022 09:32:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=29
05/30/2022 09:32:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=30
05/30/2022 09:32:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=31
05/30/2022 09:33:07 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 09:33:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=31
05/30/2022 09:33:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=32
05/30/2022 09:33:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=33
05/30/2022 09:33:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=33
05/30/2022 09:33:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 09:33:41 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 09:33:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 09:33:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=35
05/30/2022 09:33:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/30/2022 09:33:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=36
05/30/2022 09:34:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=37
05/30/2022 09:34:15 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 09:34:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 09:34:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 09:34:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=39
05/30/2022 09:34:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=39
05/30/2022 09:34:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=40
05/30/2022 09:34:48 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 09:34:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=41
05/30/2022 09:34:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=41
05/30/2022 09:35:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=42
05/30/2022 09:35:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 09:35:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=43
05/30/2022 09:35:22 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.33159268929503916 on epoch=43
05/30/2022 09:35:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=44
05/30/2022 09:35:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=44
05/30/2022 09:35:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=45
05/30/2022 09:35:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=46
05/30/2022 09:35:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 09:35:56 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 09:36:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/30/2022 09:36:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 09:36:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=48
05/30/2022 09:36:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=49
05/30/2022 09:36:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=49
05/30/2022 09:36:30 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 09:36:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=50
05/30/2022 09:36:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 09:36:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 09:36:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=52
05/30/2022 09:36:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=53
05/30/2022 09:37:04 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 09:37:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 09:37:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/30/2022 09:37:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/30/2022 09:37:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/30/2022 09:37:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/30/2022 09:37:38 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 09:37:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/30/2022 09:37:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=57
05/30/2022 09:37:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/30/2022 09:37:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/30/2022 09:38:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 09:38:12 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 09:38:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=59
05/30/2022 09:38:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=60
05/30/2022 09:38:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/30/2022 09:38:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=61
05/30/2022 09:38:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 09:38:45 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 09:38:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=63
05/30/2022 09:38:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/30/2022 09:38:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 09:39:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 09:39:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 09:39:20 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 09:39:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/30/2022 09:39:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/30/2022 09:39:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
05/30/2022 09:39:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 09:39:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 09:39:53 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3580357811213414 on epoch=68
05/30/2022 09:39:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3580357811213414 on epoch=68, global_step=1100
05/30/2022 09:39:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=69
05/30/2022 09:40:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 09:40:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/30/2022 09:40:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 09:40:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/30/2022 09:40:27 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 09:40:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/30/2022 09:40:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 09:40:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=73
05/30/2022 09:40:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/30/2022 09:40:49 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/30/2022 09:41:01 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 09:41:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 09:41:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/30/2022 09:41:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/30/2022 09:41:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/30/2022 09:41:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=78
05/30/2022 09:41:34 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3486005089058525 on epoch=78
05/30/2022 09:41:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/30/2022 09:41:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 09:41:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/30/2022 09:41:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=80
05/30/2022 09:41:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/30/2022 09:42:08 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.34485289741504155 on epoch=81
05/30/2022 09:42:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=81
05/30/2022 09:42:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 09:42:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
05/30/2022 09:42:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=83
05/30/2022 09:42:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=84
05/30/2022 09:42:42 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 09:42:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=84
05/30/2022 09:42:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=85
05/30/2022 09:42:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=86
05/30/2022 09:43:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=86
05/30/2022 09:43:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/30/2022 09:43:16 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.3383422492035824 on epoch=87
05/30/2022 09:43:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/30/2022 09:43:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/30/2022 09:43:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
05/30/2022 09:43:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 09:43:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/30/2022 09:43:50 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.35307589038932324 on epoch=90
05/30/2022 09:43:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/30/2022 09:43:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/30/2022 09:44:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=92
05/30/2022 09:44:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/30/2022 09:44:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/30/2022 09:44:24 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.44564013554839244 on epoch=93
05/30/2022 09:44:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3580357811213414 -> 0.44564013554839244 on epoch=93, global_step=1500
05/30/2022 09:44:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/30/2022 09:44:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/30/2022 09:44:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/30/2022 09:44:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/30/2022 09:44:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
05/30/2022 09:44:58 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.3432985515073196 on epoch=96
05/30/2022 09:45:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=97
05/30/2022 09:45:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/30/2022 09:45:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/30/2022 09:45:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/30/2022 09:45:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=99
05/30/2022 09:45:31 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.24898389281950925 on epoch=99
05/30/2022 09:45:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
05/30/2022 09:45:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/30/2022 09:45:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=101
05/30/2022 09:45:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
05/30/2022 09:45:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/30/2022 09:46:06 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.28878692747323514 on epoch=103
05/30/2022 09:46:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/30/2022 09:46:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
05/30/2022 09:46:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/30/2022 09:46:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=105
05/30/2022 09:46:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/30/2022 09:46:39 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.2727725011307101 on epoch=106
05/30/2022 09:46:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=106
05/30/2022 09:46:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/30/2022 09:46:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=108
05/30/2022 09:46:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
05/30/2022 09:47:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
05/30/2022 09:47:13 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.41609644835451287 on epoch=109
05/30/2022 09:47:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
05/30/2022 09:47:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
05/30/2022 09:47:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/30/2022 09:47:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/30/2022 09:47:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
05/30/2022 09:47:47 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.2622405566133089 on epoch=112
05/30/2022 09:47:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=113
05/30/2022 09:47:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/30/2022 09:48:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
05/30/2022 09:48:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 09:48:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/30/2022 09:48:21 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.2697901888185289 on epoch=115
05/30/2022 09:48:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=116
05/30/2022 09:48:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
05/30/2022 09:48:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=117
05/30/2022 09:48:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
05/30/2022 09:48:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=118
05/30/2022 09:48:55 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.2272790903859016 on epoch=118
05/30/2022 09:48:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=119
05/30/2022 09:49:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
05/30/2022 09:49:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/30/2022 09:49:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=121
05/30/2022 09:49:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=121
05/30/2022 09:49:29 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.11314655172413793 on epoch=121
05/30/2022 09:49:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/30/2022 09:49:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=123
05/30/2022 09:49:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
05/30/2022 09:49:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=124
05/30/2022 09:49:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=124
05/30/2022 09:50:03 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.22624007936507937 on epoch=124
05/30/2022 09:50:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
05/30/2022 09:50:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=126
05/30/2022 09:50:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=126
05/30/2022 09:50:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
05/30/2022 09:50:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.15 on epoch=128
05/30/2022 09:50:36 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.07585351907960479 on epoch=128
05/30/2022 09:50:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=128
05/30/2022 09:50:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=129
05/30/2022 09:50:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/30/2022 09:50:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/30/2022 09:50:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/30/2022 09:51:10 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.11186224281014193 on epoch=131
05/30/2022 09:51:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
05/30/2022 09:51:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=132
05/30/2022 09:51:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=133
05/30/2022 09:51:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=133
05/30/2022 09:51:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
05/30/2022 09:51:44 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.09630122307209105 on epoch=134
05/30/2022 09:51:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/30/2022 09:51:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/30/2022 09:51:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=136
05/30/2022 09:52:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/30/2022 09:52:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/30/2022 09:52:18 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.11036047286047285 on epoch=137
05/30/2022 09:52:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
05/30/2022 09:52:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/30/2022 09:52:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=139
05/30/2022 09:52:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/30/2022 09:52:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=140
05/30/2022 09:52:51 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.08852506801399145 on epoch=140
05/30/2022 09:52:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
05/30/2022 09:53:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/30/2022 09:53:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
05/30/2022 09:53:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=143
05/30/2022 09:53:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/30/2022 09:53:25 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.06370119552058111 on epoch=143
05/30/2022 09:53:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/30/2022 09:53:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=144
05/30/2022 09:53:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
05/30/2022 09:53:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=146
05/30/2022 09:53:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=146
05/30/2022 09:53:59 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.07541548307049335 on epoch=146
05/30/2022 09:54:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=147
05/30/2022 09:54:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=148
05/30/2022 09:54:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
05/30/2022 09:54:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
05/30/2022 09:54:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.16 on epoch=149
05/30/2022 09:54:33 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.08638001843935965 on epoch=149
05/30/2022 09:54:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
05/30/2022 09:54:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
05/30/2022 09:54:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/30/2022 09:54:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=152
05/30/2022 09:54:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/30/2022 09:55:06 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.0755529602702021 on epoch=153
05/30/2022 09:55:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
05/30/2022 09:55:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
05/30/2022 09:55:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=154
05/30/2022 09:55:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/30/2022 09:55:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=156
05/30/2022 09:55:40 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.1106384729307224 on epoch=156
05/30/2022 09:55:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
05/30/2022 09:55:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/30/2022 09:55:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/30/2022 09:55:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/30/2022 09:56:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=159
05/30/2022 09:56:14 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.13696201261228105 on epoch=159
05/30/2022 09:56:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
05/30/2022 09:56:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=160
05/30/2022 09:56:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=161
05/30/2022 09:56:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=161
05/30/2022 09:56:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=162
05/30/2022 09:56:48 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.09111290467222671 on epoch=162
05/30/2022 09:56:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=163
05/30/2022 09:56:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
05/30/2022 09:57:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
05/30/2022 09:57:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=164
05/30/2022 09:57:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
05/30/2022 09:57:21 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.08087367178276268 on epoch=165
05/30/2022 09:57:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/30/2022 09:57:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/30/2022 09:57:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/30/2022 09:57:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/30/2022 09:57:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
05/30/2022 09:57:55 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.07807793449677265 on epoch=168
05/30/2022 09:58:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
05/30/2022 09:58:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=169
05/30/2022 09:58:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=170
05/30/2022 09:58:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=171
05/30/2022 09:58:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/30/2022 09:58:29 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.0628143392188336 on epoch=171
05/30/2022 09:58:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
05/30/2022 09:58:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/30/2022 09:58:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
05/30/2022 09:58:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/30/2022 09:58:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/30/2022 09:59:03 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.09980892435652468 on epoch=174
05/30/2022 09:59:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=175
05/30/2022 09:59:12 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
05/30/2022 09:59:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=176
05/30/2022 09:59:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=177
05/30/2022 09:59:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
05/30/2022 09:59:37 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.06513035836344107 on epoch=178
05/30/2022 09:59:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=178
05/30/2022 09:59:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/30/2022 09:59:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
05/30/2022 09:59:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/30/2022 09:59:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
05/30/2022 10:00:10 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.06344189172343896 on epoch=181
05/30/2022 10:00:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
05/30/2022 10:00:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=182
05/30/2022 10:00:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/30/2022 10:00:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/30/2022 10:00:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/30/2022 10:00:44 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.06947866947866947 on epoch=184
05/30/2022 10:00:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=184
05/30/2022 10:00:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
05/30/2022 10:00:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/30/2022 10:01:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
05/30/2022 10:01:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/30/2022 10:01:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:01:08 - INFO - __main__ - Printing 3 examples
05/30/2022 10:01:08 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 10:01:08 - INFO - __main__ - ['refuted']
05/30/2022 10:01:08 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 10:01:08 - INFO - __main__ - ['refuted']
05/30/2022 10:01:08 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 10:01:08 - INFO - __main__ - ['refuted']
05/30/2022 10:01:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:01:08 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:01:09 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 10:01:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:01:09 - INFO - __main__ - Printing 3 examples
05/30/2022 10:01:09 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 10:01:09 - INFO - __main__ - ['refuted']
05/30/2022 10:01:09 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 10:01:09 - INFO - __main__ - ['refuted']
05/30/2022 10:01:09 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 10:01:09 - INFO - __main__ - ['refuted']
05/30/2022 10:01:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:01:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:01:09 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 10:01:18 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.11770969459367892 on epoch=187
05/30/2022 10:01:18 - INFO - __main__ - save last model!
05/30/2022 10:01:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 10:01:18 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 10:01:18 - INFO - __main__ - Printing 3 examples
05/30/2022 10:01:18 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:01:18 - INFO - __main__ - ['entailed']
05/30/2022 10:01:18 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:01:18 - INFO - __main__ - ['entailed']
05/30/2022 10:01:18 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:01:18 - INFO - __main__ - ['entailed']
05/30/2022 10:01:18 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:01:28 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 10:01:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 10:01:28 - INFO - __main__ - Starting training!
05/30/2022 10:01:42 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:01:55 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 10:11:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_100_0.4_8_predictions.txt
05/30/2022 10:11:13 - INFO - __main__ - Classification-F1 on test data: 0.0118
05/30/2022 10:11:13 - INFO - __main__ - prefix=tab_fact_128_100, lr=0.4, bsz=8, dev_performance=0.44564013554839244, test_performance=0.011844089807540746
05/30/2022 10:11:13 - INFO - __main__ - Running ... prefix=tab_fact_128_100, lr=0.3, bsz=8 ...
05/30/2022 10:11:14 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:11:14 - INFO - __main__ - Printing 3 examples
05/30/2022 10:11:14 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 10:11:14 - INFO - __main__ - ['refuted']
05/30/2022 10:11:14 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 10:11:14 - INFO - __main__ - ['refuted']
05/30/2022 10:11:14 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 10:11:14 - INFO - __main__ - ['refuted']
05/30/2022 10:11:14 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:11:15 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:11:15 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 10:11:15 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:11:15 - INFO - __main__ - Printing 3 examples
05/30/2022 10:11:15 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 10:11:15 - INFO - __main__ - ['refuted']
05/30/2022 10:11:15 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 10:11:15 - INFO - __main__ - ['refuted']
05/30/2022 10:11:15 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 10:11:15 - INFO - __main__ - ['refuted']
05/30/2022 10:11:15 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:11:16 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:11:16 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 10:11:32 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 10:11:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 10:11:33 - INFO - __main__ - Starting training!
05/30/2022 10:11:38 - INFO - __main__ - Step 10 Global step 10 Train loss 4.91 on epoch=0
05/30/2022 10:11:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.37 on epoch=1
05/30/2022 10:11:47 - INFO - __main__ - Step 30 Global step 30 Train loss 2.70 on epoch=1
05/30/2022 10:11:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.13 on epoch=2
05/30/2022 10:11:55 - INFO - __main__ - Step 50 Global step 50 Train loss 1.61 on epoch=3
05/30/2022 10:12:07 - INFO - __main__ - Global step 50 Train loss 2.94 Classification-F1 0.017051705170517052 on epoch=3
05/30/2022 10:12:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.017051705170517052 on epoch=3, global_step=50
05/30/2022 10:12:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.27 on epoch=3
05/30/2022 10:12:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=4
05/30/2022 10:12:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=4
05/30/2022 10:12:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/30/2022 10:12:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/30/2022 10:12:40 - INFO - __main__ - Global step 100 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 10:12:40 - INFO - __main__ - Saving model with best Classification-F1: 0.017051705170517052 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 10:12:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/30/2022 10:12:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=7
05/30/2022 10:12:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.35 on epoch=8
05/30/2022 10:12:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=8
05/30/2022 10:13:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=9
05/30/2022 10:13:14 - INFO - __main__ - Global step 150 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 10:13:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=9
05/30/2022 10:13:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=10
05/30/2022 10:13:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=11
05/30/2022 10:13:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=11
05/30/2022 10:13:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=12
05/30/2022 10:13:47 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 10:13:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=13
05/30/2022 10:13:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=13
05/30/2022 10:14:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=14
05/30/2022 10:14:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=14
05/30/2022 10:14:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=15
05/30/2022 10:14:21 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 10:14:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=16
05/30/2022 10:14:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=16
05/30/2022 10:14:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=17
05/30/2022 10:14:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=18
05/30/2022 10:14:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=18
05/30/2022 10:14:54 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 10:14:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=19
05/30/2022 10:15:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=19
05/30/2022 10:15:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=20
05/30/2022 10:15:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=21
05/30/2022 10:15:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=21
05/30/2022 10:15:28 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 10:15:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 10:15:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=23
05/30/2022 10:15:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=23
05/30/2022 10:15:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=24
05/30/2022 10:15:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=24
05/30/2022 10:16:01 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 10:16:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=25
05/30/2022 10:16:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=26
05/30/2022 10:16:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=26
05/30/2022 10:16:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=27
05/30/2022 10:16:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=28
05/30/2022 10:16:35 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 10:16:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=28
05/30/2022 10:16:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=29
05/30/2022 10:16:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 10:16:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=30
05/30/2022 10:16:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=31
05/30/2022 10:17:09 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 10:17:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 10:17:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=32
05/30/2022 10:17:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=33
05/30/2022 10:17:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=33
05/30/2022 10:17:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=34
05/30/2022 10:17:43 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 10:17:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=34
05/30/2022 10:17:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=35
05/30/2022 10:17:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=36
05/30/2022 10:18:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=36
05/30/2022 10:18:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=37
05/30/2022 10:18:16 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 10:18:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=38
05/30/2022 10:18:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 10:18:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=39
05/30/2022 10:18:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=39
05/30/2022 10:18:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=40
05/30/2022 10:18:50 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 10:18:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=41
05/30/2022 10:18:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=41
05/30/2022 10:19:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 10:19:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=43
05/30/2022 10:19:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=43
05/30/2022 10:19:24 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 10:19:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=44
05/30/2022 10:19:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=44
05/30/2022 10:19:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=45
05/30/2022 10:19:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=46
05/30/2022 10:19:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/30/2022 10:19:57 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 10:20:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=47
05/30/2022 10:20:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 10:20:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/30/2022 10:20:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=49
05/30/2022 10:20:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/30/2022 10:20:31 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 10:20:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=50
05/30/2022 10:20:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=51
05/30/2022 10:20:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 10:20:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=52
05/30/2022 10:20:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=53
05/30/2022 10:21:05 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 10:21:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 10:21:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/30/2022 10:21:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=54
05/30/2022 10:21:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 10:21:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=56
05/30/2022 10:21:39 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 10:21:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=56
05/30/2022 10:21:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/30/2022 10:21:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 10:21:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/30/2022 10:22:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 10:22:13 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 10:22:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/30/2022 10:22:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=60
05/30/2022 10:22:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 10:22:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=61
05/30/2022 10:22:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=62
05/30/2022 10:22:47 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 10:22:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=63
05/30/2022 10:22:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/30/2022 10:23:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=64
05/30/2022 10:23:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=64
05/30/2022 10:23:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 10:23:21 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 10:23:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=66
05/30/2022 10:23:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/30/2022 10:23:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 10:23:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/30/2022 10:23:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 10:23:55 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 10:23:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 10:24:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/30/2022 10:24:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/30/2022 10:24:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=71
05/30/2022 10:24:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
05/30/2022 10:24:29 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 10:24:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
05/30/2022 10:24:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=73
05/30/2022 10:24:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/30/2022 10:24:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=74
05/30/2022 10:24:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=74
05/30/2022 10:25:02 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 10:25:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 10:25:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/30/2022 10:25:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/30/2022 10:25:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=77
05/30/2022 10:25:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=78
05/30/2022 10:25:36 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 10:25:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/30/2022 10:25:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=79
05/30/2022 10:25:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=79
05/30/2022 10:25:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/30/2022 10:25:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=81
05/30/2022 10:26:10 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 10:26:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/30/2022 10:26:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 10:26:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
05/30/2022 10:26:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/30/2022 10:26:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=84
05/30/2022 10:26:44 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 10:26:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/30/2022 10:26:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/30/2022 10:26:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/30/2022 10:27:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/30/2022 10:27:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
05/30/2022 10:27:18 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 10:27:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=88
05/30/2022 10:27:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
05/30/2022 10:27:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/30/2022 10:27:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=89
05/30/2022 10:27:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=90
05/30/2022 10:27:51 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.33159268929503916 on epoch=90
05/30/2022 10:27:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/30/2022 10:28:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/30/2022 10:28:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/30/2022 10:28:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/30/2022 10:28:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
05/30/2022 10:28:25 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 10:28:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=94
05/30/2022 10:28:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/30/2022 10:28:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/30/2022 10:28:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/30/2022 10:28:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=96
05/30/2022 10:28:59 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 10:29:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/30/2022 10:29:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/30/2022 10:29:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=98
05/30/2022 10:29:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/30/2022 10:29:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
05/30/2022 10:29:33 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 10:29:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
05/30/2022 10:29:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=101
05/30/2022 10:29:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/30/2022 10:29:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=102
05/30/2022 10:29:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 10:30:07 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.34485289741504155 on epoch=103
05/30/2022 10:30:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34485289741504155 on epoch=103, global_step=1650
05/30/2022 10:30:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=103
05/30/2022 10:30:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/30/2022 10:30:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
05/30/2022 10:30:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/30/2022 10:30:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
05/30/2022 10:30:40 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=106
05/30/2022 10:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.34485289741504155 -> 0.3486005089058525 on epoch=106, global_step=1700
05/30/2022 10:30:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
05/30/2022 10:30:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/30/2022 10:30:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/30/2022 10:30:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/30/2022 10:31:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=109
05/30/2022 10:31:14 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.34485289741504155 on epoch=109
05/30/2022 10:31:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/30/2022 10:31:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/30/2022 10:31:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=111
05/30/2022 10:31:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=111
05/30/2022 10:31:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
05/30/2022 10:31:48 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 10:31:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
05/30/2022 10:31:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 10:32:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=114
05/30/2022 10:32:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 10:32:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
05/30/2022 10:32:21 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.35719708029197084 on epoch=115
05/30/2022 10:32:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3486005089058525 -> 0.35719708029197084 on epoch=115, global_step=1850
05/30/2022 10:32:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/30/2022 10:32:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
05/30/2022 10:32:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/30/2022 10:32:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/30/2022 10:32:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/30/2022 10:32:55 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.15823312489979158 on epoch=118
05/30/2022 10:33:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/30/2022 10:33:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=119
05/30/2022 10:33:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=120
05/30/2022 10:33:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=121
05/30/2022 10:33:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 10:33:29 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.39756367663344405 on epoch=121
05/30/2022 10:33:29 - INFO - __main__ - Saving model with best Classification-F1: 0.35719708029197084 -> 0.39756367663344405 on epoch=121, global_step=1950
05/30/2022 10:33:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=122
05/30/2022 10:33:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
05/30/2022 10:33:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
05/30/2022 10:33:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/30/2022 10:33:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/30/2022 10:34:03 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.22234432234432233 on epoch=124
05/30/2022 10:34:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/30/2022 10:34:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=126
05/30/2022 10:34:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
05/30/2022 10:34:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=127
05/30/2022 10:34:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/30/2022 10:34:37 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.1834929235368303 on epoch=128
05/30/2022 10:34:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=128
05/30/2022 10:34:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=129
05/30/2022 10:34:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/30/2022 10:34:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/30/2022 10:34:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=131
05/30/2022 10:35:11 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.22277908805031446 on epoch=131
05/30/2022 10:35:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=131
05/30/2022 10:35:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=132
05/30/2022 10:35:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/30/2022 10:35:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=133
05/30/2022 10:35:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/30/2022 10:35:44 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.1815179991860594 on epoch=134
05/30/2022 10:35:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 10:35:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/30/2022 10:35:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/30/2022 10:36:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=136
05/30/2022 10:36:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=137
05/30/2022 10:36:18 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.11968069321621459 on epoch=137
05/30/2022 10:36:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
05/30/2022 10:36:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
05/30/2022 10:36:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/30/2022 10:36:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=139
05/30/2022 10:36:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
05/30/2022 10:36:52 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.16321112515802783 on epoch=140
05/30/2022 10:36:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=141
05/30/2022 10:37:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=141
05/30/2022 10:37:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=142
05/30/2022 10:37:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/30/2022 10:37:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/30/2022 10:37:26 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.08446942148760331 on epoch=143
05/30/2022 10:37:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/30/2022 10:37:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=144
05/30/2022 10:37:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
05/30/2022 10:37:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
05/30/2022 10:37:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/30/2022 10:37:59 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.1034358180343582 on epoch=146
05/30/2022 10:38:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=147
05/30/2022 10:38:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
05/30/2022 10:38:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/30/2022 10:38:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=149
05/30/2022 10:38:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
05/30/2022 10:38:33 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.09886757348686281 on epoch=149
05/30/2022 10:38:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/30/2022 10:38:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/30/2022 10:38:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/30/2022 10:38:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/30/2022 10:38:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
05/30/2022 10:39:07 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.11449154560609669 on epoch=153
05/30/2022 10:39:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=153
05/30/2022 10:39:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
05/30/2022 10:39:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=154
05/30/2022 10:39:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=155
05/30/2022 10:39:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
05/30/2022 10:39:41 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.10159621987578976 on epoch=156
05/30/2022 10:39:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
05/30/2022 10:39:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/30/2022 10:39:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/30/2022 10:39:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/30/2022 10:40:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
05/30/2022 10:40:15 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.10172958072117735 on epoch=159
05/30/2022 10:40:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
05/30/2022 10:40:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
05/30/2022 10:40:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=161
05/30/2022 10:40:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
05/30/2022 10:40:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=162
05/30/2022 10:40:49 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.10645174955519783 on epoch=162
05/30/2022 10:40:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=163
05/30/2022 10:40:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/30/2022 10:41:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=164
05/30/2022 10:41:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/30/2022 10:41:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/30/2022 10:41:23 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.08959503352053633 on epoch=165
05/30/2022 10:41:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=166
05/30/2022 10:41:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/30/2022 10:41:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/30/2022 10:41:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
05/30/2022 10:41:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
05/30/2022 10:41:57 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.07324851074851074 on epoch=168
05/30/2022 10:42:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
05/30/2022 10:42:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/30/2022 10:42:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/30/2022 10:42:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/30/2022 10:42:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/30/2022 10:42:31 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.06535275766044997 on epoch=171
05/30/2022 10:42:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/30/2022 10:42:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/30/2022 10:42:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=173
05/30/2022 10:42:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/30/2022 10:42:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
05/30/2022 10:43:05 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.08723468768950879 on epoch=174
05/30/2022 10:43:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
05/30/2022 10:43:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=176
05/30/2022 10:43:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/30/2022 10:43:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=177
05/30/2022 10:43:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/30/2022 10:43:38 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.062218831201548995 on epoch=178
05/30/2022 10:43:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
05/30/2022 10:43:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=179
05/30/2022 10:43:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=179
05/30/2022 10:43:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=180
05/30/2022 10:44:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/30/2022 10:44:12 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.0764957264957265 on epoch=181
05/30/2022 10:44:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
05/30/2022 10:44:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=182
05/30/2022 10:44:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
05/30/2022 10:44:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=183
05/30/2022 10:44:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
05/30/2022 10:44:46 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.057036353185040675 on epoch=184
05/30/2022 10:44:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=184
05/30/2022 10:44:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/30/2022 10:45:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
05/30/2022 10:45:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=186
05/30/2022 10:45:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
05/30/2022 10:45:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:45:10 - INFO - __main__ - Printing 3 examples
05/30/2022 10:45:10 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 10:45:10 - INFO - __main__ - ['refuted']
05/30/2022 10:45:10 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 10:45:10 - INFO - __main__ - ['refuted']
05/30/2022 10:45:10 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 10:45:10 - INFO - __main__ - ['refuted']
05/30/2022 10:45:10 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:45:10 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:45:11 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 10:45:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:45:11 - INFO - __main__ - Printing 3 examples
05/30/2022 10:45:11 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 10:45:11 - INFO - __main__ - ['refuted']
05/30/2022 10:45:11 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 10:45:11 - INFO - __main__ - ['refuted']
05/30/2022 10:45:11 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 10:45:11 - INFO - __main__ - ['refuted']
05/30/2022 10:45:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:45:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:45:11 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 10:45:20 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.07975263707747783 on epoch=187
05/30/2022 10:45:20 - INFO - __main__ - save last model!
05/30/2022 10:45:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 10:45:20 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 10:45:20 - INFO - __main__ - Printing 3 examples
05/30/2022 10:45:20 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:45:20 - INFO - __main__ - ['entailed']
05/30/2022 10:45:20 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:45:20 - INFO - __main__ - ['entailed']
05/30/2022 10:45:20 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 10:45:20 - INFO - __main__ - ['entailed']
05/30/2022 10:45:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:45:28 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 10:45:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 10:45:29 - INFO - __main__ - Starting training!
05/30/2022 10:45:44 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:45:57 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 10:55:15 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_100_0.3_8_predictions.txt
05/30/2022 10:55:15 - INFO - __main__ - Classification-F1 on test data: 0.0178
05/30/2022 10:55:16 - INFO - __main__ - prefix=tab_fact_128_100, lr=0.3, bsz=8, dev_performance=0.39756367663344405, test_performance=0.017815233429909795
05/30/2022 10:55:16 - INFO - __main__ - Running ... prefix=tab_fact_128_100, lr=0.2, bsz=8 ...
05/30/2022 10:55:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:55:17 - INFO - __main__ - Printing 3 examples
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:55:17 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:55:17 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 10:55:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 10:55:17 - INFO - __main__ - Printing 3 examples
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: tricia flores be ranked 37th [SEP] table_caption: athletics at the 2008 summer olympics - women 's long jump [SEP] table_text: rank#group#name#nationality#mark [n] 1#b#brittney reese#united states#6.87 [n] 2#a#maurren higa maggi#brazil#6.79 [n] 3#b#tatyana lebedeva#russia#6.70 [n] 4#a#carolina klüft#sweden#6.70 [n] 5#a#grace upshaw#united states#6.68 [n] 6#a#oksana udmurtova#russia#6.63 [n] 7#b#keila costa#brazil#6.62 [n] 8#a#tabia charles#canada#6.61 [n] 9#a#funmi jimoh#united states#6.61 [n] 10#b#jade johnson#great britain#6.61 [n] 11#a#chelsea hammond#jamaica#6.60 [n] 12#b#blessing okagbare#nigeria#6.59 [n] 13#b#tatyana kotova#russia#6.57 [n] 14#a#hrysopiyí devetzí#greece#6.57 [n] 15#b#concepción montaner#spain#6.53 [n] 16#b#bronwyn thompson#australia#6.53 [n] 17#a#yargelis savigne#cuba#6.49 [n] 18#b#iryna charnushenka - stasiuk#belarus#6.48 [n] 19#b#kumiko ikeda#japan#6.47 [n] 20#a#denisa ščerbová#czech republic#6.46 [n] 21#a#patricia sylvester#grenada#6.44 [n] 22#b#viorica țigău#romania#6.44 [n] 23#a#viktoriya rybalko#ukraine#6.43 [n] 24#a#karin melis mey#turkey#6.42 [n] 25#b#ruky abdulai#canada#6.41 [n] 26#a#nina kolarič#slovenia#6.40 [n] 27#b#ksenija balta#estonia#6.38 [n] 28#b#soon - ok jung#south korea#6.33 [n] 29#b#olga rypakova#kazakhstan#6.30 [n] 30#b#ivana španović#serbia#6.30 [n] 31#a#naide gomes#portugal#6.29 [n] 32#a#volha sergeenka#belarus#6.25 [n] 33#b#oleksandra stadnyuk#ukraine#6.19 [n] 34#b#marestella torres#philippines#6.17 [n] 35#a#pamela mouele - mboussi#congo#6.06 [n] 36#b#arantxa king#bermuda#6.01 [n] 37#a#rhonda watkins#trinidad and tobago#5.88 [n] 38#a#tricia flores#belize#5.25 [n] n / a#b#anju bobby george#india#nm [n] n / a#b#jackie edwards#bahamas#nm [n] n / a#a#jana velďáková#slovakia#nm [n] n / a#a#lyudmyla blonska#ukraine#dsq [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: 1943 be the year that have the least candidate elect [SEP] table_caption: co - operative commonwealth federation (ontario section) [SEP] table_text: year of election#candidates elected#of seats available#of votes#% of popular vote [n] 1934#1#90#na#7.0% [n] 1937#0#90#na#5.6% [n] 1943#34#90#na#31.7% [n] 1945#8#90#na#22.4% [n] 1948#21#90#na#27.0% [n] 1951#2#90#na#19.1% [n] 1955#3#98#na#16.5% [n] 1959#5#98#na#16.7% [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ -  [tab_fact] statement: the opponent of the phoenix sun play more than 17 game [SEP] table_caption: 1982 - 83 philadelphia 76ers season [SEP] table_text: game#date#opponent#score#location#record [n] 17#december 2#phoenix suns#116 - 108#arizona veterans memorial coliseum#14 - 3 [n] 18#december 3#san diego clippers#127 - 110#san diego sports arena#15 - 3 [n] 19#december 5#los angeles lakers#114 - 104#the forum#16 - 3 [n] 20#december 8#atlanta hawks#132 - 85#philadelphia spectrum#17 - 3 [n] 21#december 10#boston celtics#97 - 123#boston garden#17 - 4 [n] 22#december 11#detroit pistons#128 - 111#philadelphia spectrum#18 - 4 [n] 23#december 15#cleveland cavaliers#99 - 93#philadelphia spectrum#19 - 4 [n] 24#december 17#new york knicks#109 - 95#philadelphia spectrum#20 - 4 [n] 25#december 18#washington bullets#97 - 100#capital centre#20 - 5 [n] 26#december 21#boston celtics#122 - 105#philadelphia spectrum#21 - 5 [n] 27#december 26#san antonio spurs#124 - 122#hemisfair arena#22 - 5 [n] 28#december 28#houston rockets#104 - 93#the summit#23 - 5 [n] 29#december 29#dallas mavericks#126 - 116#reunion arena#24 - 5 [n] 
05/30/2022 10:55:17 - INFO - __main__ - ['refuted']
05/30/2022 10:55:17 - INFO - __main__ - Tokenizing Input ...
05/30/2022 10:55:18 - INFO - __main__ - Tokenizing Output ...
05/30/2022 10:55:18 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 10:55:34 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 10:55:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 10:55:34 - INFO - __main__ - Starting training!
05/30/2022 10:55:39 - INFO - __main__ - Step 10 Global step 10 Train loss 5.63 on epoch=0
05/30/2022 10:55:44 - INFO - __main__ - Step 20 Global step 20 Train loss 3.82 on epoch=1
05/30/2022 10:55:48 - INFO - __main__ - Step 30 Global step 30 Train loss 3.17 on epoch=1
05/30/2022 10:55:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.67 on epoch=2
05/30/2022 10:55:57 - INFO - __main__ - Step 50 Global step 50 Train loss 2.18 on epoch=3
05/30/2022 10:56:08 - INFO - __main__ - Global step 50 Train loss 3.50 Classification-F1 0.0 on epoch=3
05/30/2022 10:56:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/30/2022 10:56:13 - INFO - __main__ - Step 60 Global step 60 Train loss 1.89 on epoch=3
05/30/2022 10:56:17 - INFO - __main__ - Step 70 Global step 70 Train loss 1.60 on epoch=4
05/30/2022 10:56:22 - INFO - __main__ - Step 80 Global step 80 Train loss 1.29 on epoch=4
05/30/2022 10:56:26 - INFO - __main__ - Step 90 Global step 90 Train loss 1.08 on epoch=5
05/30/2022 10:56:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=6
05/30/2022 10:56:42 - INFO - __main__ - Global step 100 Train loss 1.34 Classification-F1 0.22106179286335945 on epoch=6
05/30/2022 10:56:42 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.22106179286335945 on epoch=6, global_step=100
05/30/2022 10:56:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=6
05/30/2022 10:56:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=7
05/30/2022 10:56:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=8
05/30/2022 10:56:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/30/2022 10:57:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/30/2022 10:57:15 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 10:57:15 - INFO - __main__ - Saving model with best Classification-F1: 0.22106179286335945 -> 0.3333333333333333 on epoch=9, global_step=150
05/30/2022 10:57:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=9
05/30/2022 10:57:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=10
05/30/2022 10:57:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=11
05/30/2022 10:57:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=11
05/30/2022 10:57:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=12
05/30/2022 10:57:48 - INFO - __main__ - Global step 200 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 10:57:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.34 on epoch=13
05/30/2022 10:57:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.30 on epoch=13
05/30/2022 10:58:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=14
05/30/2022 10:58:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=14
05/30/2022 10:58:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=15
05/30/2022 10:58:22 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 10:58:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=16
05/30/2022 10:58:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=16
05/30/2022 10:58:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=17
05/30/2022 10:58:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=18
05/30/2022 10:58:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=18
05/30/2022 10:58:55 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 10:59:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=19
05/30/2022 10:59:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=19
05/30/2022 10:59:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=20
05/30/2022 10:59:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=21
05/30/2022 10:59:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=21
05/30/2022 10:59:29 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 10:59:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=22
05/30/2022 10:59:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=23
05/30/2022 10:59:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=23
05/30/2022 10:59:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=24
05/30/2022 10:59:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=24
05/30/2022 11:00:03 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 11:00:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=25
05/30/2022 11:00:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 11:00:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=26
05/30/2022 11:00:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=27
05/30/2022 11:00:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=28
05/30/2022 11:00:36 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 11:00:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=28
05/30/2022 11:00:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=29
05/30/2022 11:00:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=29
05/30/2022 11:00:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=30
05/30/2022 11:00:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=31
05/30/2022 11:01:10 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 11:01:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 11:01:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=32
05/30/2022 11:01:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 11:01:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=33
05/30/2022 11:01:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=34
05/30/2022 11:01:43 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 11:01:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=34
05/30/2022 11:01:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=35
05/30/2022 11:01:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=36
05/30/2022 11:02:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=36
05/30/2022 11:02:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=37
05/30/2022 11:02:17 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 11:02:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=38
05/30/2022 11:02:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 11:02:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=39
05/30/2022 11:02:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=39
05/30/2022 11:02:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=40
05/30/2022 11:02:51 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 11:02:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=41
05/30/2022 11:03:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=41
05/30/2022 11:03:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=42
05/30/2022 11:03:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/30/2022 11:03:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=43
05/30/2022 11:03:24 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 11:03:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=44
05/30/2022 11:03:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=44
05/30/2022 11:03:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=45
05/30/2022 11:03:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=46
05/30/2022 11:03:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/30/2022 11:03:58 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 11:04:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=47
05/30/2022 11:04:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=48
05/30/2022 11:04:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=48
05/30/2022 11:04:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=49
05/30/2022 11:04:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=49
05/30/2022 11:04:31 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 11:04:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=50
05/30/2022 11:04:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=51
05/30/2022 11:04:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 11:04:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=52
05/30/2022 11:04:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/30/2022 11:05:05 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 11:05:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=53
05/30/2022 11:05:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=54
05/30/2022 11:05:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=54
05/30/2022 11:05:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=55
05/30/2022 11:05:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 11:05:39 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 11:05:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/30/2022 11:05:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=57
05/30/2022 11:05:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=58
05/30/2022 11:05:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=58
05/30/2022 11:06:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=59
05/30/2022 11:06:13 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 11:06:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=59
05/30/2022 11:06:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=60
05/30/2022 11:06:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
05/30/2022 11:06:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 11:06:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=62
05/30/2022 11:06:46 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 11:06:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
05/30/2022 11:06:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/30/2022 11:07:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/30/2022 11:07:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=64
05/30/2022 11:07:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/30/2022 11:07:20 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 11:07:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/30/2022 11:07:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=66
05/30/2022 11:07:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/30/2022 11:07:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/30/2022 11:07:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 11:07:54 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 11:07:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 11:08:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=69
05/30/2022 11:08:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/30/2022 11:08:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=71
05/30/2022 11:08:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
05/30/2022 11:08:28 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 11:08:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/30/2022 11:08:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 11:08:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/30/2022 11:08:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/30/2022 11:08:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 11:09:02 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 11:09:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=75
05/30/2022 11:09:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 11:09:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=76
05/30/2022 11:09:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/30/2022 11:09:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 11:09:36 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 11:09:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/30/2022 11:09:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 11:09:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=79
05/30/2022 11:09:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=80
05/30/2022 11:09:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=81
05/30/2022 11:10:10 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 11:10:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=81
05/30/2022 11:10:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/30/2022 11:10:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
05/30/2022 11:10:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/30/2022 11:10:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/30/2022 11:10:44 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 11:10:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=84
05/30/2022 11:10:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=85
05/30/2022 11:10:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
05/30/2022 11:11:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=86
05/30/2022 11:11:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
05/30/2022 11:11:17 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 11:11:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/30/2022 11:11:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
05/30/2022 11:11:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/30/2022 11:11:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 11:11:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=90
05/30/2022 11:11:51 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 11:11:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 11:12:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/30/2022 11:12:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
05/30/2022 11:12:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=93
05/30/2022 11:12:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/30/2022 11:12:25 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 11:12:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/30/2022 11:12:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=94
05/30/2022 11:12:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/30/2022 11:12:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/30/2022 11:12:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=96
05/30/2022 11:12:59 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 11:13:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=97
05/30/2022 11:13:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=98
05/30/2022 11:13:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/30/2022 11:13:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=99
05/30/2022 11:13:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
05/30/2022 11:13:33 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 11:13:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/30/2022 11:13:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=101
05/30/2022 11:13:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/30/2022 11:13:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=102
05/30/2022 11:13:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/30/2022 11:14:06 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 11:14:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
05/30/2022 11:14:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=104
05/30/2022 11:14:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/30/2022 11:14:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/30/2022 11:14:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=106
05/30/2022 11:14:40 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 11:14:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 11:14:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/30/2022 11:14:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
05/30/2022 11:14:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=108
05/30/2022 11:15:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=109
05/30/2022 11:15:14 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 11:15:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/30/2022 11:15:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/30/2022 11:15:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 11:15:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=111
05/30/2022 11:15:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
05/30/2022 11:15:48 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 11:15:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/30/2022 11:15:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 11:16:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/30/2022 11:16:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/30/2022 11:16:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=115
05/30/2022 11:16:22 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=115
05/30/2022 11:16:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=116
05/30/2022 11:16:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=116
05/30/2022 11:16:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/30/2022 11:16:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
05/30/2022 11:16:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/30/2022 11:16:56 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=118
05/30/2022 11:17:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/30/2022 11:17:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 11:17:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=120
05/30/2022 11:17:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/30/2022 11:17:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/30/2022 11:17:30 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=121
05/30/2022 11:17:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
05/30/2022 11:17:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
05/30/2022 11:17:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=123
05/30/2022 11:17:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/30/2022 11:17:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=124
05/30/2022 11:18:04 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 11:18:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=125
05/30/2022 11:18:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=126
05/30/2022 11:18:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 11:18:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
05/30/2022 11:18:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/30/2022 11:18:37 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=128
05/30/2022 11:18:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=128
05/30/2022 11:18:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=129
05/30/2022 11:18:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
05/30/2022 11:18:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=130
05/30/2022 11:19:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
05/30/2022 11:19:11 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=131
05/30/2022 11:19:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
05/30/2022 11:19:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
05/30/2022 11:19:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=133
05/30/2022 11:19:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/30/2022 11:19:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
05/30/2022 11:19:45 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=134
05/30/2022 11:19:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
05/30/2022 11:19:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.20 on epoch=135
05/30/2022 11:19:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/30/2022 11:20:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/30/2022 11:20:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=137
05/30/2022 11:20:19 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=137
05/30/2022 11:20:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
05/30/2022 11:20:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/30/2022 11:20:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/30/2022 11:20:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=139
05/30/2022 11:20:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/30/2022 11:20:53 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=140
05/30/2022 11:20:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
05/30/2022 11:21:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=141
05/30/2022 11:21:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/30/2022 11:21:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/30/2022 11:21:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/30/2022 11:21:27 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=143
05/30/2022 11:21:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/30/2022 11:21:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/30/2022 11:21:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=145
05/30/2022 11:21:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/30/2022 11:21:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/30/2022 11:22:01 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=146
05/30/2022 11:22:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=147
05/30/2022 11:22:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=148
05/30/2022 11:22:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/30/2022 11:22:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
05/30/2022 11:22:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/30/2022 11:22:35 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 11:22:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/30/2022 11:22:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
05/30/2022 11:22:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=151
05/30/2022 11:22:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/30/2022 11:22:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.22 on epoch=153
05/30/2022 11:23:09 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=153
05/30/2022 11:23:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
05/30/2022 11:23:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=154
05/30/2022 11:23:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=154
05/30/2022 11:23:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=155
05/30/2022 11:23:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=156
05/30/2022 11:23:43 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.22106179286335945 on epoch=156
05/30/2022 11:23:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
05/30/2022 11:23:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/30/2022 11:23:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
05/30/2022 11:24:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
05/30/2022 11:24:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=159
05/30/2022 11:24:17 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.22560064607308705 on epoch=159
05/30/2022 11:24:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/30/2022 11:24:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=160
05/30/2022 11:24:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/30/2022 11:24:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
05/30/2022 11:24:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=162
05/30/2022 11:24:51 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.22106179286335945 on epoch=162
05/30/2022 11:24:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=163
05/30/2022 11:25:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
05/30/2022 11:25:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=164
05/30/2022 11:25:09 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
05/30/2022 11:25:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=165
05/30/2022 11:25:25 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.22106179286335945 on epoch=165
05/30/2022 11:25:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=166
05/30/2022 11:25:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/30/2022 11:25:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=167
05/30/2022 11:25:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=168
05/30/2022 11:25:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/30/2022 11:25:58 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.23944294699011678 on epoch=168
05/30/2022 11:26:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/30/2022 11:26:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/30/2022 11:26:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=170
05/30/2022 11:26:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=171
05/30/2022 11:26:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
05/30/2022 11:26:32 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.23693352589874703 on epoch=171
05/30/2022 11:26:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
05/30/2022 11:26:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=173
05/30/2022 11:26:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=173
05/30/2022 11:26:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
05/30/2022 11:26:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=174
05/30/2022 11:27:06 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.2658841940532081 on epoch=174
05/30/2022 11:27:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/30/2022 11:27:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=176
05/30/2022 11:27:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=176
05/30/2022 11:27:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=177
05/30/2022 11:27:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=178
05/30/2022 11:27:40 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.3704956828812838 on epoch=178
05/30/2022 11:27:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3704956828812838 on epoch=178, global_step=2850
05/30/2022 11:27:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=178
05/30/2022 11:27:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=179
05/30/2022 11:27:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=179
05/30/2022 11:27:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=180
05/30/2022 11:28:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
05/30/2022 11:28:14 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.20513880897583428 on epoch=181
05/30/2022 11:28:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=181
05/30/2022 11:28:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=182
05/30/2022 11:28:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.21 on epoch=183
05/30/2022 11:28:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=183
05/30/2022 11:28:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=184
05/30/2022 11:28:48 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.2615280843757665 on epoch=184
05/30/2022 11:28:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=184
05/30/2022 11:28:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=185
05/30/2022 11:29:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=186
05/30/2022 11:29:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=186
05/30/2022 11:29:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=187
05/30/2022 11:29:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 11:29:12 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:12 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 11:29:12 - INFO - __main__ - ['refuted']
05/30/2022 11:29:12 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 11:29:12 - INFO - __main__ - ['refuted']
05/30/2022 11:29:12 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 11:29:12 - INFO - __main__ - ['refuted']
05/30/2022 11:29:12 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:12 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:13 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 11:29:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 11:29:13 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:13 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 11:29:13 - INFO - __main__ - ['refuted']
05/30/2022 11:29:13 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 11:29:13 - INFO - __main__ - ['refuted']
05/30/2022 11:29:13 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 11:29:13 - INFO - __main__ - ['refuted']
05/30/2022 11:29:13 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:13 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 11:29:22 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.15721524564882133 on epoch=187
05/30/2022 11:29:22 - INFO - __main__ - save last model!
05/30/2022 11:29:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 11:29:22 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 11:29:22 - INFO - __main__ - Printing 3 examples
05/30/2022 11:29:22 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:22 - INFO - __main__ - ['entailed']
05/30/2022 11:29:22 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:22 - INFO - __main__ - ['entailed']
05/30/2022 11:29:22 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 11:29:22 - INFO - __main__ - ['entailed']
05/30/2022 11:29:22 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:29:32 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:29:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 11:29:33 - INFO - __main__ - Starting training!
05/30/2022 11:29:46 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:29:59 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 11:39:19 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_100_0.2_8_predictions.txt
05/30/2022 11:39:19 - INFO - __main__ - Classification-F1 on test data: 0.0911
05/30/2022 11:39:19 - INFO - __main__ - prefix=tab_fact_128_100, lr=0.2, bsz=8, dev_performance=0.3704956828812838, test_performance=0.09109826006619598
05/30/2022 11:39:19 - INFO - __main__ - Running ... prefix=tab_fact_128_13, lr=0.5, bsz=8 ...
05/30/2022 11:39:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 11:39:20 - INFO - __main__ - Printing 3 examples
05/30/2022 11:39:20 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 11:39:20 - INFO - __main__ - ['refuted']
05/30/2022 11:39:20 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 11:39:20 - INFO - __main__ - ['refuted']
05/30/2022 11:39:20 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 11:39:20 - INFO - __main__ - ['refuted']
05/30/2022 11:39:20 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:39:20 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:39:21 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 11:39:21 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 11:39:21 - INFO - __main__ - Printing 3 examples
05/30/2022 11:39:21 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 11:39:21 - INFO - __main__ - ['refuted']
05/30/2022 11:39:21 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 11:39:21 - INFO - __main__ - ['refuted']
05/30/2022 11:39:21 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 11:39:21 - INFO - __main__ - ['refuted']
05/30/2022 11:39:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 11:39:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 11:39:21 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 11:39:37 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 11:39:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 11:39:38 - INFO - __main__ - Starting training!
05/30/2022 11:39:43 - INFO - __main__ - Step 10 Global step 10 Train loss 4.18 on epoch=0
05/30/2022 11:39:47 - INFO - __main__ - Step 20 Global step 20 Train loss 2.52 on epoch=1
05/30/2022 11:39:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.73 on epoch=1
05/30/2022 11:39:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.09 on epoch=2
05/30/2022 11:40:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=3
05/30/2022 11:40:13 - INFO - __main__ - Global step 50 Train loss 2.02 Classification-F1 0.3333333333333333 on epoch=3
05/30/2022 11:40:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/30/2022 11:40:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=3
05/30/2022 11:40:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=4
05/30/2022 11:40:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=4
05/30/2022 11:40:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=5
05/30/2022 11:40:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=6
05/30/2022 11:40:45 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 11:40:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=6
05/30/2022 11:40:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=7
05/30/2022 11:40:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=8
05/30/2022 11:41:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=8
05/30/2022 11:41:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=9
05/30/2022 11:41:19 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 11:41:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=9
05/30/2022 11:41:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=10
05/30/2022 11:41:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=11
05/30/2022 11:41:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=11
05/30/2022 11:41:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=12
05/30/2022 11:41:53 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 11:41:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=13
05/30/2022 11:42:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=13
05/30/2022 11:42:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=14
05/30/2022 11:42:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=14
05/30/2022 11:42:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=15
05/30/2022 11:42:26 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 11:42:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=16
05/30/2022 11:42:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=16
05/30/2022 11:42:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=17
05/30/2022 11:42:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=18
05/30/2022 11:42:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=18
05/30/2022 11:43:00 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 11:43:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=19
05/30/2022 11:43:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=19
05/30/2022 11:43:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=20
05/30/2022 11:43:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=21
05/30/2022 11:43:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=21
05/30/2022 11:43:34 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 11:43:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=22
05/30/2022 11:43:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 11:43:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=23
05/30/2022 11:43:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=24
05/30/2022 11:43:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=24
05/30/2022 11:44:08 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 11:44:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=25
05/30/2022 11:44:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=26
05/30/2022 11:44:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=26
05/30/2022 11:44:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=27
05/30/2022 11:44:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=28
05/30/2022 11:44:41 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 11:44:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=28
05/30/2022 11:44:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 11:44:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 11:44:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=30
05/30/2022 11:45:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=31
05/30/2022 11:45:15 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 11:45:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=31
05/30/2022 11:45:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=32
05/30/2022 11:45:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 11:45:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=33
05/30/2022 11:45:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 11:45:49 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 11:45:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=34
05/30/2022 11:45:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=35
05/30/2022 11:46:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/30/2022 11:46:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=36
05/30/2022 11:46:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=37
05/30/2022 11:46:22 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 11:46:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=38
05/30/2022 11:46:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=38
05/30/2022 11:46:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=39
05/30/2022 11:46:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=39
05/30/2022 11:46:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=40
05/30/2022 11:46:56 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=40
05/30/2022 11:46:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3486005089058525 on epoch=40, global_step=650
05/30/2022 11:47:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=41
05/30/2022 11:47:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=41
05/30/2022 11:47:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=42
05/30/2022 11:47:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=43
05/30/2022 11:47:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=43
05/30/2022 11:47:30 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.42840679919331603 on epoch=43
05/30/2022 11:47:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3486005089058525 -> 0.42840679919331603 on epoch=43, global_step=700
05/30/2022 11:47:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=44
05/30/2022 11:47:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=44
05/30/2022 11:47:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=45
05/30/2022 11:47:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=46
05/30/2022 11:47:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/30/2022 11:48:04 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 11:48:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=47
05/30/2022 11:48:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=48
05/30/2022 11:48:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=48
05/30/2022 11:48:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=49
05/30/2022 11:48:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=49
05/30/2022 11:48:38 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 11:48:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=50
05/30/2022 11:48:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=51
05/30/2022 11:48:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=51
05/30/2022 11:48:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=52
05/30/2022 11:49:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=53
05/30/2022 11:49:11 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.3651088894055646 on epoch=53
05/30/2022 11:49:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=53
05/30/2022 11:49:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/30/2022 11:49:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=54
05/30/2022 11:49:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=55
05/30/2022 11:49:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 11:49:45 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.34673046251993617 on epoch=56
05/30/2022 11:49:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/30/2022 11:49:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=57
05/30/2022 11:49:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 11:50:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/30/2022 11:50:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/30/2022 11:50:19 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.33159268929503916 on epoch=59
05/30/2022 11:50:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=59
05/30/2022 11:50:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=60
05/30/2022 11:50:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 11:50:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=61
05/30/2022 11:50:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=62
05/30/2022 11:50:53 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.33652312599681017 on epoch=62
05/30/2022 11:50:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
05/30/2022 11:51:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/30/2022 11:51:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=64
05/30/2022 11:51:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/30/2022 11:51:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=65
05/30/2022 11:51:26 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.4518589301197997 on epoch=65
05/30/2022 11:51:27 - INFO - __main__ - Saving model with best Classification-F1: 0.42840679919331603 -> 0.4518589301197997 on epoch=65, global_step=1050
05/30/2022 11:51:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/30/2022 11:51:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=66
05/30/2022 11:51:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=67
05/30/2022 11:51:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/30/2022 11:51:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/30/2022 11:52:00 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.5075419847328244 on epoch=68
05/30/2022 11:52:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4518589301197997 -> 0.5075419847328244 on epoch=68, global_step=1100
05/30/2022 11:52:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=69
05/30/2022 11:52:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/30/2022 11:52:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/30/2022 11:52:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
05/30/2022 11:52:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/30/2022 11:52:35 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.29574855252274607 on epoch=71
05/30/2022 11:52:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/30/2022 11:52:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 11:52:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 11:52:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=74
05/30/2022 11:52:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=74
05/30/2022 11:53:09 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.4342541436464089 on epoch=74
05/30/2022 11:53:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=75
05/30/2022 11:53:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/30/2022 11:53:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=76
05/30/2022 11:53:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=77
05/30/2022 11:53:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=78
05/30/2022 11:53:43 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.49153888933026135 on epoch=78
05/30/2022 11:53:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/30/2022 11:53:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 11:53:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 11:54:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/30/2022 11:54:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/30/2022 11:54:17 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.4464816214600875 on epoch=81
05/30/2022 11:54:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=81
05/30/2022 11:54:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=82
05/30/2022 11:54:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
05/30/2022 11:54:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=83
05/30/2022 11:54:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=84
05/30/2022 11:54:51 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.4428347956199415 on epoch=84
05/30/2022 11:54:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/30/2022 11:55:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=85
05/30/2022 11:55:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=86
05/30/2022 11:55:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=86
05/30/2022 11:55:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=87
05/30/2022 11:55:25 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.4735281457745468 on epoch=87
05/30/2022 11:55:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=88
05/30/2022 11:55:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=88
05/30/2022 11:55:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
05/30/2022 11:55:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=89
05/30/2022 11:55:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=90
05/30/2022 11:55:59 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.45400756793945646 on epoch=90
05/30/2022 11:56:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=91
05/30/2022 11:56:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=91
05/30/2022 11:56:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=92
05/30/2022 11:56:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/30/2022 11:56:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/30/2022 11:56:33 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.31307713342167814 on epoch=93
05/30/2022 11:56:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=94
05/30/2022 11:56:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=94
05/30/2022 11:56:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=95
05/30/2022 11:56:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=96
05/30/2022 11:56:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=96
05/30/2022 11:57:07 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.22134499983237788 on epoch=96
05/30/2022 11:57:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=97
05/30/2022 11:57:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=98
05/30/2022 11:57:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=98
05/30/2022 11:57:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=99
05/30/2022 11:57:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=99
05/30/2022 11:57:41 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.28238095238095234 on epoch=99
05/30/2022 11:57:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/30/2022 11:57:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/30/2022 11:57:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
05/30/2022 11:57:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=102
05/30/2022 11:58:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/30/2022 11:58:15 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.3075739850080159 on epoch=103
05/30/2022 11:58:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
05/30/2022 11:58:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=104
05/30/2022 11:58:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/30/2022 11:58:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=105
05/30/2022 11:58:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/30/2022 11:58:49 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.29657333029800426 on epoch=106
05/30/2022 11:58:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=106
05/30/2022 11:58:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=107
05/30/2022 11:59:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/30/2022 11:59:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
05/30/2022 11:59:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/30/2022 11:59:24 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.3207491113586927 on epoch=109
05/30/2022 11:59:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=109
05/30/2022 11:59:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=110
05/30/2022 11:59:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=111
05/30/2022 11:59:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=111
05/30/2022 11:59:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=112
05/30/2022 11:59:57 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.162603550295858 on epoch=112
05/30/2022 12:00:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=113
05/30/2022 12:00:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=113
05/30/2022 12:00:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/30/2022 12:00:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=114
05/30/2022 12:00:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=115
05/30/2022 12:00:31 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.18081305197869618 on epoch=115
05/30/2022 12:00:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/30/2022 12:00:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=116
05/30/2022 12:00:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=117
05/30/2022 12:00:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=118
05/30/2022 12:00:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
05/30/2022 12:01:05 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.17584921292460645 on epoch=118
05/30/2022 12:01:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=119
05/30/2022 12:01:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
05/30/2022 12:01:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/30/2022 12:01:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/30/2022 12:01:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=121
05/30/2022 12:01:39 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.16078342798141454 on epoch=121
05/30/2022 12:01:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=122
05/30/2022 12:01:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/30/2022 12:01:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=123
05/30/2022 12:01:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=124
05/30/2022 12:02:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=124
05/30/2022 12:02:13 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.17738825591586327 on epoch=124
05/30/2022 12:02:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=125
05/30/2022 12:02:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/30/2022 12:02:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
05/30/2022 12:02:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=127
05/30/2022 12:02:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=128
05/30/2022 12:02:47 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.11864876385336744 on epoch=128
05/30/2022 12:02:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
05/30/2022 12:02:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=129
05/30/2022 12:03:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=129
05/30/2022 12:03:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=130
05/30/2022 12:03:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/30/2022 12:03:21 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.10959224208435564 on epoch=131
05/30/2022 12:03:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/30/2022 12:03:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=132
05/30/2022 12:03:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=133
05/30/2022 12:03:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
05/30/2022 12:03:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=134
05/30/2022 12:03:55 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.13278388278388276 on epoch=134
05/30/2022 12:04:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/30/2022 12:04:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
05/30/2022 12:04:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=136
05/30/2022 12:04:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=136
05/30/2022 12:04:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/30/2022 12:04:29 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.16911983032873806 on epoch=137
05/30/2022 12:04:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=138
05/30/2022 12:04:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=138
05/30/2022 12:04:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
05/30/2022 12:04:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=139
05/30/2022 12:04:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=140
05/30/2022 12:05:04 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.1836408684879703 on epoch=140
05/30/2022 12:05:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=141
05/30/2022 12:05:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/30/2022 12:05:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=142
05/30/2022 12:05:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
05/30/2022 12:05:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=143
05/30/2022 12:05:38 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.17453324942311724 on epoch=143
05/30/2022 12:05:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/30/2022 12:05:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/30/2022 12:05:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/30/2022 12:05:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
05/30/2022 12:06:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=146
05/30/2022 12:06:12 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.19749397284608552 on epoch=146
05/30/2022 12:06:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=147
05/30/2022 12:06:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/30/2022 12:06:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
05/30/2022 12:06:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=149
05/30/2022 12:06:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
05/30/2022 12:06:45 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.12953339230429012 on epoch=149
05/30/2022 12:06:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
05/30/2022 12:06:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=151
05/30/2022 12:06:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=151
05/30/2022 12:07:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/30/2022 12:07:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
05/30/2022 12:07:19 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.16602923678395376 on epoch=153
05/30/2022 12:07:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
05/30/2022 12:07:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/30/2022 12:07:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=154
05/30/2022 12:07:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/30/2022 12:07:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/30/2022 12:07:53 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.1327353379916778 on epoch=156
05/30/2022 12:07:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/30/2022 12:08:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/30/2022 12:08:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
05/30/2022 12:08:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/30/2022 12:08:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
05/30/2022 12:08:26 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.09768009768009768 on epoch=159
05/30/2022 12:08:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
05/30/2022 12:08:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/30/2022 12:08:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/30/2022 12:08:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=161
05/30/2022 12:08:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=162
05/30/2022 12:09:00 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.07495673243336795 on epoch=162
05/30/2022 12:09:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=163
05/30/2022 12:09:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
05/30/2022 12:09:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
05/30/2022 12:09:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=164
05/30/2022 12:09:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=165
05/30/2022 12:09:34 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.10118110236220472 on epoch=165
05/30/2022 12:09:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/30/2022 12:09:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/30/2022 12:09:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
05/30/2022 12:09:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/30/2022 12:09:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
05/30/2022 12:10:07 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.1152546274497494 on epoch=168
05/30/2022 12:10:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
05/30/2022 12:10:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/30/2022 12:10:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
05/30/2022 12:10:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/30/2022 12:10:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=171
05/30/2022 12:10:41 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.20950570342205319 on epoch=171
05/30/2022 12:10:46 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/30/2022 12:10:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=173
05/30/2022 12:10:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/30/2022 12:10:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=174
05/30/2022 12:11:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=174
05/30/2022 12:11:15 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.1134337447510604 on epoch=174
05/30/2022 12:11:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/30/2022 12:11:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/30/2022 12:11:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/30/2022 12:11:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/30/2022 12:11:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/30/2022 12:11:49 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.11430576388340657 on epoch=178
05/30/2022 12:11:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
05/30/2022 12:11:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
05/30/2022 12:12:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/30/2022 12:12:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/30/2022 12:12:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
05/30/2022 12:12:23 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.12915643302474966 on epoch=181
05/30/2022 12:12:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=181
05/30/2022 12:12:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/30/2022 12:12:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=183
05/30/2022 12:12:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
05/30/2022 12:12:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/30/2022 12:12:57 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.12012987012987013 on epoch=184
05/30/2022 12:13:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
05/30/2022 12:13:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=185
05/30/2022 12:13:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
05/30/2022 12:13:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/30/2022 12:13:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/30/2022 12:13:21 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:13:21 - INFO - __main__ - Printing 3 examples
05/30/2022 12:13:21 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:13:21 - INFO - __main__ - ['refuted']
05/30/2022 12:13:21 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:13:21 - INFO - __main__ - ['refuted']
05/30/2022 12:13:21 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:13:21 - INFO - __main__ - ['refuted']
05/30/2022 12:13:21 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:13:21 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:13:22 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 12:13:22 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:13:22 - INFO - __main__ - Printing 3 examples
05/30/2022 12:13:22 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 12:13:22 - INFO - __main__ - ['refuted']
05/30/2022 12:13:22 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 12:13:22 - INFO - __main__ - ['refuted']
05/30/2022 12:13:22 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 12:13:22 - INFO - __main__ - ['refuted']
05/30/2022 12:13:22 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:13:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:13:22 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 12:13:31 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.18306387789711703 on epoch=187
05/30/2022 12:13:31 - INFO - __main__ - save last model!
05/30/2022 12:13:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:13:31 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:13:31 - INFO - __main__ - Printing 3 examples
05/30/2022 12:13:31 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:13:31 - INFO - __main__ - ['entailed']
05/30/2022 12:13:31 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:13:31 - INFO - __main__ - ['entailed']
05/30/2022 12:13:31 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:13:31 - INFO - __main__ - ['entailed']
05/30/2022 12:13:31 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:13:39 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:13:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 12:13:40 - INFO - __main__ - Starting training!
05/30/2022 12:13:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:14:09 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 12:23:27 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_13_0.5_8_predictions.txt
05/30/2022 12:23:27 - INFO - __main__ - Classification-F1 on test data: 0.0113
05/30/2022 12:23:27 - INFO - __main__ - prefix=tab_fact_128_13, lr=0.5, bsz=8, dev_performance=0.5075419847328244, test_performance=0.011329572156457265
05/30/2022 12:23:27 - INFO - __main__ - Running ... prefix=tab_fact_128_13, lr=0.4, bsz=8 ...
05/30/2022 12:23:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:23:28 - INFO - __main__ - Printing 3 examples
05/30/2022 12:23:28 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:23:28 - INFO - __main__ - ['refuted']
05/30/2022 12:23:28 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:23:28 - INFO - __main__ - ['refuted']
05/30/2022 12:23:28 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:23:28 - INFO - __main__ - ['refuted']
05/30/2022 12:23:28 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:23:28 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:23:29 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 12:23:29 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:23:29 - INFO - __main__ - Printing 3 examples
05/30/2022 12:23:29 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 12:23:29 - INFO - __main__ - ['refuted']
05/30/2022 12:23:29 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 12:23:29 - INFO - __main__ - ['refuted']
05/30/2022 12:23:29 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 12:23:29 - INFO - __main__ - ['refuted']
05/30/2022 12:23:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:23:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:23:29 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 12:23:45 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:23:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 12:23:45 - INFO - __main__ - Starting training!
05/30/2022 12:23:50 - INFO - __main__ - Step 10 Global step 10 Train loss 4.84 on epoch=0
05/30/2022 12:23:55 - INFO - __main__ - Step 20 Global step 20 Train loss 2.89 on epoch=1
05/30/2022 12:23:59 - INFO - __main__ - Step 30 Global step 30 Train loss 2.03 on epoch=1
05/30/2022 12:24:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=2
05/30/2022 12:24:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=3
05/30/2022 12:24:19 - INFO - __main__ - Global step 50 Train loss 2.48 Classification-F1 0.16180371352785147 on epoch=3
05/30/2022 12:24:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16180371352785147 on epoch=3, global_step=50
05/30/2022 12:24:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=3
05/30/2022 12:24:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=4
05/30/2022 12:24:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=4
05/30/2022 12:24:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=5
05/30/2022 12:24:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=6
05/30/2022 12:24:53 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 12:24:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16180371352785147 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 12:24:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=6
05/30/2022 12:25:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=7
05/30/2022 12:25:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=8
05/30/2022 12:25:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=8
05/30/2022 12:25:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=9
05/30/2022 12:25:27 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 12:25:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 12:25:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=10
05/30/2022 12:25:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=11
05/30/2022 12:25:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=11
05/30/2022 12:25:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=12
05/30/2022 12:26:00 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 12:26:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=13
05/30/2022 12:26:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=13
05/30/2022 12:26:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=14
05/30/2022 12:26:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=14
05/30/2022 12:26:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=15
05/30/2022 12:26:34 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 12:26:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=16
05/30/2022 12:26:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=16
05/30/2022 12:26:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=17
05/30/2022 12:26:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=18
05/30/2022 12:26:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=18
05/30/2022 12:27:08 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 12:27:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=19
05/30/2022 12:27:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=19
05/30/2022 12:27:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=20
05/30/2022 12:27:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=21
05/30/2022 12:27:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=21
05/30/2022 12:27:41 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 12:27:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=22
05/30/2022 12:27:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=23
05/30/2022 12:27:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=23
05/30/2022 12:27:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.19 on epoch=24
05/30/2022 12:28:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=24
05/30/2022 12:28:15 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 12:28:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=25
05/30/2022 12:28:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=26
05/30/2022 12:28:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=26
05/30/2022 12:28:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=27
05/30/2022 12:28:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=28
05/30/2022 12:28:49 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.34195559333697656 on epoch=28
05/30/2022 12:28:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=28, global_step=450
05/30/2022 12:28:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=28
05/30/2022 12:28:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=29
05/30/2022 12:29:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=29
05/30/2022 12:29:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=30
05/30/2022 12:29:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=31
05/30/2022 12:29:23 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 12:29:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=31
05/30/2022 12:29:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=32
05/30/2022 12:29:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=33
05/30/2022 12:29:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=33
05/30/2022 12:29:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=34
05/30/2022 12:29:56 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 12:30:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 12:30:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=35
05/30/2022 12:30:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=36
05/30/2022 12:30:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=36
05/30/2022 12:30:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=37
05/30/2022 12:30:30 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 12:30:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=38
05/30/2022 12:30:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=38
05/30/2022 12:30:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=39
05/30/2022 12:30:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=39
05/30/2022 12:30:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=40
05/30/2022 12:31:04 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.34195559333697656 on epoch=40
05/30/2022 12:31:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=41
05/30/2022 12:31:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=41
05/30/2022 12:31:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 12:31:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=43
05/30/2022 12:31:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=43
05/30/2022 12:31:37 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.34195559333697656 on epoch=43
05/30/2022 12:31:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=44
05/30/2022 12:31:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=44
05/30/2022 12:31:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=45
05/30/2022 12:31:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=46
05/30/2022 12:32:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=46
05/30/2022 12:32:11 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.34195559333697656 on epoch=46
05/30/2022 12:32:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=47
05/30/2022 12:32:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 12:32:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/30/2022 12:32:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=49
05/30/2022 12:32:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/30/2022 12:32:45 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.34195559333697656 on epoch=49
05/30/2022 12:32:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=50
05/30/2022 12:32:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=51
05/30/2022 12:32:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 12:33:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/30/2022 12:33:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=53
05/30/2022 12:33:19 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.3750290630086026 on epoch=53
05/30/2022 12:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.3750290630086026 on epoch=53, global_step=850
05/30/2022 12:33:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=53
05/30/2022 12:33:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=54
05/30/2022 12:33:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=54
05/30/2022 12:33:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 12:33:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=56
05/30/2022 12:33:53 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3486005089058525 on epoch=56
05/30/2022 12:33:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/30/2022 12:34:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=57
05/30/2022 12:34:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=58
05/30/2022 12:34:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=58
05/30/2022 12:34:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/30/2022 12:34:27 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 12:34:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=59
05/30/2022 12:34:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=60
05/30/2022 12:34:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=61
05/30/2022 12:34:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 12:34:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 12:35:00 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 12:35:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
05/30/2022 12:35:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=63
05/30/2022 12:35:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 12:35:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 12:35:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 12:35:34 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.36304897101085887 on epoch=65
05/30/2022 12:35:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=66
05/30/2022 12:35:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=66
05/30/2022 12:35:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 12:35:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=68
05/30/2022 12:35:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/30/2022 12:36:08 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.4241417497231451 on epoch=68
05/30/2022 12:36:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3750290630086026 -> 0.4241417497231451 on epoch=68, global_step=1100
05/30/2022 12:36:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 12:36:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/30/2022 12:36:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=70
05/30/2022 12:36:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=71
05/30/2022 12:36:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/30/2022 12:36:42 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.36869874405000863 on epoch=71
05/30/2022 12:36:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=72
05/30/2022 12:36:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=73
05/30/2022 12:36:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=73
05/30/2022 12:37:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/30/2022 12:37:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 12:37:16 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.34195559333697656 on epoch=74
05/30/2022 12:37:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/30/2022 12:37:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=76
05/30/2022 12:37:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=76
05/30/2022 12:37:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=77
05/30/2022 12:37:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=78
05/30/2022 12:37:50 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.2249128043172708 on epoch=78
05/30/2022 12:37:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=78
05/30/2022 12:37:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 12:38:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=79
05/30/2022 12:38:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=80
05/30/2022 12:38:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=81
05/30/2022 12:38:25 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3708141321044547 on epoch=81
05/30/2022 12:38:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/30/2022 12:38:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=82
05/30/2022 12:38:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/30/2022 12:38:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=83
05/30/2022 12:38:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=84
05/30/2022 12:38:59 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.3651088894055646 on epoch=84
05/30/2022 12:39:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=84
05/30/2022 12:39:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=85
05/30/2022 12:39:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=86
05/30/2022 12:39:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/30/2022 12:39:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 12:39:33 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.24316449737267323 on epoch=87
05/30/2022 12:39:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=88
05/30/2022 12:39:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/30/2022 12:39:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/30/2022 12:39:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
05/30/2022 12:39:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=90
05/30/2022 12:40:07 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.14854216821429936 on epoch=90
05/30/2022 12:40:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=91
05/30/2022 12:40:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=91
05/30/2022 12:40:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/30/2022 12:40:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
05/30/2022 12:40:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/30/2022 12:40:41 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.1705815243721381 on epoch=93
05/30/2022 12:40:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=94
05/30/2022 12:40:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=94
05/30/2022 12:40:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/30/2022 12:40:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
05/30/2022 12:41:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
05/30/2022 12:41:14 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.13350340136054423 on epoch=96
05/30/2022 12:41:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/30/2022 12:41:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/30/2022 12:41:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
05/30/2022 12:41:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/30/2022 12:41:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/30/2022 12:41:48 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.13156660269883333 on epoch=99
05/30/2022 12:41:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=100
05/30/2022 12:41:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/30/2022 12:42:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/30/2022 12:42:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
05/30/2022 12:42:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=103
05/30/2022 12:42:22 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.11642258689932142 on epoch=103
05/30/2022 12:42:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/30/2022 12:42:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=104
05/30/2022 12:42:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=104
05/30/2022 12:42:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/30/2022 12:42:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/30/2022 12:42:55 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.1040689121176926 on epoch=106
05/30/2022 12:43:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/30/2022 12:43:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
05/30/2022 12:43:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/30/2022 12:43:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/30/2022 12:43:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=109
05/30/2022 12:43:29 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.0867283950617284 on epoch=109
05/30/2022 12:43:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/30/2022 12:43:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=110
05/30/2022 12:43:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=111
05/30/2022 12:43:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/30/2022 12:43:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=112
05/30/2022 12:44:03 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.08948341378247919 on epoch=112
05/30/2022 12:44:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=113
05/30/2022 12:44:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=113
05/30/2022 12:44:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
05/30/2022 12:44:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=114
05/30/2022 12:44:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
05/30/2022 12:44:36 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.10603452371815254 on epoch=115
05/30/2022 12:44:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
05/30/2022 12:44:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
05/30/2022 12:44:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/30/2022 12:44:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=118
05/30/2022 12:44:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=118
05/30/2022 12:45:10 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.10555370014012805 on epoch=118
05/30/2022 12:45:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=119
05/30/2022 12:45:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/30/2022 12:45:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=120
05/30/2022 12:45:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/30/2022 12:45:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 12:45:43 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.07503267105234013 on epoch=121
05/30/2022 12:45:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
05/30/2022 12:45:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=123
05/30/2022 12:45:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
05/30/2022 12:46:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=124
05/30/2022 12:46:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=124
05/30/2022 12:46:17 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.09595238095238094 on epoch=124
05/30/2022 12:46:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=125
05/30/2022 12:46:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
05/30/2022 12:46:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=126
05/30/2022 12:46:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=127
05/30/2022 12:46:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=128
05/30/2022 12:46:51 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.12275391922412647 on epoch=128
05/30/2022 12:46:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=128
05/30/2022 12:46:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=129
05/30/2022 12:47:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
05/30/2022 12:47:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=130
05/30/2022 12:47:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=131
05/30/2022 12:47:24 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.10859090271937204 on epoch=131
05/30/2022 12:47:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=131
05/30/2022 12:47:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/30/2022 12:47:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=133
05/30/2022 12:47:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/30/2022 12:47:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=134
05/30/2022 12:47:58 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.15223615186271894 on epoch=134
05/30/2022 12:48:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=134
05/30/2022 12:48:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/30/2022 12:48:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=136
05/30/2022 12:48:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/30/2022 12:48:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/30/2022 12:48:31 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.10417888563049854 on epoch=137
05/30/2022 12:48:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
05/30/2022 12:48:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=138
05/30/2022 12:48:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
05/30/2022 12:48:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/30/2022 12:48:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=140
05/30/2022 12:49:05 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.11047060056597842 on epoch=140
05/30/2022 12:49:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
05/30/2022 12:49:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/30/2022 12:49:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=142
05/30/2022 12:49:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=143
05/30/2022 12:49:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
05/30/2022 12:49:39 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.10966087657816982 on epoch=143
05/30/2022 12:49:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
05/30/2022 12:49:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=144
05/30/2022 12:49:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=145
05/30/2022 12:49:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=146
05/30/2022 12:50:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/30/2022 12:50:12 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.1473159016652859 on epoch=146
05/30/2022 12:50:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
05/30/2022 12:50:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
05/30/2022 12:50:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
05/30/2022 12:50:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
05/30/2022 12:50:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=149
05/30/2022 12:50:46 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.1593755245929159 on epoch=149
05/30/2022 12:50:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/30/2022 12:50:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
05/30/2022 12:50:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/30/2022 12:51:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=152
05/30/2022 12:51:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/30/2022 12:51:20 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.07788825757575758 on epoch=153
05/30/2022 12:51:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=153
05/30/2022 12:51:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=154
05/30/2022 12:51:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
05/30/2022 12:51:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=155
05/30/2022 12:51:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/30/2022 12:51:54 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.09290051702002153 on epoch=156
05/30/2022 12:51:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/30/2022 12:52:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
05/30/2022 12:52:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=158
05/30/2022 12:52:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/30/2022 12:52:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=159
05/30/2022 12:52:27 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.08090195538637036 on epoch=159
05/30/2022 12:52:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=159
05/30/2022 12:52:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
05/30/2022 12:52:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/30/2022 12:52:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=161
05/30/2022 12:52:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
05/30/2022 12:53:01 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.08558859384109821 on epoch=162
05/30/2022 12:53:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
05/30/2022 12:53:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=163
05/30/2022 12:53:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
05/30/2022 12:53:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/30/2022 12:53:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
05/30/2022 12:53:35 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.06962599441814664 on epoch=165
05/30/2022 12:53:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=166
05/30/2022 12:53:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/30/2022 12:53:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=167
05/30/2022 12:53:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=168
05/30/2022 12:53:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
05/30/2022 12:54:09 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.08652250781993744 on epoch=168
05/30/2022 12:54:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
05/30/2022 12:54:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
05/30/2022 12:54:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=170
05/30/2022 12:54:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=171
05/30/2022 12:54:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
05/30/2022 12:54:43 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.1009082696887575 on epoch=171
05/30/2022 12:54:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
05/30/2022 12:54:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/30/2022 12:54:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=173
05/30/2022 12:55:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=174
05/30/2022 12:55:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=174
05/30/2022 12:55:17 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.11175548589341693 on epoch=174
05/30/2022 12:55:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/30/2022 12:55:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=176
05/30/2022 12:55:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/30/2022 12:55:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
05/30/2022 12:55:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=178
05/30/2022 12:55:51 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.19528358397175194 on epoch=178
05/30/2022 12:55:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/30/2022 12:56:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
05/30/2022 12:56:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/30/2022 12:56:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/30/2022 12:56:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/30/2022 12:56:24 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.08998013834668858 on epoch=181
05/30/2022 12:56:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=181
05/30/2022 12:56:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
05/30/2022 12:56:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/30/2022 12:56:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=183
05/30/2022 12:56:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
05/30/2022 12:56:58 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.07789351851851851 on epoch=184
05/30/2022 12:57:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/30/2022 12:57:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
05/30/2022 12:57:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/30/2022 12:57:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
05/30/2022 12:57:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/30/2022 12:57:22 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:57:22 - INFO - __main__ - Printing 3 examples
05/30/2022 12:57:22 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 12:57:22 - INFO - __main__ - ['refuted']
05/30/2022 12:57:22 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 12:57:22 - INFO - __main__ - ['refuted']
05/30/2022 12:57:22 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 12:57:22 - INFO - __main__ - ['refuted']
05/30/2022 12:57:22 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:57:22 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:57:23 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 12:57:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 12:57:23 - INFO - __main__ - Printing 3 examples
05/30/2022 12:57:23 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 12:57:23 - INFO - __main__ - ['refuted']
05/30/2022 12:57:23 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 12:57:23 - INFO - __main__ - ['refuted']
05/30/2022 12:57:23 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 12:57:23 - INFO - __main__ - ['refuted']
05/30/2022 12:57:23 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:57:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:57:23 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 12:57:32 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.07308931477729431 on epoch=187
05/30/2022 12:57:32 - INFO - __main__ - save last model!
05/30/2022 12:57:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 12:57:32 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 12:57:32 - INFO - __main__ - Printing 3 examples
05/30/2022 12:57:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:57:32 - INFO - __main__ - ['entailed']
05/30/2022 12:57:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:57:32 - INFO - __main__ - ['entailed']
05/30/2022 12:57:32 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 12:57:32 - INFO - __main__ - ['entailed']
05/30/2022 12:57:32 - INFO - __main__ - Tokenizing Input ...
05/30/2022 12:57:39 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 12:57:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 12:57:40 - INFO - __main__ - Starting training!
05/30/2022 12:57:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 12:58:09 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:07:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_13_0.4_8_predictions.txt
05/30/2022 13:07:29 - INFO - __main__ - Classification-F1 on test data: 0.0112
05/30/2022 13:07:29 - INFO - __main__ - prefix=tab_fact_128_13, lr=0.4, bsz=8, dev_performance=0.4241417497231451, test_performance=0.01117035733367622
05/30/2022 13:07:29 - INFO - __main__ - Running ... prefix=tab_fact_128_13, lr=0.3, bsz=8 ...
05/30/2022 13:07:30 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:07:30 - INFO - __main__ - Printing 3 examples
05/30/2022 13:07:30 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 13:07:30 - INFO - __main__ - ['refuted']
05/30/2022 13:07:30 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 13:07:30 - INFO - __main__ - ['refuted']
05/30/2022 13:07:30 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 13:07:30 - INFO - __main__ - ['refuted']
05/30/2022 13:07:30 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:07:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:07:31 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 13:07:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:07:31 - INFO - __main__ - Printing 3 examples
05/30/2022 13:07:31 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 13:07:31 - INFO - __main__ - ['refuted']
05/30/2022 13:07:31 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 13:07:31 - INFO - __main__ - ['refuted']
05/30/2022 13:07:31 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 13:07:31 - INFO - __main__ - ['refuted']
05/30/2022 13:07:31 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:07:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:07:31 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 13:07:50 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:07:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 13:07:51 - INFO - __main__ - Starting training!
05/30/2022 13:07:56 - INFO - __main__ - Step 10 Global step 10 Train loss 5.00 on epoch=0
05/30/2022 13:08:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.30 on epoch=1
05/30/2022 13:08:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=1
05/30/2022 13:08:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.94 on epoch=2
05/30/2022 13:08:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=3
05/30/2022 13:08:25 - INFO - __main__ - Global step 50 Train loss 2.86 Classification-F1 0.08546292417260158 on epoch=3
05/30/2022 13:08:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.08546292417260158 on epoch=3, global_step=50
05/30/2022 13:08:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.15 on epoch=3
05/30/2022 13:08:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.87 on epoch=4
05/30/2022 13:08:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.68 on epoch=4
05/30/2022 13:08:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
05/30/2022 13:08:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/30/2022 13:08:59 - INFO - __main__ - Global step 100 Train loss 0.71 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 13:08:59 - INFO - __main__ - Saving model with best Classification-F1: 0.08546292417260158 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 13:09:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=6
05/30/2022 13:09:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=7
05/30/2022 13:09:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=8
05/30/2022 13:09:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=8
05/30/2022 13:09:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=9
05/30/2022 13:09:33 - INFO - __main__ - Global step 150 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 13:09:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 13:09:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=10
05/30/2022 13:09:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=11
05/30/2022 13:09:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=11
05/30/2022 13:09:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=12
05/30/2022 13:10:07 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 13:10:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=13
05/30/2022 13:10:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=13
05/30/2022 13:10:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=14
05/30/2022 13:10:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=14
05/30/2022 13:10:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=15
05/30/2022 13:10:41 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 13:10:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=16
05/30/2022 13:10:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=16
05/30/2022 13:10:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=17
05/30/2022 13:10:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=18
05/30/2022 13:11:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=18
05/30/2022 13:11:14 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 13:11:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=19
05/30/2022 13:11:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=19
05/30/2022 13:11:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=20
05/30/2022 13:11:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=21
05/30/2022 13:11:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=21
05/30/2022 13:11:48 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 13:11:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 13:11:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 13:12:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=23
05/30/2022 13:12:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=24
05/30/2022 13:12:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=24
05/30/2022 13:12:20 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 13:12:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=25
05/30/2022 13:12:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 13:12:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=26
05/30/2022 13:12:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=27
05/30/2022 13:12:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=28
05/30/2022 13:12:54 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 13:12:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=28
05/30/2022 13:13:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 13:13:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=29
05/30/2022 13:13:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=30
05/30/2022 13:13:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=31
05/30/2022 13:13:28 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 13:13:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 13:13:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=32
05/30/2022 13:13:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=33
05/30/2022 13:13:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=33
05/30/2022 13:13:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 13:14:02 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 13:14:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=34
05/30/2022 13:14:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=35
05/30/2022 13:14:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/30/2022 13:14:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=36
05/30/2022 13:14:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=37
05/30/2022 13:14:36 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 13:14:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 13:14:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 13:14:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=39
05/30/2022 13:14:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=39
05/30/2022 13:14:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=40
05/30/2022 13:15:10 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 13:15:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=41
05/30/2022 13:15:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=41
05/30/2022 13:15:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=42
05/30/2022 13:15:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 13:15:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=43
05/30/2022 13:15:44 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 13:15:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=44
05/30/2022 13:15:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=44
05/30/2022 13:15:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=45
05/30/2022 13:16:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=46
05/30/2022 13:16:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 13:16:18 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 13:16:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=47
05/30/2022 13:16:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=48
05/30/2022 13:16:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/30/2022 13:16:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=49
05/30/2022 13:16:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=49
05/30/2022 13:16:52 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 13:16:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=50
05/30/2022 13:17:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 13:17:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/30/2022 13:17:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=52
05/30/2022 13:17:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/30/2022 13:17:26 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.36516753625488524 on epoch=53
05/30/2022 13:17:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36516753625488524 on epoch=53, global_step=850
05/30/2022 13:17:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=53
05/30/2022 13:17:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/30/2022 13:17:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=54
05/30/2022 13:17:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 13:17:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 13:17:59 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 13:18:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/30/2022 13:18:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=57
05/30/2022 13:18:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 13:18:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/30/2022 13:18:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 13:18:33 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 13:18:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=59
05/30/2022 13:18:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=60
05/30/2022 13:18:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=61
05/30/2022 13:18:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=61
05/30/2022 13:18:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/30/2022 13:19:07 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 13:19:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
05/30/2022 13:19:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/30/2022 13:19:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=64
05/30/2022 13:19:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 13:19:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/30/2022 13:19:41 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 13:19:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/30/2022 13:19:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/30/2022 13:19:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/30/2022 13:19:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/30/2022 13:20:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/30/2022 13:20:15 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 13:20:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/30/2022 13:20:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/30/2022 13:20:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/30/2022 13:20:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
05/30/2022 13:20:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/30/2022 13:20:49 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.33159268929503916 on epoch=71
05/30/2022 13:20:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=72
05/30/2022 13:20:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=73
05/30/2022 13:21:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/30/2022 13:21:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/30/2022 13:21:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=74
05/30/2022 13:21:23 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 13:21:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/30/2022 13:21:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 13:21:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=76
05/30/2022 13:21:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/30/2022 13:21:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
05/30/2022 13:21:56 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.34195559333697656 on epoch=78
05/30/2022 13:22:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
05/30/2022 13:22:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
05/30/2022 13:22:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/30/2022 13:22:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=80
05/30/2022 13:22:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/30/2022 13:22:30 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.34195559333697656 on epoch=81
05/30/2022 13:22:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/30/2022 13:22:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/30/2022 13:22:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/30/2022 13:22:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/30/2022 13:22:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
05/30/2022 13:23:04 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 13:23:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=84
05/30/2022 13:23:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/30/2022 13:23:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/30/2022 13:23:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
05/30/2022 13:23:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 13:23:38 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.341074761764417 on epoch=87
05/30/2022 13:23:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/30/2022 13:23:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/30/2022 13:23:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/30/2022 13:23:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
05/30/2022 13:24:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/30/2022 13:24:12 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.35113468906572354 on epoch=90
05/30/2022 13:24:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/30/2022 13:24:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=91
05/30/2022 13:24:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/30/2022 13:24:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/30/2022 13:24:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/30/2022 13:24:46 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.43636363636363634 on epoch=93
05/30/2022 13:24:46 - INFO - __main__ - Saving model with best Classification-F1: 0.36516753625488524 -> 0.43636363636363634 on epoch=93, global_step=1500
05/30/2022 13:24:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/30/2022 13:24:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
05/30/2022 13:24:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/30/2022 13:25:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
05/30/2022 13:25:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=96
05/30/2022 13:25:20 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.34673046251993617 on epoch=96
05/30/2022 13:25:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/30/2022 13:25:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/30/2022 13:25:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=98
05/30/2022 13:25:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=99
05/30/2022 13:25:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/30/2022 13:25:53 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.3692115143929912 on epoch=99
05/30/2022 13:25:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=100
05/30/2022 13:26:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/30/2022 13:26:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/30/2022 13:26:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
05/30/2022 13:26:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/30/2022 13:26:28 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.40116959064327484 on epoch=103
05/30/2022 13:26:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/30/2022 13:26:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/30/2022 13:26:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/30/2022 13:26:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 13:26:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/30/2022 13:27:02 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.3784863604213263 on epoch=106
05/30/2022 13:27:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 13:27:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/30/2022 13:27:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 13:27:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/30/2022 13:27:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
05/30/2022 13:27:36 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.38279939051439815 on epoch=109
05/30/2022 13:27:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/30/2022 13:27:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=110
05/30/2022 13:27:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=111
05/30/2022 13:27:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=111
05/30/2022 13:27:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
05/30/2022 13:28:10 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.3784863604213263 on epoch=112
05/30/2022 13:28:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=113
05/30/2022 13:28:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/30/2022 13:28:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
05/30/2022 13:28:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=114
05/30/2022 13:28:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/30/2022 13:28:44 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.19560690847571066 on epoch=115
05/30/2022 13:28:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/30/2022 13:28:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/30/2022 13:28:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/30/2022 13:29:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
05/30/2022 13:29:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=118
05/30/2022 13:29:18 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.1843473535854375 on epoch=118
05/30/2022 13:29:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/30/2022 13:29:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/30/2022 13:29:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=120
05/30/2022 13:29:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/30/2022 13:29:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 13:29:52 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.1624000646047 on epoch=121
05/30/2022 13:29:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/30/2022 13:30:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=123
05/30/2022 13:30:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/30/2022 13:30:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/30/2022 13:30:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=124
05/30/2022 13:30:25 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.1826731639401323 on epoch=124
05/30/2022 13:30:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=125
05/30/2022 13:30:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/30/2022 13:30:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 13:30:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
05/30/2022 13:30:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/30/2022 13:31:00 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.1322650598420765 on epoch=128
05/30/2022 13:31:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
05/30/2022 13:31:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/30/2022 13:31:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
05/30/2022 13:31:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/30/2022 13:31:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
05/30/2022 13:31:34 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.10009189038735039 on epoch=131
05/30/2022 13:31:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
05/30/2022 13:31:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=132
05/30/2022 13:31:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/30/2022 13:31:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=133
05/30/2022 13:31:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
05/30/2022 13:32:08 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.09830336896663426 on epoch=134
05/30/2022 13:32:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=134
05/30/2022 13:32:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/30/2022 13:32:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=136
05/30/2022 13:32:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=136
05/30/2022 13:32:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
05/30/2022 13:32:42 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.09850836039937448 on epoch=137
05/30/2022 13:32:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
05/30/2022 13:32:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=138
05/30/2022 13:32:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/30/2022 13:33:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/30/2022 13:33:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=140
05/30/2022 13:33:16 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.11209406686763498 on epoch=140
05/30/2022 13:33:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/30/2022 13:33:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
05/30/2022 13:33:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=142
05/30/2022 13:33:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/30/2022 13:33:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/30/2022 13:33:50 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.12418091027513521 on epoch=143
05/30/2022 13:33:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
05/30/2022 13:33:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=144
05/30/2022 13:34:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=145
05/30/2022 13:34:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=146
05/30/2022 13:34:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=146
05/30/2022 13:34:24 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.12293987362386816 on epoch=146
05/30/2022 13:34:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=147
05/30/2022 13:34:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/30/2022 13:34:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=148
05/30/2022 13:34:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
05/30/2022 13:34:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/30/2022 13:34:58 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.17832469323279398 on epoch=149
05/30/2022 13:35:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/30/2022 13:35:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/30/2022 13:35:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=151
05/30/2022 13:35:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
05/30/2022 13:35:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
05/30/2022 13:35:32 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.12879151660664265 on epoch=153
05/30/2022 13:35:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=153
05/30/2022 13:35:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
05/30/2022 13:35:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=154
05/30/2022 13:35:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
05/30/2022 13:35:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=156
05/30/2022 13:36:07 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.1547008547008547 on epoch=156
05/30/2022 13:36:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
05/30/2022 13:36:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=157
05/30/2022 13:36:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
05/30/2022 13:36:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/30/2022 13:36:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
05/30/2022 13:36:41 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.13624078853851046 on epoch=159
05/30/2022 13:36:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/30/2022 13:36:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/30/2022 13:36:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/30/2022 13:36:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=161
05/30/2022 13:37:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
05/30/2022 13:37:15 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.1742199810904507 on epoch=162
05/30/2022 13:37:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/30/2022 13:37:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/30/2022 13:37:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/30/2022 13:37:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/30/2022 13:37:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/30/2022 13:37:49 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.15786749482401655 on epoch=165
05/30/2022 13:37:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/30/2022 13:37:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/30/2022 13:38:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
05/30/2022 13:38:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=168
05/30/2022 13:38:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/30/2022 13:38:23 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.09822967052653865 on epoch=168
05/30/2022 13:38:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
05/30/2022 13:38:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=169
05/30/2022 13:38:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/30/2022 13:38:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
05/30/2022 13:38:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=171
05/30/2022 13:38:57 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.13124659146358966 on epoch=171
05/30/2022 13:39:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/30/2022 13:39:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=173
05/30/2022 13:39:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/30/2022 13:39:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/30/2022 13:39:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/30/2022 13:39:32 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.1231000678216906 on epoch=174
05/30/2022 13:39:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
05/30/2022 13:39:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
05/30/2022 13:39:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/30/2022 13:39:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=177
05/30/2022 13:39:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
05/30/2022 13:40:06 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.11050642479213908 on epoch=178
05/30/2022 13:40:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
05/30/2022 13:40:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=179
05/30/2022 13:40:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=179
05/30/2022 13:40:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/30/2022 13:40:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
05/30/2022 13:40:40 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.09933056456353946 on epoch=181
05/30/2022 13:40:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=181
05/30/2022 13:40:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
05/30/2022 13:40:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/30/2022 13:40:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=183
05/30/2022 13:41:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=184
05/30/2022 13:41:15 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.11278044871794872 on epoch=184
05/30/2022 13:41:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=184
05/30/2022 13:41:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/30/2022 13:41:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/30/2022 13:41:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=186
05/30/2022 13:41:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
05/30/2022 13:41:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:41:39 - INFO - __main__ - Printing 3 examples
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:41:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:41:39 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 13:41:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:41:39 - INFO - __main__ - Printing 3 examples
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 13:41:39 - INFO - __main__ - ['refuted']
05/30/2022 13:41:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:41:40 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:41:40 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 13:41:49 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.12363315696649028 on epoch=187
05/30/2022 13:41:49 - INFO - __main__ - save last model!
05/30/2022 13:41:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 13:41:49 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 13:41:49 - INFO - __main__ - Printing 3 examples
05/30/2022 13:41:49 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:41:49 - INFO - __main__ - ['entailed']
05/30/2022 13:41:49 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:41:49 - INFO - __main__ - ['entailed']
05/30/2022 13:41:49 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 13:41:49 - INFO - __main__ - ['entailed']
05/30/2022 13:41:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:41:55 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 13:41:56 - INFO - __main__ - Starting training!
05/30/2022 13:42:13 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:42:26 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 13:51:47 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_13_0.3_8_predictions.txt
05/30/2022 13:51:47 - INFO - __main__ - Classification-F1 on test data: 0.0193
05/30/2022 13:51:47 - INFO - __main__ - prefix=tab_fact_128_13, lr=0.3, bsz=8, dev_performance=0.43636363636363634, test_performance=0.019334729663775665
05/30/2022 13:51:47 - INFO - __main__ - Running ... prefix=tab_fact_128_13, lr=0.2, bsz=8 ...
05/30/2022 13:51:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:51:48 - INFO - __main__ - Printing 3 examples
05/30/2022 13:51:48 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
05/30/2022 13:51:48 - INFO - __main__ - ['refuted']
05/30/2022 13:51:48 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
05/30/2022 13:51:48 - INFO - __main__ - ['refuted']
05/30/2022 13:51:48 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
05/30/2022 13:51:48 - INFO - __main__ - ['refuted']
05/30/2022 13:51:48 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:51:48 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:51:49 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 13:51:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 13:51:49 - INFO - __main__ - Printing 3 examples
05/30/2022 13:51:49 - INFO - __main__ -  [tab_fact] statement: marisa monte be the original artist for the song tanto [SEP] table_caption: thaeme mariôto [SEP] table_text: week#theme#song choice#original artist#order#result [n] audition#auditioner 's choice#os outros#kid abelha#n / a#advanced [n] theater#first solo#eu tive um sonho#kid abelha#n / a#advanced [n] top 32#top 16 women#me chama#lobão#7#advanced [n] top 12#my idol#como eu quero#kid abelha#2#bottom 2 [n] top 10#male singers#por onde andei#nando reis#9#safe [n] top 9#jovem pan number 1 hits#amor perfeito#claudia leitte#7#safe [n] top 8#dedicate a song#a sua#marisa monte#7#safe [n] top 7#female singers#agora só falta você#rita lee#1#safe [n] top 6#skank#tanto#skank#4#bottom 2 [n] top 5#popular classics#nem um toque#rosana#3#safe [n] top 5#popular classics#agüenta coração#josé augusto#8#safe [n] top 4#raul seixas#maluco beleza#raul seixas#2#safe [n] top 4#contestant 's choice#espirais#marjorie estiano#6#safe [n] top 3#fans' choice#os outros#leoni#1#safe [n] top 3#fans' choice#sozinho#caetano veloso#4#safe [n] top 2#judges' choice#devolva - me#adriana calcanhotto#1#winner [n] top 2#best of the season#por onde andei#nando reis#3#winner [n] 
05/30/2022 13:51:49 - INFO - __main__ - ['refuted']
05/30/2022 13:51:49 - INFO - __main__ -  [tab_fact] statement: teel bivins senator have a party of republican a home town of houston , and a district of 7 [SEP] table_caption: seventy - third texas legislature [SEP] table_text: senator#party#district#home town#took office [n] bill ratliff#republican#1#mount pleasant#1989 [n] florence shapiro#republican#2#plano#1993 [n] bill haley#democratic#3#center#1992 [n] carl a parker#democratic#4#port arthur#1977 [n] jim turner#democratic#5#crockett#1991 [n] dan shelley#republican#6#crosby#1993 [n] don henderson#republican#7#houston#1993 [n] oh ike harris#republican#8#dallas#1967 [n] david sibley#republican#9#waco#1991 [n] chris harris#republican#10#arlington#1991 [n] jerry e patterson#republican#11#houston#1993 [n] mike moncrief#democratic#12#fort worth#1991 [n] craig a washington#democratic#13#houston#1983 [n] gonzalo barrientos#democratic#14#austin#1985 [n] john whitmire#democratic#15#houston#1983 [n] john n leedom#republican#16#dallas#1981 [n] j e buster brown#republican#17#lake jackson#1981 [n] ken armbrister#democratic#18#victoria#1987 [n] gregory luna#democratic#19#san antonio#1985 [n] carlos f truan#democratic#20#corpus christi#1977 [n] judith zaffirini#democratic#21#laredo#1987 [n] jane nelson#republican#22#lewisville#1993 [n] royce west#democratic#23#dallas#1993 [n] frank l madla#democratic#24#san antonio#1993 [n] bill sims#democratic#25#san antonio#1983 [n] jeff wentworth#republican#26#san antonio#1992 [n] eddie lucio , jr#democratic#27#brownsville#1991 [n] john montford#democratic#28#lubbock#1982 [n] peggy rosson#democratic#29#el paso#1991 [n] steve carriker#democratic#30#roby#1988 [n] teel bivins#republican#31#amarillo#1989 [n] 
05/30/2022 13:51:49 - INFO - __main__ - ['refuted']
05/30/2022 13:51:49 - INFO - __main__ -  [tab_fact] statement: the call sign dxgh have no know location [SEP] table_caption: list of manila broadcasting company stations [SEP] table_text: branding#callsign#frequency#power (kw)#station type#location [n] dzrh#dzrh#666khz#50 kw#originating#metro manila [n] dzrh laoag#dzmt#990khz#5 kw#relay#laoag [n] dzrh dagupan#dwdh#1440khz#10 kw#relay#dagupan [n] dzrh tuguegarao#dwrh#576khz#5 kw#relay#tuguegarao [n] dzrh isabela#dwrh#828khz#5 kw#relay#santiago , isabela [n] dzrh lucena#dwsr#1224khz#5 kw#relay#lucena [n] dzrh palawan#dyph#693khz#10 kw#relay#puerto princesa [n] dzrh naga#dwmt#981khz#5 kw#relay#naga [n] dzrh sorsogon#dzzh#1287khz#5 kw#relay#sorsogon [n] dzrh kalibo#dykx#693khz#1 kw#relay#kalibo , aklan [n] dzrh iloilo#dydh#1485khz#5 kw#relay#iloilo [n] dzrh bacolod#dybh#1080khz#5 kw#relay#bacolod [n] dzrh cebu#dyrh#1395khz#10 kw#relay#cebu [n] dzrh tacloban#dyth#990khz#5 kw#relay#tacloban [n] dzrh zamboanga#dxzh#855khz#5 kw#relay#zamboanga [n] dzrh cagayan de oro#dxkh#972khz#5 kw#relay#cagayan de oro [n] dzrh davao#dxrf#1260khz#10 kw#relay#davao [n] dzrh general santos#dxgh#531khz#5 kw#relay#general santos [n] dzrh bislig#dxrh#1035khz#1 kw#relay#bislig , surigao del sur [n] 
05/30/2022 13:51:49 - INFO - __main__ - ['refuted']
05/30/2022 13:51:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 13:51:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 13:51:49 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 13:52:08 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 13:52:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 13:52:09 - INFO - __main__ - Starting training!
05/30/2022 13:52:14 - INFO - __main__ - Step 10 Global step 10 Train loss 5.64 on epoch=0
05/30/2022 13:52:18 - INFO - __main__ - Step 20 Global step 20 Train loss 3.89 on epoch=1
05/30/2022 13:52:23 - INFO - __main__ - Step 30 Global step 30 Train loss 3.23 on epoch=1
05/30/2022 13:52:27 - INFO - __main__ - Step 40 Global step 40 Train loss 2.64 on epoch=2
05/30/2022 13:52:32 - INFO - __main__ - Step 50 Global step 50 Train loss 2.22 on epoch=3
05/30/2022 13:52:43 - INFO - __main__ - Global step 50 Train loss 3.52 Classification-F1 0.0 on epoch=3
05/30/2022 13:52:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/30/2022 13:52:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.86 on epoch=3
05/30/2022 13:52:52 - INFO - __main__ - Step 70 Global step 70 Train loss 1.58 on epoch=4
05/30/2022 13:52:57 - INFO - __main__ - Step 80 Global step 80 Train loss 1.40 on epoch=4
05/30/2022 13:53:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.23 on epoch=5
05/30/2022 13:53:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=6
05/30/2022 13:53:16 - INFO - __main__ - Global step 100 Train loss 1.40 Classification-F1 0.2228024369016536 on epoch=6
05/30/2022 13:53:16 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.2228024369016536 on epoch=6, global_step=100
05/30/2022 13:53:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.74 on epoch=6
05/30/2022 13:53:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=7
05/30/2022 13:53:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/30/2022 13:53:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
05/30/2022 13:53:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=9
05/30/2022 13:53:50 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 13:53:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2228024369016536 -> 0.3333333333333333 on epoch=9, global_step=150
05/30/2022 13:53:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=9
05/30/2022 13:53:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=10
05/30/2022 13:54:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=11
05/30/2022 13:54:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=11
05/30/2022 13:54:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=12
05/30/2022 13:54:24 - INFO - __main__ - Global step 200 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 13:54:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=13
05/30/2022 13:54:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
05/30/2022 13:54:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=14
05/30/2022 13:54:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=14
05/30/2022 13:54:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=15
05/30/2022 13:54:58 - INFO - __main__ - Global step 250 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 13:55:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=16
05/30/2022 13:55:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=16
05/30/2022 13:55:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=17
05/30/2022 13:55:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=18
05/30/2022 13:55:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=18
05/30/2022 13:55:31 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 13:55:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=19
05/30/2022 13:55:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=19
05/30/2022 13:55:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=20
05/30/2022 13:55:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=21
05/30/2022 13:55:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=21
05/30/2022 13:56:04 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 13:56:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=22
05/30/2022 13:56:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=23
05/30/2022 13:56:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=23
05/30/2022 13:56:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=24
05/30/2022 13:56:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=24
05/30/2022 13:56:38 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 13:56:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=25
05/30/2022 13:56:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=26
05/30/2022 13:56:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=26
05/30/2022 13:56:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=27
05/30/2022 13:57:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 13:57:12 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 13:57:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=28
05/30/2022 13:57:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=29
05/30/2022 13:57:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 13:57:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=30
05/30/2022 13:57:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=31
05/30/2022 13:57:45 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 13:57:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=31
05/30/2022 13:57:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=32
05/30/2022 13:57:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=33
05/30/2022 13:58:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=33
05/30/2022 13:58:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=34
05/30/2022 13:58:18 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 13:58:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=34
05/30/2022 13:58:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=35
05/30/2022 13:58:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=36
05/30/2022 13:58:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=36
05/30/2022 13:58:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=37
05/30/2022 13:58:52 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 13:58:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 13:59:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 13:59:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=39
05/30/2022 13:59:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=39
05/30/2022 13:59:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=40
05/30/2022 13:59:26 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 13:59:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=41
05/30/2022 13:59:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=41
05/30/2022 13:59:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 13:59:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=43
05/30/2022 13:59:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=43
05/30/2022 13:59:59 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 14:00:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 14:00:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=44
05/30/2022 14:00:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=45
05/30/2022 14:00:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=46
05/30/2022 14:00:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 14:00:33 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 14:00:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=47
05/30/2022 14:00:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 14:00:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=48
05/30/2022 14:00:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=49
05/30/2022 14:00:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=49
05/30/2022 14:01:06 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 14:01:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=50
05/30/2022 14:01:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 14:01:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=51
05/30/2022 14:01:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=52
05/30/2022 14:01:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=53
05/30/2022 14:01:40 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 14:01:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/30/2022 14:01:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=54
05/30/2022 14:01:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=54
05/30/2022 14:01:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/30/2022 14:02:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=56
05/30/2022 14:02:13 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 14:02:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/30/2022 14:02:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/30/2022 14:02:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=58
05/30/2022 14:02:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
05/30/2022 14:02:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=59
05/30/2022 14:02:47 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 14:02:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/30/2022 14:02:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/30/2022 14:03:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=61
05/30/2022 14:03:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 14:03:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 14:03:20 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 14:03:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=63
05/30/2022 14:03:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/30/2022 14:03:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/30/2022 14:03:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=64
05/30/2022 14:03:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=65
05/30/2022 14:03:54 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 14:03:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/30/2022 14:04:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/30/2022 14:04:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 14:04:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=68
05/30/2022 14:04:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/30/2022 14:04:28 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 14:04:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/30/2022 14:04:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/30/2022 14:04:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/30/2022 14:04:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=71
05/30/2022 14:04:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/30/2022 14:05:01 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 14:05:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 14:05:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=73
05/30/2022 14:05:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 14:05:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/30/2022 14:05:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 14:05:35 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 14:05:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=75
05/30/2022 14:05:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/30/2022 14:05:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/30/2022 14:05:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=77
05/30/2022 14:05:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 14:06:09 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 14:06:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/30/2022 14:06:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
05/30/2022 14:06:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 14:06:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=80
05/30/2022 14:06:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=81
05/30/2022 14:06:42 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 14:06:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/30/2022 14:06:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 14:06:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
05/30/2022 14:07:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=83
05/30/2022 14:07:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/30/2022 14:07:16 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 14:07:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/30/2022 14:07:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=85
05/30/2022 14:07:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/30/2022 14:07:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=86
05/30/2022 14:07:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=87
05/30/2022 14:07:50 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 14:07:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=88
05/30/2022 14:07:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/30/2022 14:08:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/30/2022 14:08:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
05/30/2022 14:08:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/30/2022 14:08:24 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 14:08:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
05/30/2022 14:08:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=91
05/30/2022 14:08:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
05/30/2022 14:08:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
05/30/2022 14:08:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/30/2022 14:08:57 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 14:09:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=94
05/30/2022 14:09:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 14:09:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/30/2022 14:09:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/30/2022 14:09:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/30/2022 14:09:31 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 14:09:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=97
05/30/2022 14:09:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/30/2022 14:09:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/30/2022 14:09:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/30/2022 14:09:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=99
05/30/2022 14:10:05 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 14:10:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
05/30/2022 14:10:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/30/2022 14:10:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=101
05/30/2022 14:10:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=102
05/30/2022 14:10:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 14:10:39 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 14:10:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/30/2022 14:10:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
05/30/2022 14:10:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/30/2022 14:10:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/30/2022 14:11:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
05/30/2022 14:11:13 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.33159268929503916 on epoch=106
05/30/2022 14:11:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/30/2022 14:11:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/30/2022 14:11:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/30/2022 14:11:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
05/30/2022 14:11:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/30/2022 14:11:46 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.33159268929503916 on epoch=109
05/30/2022 14:11:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=109
05/30/2022 14:11:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
05/30/2022 14:12:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/30/2022 14:12:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/30/2022 14:12:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/30/2022 14:12:20 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 14:12:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
05/30/2022 14:12:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 14:12:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=114
05/30/2022 14:12:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
05/30/2022 14:12:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
05/30/2022 14:12:54 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.33159268929503916 on epoch=115
05/30/2022 14:12:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/30/2022 14:13:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/30/2022 14:13:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/30/2022 14:13:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
05/30/2022 14:13:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=118
05/30/2022 14:13:28 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.3383422492035824 on epoch=118
05/30/2022 14:13:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3383422492035824 on epoch=118, global_step=1900
05/30/2022 14:13:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/30/2022 14:13:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
05/30/2022 14:13:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=120
05/30/2022 14:13:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/30/2022 14:13:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 14:14:02 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3671630170316302 on epoch=121
05/30/2022 14:14:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3383422492035824 -> 0.3671630170316302 on epoch=121, global_step=1950
05/30/2022 14:14:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/30/2022 14:14:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/30/2022 14:14:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/30/2022 14:14:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=124
05/30/2022 14:14:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=124
05/30/2022 14:14:35 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.33159268929503916 on epoch=124
05/30/2022 14:14:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/30/2022 14:14:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
05/30/2022 14:14:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 14:14:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=127
05/30/2022 14:14:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=128
05/30/2022 14:15:09 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.35307589038932324 on epoch=128
05/30/2022 14:15:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 14:15:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
05/30/2022 14:15:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/30/2022 14:15:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=130
05/30/2022 14:15:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 14:15:43 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.3383422492035824 on epoch=131
05/30/2022 14:15:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/30/2022 14:15:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
05/30/2022 14:15:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/30/2022 14:16:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/30/2022 14:16:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.20 on epoch=134
05/30/2022 14:16:17 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.2261885999474652 on epoch=134
05/30/2022 14:16:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 14:16:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/30/2022 14:16:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/30/2022 14:16:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/30/2022 14:16:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=137
05/30/2022 14:16:51 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.23944294699011678 on epoch=137
05/30/2022 14:16:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
05/30/2022 14:17:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/30/2022 14:17:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/30/2022 14:17:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=139
05/30/2022 14:17:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=140
05/30/2022 14:17:24 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.23693352589874703 on epoch=140
05/30/2022 14:17:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
05/30/2022 14:17:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=141
05/30/2022 14:17:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
05/30/2022 14:17:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/30/2022 14:17:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
05/30/2022 14:17:58 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.2897722567287785 on epoch=143
05/30/2022 14:18:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/30/2022 14:18:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=144
05/30/2022 14:18:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
05/30/2022 14:18:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=146
05/30/2022 14:18:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/30/2022 14:18:32 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.180019305019305 on epoch=146
05/30/2022 14:18:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/30/2022 14:18:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
05/30/2022 14:18:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 14:18:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/30/2022 14:18:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/30/2022 14:19:06 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.23985687815475046 on epoch=149
05/30/2022 14:19:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
05/30/2022 14:19:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/30/2022 14:19:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
05/30/2022 14:19:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
05/30/2022 14:19:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
05/30/2022 14:19:40 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.1441182189383628 on epoch=153
05/30/2022 14:19:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/30/2022 14:19:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
05/30/2022 14:19:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/30/2022 14:19:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=155
05/30/2022 14:20:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
05/30/2022 14:20:14 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.16853850374977136 on epoch=156
05/30/2022 14:20:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/30/2022 14:20:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
05/30/2022 14:20:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
05/30/2022 14:20:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/30/2022 14:20:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=159
05/30/2022 14:20:47 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.24880115343330603 on epoch=159
05/30/2022 14:20:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/30/2022 14:20:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=160
05/30/2022 14:21:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
05/30/2022 14:21:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=161
05/30/2022 14:21:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=162
05/30/2022 14:21:21 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.2716044792305738 on epoch=162
05/30/2022 14:21:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=163
05/30/2022 14:21:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/30/2022 14:21:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=164
05/30/2022 14:21:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/30/2022 14:21:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=165
05/30/2022 14:21:55 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.19404818138875768 on epoch=165
05/30/2022 14:22:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
05/30/2022 14:22:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=166
05/30/2022 14:22:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/30/2022 14:22:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=168
05/30/2022 14:22:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/30/2022 14:22:29 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.2826943467827446 on epoch=168
05/30/2022 14:22:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/30/2022 14:22:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.19 on epoch=169
05/30/2022 14:22:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=170
05/30/2022 14:22:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=171
05/30/2022 14:22:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=171
05/30/2022 14:23:03 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.21242850183143575 on epoch=171
05/30/2022 14:23:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=172
05/30/2022 14:23:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/30/2022 14:23:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=173
05/30/2022 14:23:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=174
05/30/2022 14:23:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=174
05/30/2022 14:23:37 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.21803234501347712 on epoch=174
05/30/2022 14:23:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
05/30/2022 14:23:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
05/30/2022 14:23:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=176
05/30/2022 14:23:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=177
05/30/2022 14:23:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=178
05/30/2022 14:24:10 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.17801839319798715 on epoch=178
05/30/2022 14:24:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=178
05/30/2022 14:24:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=179
05/30/2022 14:24:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/30/2022 14:24:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=180
05/30/2022 14:24:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
05/30/2022 14:24:44 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.29473075365839546 on epoch=181
05/30/2022 14:24:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
05/30/2022 14:24:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=182
05/30/2022 14:24:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
05/30/2022 14:25:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=183
05/30/2022 14:25:07 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
05/30/2022 14:25:18 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.29385964912280704 on epoch=184
05/30/2022 14:25:22 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=184
05/30/2022 14:25:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
05/30/2022 14:25:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
05/30/2022 14:25:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=186
05/30/2022 14:25:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=187
05/30/2022 14:25:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 14:25:42 - INFO - __main__ - Printing 3 examples
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:25:42 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:25:42 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 14:25:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 14:25:42 - INFO - __main__ - Printing 3 examples
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 14:25:42 - INFO - __main__ - ['entailed']
05/30/2022 14:25:42 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:25:43 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:25:43 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 14:25:52 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.21767536618778321 on epoch=187
05/30/2022 14:25:52 - INFO - __main__ - save last model!
05/30/2022 14:25:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 14:25:52 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 14:25:52 - INFO - __main__ - Printing 3 examples
05/30/2022 14:25:52 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:25:52 - INFO - __main__ - ['entailed']
05/30/2022 14:25:52 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:25:52 - INFO - __main__ - ['entailed']
05/30/2022 14:25:52 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 14:25:52 - INFO - __main__ - ['entailed']
05/30/2022 14:25:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:25:59 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:26:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 14:26:00 - INFO - __main__ - Starting training!
05/30/2022 14:26:16 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:26:28 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 14:35:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_13_0.2_8_predictions.txt
05/30/2022 14:35:43 - INFO - __main__ - Classification-F1 on test data: 0.0438
05/30/2022 14:35:44 - INFO - __main__ - prefix=tab_fact_128_13, lr=0.2, bsz=8, dev_performance=0.3671630170316302, test_performance=0.04375788620616798
05/30/2022 14:35:44 - INFO - __main__ - Running ... prefix=tab_fact_128_21, lr=0.5, bsz=8 ...
05/30/2022 14:35:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 14:35:45 - INFO - __main__ - Printing 3 examples
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:35:45 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:35:45 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 14:35:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 14:35:45 - INFO - __main__ - Printing 3 examples
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 14:35:45 - INFO - __main__ - ['entailed']
05/30/2022 14:35:45 - INFO - __main__ - Tokenizing Input ...
05/30/2022 14:35:46 - INFO - __main__ - Tokenizing Output ...
05/30/2022 14:35:46 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 14:36:04 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 14:36:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 14:36:05 - INFO - __main__ - Starting training!
05/30/2022 14:36:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.80 on epoch=0
05/30/2022 14:36:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=1
05/30/2022 14:36:19 - INFO - __main__ - Step 30 Global step 30 Train loss 1.84 on epoch=1
05/30/2022 14:36:24 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=2
05/30/2022 14:36:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.72 on epoch=3
05/30/2022 14:36:40 - INFO - __main__ - Global step 50 Train loss 2.25 Classification-F1 0.3333333333333333 on epoch=3
05/30/2022 14:36:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
05/30/2022 14:36:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
05/30/2022 14:36:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=4
05/30/2022 14:36:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=4
05/30/2022 14:36:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=5
05/30/2022 14:37:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=6
05/30/2022 14:37:13 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 14:37:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=6
05/30/2022 14:37:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=7
05/30/2022 14:37:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=8
05/30/2022 14:37:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=8
05/30/2022 14:37:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=9
05/30/2022 14:37:46 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 14:37:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=9
05/30/2022 14:37:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=10
05/30/2022 14:37:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=11
05/30/2022 14:38:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=11
05/30/2022 14:38:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=12
05/30/2022 14:38:19 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 14:38:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=13
05/30/2022 14:38:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=13
05/30/2022 14:38:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=14
05/30/2022 14:38:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=14
05/30/2022 14:38:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=15
05/30/2022 14:38:53 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 14:38:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=16
05/30/2022 14:39:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=16
05/30/2022 14:39:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=17
05/30/2022 14:39:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=18
05/30/2022 14:39:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=18
05/30/2022 14:39:26 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 14:39:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=19
05/30/2022 14:39:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=19
05/30/2022 14:39:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=20
05/30/2022 14:39:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=21
05/30/2022 14:39:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=21
05/30/2022 14:40:00 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 14:40:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=22
05/30/2022 14:40:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=23
05/30/2022 14:40:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.18 on epoch=23
05/30/2022 14:40:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=24
05/30/2022 14:40:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=24
05/30/2022 14:40:34 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 14:40:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=25
05/30/2022 14:40:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=26
05/30/2022 14:40:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=26
05/30/2022 14:40:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=27
05/30/2022 14:40:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=28
05/30/2022 14:41:07 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 14:41:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=28
05/30/2022 14:41:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 14:41:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 14:41:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=30
05/30/2022 14:41:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=31
05/30/2022 14:41:41 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 14:41:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=31
05/30/2022 14:41:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=32
05/30/2022 14:41:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=33
05/30/2022 14:41:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=33
05/30/2022 14:42:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=34
05/30/2022 14:42:15 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 14:42:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 14:42:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=35
05/30/2022 14:42:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=36
05/30/2022 14:42:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=36
05/30/2022 14:42:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=37
05/30/2022 14:42:49 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 14:42:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 14:42:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=38
05/30/2022 14:43:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=39
05/30/2022 14:43:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=39
05/30/2022 14:43:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=40
05/30/2022 14:43:22 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 14:43:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=41
05/30/2022 14:43:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=41
05/30/2022 14:43:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 14:43:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=43
05/30/2022 14:43:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=43
05/30/2022 14:43:56 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 14:44:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 14:44:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=44
05/30/2022 14:44:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 14:44:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=46
05/30/2022 14:44:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=46
05/30/2022 14:44:30 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 14:44:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=47
05/30/2022 14:44:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=48
05/30/2022 14:44:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=48
05/30/2022 14:44:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=49
05/30/2022 14:44:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=49
05/30/2022 14:45:03 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.35113468906572354 on epoch=49
05/30/2022 14:45:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35113468906572354 on epoch=49, global_step=800
05/30/2022 14:45:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=50
05/30/2022 14:45:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=51
05/30/2022 14:45:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/30/2022 14:45:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/30/2022 14:45:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/30/2022 14:45:37 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 14:45:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 14:45:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=54
05/30/2022 14:45:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=54
05/30/2022 14:45:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=55
05/30/2022 14:45:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 14:46:11 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.33652312599681017 on epoch=56
05/30/2022 14:46:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/30/2022 14:46:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/30/2022 14:46:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/30/2022 14:46:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/30/2022 14:46:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=59
05/30/2022 14:46:45 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.34526854219948844 on epoch=59
05/30/2022 14:46:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/30/2022 14:46:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=60
05/30/2022 14:46:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 14:47:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=61
05/30/2022 14:47:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 14:47:19 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.34485289741504155 on epoch=62
05/30/2022 14:47:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
05/30/2022 14:47:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/30/2022 14:47:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 14:47:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/30/2022 14:47:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/30/2022 14:47:53 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.33917396745932415 on epoch=65
05/30/2022 14:47:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/30/2022 14:48:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=66
05/30/2022 14:48:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/30/2022 14:48:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 14:48:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=68
05/30/2022 14:48:26 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 14:48:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 14:48:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=69
05/30/2022 14:48:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/30/2022 14:48:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=71
05/30/2022 14:48:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/30/2022 14:49:00 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.3568328892272554 on epoch=71
05/30/2022 14:49:00 - INFO - __main__ - Saving model with best Classification-F1: 0.35113468906572354 -> 0.3568328892272554 on epoch=71, global_step=1150
05/30/2022 14:49:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
05/30/2022 14:49:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
05/30/2022 14:49:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=73
05/30/2022 14:49:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=74
05/30/2022 14:49:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 14:49:34 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.24723897451170176 on epoch=74
05/30/2022 14:49:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 14:49:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/30/2022 14:49:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/30/2022 14:49:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/30/2022 14:49:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 14:50:08 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.36657784545108485 on epoch=78
05/30/2022 14:50:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3568328892272554 -> 0.36657784545108485 on epoch=78, global_step=1250
05/30/2022 14:50:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/30/2022 14:50:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 14:50:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 14:50:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=80
05/30/2022 14:50:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=81
05/30/2022 14:50:42 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.24174014391405696 on epoch=81
05/30/2022 14:50:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/30/2022 14:50:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/30/2022 14:50:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
05/30/2022 14:51:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/30/2022 14:51:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/30/2022 14:51:16 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.30320541656574046 on epoch=84
05/30/2022 14:51:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=84
05/30/2022 14:51:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/30/2022 14:51:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/30/2022 14:51:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=86
05/30/2022 14:51:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/30/2022 14:51:49 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.4056446346949141 on epoch=87
05/30/2022 14:51:49 - INFO - __main__ - Saving model with best Classification-F1: 0.36657784545108485 -> 0.4056446346949141 on epoch=87, global_step=1400
05/30/2022 14:51:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=88
05/30/2022 14:51:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/30/2022 14:52:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/30/2022 14:52:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/30/2022 14:52:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
05/30/2022 14:52:23 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.25052631578947365 on epoch=90
05/30/2022 14:52:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
05/30/2022 14:52:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/30/2022 14:52:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/30/2022 14:52:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
05/30/2022 14:52:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/30/2022 14:52:57 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.36657784545108485 on epoch=93
05/30/2022 14:53:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/30/2022 14:53:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=94
05/30/2022 14:53:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=95
05/30/2022 14:53:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/30/2022 14:53:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=96
05/30/2022 14:53:31 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.36657784545108485 on epoch=96
05/30/2022 14:53:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=97
05/30/2022 14:53:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/30/2022 14:53:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
05/30/2022 14:53:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=99
05/30/2022 14:53:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=99
05/30/2022 14:54:04 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.3290391841116479 on epoch=99
05/30/2022 14:54:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/30/2022 14:54:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/30/2022 14:54:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/30/2022 14:54:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
05/30/2022 14:54:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/30/2022 14:54:38 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.3771988921326446 on epoch=103
05/30/2022 14:54:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/30/2022 14:54:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/30/2022 14:54:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/30/2022 14:54:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
05/30/2022 14:55:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/30/2022 14:55:12 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.3845121610287951 on epoch=106
05/30/2022 14:55:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/30/2022 14:55:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=107
05/30/2022 14:55:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=108
05/30/2022 14:55:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/30/2022 14:55:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/30/2022 14:55:46 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.45705196182396607 on epoch=109
05/30/2022 14:55:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4056446346949141 -> 0.45705196182396607 on epoch=109, global_step=1750
05/30/2022 14:55:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
05/30/2022 14:55:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
05/30/2022 14:55:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/30/2022 14:56:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/30/2022 14:56:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/30/2022 14:56:20 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.5069276172724448 on epoch=112
05/30/2022 14:56:20 - INFO - __main__ - Saving model with best Classification-F1: 0.45705196182396607 -> 0.5069276172724448 on epoch=112, global_step=1800
05/30/2022 14:56:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=113
05/30/2022 14:56:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 14:56:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
05/30/2022 14:56:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/30/2022 14:56:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/30/2022 14:56:54 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.32008006058638977 on epoch=115
05/30/2022 14:56:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=116
05/30/2022 14:57:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
05/30/2022 14:57:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/30/2022 14:57:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
05/30/2022 14:57:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
05/30/2022 14:57:28 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.255261221486387 on epoch=118
05/30/2022 14:57:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=119
05/30/2022 14:57:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=119
05/30/2022 14:57:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=120
05/30/2022 14:57:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/30/2022 14:57:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 14:58:02 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.26752446538600944 on epoch=121
05/30/2022 14:58:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=122
05/30/2022 14:58:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/30/2022 14:58:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=123
05/30/2022 14:58:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/30/2022 14:58:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
05/30/2022 14:58:36 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.30408611981887845 on epoch=124
05/30/2022 14:58:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/30/2022 14:58:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
05/30/2022 14:58:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=126
05/30/2022 14:58:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/30/2022 14:58:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=128
05/30/2022 14:59:11 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.28048458440615304 on epoch=128
05/30/2022 14:59:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=128
05/30/2022 14:59:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=129
05/30/2022 14:59:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/30/2022 14:59:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/30/2022 14:59:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/30/2022 14:59:45 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.19200802939575395 on epoch=131
05/30/2022 14:59:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/30/2022 14:59:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
05/30/2022 14:59:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=133
05/30/2022 15:00:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=133
05/30/2022 15:00:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=134
05/30/2022 15:00:19 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.2292823327723691 on epoch=134
05/30/2022 15:00:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=134
05/30/2022 15:00:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/30/2022 15:00:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/30/2022 15:00:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/30/2022 15:00:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
05/30/2022 15:00:53 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.24901574803149606 on epoch=137
05/30/2022 15:00:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=138
05/30/2022 15:01:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
05/30/2022 15:01:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/30/2022 15:01:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=139
05/30/2022 15:01:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=140
05/30/2022 15:01:27 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.19477911646586346 on epoch=140
05/30/2022 15:01:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/30/2022 15:01:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=141
05/30/2022 15:01:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=142
05/30/2022 15:01:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/30/2022 15:01:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/30/2022 15:02:01 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.20264202172096907 on epoch=143
05/30/2022 15:02:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=144
05/30/2022 15:02:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=144
05/30/2022 15:02:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/30/2022 15:02:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=146
05/30/2022 15:02:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=146
05/30/2022 15:02:35 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.21776082987283685 on epoch=146
05/30/2022 15:02:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
05/30/2022 15:02:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=148
05/30/2022 15:02:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/30/2022 15:02:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/30/2022 15:02:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.16 on epoch=149
05/30/2022 15:03:09 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.2568590059055118 on epoch=149
05/30/2022 15:03:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
05/30/2022 15:03:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/30/2022 15:03:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/30/2022 15:03:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=152
05/30/2022 15:03:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
05/30/2022 15:03:43 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.3200033208283391 on epoch=153
05/30/2022 15:03:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=153
05/30/2022 15:03:52 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
05/30/2022 15:03:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/30/2022 15:04:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/30/2022 15:04:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
05/30/2022 15:04:17 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.23624728326491187 on epoch=156
05/30/2022 15:04:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
05/30/2022 15:04:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/30/2022 15:04:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/30/2022 15:04:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/30/2022 15:04:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
05/30/2022 15:04:52 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.1648424543946932 on epoch=159
05/30/2022 15:04:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
05/30/2022 15:05:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
05/30/2022 15:05:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=161
05/30/2022 15:05:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/30/2022 15:05:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=162
05/30/2022 15:05:26 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.18228381096028157 on epoch=162
05/30/2022 15:05:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=163
05/30/2022 15:05:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/30/2022 15:05:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
05/30/2022 15:05:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
05/30/2022 15:05:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/30/2022 15:06:00 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.17654239490339077 on epoch=165
05/30/2022 15:06:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
05/30/2022 15:06:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/30/2022 15:06:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/30/2022 15:06:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/30/2022 15:06:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=168
05/30/2022 15:06:34 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.3128276287388221 on epoch=168
05/30/2022 15:06:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
05/30/2022 15:06:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/30/2022 15:06:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/30/2022 15:06:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/30/2022 15:06:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
05/30/2022 15:07:09 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.22525252525252523 on epoch=171
05/30/2022 15:07:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/30/2022 15:07:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/30/2022 15:07:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=173
05/30/2022 15:07:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/30/2022 15:07:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=174
05/30/2022 15:07:43 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.1723185931091713 on epoch=174
05/30/2022 15:07:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=175
05/30/2022 15:07:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=176
05/30/2022 15:07:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=176
05/30/2022 15:08:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/30/2022 15:08:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/30/2022 15:08:17 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.214940690179434 on epoch=178
05/30/2022 15:08:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
05/30/2022 15:08:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/30/2022 15:08:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
05/30/2022 15:08:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
05/30/2022 15:08:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=181
05/30/2022 15:08:51 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.20699300699300696 on epoch=181
05/30/2022 15:08:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
05/30/2022 15:09:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
05/30/2022 15:09:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/30/2022 15:09:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=183
05/30/2022 15:09:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=184
05/30/2022 15:09:25 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.19981065387528402 on epoch=184
05/30/2022 15:09:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/30/2022 15:09:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
05/30/2022 15:09:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=186
05/30/2022 15:09:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/30/2022 15:09:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
05/30/2022 15:09:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:09:49 - INFO - __main__ - Printing 3 examples
05/30/2022 15:09:49 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 15:09:49 - INFO - __main__ - ['entailed']
05/30/2022 15:09:49 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 15:09:49 - INFO - __main__ - ['entailed']
05/30/2022 15:09:49 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 15:09:49 - INFO - __main__ - ['entailed']
05/30/2022 15:09:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:09:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:09:50 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 15:09:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:09:50 - INFO - __main__ - Printing 3 examples
05/30/2022 15:09:50 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 15:09:50 - INFO - __main__ - ['entailed']
05/30/2022 15:09:50 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 15:09:50 - INFO - __main__ - ['entailed']
05/30/2022 15:09:50 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 15:09:50 - INFO - __main__ - ['entailed']
05/30/2022 15:09:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:09:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:09:50 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 15:10:00 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.17654043244594422 on epoch=187
05/30/2022 15:10:00 - INFO - __main__ - save last model!
05/30/2022 15:10:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 15:10:00 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 15:10:00 - INFO - __main__ - Printing 3 examples
05/30/2022 15:10:00 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:10:00 - INFO - __main__ - ['entailed']
05/30/2022 15:10:00 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:10:00 - INFO - __main__ - ['entailed']
05/30/2022 15:10:00 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:10:00 - INFO - __main__ - ['entailed']
05/30/2022 15:10:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:10:08 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:10:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 15:10:09 - INFO - __main__ - Starting training!
05/30/2022 15:10:24 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:10:37 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 15:19:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_21_0.5_8_predictions.txt
05/30/2022 15:19:58 - INFO - __main__ - Classification-F1 on test data: 0.0589
05/30/2022 15:19:58 - INFO - __main__ - prefix=tab_fact_128_21, lr=0.5, bsz=8, dev_performance=0.5069276172724448, test_performance=0.058871752942251165
05/30/2022 15:19:58 - INFO - __main__ - Running ... prefix=tab_fact_128_21, lr=0.4, bsz=8 ...
05/30/2022 15:19:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:19:59 - INFO - __main__ - Printing 3 examples
05/30/2022 15:19:59 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 15:19:59 - INFO - __main__ - ['entailed']
05/30/2022 15:19:59 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 15:19:59 - INFO - __main__ - ['entailed']
05/30/2022 15:19:59 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 15:19:59 - INFO - __main__ - ['entailed']
05/30/2022 15:19:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:20:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:20:00 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 15:20:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:20:00 - INFO - __main__ - Printing 3 examples
05/30/2022 15:20:00 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 15:20:00 - INFO - __main__ - ['entailed']
05/30/2022 15:20:00 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 15:20:00 - INFO - __main__ - ['entailed']
05/30/2022 15:20:00 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 15:20:00 - INFO - __main__ - ['entailed']
05/30/2022 15:20:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:20:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:20:01 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 15:20:16 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:20:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 15:20:17 - INFO - __main__ - Starting training!
05/30/2022 15:20:22 - INFO - __main__ - Step 10 Global step 10 Train loss 5.19 on epoch=0
05/30/2022 15:20:26 - INFO - __main__ - Step 20 Global step 20 Train loss 3.05 on epoch=1
05/30/2022 15:20:31 - INFO - __main__ - Step 30 Global step 30 Train loss 2.15 on epoch=1
05/30/2022 15:20:35 - INFO - __main__ - Step 40 Global step 40 Train loss 1.75 on epoch=2
05/30/2022 15:20:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.24 on epoch=3
05/30/2022 15:20:51 - INFO - __main__ - Global step 50 Train loss 2.68 Classification-F1 0.21693121693121695 on epoch=3
05/30/2022 15:20:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21693121693121695 on epoch=3, global_step=50
05/30/2022 15:20:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=3
05/30/2022 15:21:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=4
05/30/2022 15:21:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=4
05/30/2022 15:21:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/30/2022 15:21:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=6
05/30/2022 15:21:24 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 15:21:25 - INFO - __main__ - Saving model with best Classification-F1: 0.21693121693121695 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 15:21:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=6
05/30/2022 15:21:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=7
05/30/2022 15:21:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=8
05/30/2022 15:21:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=8
05/30/2022 15:21:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=9
05/30/2022 15:21:57 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 15:22:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=9
05/30/2022 15:22:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=10
05/30/2022 15:22:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=11
05/30/2022 15:22:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=11
05/30/2022 15:22:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=12
05/30/2022 15:22:29 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 15:22:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=13
05/30/2022 15:22:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=13
05/30/2022 15:22:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=14
05/30/2022 15:22:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=14
05/30/2022 15:22:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=15
05/30/2022 15:23:02 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 15:23:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=16
05/30/2022 15:23:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=16
05/30/2022 15:23:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=17
05/30/2022 15:23:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=18
05/30/2022 15:23:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=18
05/30/2022 15:23:36 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 15:23:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=19
05/30/2022 15:23:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=19
05/30/2022 15:23:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=20
05/30/2022 15:23:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=21
05/30/2022 15:23:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=21
05/30/2022 15:24:09 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 15:24:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=22
05/30/2022 15:24:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=23
05/30/2022 15:24:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=23
05/30/2022 15:24:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=24
05/30/2022 15:24:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=24
05/30/2022 15:24:42 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 15:24:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=25
05/30/2022 15:24:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=26
05/30/2022 15:24:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=26
05/30/2022 15:25:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=27
05/30/2022 15:25:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=28
05/30/2022 15:25:16 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 15:25:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=28
05/30/2022 15:25:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 15:25:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=29
05/30/2022 15:25:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=30
05/30/2022 15:25:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=31
05/30/2022 15:25:50 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 15:25:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=31
05/30/2022 15:25:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=32
05/30/2022 15:26:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=33
05/30/2022 15:26:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=33
05/30/2022 15:26:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=34
05/30/2022 15:26:24 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 15:26:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=34
05/30/2022 15:26:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=35
05/30/2022 15:26:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=36
05/30/2022 15:26:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=36
05/30/2022 15:26:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.16 on epoch=37
05/30/2022 15:26:57 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 15:27:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 15:27:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=38
05/30/2022 15:27:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=39
05/30/2022 15:27:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=39
05/30/2022 15:27:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=40
05/30/2022 15:27:31 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 15:27:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=41
05/30/2022 15:27:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=41
05/30/2022 15:27:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=42
05/30/2022 15:27:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 15:27:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=43
05/30/2022 15:28:05 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 15:28:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=44
05/30/2022 15:28:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=44
05/30/2022 15:28:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=45
05/30/2022 15:28:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=46
05/30/2022 15:28:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=46
05/30/2022 15:28:39 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 15:28:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=47
05/30/2022 15:28:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=48
05/30/2022 15:28:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/30/2022 15:28:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=49
05/30/2022 15:29:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/30/2022 15:29:13 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 15:29:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/30/2022 15:29:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/30/2022 15:29:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=51
05/30/2022 15:29:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=52
05/30/2022 15:29:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=53
05/30/2022 15:29:47 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 15:29:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=53
05/30/2022 15:29:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=54
05/30/2022 15:30:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=54
05/30/2022 15:30:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 15:30:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 15:30:22 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 15:30:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/30/2022 15:30:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=57
05/30/2022 15:30:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=58
05/30/2022 15:30:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=58
05/30/2022 15:30:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 15:30:56 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 15:31:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/30/2022 15:31:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/30/2022 15:31:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=61
05/30/2022 15:31:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 15:31:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 15:31:30 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 15:31:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/30/2022 15:31:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=63
05/30/2022 15:31:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 15:31:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=64
05/30/2022 15:31:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 15:32:03 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 15:32:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=66
05/30/2022 15:32:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=66
05/30/2022 15:32:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=67
05/30/2022 15:32:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/30/2022 15:32:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=68
05/30/2022 15:32:37 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 15:32:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 15:32:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 15:32:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=70
05/30/2022 15:32:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 15:33:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/30/2022 15:33:11 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 15:33:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/30/2022 15:33:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 15:33:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 15:33:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=74
05/30/2022 15:33:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
05/30/2022 15:33:45 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 15:33:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=75
05/30/2022 15:33:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/30/2022 15:33:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=76
05/30/2022 15:34:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/30/2022 15:34:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=78
05/30/2022 15:34:18 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.35307589038932324 on epoch=78
05/30/2022 15:34:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35307589038932324 on epoch=78, global_step=1250
05/30/2022 15:34:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
05/30/2022 15:34:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=79
05/30/2022 15:34:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/30/2022 15:34:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/30/2022 15:34:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/30/2022 15:34:52 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.34723114355231144 on epoch=81
05/30/2022 15:34:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/30/2022 15:35:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 15:35:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/30/2022 15:35:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=83
05/30/2022 15:35:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/30/2022 15:35:26 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.35518871580252653 on epoch=84
05/30/2022 15:35:26 - INFO - __main__ - Saving model with best Classification-F1: 0.35307589038932324 -> 0.35518871580252653 on epoch=84, global_step=1350
05/30/2022 15:35:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=84
05/30/2022 15:35:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=85
05/30/2022 15:35:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/30/2022 15:35:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/30/2022 15:35:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 15:36:00 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.36869874405000863 on epoch=87
05/30/2022 15:36:00 - INFO - __main__ - Saving model with best Classification-F1: 0.35518871580252653 -> 0.36869874405000863 on epoch=87, global_step=1400
05/30/2022 15:36:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=88
05/30/2022 15:36:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/30/2022 15:36:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/30/2022 15:36:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 15:36:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=90
05/30/2022 15:36:33 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.35518871580252653 on epoch=90
05/30/2022 15:36:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 15:36:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/30/2022 15:36:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=92
05/30/2022 15:36:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/30/2022 15:36:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/30/2022 15:37:07 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.34723114355231144 on epoch=93
05/30/2022 15:37:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/30/2022 15:37:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/30/2022 15:37:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=95
05/30/2022 15:37:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/30/2022 15:37:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=96
05/30/2022 15:37:41 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.34723114355231144 on epoch=96
05/30/2022 15:37:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/30/2022 15:37:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/30/2022 15:37:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=98
05/30/2022 15:37:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/30/2022 15:38:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/30/2022 15:38:15 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.36304897101085887 on epoch=99
05/30/2022 15:38:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=100
05/30/2022 15:38:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/30/2022 15:38:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
05/30/2022 15:38:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/30/2022 15:38:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/30/2022 15:38:49 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.34723114355231144 on epoch=103
05/30/2022 15:38:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/30/2022 15:38:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=104
05/30/2022 15:39:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
05/30/2022 15:39:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 15:39:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/30/2022 15:39:22 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.4376260443676175 on epoch=106
05/30/2022 15:39:22 - INFO - __main__ - Saving model with best Classification-F1: 0.36869874405000863 -> 0.4376260443676175 on epoch=106, global_step=1700
05/30/2022 15:39:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 15:39:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/30/2022 15:39:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=108
05/30/2022 15:39:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/30/2022 15:39:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/30/2022 15:39:56 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.39866578972094335 on epoch=109
05/30/2022 15:40:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/30/2022 15:40:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=110
05/30/2022 15:40:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/30/2022 15:40:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=111
05/30/2022 15:40:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
05/30/2022 15:40:30 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.4059668508287293 on epoch=112
05/30/2022 15:40:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=113
05/30/2022 15:40:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=113
05/30/2022 15:40:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/30/2022 15:40:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 15:40:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/30/2022 15:41:04 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.38904444235647845 on epoch=115
05/30/2022 15:41:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=116
05/30/2022 15:41:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
05/30/2022 15:41:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
05/30/2022 15:41:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=118
05/30/2022 15:41:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/30/2022 15:41:38 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.39636200314394787 on epoch=118
05/30/2022 15:41:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=119
05/30/2022 15:41:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=119
05/30/2022 15:41:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/30/2022 15:41:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/30/2022 15:42:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=121
05/30/2022 15:42:12 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.42235512098475536 on epoch=121
05/30/2022 15:42:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/30/2022 15:42:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=123
05/30/2022 15:42:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/30/2022 15:42:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/30/2022 15:42:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=124
05/30/2022 15:42:46 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.47182294064487523 on epoch=124
05/30/2022 15:42:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4376260443676175 -> 0.47182294064487523 on epoch=124, global_step=2000
05/30/2022 15:42:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/30/2022 15:42:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=126
05/30/2022 15:42:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
05/30/2022 15:43:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
05/30/2022 15:43:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
05/30/2022 15:43:20 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.44160766645212046 on epoch=128
05/30/2022 15:43:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/30/2022 15:43:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=129
05/30/2022 15:43:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/30/2022 15:43:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=130
05/30/2022 15:43:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
05/30/2022 15:43:54 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.46906958449545877 on epoch=131
05/30/2022 15:43:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/30/2022 15:44:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
05/30/2022 15:44:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/30/2022 15:44:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=133
05/30/2022 15:44:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
05/30/2022 15:44:28 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.510856348792774 on epoch=134
05/30/2022 15:44:28 - INFO - __main__ - Saving model with best Classification-F1: 0.47182294064487523 -> 0.510856348792774 on epoch=134, global_step=2150
05/30/2022 15:44:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
05/30/2022 15:44:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/30/2022 15:44:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/30/2022 15:44:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=136
05/30/2022 15:44:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
05/30/2022 15:45:02 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.49573753552053734 on epoch=137
05/30/2022 15:45:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/30/2022 15:45:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
05/30/2022 15:45:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=139
05/30/2022 15:45:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/30/2022 15:45:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/30/2022 15:45:36 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.49556650246305417 on epoch=140
05/30/2022 15:45:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/30/2022 15:45:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/30/2022 15:45:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/30/2022 15:45:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/30/2022 15:45:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/30/2022 15:46:09 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.45347565738089035 on epoch=143
05/30/2022 15:46:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=144
05/30/2022 15:46:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
05/30/2022 15:46:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
05/30/2022 15:46:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=146
05/30/2022 15:46:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/30/2022 15:46:43 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.49552345338029696 on epoch=146
05/30/2022 15:46:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/30/2022 15:46:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=148
05/30/2022 15:46:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 15:47:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=149
05/30/2022 15:47:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=149
05/30/2022 15:47:17 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.4740190880169671 on epoch=149
05/30/2022 15:47:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
05/30/2022 15:47:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/30/2022 15:47:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=151
05/30/2022 15:47:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
05/30/2022 15:47:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=153
05/30/2022 15:47:51 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.477376171352075 on epoch=153
05/30/2022 15:47:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/30/2022 15:48:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
05/30/2022 15:48:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/30/2022 15:48:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
05/30/2022 15:48:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/30/2022 15:48:25 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.48169050010362363 on epoch=156
05/30/2022 15:48:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/30/2022 15:48:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/30/2022 15:48:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
05/30/2022 15:48:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/30/2022 15:48:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=159
05/30/2022 15:48:58 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.5032923382476511 on epoch=159
05/30/2022 15:49:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/30/2022 15:49:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=160
05/30/2022 15:49:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=161
05/30/2022 15:49:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/30/2022 15:49:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=162
05/30/2022 15:49:32 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.31858632087465044 on epoch=162
05/30/2022 15:49:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=163
05/30/2022 15:49:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=163
05/30/2022 15:49:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/30/2022 15:49:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
05/30/2022 15:49:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=165
05/30/2022 15:50:06 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.33655054079725466 on epoch=165
05/30/2022 15:50:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=166
05/30/2022 15:50:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=166
05/30/2022 15:50:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=167
05/30/2022 15:50:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
05/30/2022 15:50:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=168
05/30/2022 15:50:39 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.5029284441049147 on epoch=168
05/30/2022 15:50:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/30/2022 15:50:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/30/2022 15:50:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=170
05/30/2022 15:50:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
05/30/2022 15:51:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=171
05/30/2022 15:51:13 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.3066156543918381 on epoch=171
05/30/2022 15:51:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/30/2022 15:51:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/30/2022 15:51:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=173
05/30/2022 15:51:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/30/2022 15:51:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=174
05/30/2022 15:51:47 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.34619297859173254 on epoch=174
05/30/2022 15:51:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
05/30/2022 15:51:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=176
05/30/2022 15:52:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=176
05/30/2022 15:52:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=177
05/30/2022 15:52:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=178
05/30/2022 15:52:21 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.3400063650456333 on epoch=178
05/30/2022 15:52:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=178
05/30/2022 15:52:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/30/2022 15:52:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
05/30/2022 15:52:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=180
05/30/2022 15:52:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=181
05/30/2022 15:52:54 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.33632559112785104 on epoch=181
05/30/2022 15:52:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
05/30/2022 15:53:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=182
05/30/2022 15:53:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=183
05/30/2022 15:53:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
05/30/2022 15:53:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
05/30/2022 15:53:28 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.3339053339053339 on epoch=184
05/30/2022 15:53:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=184
05/30/2022 15:53:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/30/2022 15:53:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=186
05/30/2022 15:53:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/30/2022 15:53:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
05/30/2022 15:53:52 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:53:52 - INFO - __main__ - Printing 3 examples
05/30/2022 15:53:52 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 15:53:52 - INFO - __main__ - ['entailed']
05/30/2022 15:53:52 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 15:53:52 - INFO - __main__ - ['entailed']
05/30/2022 15:53:52 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 15:53:52 - INFO - __main__ - ['entailed']
05/30/2022 15:53:52 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:53:52 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:53:53 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 15:53:53 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 15:53:53 - INFO - __main__ - Printing 3 examples
05/30/2022 15:53:53 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 15:53:53 - INFO - __main__ - ['entailed']
05/30/2022 15:53:53 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 15:53:53 - INFO - __main__ - ['entailed']
05/30/2022 15:53:53 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 15:53:53 - INFO - __main__ - ['entailed']
05/30/2022 15:53:53 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:53:53 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:53:53 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 15:54:02 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.334855403348554 on epoch=187
05/30/2022 15:54:02 - INFO - __main__ - save last model!
05/30/2022 15:54:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 15:54:02 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 15:54:02 - INFO - __main__ - Printing 3 examples
05/30/2022 15:54:02 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:54:02 - INFO - __main__ - ['entailed']
05/30/2022 15:54:02 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:54:02 - INFO - __main__ - ['entailed']
05/30/2022 15:54:02 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 15:54:02 - INFO - __main__ - ['entailed']
05/30/2022 15:54:02 - INFO - __main__ - Tokenizing Input ...
05/30/2022 15:54:12 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 15:54:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 15:54:13 - INFO - __main__ - Starting training!
05/30/2022 15:54:27 - INFO - __main__ - Tokenizing Output ...
05/30/2022 15:54:40 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 16:03:59 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_21_0.4_8_predictions.txt
05/30/2022 16:03:59 - INFO - __main__ - Classification-F1 on test data: 0.1986
05/30/2022 16:04:00 - INFO - __main__ - prefix=tab_fact_128_21, lr=0.4, bsz=8, dev_performance=0.510856348792774, test_performance=0.19860518087571374
05/30/2022 16:04:00 - INFO - __main__ - Running ... prefix=tab_fact_128_21, lr=0.3, bsz=8 ...
05/30/2022 16:04:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:04:01 - INFO - __main__ - Printing 3 examples
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:04:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:04:01 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 16:04:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:04:01 - INFO - __main__ - Printing 3 examples
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 16:04:01 - INFO - __main__ - ['entailed']
05/30/2022 16:04:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:04:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:04:02 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 16:04:17 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 16:04:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 16:04:18 - INFO - __main__ - Starting training!
05/30/2022 16:04:23 - INFO - __main__ - Step 10 Global step 10 Train loss 5.14 on epoch=0
05/30/2022 16:04:28 - INFO - __main__ - Step 20 Global step 20 Train loss 3.36 on epoch=1
05/30/2022 16:04:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.52 on epoch=1
05/30/2022 16:04:37 - INFO - __main__ - Step 40 Global step 40 Train loss 2.04 on epoch=2
05/30/2022 16:04:41 - INFO - __main__ - Step 50 Global step 50 Train loss 1.53 on epoch=3
05/30/2022 16:04:52 - INFO - __main__ - Global step 50 Train loss 2.92 Classification-F1 0.05447761194029851 on epoch=3
05/30/2022 16:04:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.05447761194029851 on epoch=3, global_step=50
05/30/2022 16:04:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=3
05/30/2022 16:05:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=4
05/30/2022 16:05:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.67 on epoch=4
05/30/2022 16:05:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=5
05/30/2022 16:05:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/30/2022 16:05:26 - INFO - __main__ - Global step 100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 16:05:26 - INFO - __main__ - Saving model with best Classification-F1: 0.05447761194029851 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 16:05:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=6
05/30/2022 16:05:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=7
05/30/2022 16:05:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=8
05/30/2022 16:05:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=8
05/30/2022 16:05:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=9
05/30/2022 16:06:00 - INFO - __main__ - Global step 150 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 16:06:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 16:06:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.34 on epoch=10
05/30/2022 16:06:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=11
05/30/2022 16:06:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=11
05/30/2022 16:06:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=12
05/30/2022 16:06:33 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 16:06:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=13
05/30/2022 16:06:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=13
05/30/2022 16:06:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=14
05/30/2022 16:06:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=14
05/30/2022 16:06:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=15
05/30/2022 16:07:07 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 16:07:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=16
05/30/2022 16:07:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=16
05/30/2022 16:07:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=17
05/30/2022 16:07:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=18
05/30/2022 16:07:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=18
05/30/2022 16:07:40 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 16:07:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=19
05/30/2022 16:07:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=19
05/30/2022 16:07:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=20
05/30/2022 16:07:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=21
05/30/2022 16:08:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=21
05/30/2022 16:08:14 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 16:08:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=22
05/30/2022 16:08:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=23
05/30/2022 16:08:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=23
05/30/2022 16:08:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=24
05/30/2022 16:08:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=24
05/30/2022 16:08:47 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 16:08:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=25
05/30/2022 16:08:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=26
05/30/2022 16:09:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=26
05/30/2022 16:09:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=27
05/30/2022 16:09:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 16:09:21 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 16:09:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=28
05/30/2022 16:09:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 16:09:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 16:09:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=30
05/30/2022 16:09:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=31
05/30/2022 16:09:53 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 16:09:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=31
05/30/2022 16:10:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=32
05/30/2022 16:10:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 16:10:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=33
05/30/2022 16:10:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=34
05/30/2022 16:10:27 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 16:10:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=34
05/30/2022 16:10:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=35
05/30/2022 16:10:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=36
05/30/2022 16:10:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=36
05/30/2022 16:10:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=37
05/30/2022 16:11:01 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 16:11:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 16:11:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=38
05/30/2022 16:11:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=39
05/30/2022 16:11:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=39
05/30/2022 16:11:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=40
05/30/2022 16:11:35 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 16:11:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=41
05/30/2022 16:11:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=41
05/30/2022 16:11:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=42
05/30/2022 16:11:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 16:11:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=43
05/30/2022 16:12:08 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 16:12:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 16:12:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=44
05/30/2022 16:12:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 16:12:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=46
05/30/2022 16:12:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 16:12:42 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 16:12:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=47
05/30/2022 16:12:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 16:12:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
05/30/2022 16:13:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=49
05/30/2022 16:13:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=49
05/30/2022 16:13:16 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 16:13:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=50
05/30/2022 16:13:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/30/2022 16:13:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 16:13:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/30/2022 16:13:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=53
05/30/2022 16:13:50 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 16:13:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=53
05/30/2022 16:13:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/30/2022 16:14:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=54
05/30/2022 16:14:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 16:14:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 16:14:24 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 16:14:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=56
05/30/2022 16:14:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/30/2022 16:14:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 16:14:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=58
05/30/2022 16:14:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/30/2022 16:14:58 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 16:15:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=59
05/30/2022 16:15:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=60
05/30/2022 16:15:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=61
05/30/2022 16:15:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=61
05/30/2022 16:15:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/30/2022 16:15:31 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3401530406766009 on epoch=62
05/30/2022 16:15:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3401530406766009 on epoch=62, global_step=1000
05/30/2022 16:15:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/30/2022 16:15:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=63
05/30/2022 16:15:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=64
05/30/2022 16:15:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 16:15:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/30/2022 16:16:06 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 16:16:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/30/2022 16:16:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=66
05/30/2022 16:16:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 16:16:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/30/2022 16:16:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/30/2022 16:16:39 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 16:16:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=69
05/30/2022 16:16:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 16:16:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/30/2022 16:16:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 16:17:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=71
05/30/2022 16:17:13 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 16:17:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/30/2022 16:17:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 16:17:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 16:17:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/30/2022 16:17:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 16:17:47 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=74
05/30/2022 16:17:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/30/2022 16:17:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/30/2022 16:18:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=76
05/30/2022 16:18:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
05/30/2022 16:18:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 16:18:21 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 16:18:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
05/30/2022 16:18:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=79
05/30/2022 16:18:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/30/2022 16:18:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=80
05/30/2022 16:18:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/30/2022 16:18:55 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.33159268929503916 on epoch=81
05/30/2022 16:18:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/30/2022 16:19:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=82
05/30/2022 16:19:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/30/2022 16:19:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/30/2022 16:19:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
05/30/2022 16:19:29 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.341074761764417 on epoch=84
05/30/2022 16:19:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3401530406766009 -> 0.341074761764417 on epoch=84, global_step=1350
05/30/2022 16:19:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/30/2022 16:19:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/30/2022 16:19:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=86
05/30/2022 16:19:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/30/2022 16:19:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
05/30/2022 16:20:02 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.33917396745932415 on epoch=87
05/30/2022 16:20:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/30/2022 16:20:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/30/2022 16:20:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
05/30/2022 16:20:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 16:20:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/30/2022 16:20:36 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.3491235113073732 on epoch=90
05/30/2022 16:20:36 - INFO - __main__ - Saving model with best Classification-F1: 0.341074761764417 -> 0.3491235113073732 on epoch=90, global_step=1450
05/30/2022 16:20:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 16:20:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/30/2022 16:20:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/30/2022 16:20:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=93
05/30/2022 16:20:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=93
05/30/2022 16:21:10 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.34526854219948844 on epoch=93
05/30/2022 16:21:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/30/2022 16:21:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 16:21:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/30/2022 16:21:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/30/2022 16:21:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
05/30/2022 16:21:44 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.33101483446311036 on epoch=96
05/30/2022 16:21:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=97
05/30/2022 16:21:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/30/2022 16:21:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
05/30/2022 16:22:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=99
05/30/2022 16:22:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=99
05/30/2022 16:22:18 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.3644512668902913 on epoch=99
05/30/2022 16:22:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3491235113073732 -> 0.3644512668902913 on epoch=99, global_step=1600
05/30/2022 16:22:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=100
05/30/2022 16:22:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/30/2022 16:22:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=101
05/30/2022 16:22:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=102
05/30/2022 16:22:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/30/2022 16:22:52 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.3719806763285024 on epoch=103
05/30/2022 16:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3644512668902913 -> 0.3719806763285024 on epoch=103, global_step=1650
05/30/2022 16:22:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
05/30/2022 16:23:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
05/30/2022 16:23:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/30/2022 16:23:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 16:23:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
05/30/2022 16:23:25 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.3491235113073732 on epoch=106
05/30/2022 16:23:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
05/30/2022 16:23:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/30/2022 16:23:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/30/2022 16:23:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/30/2022 16:23:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=109
05/30/2022 16:23:59 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.3511520737327189 on epoch=109
05/30/2022 16:24:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/30/2022 16:24:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=110
05/30/2022 16:24:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/30/2022 16:24:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/30/2022 16:24:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/30/2022 16:24:33 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.3871086556169429 on epoch=112
05/30/2022 16:24:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3719806763285024 -> 0.3871086556169429 on epoch=112, global_step=1800
05/30/2022 16:24:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/30/2022 16:24:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/30/2022 16:24:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/30/2022 16:24:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
05/30/2022 16:24:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/30/2022 16:25:07 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.3644512668902913 on epoch=115
05/30/2022 16:25:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/30/2022 16:25:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/30/2022 16:25:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/30/2022 16:25:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/30/2022 16:25:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/30/2022 16:25:40 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.3589111276786909 on epoch=118
05/30/2022 16:25:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/30/2022 16:25:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=119
05/30/2022 16:25:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=120
05/30/2022 16:25:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
05/30/2022 16:26:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 16:26:14 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3644512668902913 on epoch=121
05/30/2022 16:26:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=122
05/30/2022 16:26:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/30/2022 16:26:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/30/2022 16:26:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/30/2022 16:26:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
05/30/2022 16:26:48 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.3644512668902913 on epoch=124
05/30/2022 16:26:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=125
05/30/2022 16:26:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=126
05/30/2022 16:27:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 16:27:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
05/30/2022 16:27:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
05/30/2022 16:27:22 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.3491235113073732 on epoch=128
05/30/2022 16:27:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/30/2022 16:27:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
05/30/2022 16:27:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/30/2022 16:27:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/30/2022 16:27:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
05/30/2022 16:27:56 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.3644512668902913 on epoch=131
05/30/2022 16:28:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
05/30/2022 16:28:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=132
05/30/2022 16:28:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/30/2022 16:28:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/30/2022 16:28:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/30/2022 16:28:30 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.3644512668902913 on epoch=134
05/30/2022 16:28:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 16:28:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/30/2022 16:28:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/30/2022 16:28:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/30/2022 16:28:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
05/30/2022 16:29:03 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.26146869004011863 on epoch=137
05/30/2022 16:29:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/30/2022 16:29:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
05/30/2022 16:29:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/30/2022 16:29:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=139
05/30/2022 16:29:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/30/2022 16:29:37 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.28433850969062235 on epoch=140
05/30/2022 16:29:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
05/30/2022 16:29:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/30/2022 16:29:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=142
05/30/2022 16:29:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
05/30/2022 16:30:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/30/2022 16:30:11 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.24821374277896016 on epoch=143
05/30/2022 16:30:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/30/2022 16:30:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/30/2022 16:30:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
05/30/2022 16:30:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/30/2022 16:30:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/30/2022 16:30:45 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.2583707219400052 on epoch=146
05/30/2022 16:30:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/30/2022 16:30:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
05/30/2022 16:30:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/30/2022 16:31:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/30/2022 16:31:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=149
05/30/2022 16:31:19 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.396357832112022 on epoch=149
05/30/2022 16:31:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3871086556169429 -> 0.396357832112022 on epoch=149, global_step=2400
05/30/2022 16:31:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/30/2022 16:31:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
05/30/2022 16:31:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
05/30/2022 16:31:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/30/2022 16:31:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/30/2022 16:31:53 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.25205482838320536 on epoch=153
05/30/2022 16:31:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/30/2022 16:32:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=154
05/30/2022 16:32:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=154
05/30/2022 16:32:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=155
05/30/2022 16:32:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=156
05/30/2022 16:32:27 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.24723897451170176 on epoch=156
05/30/2022 16:32:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/30/2022 16:32:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
05/30/2022 16:32:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=158
05/30/2022 16:32:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/30/2022 16:32:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/30/2022 16:33:01 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.289624881291548 on epoch=159
05/30/2022 16:33:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/30/2022 16:33:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=160
05/30/2022 16:33:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/30/2022 16:33:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=161
05/30/2022 16:33:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=162
05/30/2022 16:33:35 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.44243796196258 on epoch=162
05/30/2022 16:33:35 - INFO - __main__ - Saving model with best Classification-F1: 0.396357832112022 -> 0.44243796196258 on epoch=162, global_step=2600
05/30/2022 16:33:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=163
05/30/2022 16:33:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=163
05/30/2022 16:33:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
05/30/2022 16:33:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=164
05/30/2022 16:33:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=165
05/30/2022 16:34:09 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.42817194400395586 on epoch=165
05/30/2022 16:34:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=166
05/30/2022 16:34:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=166
05/30/2022 16:34:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=167
05/30/2022 16:34:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=168
05/30/2022 16:34:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/30/2022 16:34:43 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.2781870711448176 on epoch=168
05/30/2022 16:34:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/30/2022 16:34:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=169
05/30/2022 16:34:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
05/30/2022 16:35:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=171
05/30/2022 16:35:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=171
05/30/2022 16:35:17 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.42467838120012025 on epoch=171
05/30/2022 16:35:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
05/30/2022 16:35:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=173
05/30/2022 16:35:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/30/2022 16:35:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
05/30/2022 16:35:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=174
05/30/2022 16:35:51 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.4241417497231451 on epoch=174
05/30/2022 16:35:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
05/30/2022 16:36:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.18 on epoch=176
05/30/2022 16:36:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=176
05/30/2022 16:36:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=177
05/30/2022 16:36:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=178
05/30/2022 16:36:25 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.2781870711448176 on epoch=178
05/30/2022 16:36:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=178
05/30/2022 16:36:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/30/2022 16:36:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.20 on epoch=179
05/30/2022 16:36:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=180
05/30/2022 16:36:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=181
05/30/2022 16:36:58 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.22664816569649382 on epoch=181
05/30/2022 16:37:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=181
05/30/2022 16:37:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=182
05/30/2022 16:37:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
05/30/2022 16:37:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/30/2022 16:37:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=184
05/30/2022 16:37:32 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.46587438033082407 on epoch=184
05/30/2022 16:37:32 - INFO - __main__ - Saving model with best Classification-F1: 0.44243796196258 -> 0.46587438033082407 on epoch=184, global_step=2950
05/30/2022 16:37:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=184
05/30/2022 16:37:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/30/2022 16:37:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
05/30/2022 16:37:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=186
05/30/2022 16:37:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=187
05/30/2022 16:37:56 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:37:56 - INFO - __main__ - Printing 3 examples
05/30/2022 16:37:56 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 16:37:56 - INFO - __main__ - ['entailed']
05/30/2022 16:37:56 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 16:37:56 - INFO - __main__ - ['entailed']
05/30/2022 16:37:56 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 16:37:56 - INFO - __main__ - ['entailed']
05/30/2022 16:37:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:37:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:37:57 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 16:37:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:37:57 - INFO - __main__ - Printing 3 examples
05/30/2022 16:37:57 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 16:37:57 - INFO - __main__ - ['entailed']
05/30/2022 16:37:57 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 16:37:57 - INFO - __main__ - ['entailed']
05/30/2022 16:37:57 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 16:37:57 - INFO - __main__ - ['entailed']
05/30/2022 16:37:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:37:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:37:58 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 16:38:06 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.4729460207167213 on epoch=187
05/30/2022 16:38:07 - INFO - __main__ - Saving model with best Classification-F1: 0.46587438033082407 -> 0.4729460207167213 on epoch=187, global_step=3000
05/30/2022 16:38:07 - INFO - __main__ - save last model!
05/30/2022 16:38:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 16:38:07 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 16:38:07 - INFO - __main__ - Printing 3 examples
05/30/2022 16:38:07 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 16:38:07 - INFO - __main__ - ['entailed']
05/30/2022 16:38:07 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 16:38:07 - INFO - __main__ - ['entailed']
05/30/2022 16:38:07 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 16:38:07 - INFO - __main__ - ['entailed']
05/30/2022 16:38:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:38:14 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 16:38:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 16:38:15 - INFO - __main__ - Starting training!
05/30/2022 16:38:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:38:43 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 16:48:04 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_21_0.3_8_predictions.txt
05/30/2022 16:48:04 - INFO - __main__ - Classification-F1 on test data: 0.1025
05/30/2022 16:48:05 - INFO - __main__ - prefix=tab_fact_128_21, lr=0.3, bsz=8, dev_performance=0.4729460207167213, test_performance=0.10252689369687366
05/30/2022 16:48:05 - INFO - __main__ - Running ... prefix=tab_fact_128_21, lr=0.2, bsz=8 ...
05/30/2022 16:48:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:48:06 - INFO - __main__ - Printing 3 examples
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:48:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:48:06 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 16:48:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 16:48:06 - INFO - __main__ - Printing 3 examples
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: the european race walk cup in 2003 be 2nd position [SEP] table_caption: alessandro gandellini [SEP] table_text: year#competition#venue#position#notes [n] 1997#world championships#athens , greece#12th#20 km [n] 1997#world race walking cup#podebrady , czech republic#17th#20 km [n] 1998#european championships#budapest , hungary#12th#20 km [n] 1999#world championships#seville , spain#5th#20 km [n] 1999#world race walking cup#mézidon - canon , france#8th#20 km [n] 2000#olympic games#sydney , australia#9th#20 km [n] 2001#world championships#edmonton , canada#12th#20 km [n] 2001#mediterranean games#radès , tunisia#3rd#20 km [n] 2002#european championships#munich , germany#7th#20 km [n] 2002#world race walking cup#turin , italy#7th#20 km [n] 2003#european race walking cup#cheboksary , russia#2nd#20 km [n] 2003#world championships#paris , france#21st#20 km [n] 2004#world race walking cup#naumburg , germany#13th#20 km [n] 2004#olympic games#athens , greece#dnf#20 km [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: junction oval venue record a lower crowd participation than that of the glenferrie oval venue [SEP] table_caption: 1928 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#12.12 (84)#melbourne#17.16 (118)#brunswick street oval#17000#5 may 1928 [n] essendon#12.13 (85)#south melbourne#5.11 (41)#windy hill#22000#5 may 1928 [n] st kilda#11.11 (77)#north melbourne#10.15 (75)#junction oval#12000#5 may 1928 [n] geelong#10.17 (77)#footscray#12.9 (81)#corio oval#12500#5 may 1928 [n] richmond#5.14 (44)#collingwood#5.12 (42)#punt road oval#36000#5 may 1928 [n] hawthorn#7.17 (59)#carlton#14.9 (93)#glenferrie oval#14000#5 may 1928 [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ -  [tab_fact] statement: the specific orbital energy of - 29.8 mj / kg have a low earth orbit [SEP] table_caption: orbital period [SEP] table_text: orbit#center - to - center distance#altitude above the earth 's surface#orbital period#specific orbital energy [n] earth 's surface (for comparison)#6400 km#0 km#85 minutes#62.6 mj / kg [n] low earth orbit#6600 to 8400 km#200 to 2000 km#89 to 128 min#29.8 mj / kg [n] molniya orbit#6900 to 46300 km#500 to 39900 km#11 h 58 min#4.7 mj / kg [n] geostationary#42000 km#35786 km#23 h 56 min#4.6 mj / kg [n] orbit of the moon#363000 to 406000 km#357000 to 399000 km#27.3 days#0.5 mj / kg [n] 
05/30/2022 16:48:06 - INFO - __main__ - ['entailed']
05/30/2022 16:48:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 16:48:07 - INFO - __main__ - Tokenizing Output ...
05/30/2022 16:48:07 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 16:48:22 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 16:48:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 16:48:23 - INFO - __main__ - Starting training!
05/30/2022 16:48:28 - INFO - __main__ - Step 10 Global step 10 Train loss 5.59 on epoch=0
05/30/2022 16:48:33 - INFO - __main__ - Step 20 Global step 20 Train loss 3.80 on epoch=1
05/30/2022 16:48:37 - INFO - __main__ - Step 30 Global step 30 Train loss 3.12 on epoch=1
05/30/2022 16:48:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.71 on epoch=2
05/30/2022 16:48:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.28 on epoch=3
05/30/2022 16:48:57 - INFO - __main__ - Global step 50 Train loss 3.50 Classification-F1 0.0 on epoch=3
05/30/2022 16:48:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/30/2022 16:49:01 - INFO - __main__ - Step 60 Global step 60 Train loss 1.90 on epoch=3
05/30/2022 16:49:06 - INFO - __main__ - Step 70 Global step 70 Train loss 1.76 on epoch=4
05/30/2022 16:49:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.43 on epoch=4
05/30/2022 16:49:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.26 on epoch=5
05/30/2022 16:49:19 - INFO - __main__ - Step 100 Global step 100 Train loss 1.14 on epoch=6
05/30/2022 16:49:29 - INFO - __main__ - Global step 100 Train loss 1.50 Classification-F1 0.09848484848484848 on epoch=6
05/30/2022 16:49:29 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.09848484848484848 on epoch=6, global_step=100
05/30/2022 16:49:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=6
05/30/2022 16:49:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=7
05/30/2022 16:49:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=8
05/30/2022 16:49:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/30/2022 16:49:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
05/30/2022 16:50:02 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 16:50:02 - INFO - __main__ - Saving model with best Classification-F1: 0.09848484848484848 -> 0.3333333333333333 on epoch=9, global_step=150
05/30/2022 16:50:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=9
05/30/2022 16:50:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.34 on epoch=10
05/30/2022 16:50:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=11
05/30/2022 16:50:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=11
05/30/2022 16:50:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=12
05/30/2022 16:50:34 - INFO - __main__ - Global step 200 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 16:50:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=13
05/30/2022 16:50:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=13
05/30/2022 16:50:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=14
05/30/2022 16:50:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=14
05/30/2022 16:50:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=15
05/30/2022 16:51:06 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 16:51:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=16
05/30/2022 16:51:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=16
05/30/2022 16:51:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=17
05/30/2022 16:51:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=18
05/30/2022 16:51:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=18
05/30/2022 16:51:39 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 16:51:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=19
05/30/2022 16:51:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=19
05/30/2022 16:51:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=20
05/30/2022 16:51:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=21
05/30/2022 16:52:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=21
05/30/2022 16:52:13 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 16:52:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=22
05/30/2022 16:52:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=23
05/30/2022 16:52:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=23
05/30/2022 16:52:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=24
05/30/2022 16:52:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=24
05/30/2022 16:52:46 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 16:52:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=25
05/30/2022 16:52:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=26
05/30/2022 16:53:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=26
05/30/2022 16:53:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=27
05/30/2022 16:53:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 16:53:20 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 16:53:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=28
05/30/2022 16:53:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=29
05/30/2022 16:53:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 16:53:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=30
05/30/2022 16:53:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=31
05/30/2022 16:53:53 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 16:53:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=31
05/30/2022 16:54:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=32
05/30/2022 16:54:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=33
05/30/2022 16:54:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=33
05/30/2022 16:54:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=34
05/30/2022 16:54:27 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 16:54:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=34
05/30/2022 16:54:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=35
05/30/2022 16:54:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=36
05/30/2022 16:54:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=36
05/30/2022 16:54:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=37
05/30/2022 16:55:01 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 16:55:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=38
05/30/2022 16:55:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=38
05/30/2022 16:55:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=39
05/30/2022 16:55:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=39
05/30/2022 16:55:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=40
05/30/2022 16:55:35 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 16:55:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=41
05/30/2022 16:55:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=41
05/30/2022 16:55:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=42
05/30/2022 16:55:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 16:55:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=43
05/30/2022 16:56:09 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 16:56:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=44
05/30/2022 16:56:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=44
05/30/2022 16:56:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 16:56:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=46
05/30/2022 16:56:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 16:56:42 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 16:56:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/30/2022 16:56:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/30/2022 16:56:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=48
05/30/2022 16:57:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=49
05/30/2022 16:57:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=49
05/30/2022 16:57:16 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 16:57:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=50
05/30/2022 16:57:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/30/2022 16:57:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=51
05/30/2022 16:57:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=52
05/30/2022 16:57:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=53
05/30/2022 16:57:50 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 16:57:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=53
05/30/2022 16:57:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/30/2022 16:58:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=54
05/30/2022 16:58:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/30/2022 16:58:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 16:58:24 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 16:58:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=56
05/30/2022 16:58:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/30/2022 16:58:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/30/2022 16:58:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
05/30/2022 16:58:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=59
05/30/2022 16:58:57 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 16:59:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/30/2022 16:59:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=60
05/30/2022 16:59:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=61
05/30/2022 16:59:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/30/2022 16:59:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/30/2022 16:59:31 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 16:59:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=63
05/30/2022 16:59:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=63
05/30/2022 16:59:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/30/2022 16:59:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=64
05/30/2022 16:59:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=65
05/30/2022 17:00:05 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 17:00:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/30/2022 17:00:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/30/2022 17:00:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
05/30/2022 17:00:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=68
05/30/2022 17:00:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 17:00:39 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 17:00:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=69
05/30/2022 17:00:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/30/2022 17:00:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=70
05/30/2022 17:00:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 17:01:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/30/2022 17:01:12 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 17:01:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=72
05/30/2022 17:01:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
05/30/2022 17:01:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=73
05/30/2022 17:01:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/30/2022 17:01:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=74
05/30/2022 17:01:46 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 17:01:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 17:01:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 17:01:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=76
05/30/2022 17:02:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=77
05/30/2022 17:02:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 17:02:20 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 17:02:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=78
05/30/2022 17:02:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 17:02:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 17:02:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/30/2022 17:02:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/30/2022 17:02:54 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 17:02:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=81
05/30/2022 17:03:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 17:03:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=83
05/30/2022 17:03:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=83
05/30/2022 17:03:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/30/2022 17:03:27 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 17:03:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=84
05/30/2022 17:03:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
05/30/2022 17:03:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/30/2022 17:03:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/30/2022 17:03:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
05/30/2022 17:04:01 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 17:04:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/30/2022 17:04:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
05/30/2022 17:04:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=89
05/30/2022 17:04:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 17:04:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=90
05/30/2022 17:04:35 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 17:04:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 17:04:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=91
05/30/2022 17:04:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=92
05/30/2022 17:04:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/30/2022 17:04:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
05/30/2022 17:05:09 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 17:05:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
05/30/2022 17:05:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=94
05/30/2022 17:05:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/30/2022 17:05:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/30/2022 17:05:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=96
05/30/2022 17:05:43 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 17:05:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/30/2022 17:05:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=98
05/30/2022 17:05:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=98
05/30/2022 17:06:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=99
05/30/2022 17:06:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
05/30/2022 17:06:17 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 17:06:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/30/2022 17:06:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/30/2022 17:06:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=101
05/30/2022 17:06:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/30/2022 17:06:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/30/2022 17:06:51 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 17:06:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=103
05/30/2022 17:07:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/30/2022 17:07:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
05/30/2022 17:07:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=105
05/30/2022 17:07:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=106
05/30/2022 17:07:25 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 17:07:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/30/2022 17:07:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
05/30/2022 17:07:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 17:07:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/30/2022 17:07:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=109
05/30/2022 17:07:59 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 17:08:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/30/2022 17:08:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=110
05/30/2022 17:08:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 17:08:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=111
05/30/2022 17:08:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/30/2022 17:08:33 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=112
05/30/2022 17:08:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3401530406766009 on epoch=112, global_step=1800
05/30/2022 17:08:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
05/30/2022 17:08:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=113
05/30/2022 17:08:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/30/2022 17:08:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 17:08:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=115
05/30/2022 17:09:07 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=115
05/30/2022 17:09:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=116
05/30/2022 17:09:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/30/2022 17:09:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
05/30/2022 17:09:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=118
05/30/2022 17:09:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=118
05/30/2022 17:09:41 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=118
05/30/2022 17:09:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/30/2022 17:09:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 17:09:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=120
05/30/2022 17:09:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=121
05/30/2022 17:10:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/30/2022 17:10:15 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=121
05/30/2022 17:10:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
05/30/2022 17:10:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
05/30/2022 17:10:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=123
05/30/2022 17:10:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=124
05/30/2022 17:10:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
05/30/2022 17:10:48 - INFO - __main__ - Global step 2000 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=124
05/30/2022 17:10:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=125
05/30/2022 17:10:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=126
05/30/2022 17:11:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=126
05/30/2022 17:11:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
05/30/2022 17:11:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
05/30/2022 17:11:23 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=128
05/30/2022 17:11:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/30/2022 17:11:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=129
05/30/2022 17:11:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=129
05/30/2022 17:11:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/30/2022 17:11:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 17:11:57 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=131
05/30/2022 17:12:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=131
05/30/2022 17:12:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
05/30/2022 17:12:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=133
05/30/2022 17:12:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/30/2022 17:12:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=134
05/30/2022 17:12:31 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.3383422492035824 on epoch=134
05/30/2022 17:12:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=134
05/30/2022 17:12:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=135
05/30/2022 17:12:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/30/2022 17:12:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=136
05/30/2022 17:12:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/30/2022 17:13:05 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.3383422492035824 on epoch=137
05/30/2022 17:13:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=138
05/30/2022 17:13:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/30/2022 17:13:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=139
05/30/2022 17:13:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=139
05/30/2022 17:13:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
05/30/2022 17:13:39 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.34673046251993617 on epoch=140
05/30/2022 17:13:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3401530406766009 -> 0.34673046251993617 on epoch=140, global_step=2250
05/30/2022 17:13:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/30/2022 17:13:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=141
05/30/2022 17:13:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=142
05/30/2022 17:13:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/30/2022 17:14:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
05/30/2022 17:14:13 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.34195559333697656 on epoch=143
05/30/2022 17:14:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/30/2022 17:14:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=144
05/30/2022 17:14:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
05/30/2022 17:14:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=146
05/30/2022 17:14:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
05/30/2022 17:14:47 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=146
05/30/2022 17:14:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=147
05/30/2022 17:14:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/30/2022 17:15:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.24 on epoch=148
05/30/2022 17:15:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
05/30/2022 17:15:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/30/2022 17:15:21 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=149
05/30/2022 17:15:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/30/2022 17:15:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/30/2022 17:15:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
05/30/2022 17:15:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/30/2022 17:15:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/30/2022 17:15:55 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.34673046251993617 on epoch=153
05/30/2022 17:15:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=153
05/30/2022 17:16:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=154
05/30/2022 17:16:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/30/2022 17:16:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/30/2022 17:16:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=156
05/30/2022 17:16:28 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.34195559333697656 on epoch=156
05/30/2022 17:16:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/30/2022 17:16:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
05/30/2022 17:16:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
05/30/2022 17:16:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=158
05/30/2022 17:16:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=159
05/30/2022 17:17:02 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.34485289741504155 on epoch=159
05/30/2022 17:17:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/30/2022 17:17:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/30/2022 17:17:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=161
05/30/2022 17:17:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/30/2022 17:17:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
05/30/2022 17:17:36 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3589111276786909 on epoch=162
05/30/2022 17:17:36 - INFO - __main__ - Saving model with best Classification-F1: 0.34673046251993617 -> 0.3589111276786909 on epoch=162, global_step=2600
05/30/2022 17:17:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/30/2022 17:17:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=163
05/30/2022 17:17:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=164
05/30/2022 17:17:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=164
05/30/2022 17:17:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=165
05/30/2022 17:18:10 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.3547482327970133 on epoch=165
05/30/2022 17:18:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=166
05/30/2022 17:18:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=166
05/30/2022 17:18:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/30/2022 17:18:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=168
05/30/2022 17:18:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=168
05/30/2022 17:18:44 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.3547482327970133 on epoch=168
05/30/2022 17:18:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=169
05/30/2022 17:18:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=169
05/30/2022 17:18:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=170
05/30/2022 17:19:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=171
05/30/2022 17:19:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=171
05/30/2022 17:19:18 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.3547482327970133 on epoch=171
05/30/2022 17:19:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=172
05/30/2022 17:19:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/30/2022 17:19:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/30/2022 17:19:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
05/30/2022 17:19:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=174
05/30/2022 17:19:52 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.3547482327970133 on epoch=174
05/30/2022 17:19:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=175
05/30/2022 17:20:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.18 on epoch=176
05/30/2022 17:20:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=176
05/30/2022 17:20:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=177
05/30/2022 17:20:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=178
05/30/2022 17:20:26 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.3511520737327189 on epoch=178
05/30/2022 17:20:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
05/30/2022 17:20:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=179
05/30/2022 17:20:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/30/2022 17:20:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=180
05/30/2022 17:20:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=181
05/30/2022 17:21:00 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.36657784545108485 on epoch=181
05/30/2022 17:21:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3589111276786909 -> 0.36657784545108485 on epoch=181, global_step=2900
05/30/2022 17:21:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.21 on epoch=181
05/30/2022 17:21:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=182
05/30/2022 17:21:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
05/30/2022 17:21:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=183
05/30/2022 17:21:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=184
05/30/2022 17:21:34 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.36231884057971014 on epoch=184
05/30/2022 17:21:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=184
05/30/2022 17:21:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/30/2022 17:21:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
05/30/2022 17:21:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
05/30/2022 17:21:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=187
05/30/2022 17:21:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 17:21:57 - INFO - __main__ - Printing 3 examples
05/30/2022 17:21:57 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 17:21:57 - INFO - __main__ - ['refuted']
05/30/2022 17:21:57 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 17:21:57 - INFO - __main__ - ['refuted']
05/30/2022 17:21:57 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 17:21:57 - INFO - __main__ - ['refuted']
05/30/2022 17:21:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 17:21:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 17:21:58 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 17:21:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 17:21:58 - INFO - __main__ - Printing 3 examples
05/30/2022 17:21:58 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 17:21:58 - INFO - __main__ - ['refuted']
05/30/2022 17:21:58 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 17:21:58 - INFO - __main__ - ['refuted']
05/30/2022 17:21:58 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 17:21:58 - INFO - __main__ - ['refuted']
05/30/2022 17:21:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 17:21:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 17:21:59 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 17:22:07 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.3568328892272554 on epoch=187
05/30/2022 17:22:07 - INFO - __main__ - save last model!
05/30/2022 17:22:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 17:22:07 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 17:22:07 - INFO - __main__ - Printing 3 examples
05/30/2022 17:22:07 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 17:22:07 - INFO - __main__ - ['entailed']
05/30/2022 17:22:07 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 17:22:07 - INFO - __main__ - ['entailed']
05/30/2022 17:22:07 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 17:22:07 - INFO - __main__ - ['entailed']
05/30/2022 17:22:07 - INFO - __main__ - Tokenizing Input ...
05/30/2022 17:22:17 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 17:22:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 17:22:18 - INFO - __main__ - Starting training!
05/30/2022 17:22:32 - INFO - __main__ - Tokenizing Output ...
05/30/2022 17:22:45 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 17:32:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_21_0.2_8_predictions.txt
05/30/2022 17:32:03 - INFO - __main__ - Classification-F1 on test data: 0.3677
05/30/2022 17:32:04 - INFO - __main__ - prefix=tab_fact_128_21, lr=0.2, bsz=8, dev_performance=0.36657784545108485, test_performance=0.3676807934986121
05/30/2022 17:32:04 - INFO - __main__ - Running ... prefix=tab_fact_128_42, lr=0.5, bsz=8 ...
05/30/2022 17:32:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 17:32:05 - INFO - __main__ - Printing 3 examples
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 17:32:05 - INFO - __main__ - Tokenizing Output ...
05/30/2022 17:32:05 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 17:32:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 17:32:05 - INFO - __main__ - Printing 3 examples
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 17:32:05 - INFO - __main__ - ['refuted']
05/30/2022 17:32:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 17:32:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 17:32:06 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 17:32:25 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 17:32:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 17:32:26 - INFO - __main__ - Starting training!
05/30/2022 17:32:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.80 on epoch=0
05/30/2022 17:32:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.56 on epoch=1
05/30/2022 17:32:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.72 on epoch=1
05/30/2022 17:32:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.20 on epoch=2
05/30/2022 17:32:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.61 on epoch=3
05/30/2022 17:32:59 - INFO - __main__ - Global step 50 Train loss 2.18 Classification-F1 0.36119461636703015 on epoch=3
05/30/2022 17:32:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36119461636703015 on epoch=3, global_step=50
05/30/2022 17:33:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
05/30/2022 17:33:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=4
05/30/2022 17:33:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.34 on epoch=4
05/30/2022 17:33:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=5
05/30/2022 17:33:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=6
05/30/2022 17:33:31 - INFO - __main__ - Global step 100 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 17:33:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=6
05/30/2022 17:33:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=7
05/30/2022 17:33:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=8
05/30/2022 17:33:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=8
05/30/2022 17:33:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=9
05/30/2022 17:34:05 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 17:34:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=9
05/30/2022 17:34:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=10
05/30/2022 17:34:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=11
05/30/2022 17:34:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=11
05/30/2022 17:34:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=12
05/30/2022 17:34:39 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 17:34:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=13
05/30/2022 17:34:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=13
05/30/2022 17:34:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=14
05/30/2022 17:34:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=14
05/30/2022 17:35:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=15
05/30/2022 17:35:13 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 17:35:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=16
05/30/2022 17:35:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=16
05/30/2022 17:35:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=17
05/30/2022 17:35:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=18
05/30/2022 17:35:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=18
05/30/2022 17:35:46 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 17:35:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=19
05/30/2022 17:35:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=19
05/30/2022 17:36:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=20
05/30/2022 17:36:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=21
05/30/2022 17:36:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=21
05/30/2022 17:36:20 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 17:36:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=22
05/30/2022 17:36:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=23
05/30/2022 17:36:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=23
05/30/2022 17:36:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=24
05/30/2022 17:36:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=24
05/30/2022 17:36:54 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 17:36:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=25
05/30/2022 17:37:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=26
05/30/2022 17:37:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=26
05/30/2022 17:37:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=27
05/30/2022 17:37:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 17:37:27 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 17:37:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=28
05/30/2022 17:37:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=29
05/30/2022 17:37:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=29
05/30/2022 17:37:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=30
05/30/2022 17:37:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=31
05/30/2022 17:38:01 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 17:38:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=31
05/30/2022 17:38:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=32
05/30/2022 17:38:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=33
05/30/2022 17:38:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=33
05/30/2022 17:38:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=34
05/30/2022 17:38:34 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 17:38:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=34
05/30/2022 17:38:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=35
05/30/2022 17:38:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=36
05/30/2022 17:38:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=36
05/30/2022 17:38:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=37
05/30/2022 17:39:08 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 17:39:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=38
05/30/2022 17:39:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=38
05/30/2022 17:39:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=39
05/30/2022 17:39:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=39
05/30/2022 17:39:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=40
05/30/2022 17:39:42 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 17:39:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=41
05/30/2022 17:39:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=41
05/30/2022 17:39:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 17:39:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 17:40:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=43
05/30/2022 17:40:16 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.34195559333697656 on epoch=43
05/30/2022 17:40:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=44
05/30/2022 17:40:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=44
05/30/2022 17:40:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=45
05/30/2022 17:40:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=46
05/30/2022 17:40:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=46
05/30/2022 17:40:50 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 17:40:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/30/2022 17:40:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=48
05/30/2022 17:41:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=48
05/30/2022 17:41:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=49
05/30/2022 17:41:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=49
05/30/2022 17:41:24 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 17:41:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=50
05/30/2022 17:41:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 17:41:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 17:41:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=52
05/30/2022 17:41:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/30/2022 17:41:58 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.36516753625488524 on epoch=53
05/30/2022 17:41:58 - INFO - __main__ - Saving model with best Classification-F1: 0.36119461636703015 -> 0.36516753625488524 on epoch=53, global_step=850
05/30/2022 17:42:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/30/2022 17:42:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/30/2022 17:42:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/30/2022 17:42:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=55
05/30/2022 17:42:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/30/2022 17:42:32 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.36516753625488524 on epoch=56
05/30/2022 17:42:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/30/2022 17:42:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=57
05/30/2022 17:42:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=58
05/30/2022 17:42:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/30/2022 17:42:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 17:43:06 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 17:43:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=59
05/30/2022 17:43:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/30/2022 17:43:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 17:43:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/30/2022 17:43:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=62
05/30/2022 17:43:40 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.3383422492035824 on epoch=62
05/30/2022 17:43:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/30/2022 17:43:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=63
05/30/2022 17:43:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=64
05/30/2022 17:43:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=64
05/30/2022 17:44:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=65
05/30/2022 17:44:13 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.35501021683496337 on epoch=65
05/30/2022 17:44:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=66
05/30/2022 17:44:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=66
05/30/2022 17:44:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=67
05/30/2022 17:44:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/30/2022 17:44:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/30/2022 17:44:47 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=68
05/30/2022 17:44:47 - INFO - __main__ - Saving model with best Classification-F1: 0.36516753625488524 -> 0.37922403003754696 on epoch=68, global_step=1100
05/30/2022 17:44:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/30/2022 17:44:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=69
05/30/2022 17:45:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/30/2022 17:45:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 17:45:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=71
05/30/2022 17:45:21 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.3383422492035824 on epoch=71
05/30/2022 17:45:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 17:45:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 17:45:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/30/2022 17:45:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=74
05/30/2022 17:45:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/30/2022 17:45:55 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.350463149416029 on epoch=74
05/30/2022 17:45:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 17:46:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=76
05/30/2022 17:46:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=76
05/30/2022 17:46:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=77
05/30/2022 17:46:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=78
05/30/2022 17:46:29 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.4174085604529648 on epoch=78
05/30/2022 17:46:29 - INFO - __main__ - Saving model with best Classification-F1: 0.37922403003754696 -> 0.4174085604529648 on epoch=78, global_step=1250
05/30/2022 17:46:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/30/2022 17:46:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 17:46:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=79
05/30/2022 17:46:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/30/2022 17:46:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
05/30/2022 17:47:02 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3692115143929912 on epoch=81
05/30/2022 17:47:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/30/2022 17:47:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/30/2022 17:47:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
05/30/2022 17:47:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/30/2022 17:47:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=84
05/30/2022 17:47:36 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.2520134795789583 on epoch=84
05/30/2022 17:47:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/30/2022 17:47:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=85
05/30/2022 17:47:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/30/2022 17:47:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=86
05/30/2022 17:47:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=87
05/30/2022 17:48:10 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.3692115143929912 on epoch=87
05/30/2022 17:48:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/30/2022 17:48:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=88
05/30/2022 17:48:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/30/2022 17:48:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
05/30/2022 17:48:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/30/2022 17:48:43 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.36318407960199 on epoch=90
05/30/2022 17:48:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 17:48:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/30/2022 17:48:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/30/2022 17:49:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/30/2022 17:49:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/30/2022 17:49:17 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.4744596063277382 on epoch=93
05/30/2022 17:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4174085604529648 -> 0.4744596063277382 on epoch=93, global_step=1500
05/30/2022 17:49:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
05/30/2022 17:49:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 17:49:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
05/30/2022 17:49:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/30/2022 17:49:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
05/30/2022 17:49:51 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.38279939051439815 on epoch=96
05/30/2022 17:49:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/30/2022 17:50:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=98
05/30/2022 17:50:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/30/2022 17:50:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=99
05/30/2022 17:50:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
05/30/2022 17:50:25 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.3671630170316302 on epoch=99
05/30/2022 17:50:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=100
05/30/2022 17:50:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/30/2022 17:50:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=101
05/30/2022 17:50:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
05/30/2022 17:50:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/30/2022 17:50:59 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.4035970552604604 on epoch=103
05/30/2022 17:51:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/30/2022 17:51:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/30/2022 17:51:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/30/2022 17:51:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 17:51:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=106
05/30/2022 17:51:33 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.39889437387912063 on epoch=106
05/30/2022 17:51:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/30/2022 17:51:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/30/2022 17:51:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=108
05/30/2022 17:51:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/30/2022 17:51:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/30/2022 17:52:07 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.41539594843462246 on epoch=109
05/30/2022 17:52:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=109
05/30/2022 17:52:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/30/2022 17:52:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/30/2022 17:52:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=111
05/30/2022 17:52:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
05/30/2022 17:52:40 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.3935603691701252 on epoch=112
05/30/2022 17:52:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/30/2022 17:52:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 17:52:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/30/2022 17:52:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=114
05/30/2022 17:53:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/30/2022 17:53:14 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.4595791805094131 on epoch=115
05/30/2022 17:53:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=116
05/30/2022 17:53:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
05/30/2022 17:53:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/30/2022 17:53:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/30/2022 17:53:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=118
05/30/2022 17:53:47 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.21284906327324146 on epoch=118
05/30/2022 17:53:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
05/30/2022 17:53:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=119
05/30/2022 17:54:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/30/2022 17:54:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
05/30/2022 17:54:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/30/2022 17:54:21 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.20986889195370195 on epoch=121
05/30/2022 17:54:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/30/2022 17:54:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
05/30/2022 17:54:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
05/30/2022 17:54:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/30/2022 17:54:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=124
05/30/2022 17:54:55 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.21383758635274214 on epoch=124
05/30/2022 17:54:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=125
05/30/2022 17:55:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=126
05/30/2022 17:55:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=126
05/30/2022 17:55:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
05/30/2022 17:55:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
05/30/2022 17:55:28 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.2015126817500163 on epoch=128
05/30/2022 17:55:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/30/2022 17:55:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=129
05/30/2022 17:55:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=129
05/30/2022 17:55:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=130
05/30/2022 17:55:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=131
05/30/2022 17:56:02 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.24518738076844243 on epoch=131
05/30/2022 17:56:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=131
05/30/2022 17:56:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=132
05/30/2022 17:56:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=133
05/30/2022 17:56:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/30/2022 17:56:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/30/2022 17:56:36 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.26804476371988467 on epoch=134
05/30/2022 17:56:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 17:56:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/30/2022 17:56:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/30/2022 17:56:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/30/2022 17:56:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/30/2022 17:57:10 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.3170382399043721 on epoch=137
05/30/2022 17:57:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=138
05/30/2022 17:57:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/30/2022 17:57:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/30/2022 17:57:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=139
05/30/2022 17:57:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/30/2022 17:57:44 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.4479617713526662 on epoch=140
05/30/2022 17:57:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/30/2022 17:57:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
05/30/2022 17:57:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
05/30/2022 17:58:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/30/2022 17:58:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
05/30/2022 17:58:17 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.13299860064243235 on epoch=143
05/30/2022 17:58:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/30/2022 17:58:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=144
05/30/2022 17:58:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=145
05/30/2022 17:58:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=146
05/30/2022 17:58:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/30/2022 17:58:51 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.11277074011050188 on epoch=146
05/30/2022 17:58:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
05/30/2022 17:59:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/30/2022 17:59:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/30/2022 17:59:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=149
05/30/2022 17:59:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
05/30/2022 17:59:25 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.11388442731198915 on epoch=149
05/30/2022 17:59:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/30/2022 17:59:34 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
05/30/2022 17:59:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
05/30/2022 17:59:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=152
05/30/2022 17:59:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=153
05/30/2022 17:59:59 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.14752567693744162 on epoch=153
05/30/2022 18:00:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/30/2022 18:00:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=154
05/30/2022 18:00:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/30/2022 18:00:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
05/30/2022 18:00:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=156
05/30/2022 18:00:32 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.16801755102468244 on epoch=156
05/30/2022 18:00:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/30/2022 18:00:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=157
05/30/2022 18:00:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
05/30/2022 18:00:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/30/2022 18:00:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=159
05/30/2022 18:01:06 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.2537216828478964 on epoch=159
05/30/2022 18:01:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/30/2022 18:01:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/30/2022 18:01:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=161
05/30/2022 18:01:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=161
05/30/2022 18:01:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
05/30/2022 18:01:40 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.12551154101224465 on epoch=162
05/30/2022 18:01:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/30/2022 18:01:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=163
05/30/2022 18:01:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=164
05/30/2022 18:01:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/30/2022 18:02:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/30/2022 18:02:14 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.14116355244547013 on epoch=165
05/30/2022 18:02:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=166
05/30/2022 18:02:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/30/2022 18:02:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
05/30/2022 18:02:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
05/30/2022 18:02:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=168
05/30/2022 18:02:48 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.10198730158730158 on epoch=168
05/30/2022 18:02:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
05/30/2022 18:02:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=169
05/30/2022 18:03:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=170
05/30/2022 18:03:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/30/2022 18:03:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/30/2022 18:03:21 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.19716763366398404 on epoch=171
05/30/2022 18:03:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=172
05/30/2022 18:03:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/30/2022 18:03:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/30/2022 18:03:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=174
05/30/2022 18:03:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=174
05/30/2022 18:03:55 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.11887891863620018 on epoch=174
05/30/2022 18:03:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=175
05/30/2022 18:04:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
05/30/2022 18:04:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/30/2022 18:04:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/30/2022 18:04:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=178
05/30/2022 18:04:29 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.13101948339017305 on epoch=178
05/30/2022 18:04:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=178
05/30/2022 18:04:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/30/2022 18:04:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
05/30/2022 18:04:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=180
05/30/2022 18:04:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/30/2022 18:05:03 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.11531069827033952 on epoch=181
05/30/2022 18:05:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=181
05/30/2022 18:05:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/30/2022 18:05:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/30/2022 18:05:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
05/30/2022 18:05:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
05/30/2022 18:05:36 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.10745901380362682 on epoch=184
05/30/2022 18:05:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/30/2022 18:05:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
05/30/2022 18:05:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/30/2022 18:05:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/30/2022 18:05:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
05/30/2022 18:06:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:06:00 - INFO - __main__ - Printing 3 examples
05/30/2022 18:06:00 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 18:06:00 - INFO - __main__ - ['refuted']
05/30/2022 18:06:00 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 18:06:00 - INFO - __main__ - ['refuted']
05/30/2022 18:06:00 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 18:06:00 - INFO - __main__ - ['refuted']
05/30/2022 18:06:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:06:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:06:01 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 18:06:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:06:01 - INFO - __main__ - Printing 3 examples
05/30/2022 18:06:01 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 18:06:01 - INFO - __main__ - ['refuted']
05/30/2022 18:06:01 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 18:06:01 - INFO - __main__ - ['refuted']
05/30/2022 18:06:01 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 18:06:01 - INFO - __main__ - ['refuted']
05/30/2022 18:06:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:06:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:06:02 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 18:06:10 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.09680063795853269 on epoch=187
05/30/2022 18:06:10 - INFO - __main__ - save last model!
05/30/2022 18:06:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 18:06:11 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 18:06:11 - INFO - __main__ - Printing 3 examples
05/30/2022 18:06:11 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:06:11 - INFO - __main__ - ['entailed']
05/30/2022 18:06:11 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:06:11 - INFO - __main__ - ['entailed']
05/30/2022 18:06:11 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:06:11 - INFO - __main__ - ['entailed']
05/30/2022 18:06:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:06:18 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 18:06:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 18:06:19 - INFO - __main__ - Starting training!
05/30/2022 18:06:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:06:48 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 18:16:04 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_42_0.5_8_predictions.txt
05/30/2022 18:16:04 - INFO - __main__ - Classification-F1 on test data: 0.0075
05/30/2022 18:16:04 - INFO - __main__ - prefix=tab_fact_128_42, lr=0.5, bsz=8, dev_performance=0.4744596063277382, test_performance=0.007491832633359543
05/30/2022 18:16:04 - INFO - __main__ - Running ... prefix=tab_fact_128_42, lr=0.4, bsz=8 ...
05/30/2022 18:16:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:16:05 - INFO - __main__ - Printing 3 examples
05/30/2022 18:16:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 18:16:05 - INFO - __main__ - ['refuted']
05/30/2022 18:16:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 18:16:05 - INFO - __main__ - ['refuted']
05/30/2022 18:16:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 18:16:05 - INFO - __main__ - ['refuted']
05/30/2022 18:16:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:16:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:16:06 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 18:16:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:16:06 - INFO - __main__ - Printing 3 examples
05/30/2022 18:16:06 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 18:16:06 - INFO - __main__ - ['refuted']
05/30/2022 18:16:06 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 18:16:06 - INFO - __main__ - ['refuted']
05/30/2022 18:16:06 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 18:16:06 - INFO - __main__ - ['refuted']
05/30/2022 18:16:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:16:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:16:06 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 18:16:25 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 18:16:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 18:16:26 - INFO - __main__ - Starting training!
05/30/2022 18:16:31 - INFO - __main__ - Step 10 Global step 10 Train loss 5.14 on epoch=0
05/30/2022 18:16:36 - INFO - __main__ - Step 20 Global step 20 Train loss 3.12 on epoch=1
05/30/2022 18:16:40 - INFO - __main__ - Step 30 Global step 30 Train loss 2.17 on epoch=1
05/30/2022 18:16:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.59 on epoch=2
05/30/2022 18:16:49 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=3
05/30/2022 18:16:59 - INFO - __main__ - Global step 50 Train loss 2.63 Classification-F1 0.11152416356877323 on epoch=3
05/30/2022 18:16:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11152416356877323 on epoch=3, global_step=50
05/30/2022 18:17:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.79 on epoch=3
05/30/2022 18:17:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=4
05/30/2022 18:17:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
05/30/2022 18:17:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=5
05/30/2022 18:17:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=6
05/30/2022 18:17:32 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 18:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.11152416356877323 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 18:17:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=6
05/30/2022 18:17:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=7
05/30/2022 18:17:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=8
05/30/2022 18:17:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=8
05/30/2022 18:17:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=9
05/30/2022 18:18:06 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 18:18:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 18:18:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=10
05/30/2022 18:18:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=11
05/30/2022 18:18:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=11
05/30/2022 18:18:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=12
05/30/2022 18:18:39 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 18:18:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=13
05/30/2022 18:18:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=13
05/30/2022 18:18:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=14
05/30/2022 18:18:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=14
05/30/2022 18:19:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=15
05/30/2022 18:19:13 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 18:19:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=16
05/30/2022 18:19:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=16
05/30/2022 18:19:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=17
05/30/2022 18:19:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=18
05/30/2022 18:19:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=18
05/30/2022 18:19:47 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 18:19:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=19
05/30/2022 18:19:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=19
05/30/2022 18:20:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=20
05/30/2022 18:20:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=21
05/30/2022 18:20:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=21
05/30/2022 18:20:20 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 18:20:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=22
05/30/2022 18:20:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 18:20:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=23
05/30/2022 18:20:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=24
05/30/2022 18:20:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=24
05/30/2022 18:20:54 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 18:20:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=25
05/30/2022 18:21:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 18:21:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=26
05/30/2022 18:21:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=27
05/30/2022 18:21:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 18:21:27 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 18:21:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=28
05/30/2022 18:21:36 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=29
05/30/2022 18:21:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=29
05/30/2022 18:21:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=30
05/30/2022 18:21:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=31
05/30/2022 18:22:01 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 18:22:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=31
05/30/2022 18:22:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=32
05/30/2022 18:22:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=33
05/30/2022 18:22:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=33
05/30/2022 18:22:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=34
05/30/2022 18:22:35 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 18:22:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 18:22:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=35
05/30/2022 18:22:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/30/2022 18:22:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=36
05/30/2022 18:22:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=37
05/30/2022 18:23:08 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 18:23:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=38
05/30/2022 18:23:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=38
05/30/2022 18:23:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=39
05/30/2022 18:23:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=39
05/30/2022 18:23:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=40
05/30/2022 18:23:42 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 18:23:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=41
05/30/2022 18:23:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=41
05/30/2022 18:23:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=42
05/30/2022 18:24:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 18:24:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=43
05/30/2022 18:24:16 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 18:24:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 18:24:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=44
05/30/2022 18:24:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 18:24:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=46
05/30/2022 18:24:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=46
05/30/2022 18:24:50 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 18:24:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/30/2022 18:24:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/30/2022 18:25:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=48
05/30/2022 18:25:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=49
05/30/2022 18:25:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=49
05/30/2022 18:25:23 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 18:25:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=50
05/30/2022 18:25:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=51
05/30/2022 18:25:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/30/2022 18:25:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/30/2022 18:25:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/30/2022 18:25:57 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 18:26:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 18:26:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/30/2022 18:26:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=54
05/30/2022 18:26:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=55
05/30/2022 18:26:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 18:26:31 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 18:26:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/30/2022 18:26:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=57
05/30/2022 18:26:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/30/2022 18:26:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
05/30/2022 18:26:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=59
05/30/2022 18:27:05 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 18:27:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/30/2022 18:27:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/30/2022 18:27:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 18:27:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/30/2022 18:27:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 18:27:38 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 18:27:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/30/2022 18:27:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/30/2022 18:27:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 18:27:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=64
05/30/2022 18:28:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/30/2022 18:28:12 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.34195559333697656 on epoch=65
05/30/2022 18:28:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=65, global_step=1050
05/30/2022 18:28:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/30/2022 18:28:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=66
05/30/2022 18:28:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 18:28:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/30/2022 18:28:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=68
05/30/2022 18:28:46 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3732922688146569 on epoch=68
05/30/2022 18:28:46 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.3732922688146569 on epoch=68, global_step=1100
05/30/2022 18:28:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
05/30/2022 18:28:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=69
05/30/2022 18:28:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/30/2022 18:29:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=71
05/30/2022 18:29:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=71
05/30/2022 18:29:19 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.3732922688146569 on epoch=71
05/30/2022 18:29:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 18:29:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 18:29:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/30/2022 18:29:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=74
05/30/2022 18:29:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 18:29:53 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.36516753625488524 on epoch=74
05/30/2022 18:29:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=75
05/30/2022 18:30:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=76
05/30/2022 18:30:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/30/2022 18:30:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/30/2022 18:30:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=78
05/30/2022 18:30:27 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3712545436683367 on epoch=78
05/30/2022 18:30:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
05/30/2022 18:30:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
05/30/2022 18:30:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 18:30:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/30/2022 18:30:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=81
05/30/2022 18:31:00 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3692115143929912 on epoch=81
05/30/2022 18:31:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/30/2022 18:31:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/30/2022 18:31:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
05/30/2022 18:31:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/30/2022 18:31:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
05/30/2022 18:31:34 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.17741743410606542 on epoch=84
05/30/2022 18:31:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/30/2022 18:31:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/30/2022 18:31:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/30/2022 18:31:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/30/2022 18:31:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=87
05/30/2022 18:32:07 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.36516753625488524 on epoch=87
05/30/2022 18:32:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/30/2022 18:32:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=88
05/30/2022 18:32:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/30/2022 18:32:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 18:32:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=90
05/30/2022 18:32:41 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.3692115143929912 on epoch=90
05/30/2022 18:32:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
05/30/2022 18:32:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/30/2022 18:32:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/30/2022 18:32:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=93
05/30/2022 18:33:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/30/2022 18:33:15 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.38279939051439815 on epoch=93
05/30/2022 18:33:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3732922688146569 -> 0.38279939051439815 on epoch=93, global_step=1500
05/30/2022 18:33:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=94
05/30/2022 18:33:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
05/30/2022 18:33:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
05/30/2022 18:33:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/30/2022 18:33:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/30/2022 18:33:48 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.39581271412257324 on epoch=96
05/30/2022 18:33:48 - INFO - __main__ - Saving model with best Classification-F1: 0.38279939051439815 -> 0.39581271412257324 on epoch=96, global_step=1550
05/30/2022 18:33:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=97
05/30/2022 18:33:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=98
05/30/2022 18:34:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
05/30/2022 18:34:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/30/2022 18:34:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/30/2022 18:34:22 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.3692115143929912 on epoch=99
05/30/2022 18:34:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/30/2022 18:34:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/30/2022 18:34:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/30/2022 18:34:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
05/30/2022 18:34:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/30/2022 18:34:56 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.44014109347442687 on epoch=103
05/30/2022 18:34:56 - INFO - __main__ - Saving model with best Classification-F1: 0.39581271412257324 -> 0.44014109347442687 on epoch=103, global_step=1650
05/30/2022 18:35:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/30/2022 18:35:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/30/2022 18:35:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/30/2022 18:35:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=105
05/30/2022 18:35:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=106
05/30/2022 18:35:29 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.4059435586495995 on epoch=106
05/30/2022 18:35:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 18:35:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/30/2022 18:35:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/30/2022 18:35:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/30/2022 18:35:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/30/2022 18:36:03 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.3712545436683367 on epoch=109
05/30/2022 18:36:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=109
05/30/2022 18:36:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=110
05/30/2022 18:36:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 18:36:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=111
05/30/2022 18:36:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/30/2022 18:36:37 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.2492070859441479 on epoch=112
05/30/2022 18:36:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/30/2022 18:36:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/30/2022 18:36:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/30/2022 18:36:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
05/30/2022 18:36:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=115
05/30/2022 18:37:10 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.38064516129032255 on epoch=115
05/30/2022 18:37:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=116
05/30/2022 18:37:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
05/30/2022 18:37:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
05/30/2022 18:37:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=118
05/30/2022 18:37:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
05/30/2022 18:37:44 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.400749063670412 on epoch=118
05/30/2022 18:37:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
05/30/2022 18:37:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 18:37:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=120
05/30/2022 18:38:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
05/30/2022 18:38:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=121
05/30/2022 18:38:17 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.3816425120772947 on epoch=121
05/30/2022 18:38:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
05/30/2022 18:38:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
05/30/2022 18:38:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/30/2022 18:38:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/30/2022 18:38:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
05/30/2022 18:38:51 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3729241807626285 on epoch=124
05/30/2022 18:38:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/30/2022 18:39:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=126
05/30/2022 18:39:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=126
05/30/2022 18:39:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
05/30/2022 18:39:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=128
05/30/2022 18:39:25 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.22661484856606806 on epoch=128
05/30/2022 18:39:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 18:39:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=129
05/30/2022 18:39:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
05/30/2022 18:39:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/30/2022 18:39:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=131
05/30/2022 18:39:58 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.20544642857142856 on epoch=131
05/30/2022 18:40:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/30/2022 18:40:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
05/30/2022 18:40:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
05/30/2022 18:40:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/30/2022 18:40:20 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/30/2022 18:40:32 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.18137065637065636 on epoch=134
05/30/2022 18:40:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 18:40:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/30/2022 18:40:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/30/2022 18:40:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/30/2022 18:40:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/30/2022 18:41:05 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.20008820935019112 on epoch=137
05/30/2022 18:41:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=138
05/30/2022 18:41:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
05/30/2022 18:41:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/30/2022 18:41:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=139
05/30/2022 18:41:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/30/2022 18:41:39 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.13070194462517168 on epoch=140
05/30/2022 18:41:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/30/2022 18:41:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
05/30/2022 18:41:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=142
05/30/2022 18:41:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/30/2022 18:42:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/30/2022 18:42:13 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.14717425431711148 on epoch=143
05/30/2022 18:42:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=144
05/30/2022 18:42:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
05/30/2022 18:42:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=145
05/30/2022 18:42:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=146
05/30/2022 18:42:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
05/30/2022 18:42:46 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.1631890504704876 on epoch=146
05/30/2022 18:42:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=147
05/30/2022 18:42:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/30/2022 18:43:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/30/2022 18:43:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=149
05/30/2022 18:43:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/30/2022 18:43:20 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.1796833064949007 on epoch=149
05/30/2022 18:43:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
05/30/2022 18:43:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=151
05/30/2022 18:43:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
05/30/2022 18:43:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/30/2022 18:43:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
05/30/2022 18:43:54 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.08782585513428304 on epoch=153
05/30/2022 18:43:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=153
05/30/2022 18:44:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
05/30/2022 18:44:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/30/2022 18:44:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
05/30/2022 18:44:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=156
05/30/2022 18:44:28 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.1637731842877662 on epoch=156
05/30/2022 18:44:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/30/2022 18:44:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
05/30/2022 18:44:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/30/2022 18:44:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/30/2022 18:44:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=159
05/30/2022 18:45:01 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.13249074598992897 on epoch=159
05/30/2022 18:45:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/30/2022 18:45:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=160
05/30/2022 18:45:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=161
05/30/2022 18:45:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=161
05/30/2022 18:45:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/30/2022 18:45:35 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.1211408452787763 on epoch=162
05/30/2022 18:45:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/30/2022 18:45:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
05/30/2022 18:45:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/30/2022 18:45:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
05/30/2022 18:45:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=165
05/30/2022 18:46:09 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.13784661793711567 on epoch=165
05/30/2022 18:46:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=166
05/30/2022 18:46:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/30/2022 18:46:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
05/30/2022 18:46:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
05/30/2022 18:46:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/30/2022 18:46:43 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.1167992281626506 on epoch=168
05/30/2022 18:46:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
05/30/2022 18:46:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=169
05/30/2022 18:46:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
05/30/2022 18:47:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=171
05/30/2022 18:47:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=171
05/30/2022 18:47:16 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.16303291136579803 on epoch=171
05/30/2022 18:47:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=172
05/30/2022 18:47:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/30/2022 18:47:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/30/2022 18:47:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/30/2022 18:47:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=174
05/30/2022 18:47:50 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.12128859619606146 on epoch=174
05/30/2022 18:47:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
05/30/2022 18:47:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
05/30/2022 18:48:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/30/2022 18:48:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
05/30/2022 18:48:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
05/30/2022 18:48:24 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.08350815850815851 on epoch=178
05/30/2022 18:48:28 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
05/30/2022 18:48:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/30/2022 18:48:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
05/30/2022 18:48:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=180
05/30/2022 18:48:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
05/30/2022 18:48:58 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.07104546375801436 on epoch=181
05/30/2022 18:49:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
05/30/2022 18:49:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/30/2022 18:49:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=183
05/30/2022 18:49:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
05/30/2022 18:49:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=184
05/30/2022 18:49:32 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.12808551992225461 on epoch=184
05/30/2022 18:49:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=184
05/30/2022 18:49:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
05/30/2022 18:49:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=186
05/30/2022 18:49:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=186
05/30/2022 18:49:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
05/30/2022 18:49:56 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:49:56 - INFO - __main__ - Printing 3 examples
05/30/2022 18:49:56 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 18:49:56 - INFO - __main__ - ['refuted']
05/30/2022 18:49:56 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 18:49:56 - INFO - __main__ - ['refuted']
05/30/2022 18:49:56 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 18:49:56 - INFO - __main__ - ['refuted']
05/30/2022 18:49:56 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:49:56 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:49:57 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 18:49:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 18:49:57 - INFO - __main__ - Printing 3 examples
05/30/2022 18:49:57 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 18:49:57 - INFO - __main__ - ['refuted']
05/30/2022 18:49:57 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 18:49:57 - INFO - __main__ - ['refuted']
05/30/2022 18:49:57 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 18:49:57 - INFO - __main__ - ['refuted']
05/30/2022 18:49:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:49:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:49:57 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 18:50:06 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.12272036474164133 on epoch=187
05/30/2022 18:50:06 - INFO - __main__ - save last model!
05/30/2022 18:50:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 18:50:06 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 18:50:06 - INFO - __main__ - Printing 3 examples
05/30/2022 18:50:06 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:50:06 - INFO - __main__ - ['entailed']
05/30/2022 18:50:06 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:50:06 - INFO - __main__ - ['entailed']
05/30/2022 18:50:06 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 18:50:06 - INFO - __main__ - ['entailed']
05/30/2022 18:50:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 18:50:14 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 18:50:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 18:50:15 - INFO - __main__ - Starting training!
05/30/2022 18:50:31 - INFO - __main__ - Tokenizing Output ...
05/30/2022 18:50:43 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 19:00:04 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_42_0.4_8_predictions.txt
05/30/2022 19:00:04 - INFO - __main__ - Classification-F1 on test data: 0.0109
05/30/2022 19:00:04 - INFO - __main__ - prefix=tab_fact_128_42, lr=0.4, bsz=8, dev_performance=0.44014109347442687, test_performance=0.0108908258270901
05/30/2022 19:00:04 - INFO - __main__ - Running ... prefix=tab_fact_128_42, lr=0.3, bsz=8 ...
05/30/2022 19:00:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:00:05 - INFO - __main__ - Printing 3 examples
05/30/2022 19:00:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 19:00:05 - INFO - __main__ - ['refuted']
05/30/2022 19:00:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 19:00:05 - INFO - __main__ - ['refuted']
05/30/2022 19:00:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 19:00:05 - INFO - __main__ - ['refuted']
05/30/2022 19:00:05 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:00:05 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:00:06 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 19:00:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:00:06 - INFO - __main__ - Printing 3 examples
05/30/2022 19:00:06 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 19:00:06 - INFO - __main__ - ['refuted']
05/30/2022 19:00:06 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 19:00:06 - INFO - __main__ - ['refuted']
05/30/2022 19:00:06 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 19:00:06 - INFO - __main__ - ['refuted']
05/30/2022 19:00:06 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:00:06 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:00:06 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 19:00:22 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 19:00:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 19:00:23 - INFO - __main__ - Starting training!
05/30/2022 19:00:28 - INFO - __main__ - Step 10 Global step 10 Train loss 4.71 on epoch=0
05/30/2022 19:00:32 - INFO - __main__ - Step 20 Global step 20 Train loss 3.41 on epoch=1
05/30/2022 19:00:37 - INFO - __main__ - Step 30 Global step 30 Train loss 2.57 on epoch=1
05/30/2022 19:00:41 - INFO - __main__ - Step 40 Global step 40 Train loss 1.95 on epoch=2
05/30/2022 19:00:46 - INFO - __main__ - Step 50 Global step 50 Train loss 1.55 on epoch=3
05/30/2022 19:00:57 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.040771948899157376 on epoch=3
05/30/2022 19:00:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.040771948899157376 on epoch=3, global_step=50
05/30/2022 19:01:01 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=3
05/30/2022 19:01:06 - INFO - __main__ - Step 70 Global step 70 Train loss 0.85 on epoch=4
05/30/2022 19:01:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=4
05/30/2022 19:01:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=5
05/30/2022 19:01:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=6
05/30/2022 19:01:30 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 19:01:30 - INFO - __main__ - Saving model with best Classification-F1: 0.040771948899157376 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 19:01:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=6
05/30/2022 19:01:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=7
05/30/2022 19:01:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=8
05/30/2022 19:01:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=8
05/30/2022 19:01:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=9
05/30/2022 19:02:02 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 19:02:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 19:02:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=10
05/30/2022 19:02:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=11
05/30/2022 19:02:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=11
05/30/2022 19:02:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=12
05/30/2022 19:02:34 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 19:02:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=13
05/30/2022 19:02:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=13
05/30/2022 19:02:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=14
05/30/2022 19:02:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=14
05/30/2022 19:02:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=15
05/30/2022 19:03:07 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 19:03:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=16
05/30/2022 19:03:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.28 on epoch=16
05/30/2022 19:03:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=17
05/30/2022 19:03:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=18
05/30/2022 19:03:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=18
05/30/2022 19:03:40 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 19:03:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=19
05/30/2022 19:03:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=19
05/30/2022 19:03:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=20
05/30/2022 19:03:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=21
05/30/2022 19:04:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=21
05/30/2022 19:04:12 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 19:04:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=22
05/30/2022 19:04:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 19:04:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=23
05/30/2022 19:04:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=24
05/30/2022 19:04:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=24
05/30/2022 19:04:44 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 19:04:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=25
05/30/2022 19:04:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 19:04:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=26
05/30/2022 19:05:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=27
05/30/2022 19:05:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=28
05/30/2022 19:05:18 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 19:05:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=28
05/30/2022 19:05:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=29
05/30/2022 19:05:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=29
05/30/2022 19:05:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=30
05/30/2022 19:05:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=31
05/30/2022 19:05:51 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 19:05:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=31
05/30/2022 19:06:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=32
05/30/2022 19:06:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=33
05/30/2022 19:06:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=33
05/30/2022 19:06:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=34
05/30/2022 19:06:24 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 19:06:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 19:06:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=35
05/30/2022 19:06:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=36
05/30/2022 19:06:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=36
05/30/2022 19:06:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=37
05/30/2022 19:06:58 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 19:07:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=38
05/30/2022 19:07:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=38
05/30/2022 19:07:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=39
05/30/2022 19:07:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=39
05/30/2022 19:07:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=40
05/30/2022 19:07:32 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 19:07:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=41
05/30/2022 19:07:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=41
05/30/2022 19:07:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=42
05/30/2022 19:07:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/30/2022 19:07:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=43
05/30/2022 19:08:06 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 19:08:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 19:08:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=44
05/30/2022 19:08:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 19:08:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=46
05/30/2022 19:08:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=46
05/30/2022 19:08:40 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 19:08:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=47
05/30/2022 19:08:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/30/2022 19:08:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=48
05/30/2022 19:08:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=49
05/30/2022 19:09:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/30/2022 19:09:14 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 19:09:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=50
05/30/2022 19:09:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 19:09:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=51
05/30/2022 19:09:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=52
05/30/2022 19:09:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/30/2022 19:09:48 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 19:09:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=53
05/30/2022 19:09:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=54
05/30/2022 19:10:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=54
05/30/2022 19:10:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 19:10:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=56
05/30/2022 19:10:22 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 19:10:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=56
05/30/2022 19:10:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=57
05/30/2022 19:10:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=58
05/30/2022 19:10:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/30/2022 19:10:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 19:10:56 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 19:11:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/30/2022 19:11:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=60
05/30/2022 19:11:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=61
05/30/2022 19:11:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=61
05/30/2022 19:11:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/30/2022 19:11:29 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 19:11:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/30/2022 19:11:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/30/2022 19:11:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=64
05/30/2022 19:11:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/30/2022 19:11:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/30/2022 19:12:03 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.35693779904306216 on epoch=65
05/30/2022 19:12:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35693779904306216 on epoch=65, global_step=1050
05/30/2022 19:12:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/30/2022 19:12:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=66
05/30/2022 19:12:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/30/2022 19:12:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 19:12:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/30/2022 19:12:37 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.36516753625488524 on epoch=68
05/30/2022 19:12:37 - INFO - __main__ - Saving model with best Classification-F1: 0.35693779904306216 -> 0.36516753625488524 on epoch=68, global_step=1100
05/30/2022 19:12:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=69
05/30/2022 19:12:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 19:12:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=70
05/30/2022 19:12:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
05/30/2022 19:13:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/30/2022 19:13:11 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=71
05/30/2022 19:13:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 19:13:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=73
05/30/2022 19:13:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=73
05/30/2022 19:13:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/30/2022 19:13:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/30/2022 19:13:45 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 19:13:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/30/2022 19:13:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 19:13:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/30/2022 19:14:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
05/30/2022 19:14:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/30/2022 19:14:18 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3732922688146569 on epoch=78
05/30/2022 19:14:18 - INFO - __main__ - Saving model with best Classification-F1: 0.36516753625488524 -> 0.3732922688146569 on epoch=78, global_step=1250
05/30/2022 19:14:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/30/2022 19:14:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 19:14:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/30/2022 19:14:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/30/2022 19:14:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/30/2022 19:14:52 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=81
05/30/2022 19:14:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=81
05/30/2022 19:15:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/30/2022 19:15:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/30/2022 19:15:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/30/2022 19:15:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/30/2022 19:15:26 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 19:15:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/30/2022 19:15:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=85
05/30/2022 19:15:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/30/2022 19:15:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/30/2022 19:15:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 19:16:00 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=87
05/30/2022 19:16:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/30/2022 19:16:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/30/2022 19:16:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/30/2022 19:16:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
05/30/2022 19:16:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=90
05/30/2022 19:16:34 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.35693779904306216 on epoch=90
05/30/2022 19:16:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 19:16:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/30/2022 19:16:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/30/2022 19:16:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/30/2022 19:16:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/30/2022 19:17:08 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3712545436683367 on epoch=93
05/30/2022 19:17:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/30/2022 19:17:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/30/2022 19:17:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/30/2022 19:17:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=96
05/30/2022 19:17:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=96
05/30/2022 19:17:42 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.36516753625488524 on epoch=96
05/30/2022 19:17:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/30/2022 19:17:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/30/2022 19:17:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/30/2022 19:18:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/30/2022 19:18:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/30/2022 19:18:15 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.36516753625488524 on epoch=99
05/30/2022 19:18:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
05/30/2022 19:18:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/30/2022 19:18:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/30/2022 19:18:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/30/2022 19:18:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/30/2022 19:18:49 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=103
05/30/2022 19:18:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/30/2022 19:18:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/30/2022 19:19:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/30/2022 19:19:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 19:19:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
05/30/2022 19:19:23 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 19:19:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
05/30/2022 19:19:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/30/2022 19:19:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/30/2022 19:19:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=108
05/30/2022 19:19:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/30/2022 19:19:57 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 19:20:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/30/2022 19:20:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/30/2022 19:20:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 19:20:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
05/30/2022 19:20:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/30/2022 19:20:31 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 19:20:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/30/2022 19:20:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/30/2022 19:20:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/30/2022 19:20:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=114
05/30/2022 19:20:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/30/2022 19:21:04 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.35501021683496337 on epoch=115
05/30/2022 19:21:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=116
05/30/2022 19:21:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/30/2022 19:21:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=117
05/30/2022 19:21:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
05/30/2022 19:21:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
05/30/2022 19:21:38 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.36318407960199 on epoch=118
05/30/2022 19:21:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
05/30/2022 19:21:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=119
05/30/2022 19:21:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=120
05/30/2022 19:21:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
05/30/2022 19:22:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=121
05/30/2022 19:22:12 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.36318407960199 on epoch=121
05/30/2022 19:22:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=122
05/30/2022 19:22:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/30/2022 19:22:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/30/2022 19:22:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=124
05/30/2022 19:22:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/30/2022 19:22:46 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.3486005089058525 on epoch=124
05/30/2022 19:22:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/30/2022 19:22:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
05/30/2022 19:22:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 19:23:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
05/30/2022 19:23:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
05/30/2022 19:23:20 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=128
05/30/2022 19:23:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3732922688146569 -> 0.37922403003754696 on epoch=128, global_step=2050
05/30/2022 19:23:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/30/2022 19:23:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=129
05/30/2022 19:23:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=129
05/30/2022 19:23:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/30/2022 19:23:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=131
05/30/2022 19:23:54 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.24330475283158673 on epoch=131
05/30/2022 19:23:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/30/2022 19:24:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
05/30/2022 19:24:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=133
05/30/2022 19:24:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=133
05/30/2022 19:24:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/30/2022 19:24:27 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.34673046251993617 on epoch=134
05/30/2022 19:24:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=134
05/30/2022 19:24:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/30/2022 19:24:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/30/2022 19:24:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/30/2022 19:24:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/30/2022 19:25:02 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.34673046251993617 on epoch=137
05/30/2022 19:25:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
05/30/2022 19:25:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/30/2022 19:25:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/30/2022 19:25:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/30/2022 19:25:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=140
05/30/2022 19:25:36 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.3750290630086026 on epoch=140
05/30/2022 19:25:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
05/30/2022 19:25:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=141
05/30/2022 19:25:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
05/30/2022 19:25:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/30/2022 19:25:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
05/30/2022 19:26:10 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.3750290630086026 on epoch=143
05/30/2022 19:26:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=144
05/30/2022 19:26:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/30/2022 19:26:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=145
05/30/2022 19:26:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=146
05/30/2022 19:26:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/30/2022 19:26:45 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.35307589038932324 on epoch=146
05/30/2022 19:26:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/30/2022 19:26:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/30/2022 19:26:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 19:27:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=149
05/30/2022 19:27:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/30/2022 19:27:19 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.35501021683496337 on epoch=149
05/30/2022 19:27:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/30/2022 19:27:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=151
05/30/2022 19:27:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
05/30/2022 19:27:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
05/30/2022 19:27:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/30/2022 19:27:53 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.183878221760941 on epoch=153
05/30/2022 19:27:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=153
05/30/2022 19:28:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=154
05/30/2022 19:28:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=154
05/30/2022 19:28:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/30/2022 19:28:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=156
05/30/2022 19:28:27 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.2480956133438403 on epoch=156
05/30/2022 19:28:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/30/2022 19:28:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
05/30/2022 19:28:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/30/2022 19:28:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=158
05/30/2022 19:28:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
05/30/2022 19:29:01 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.17824546026180904 on epoch=159
05/30/2022 19:29:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/30/2022 19:29:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=160
05/30/2022 19:29:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=161
05/30/2022 19:29:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/30/2022 19:29:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=162
05/30/2022 19:29:35 - INFO - __main__ - Global step 2600 Train loss 0.21 Classification-F1 0.16913179413179413 on epoch=162
05/30/2022 19:29:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.22 on epoch=163
05/30/2022 19:29:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
05/30/2022 19:29:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
05/30/2022 19:29:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=164
05/30/2022 19:29:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=165
05/30/2022 19:30:10 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.17644444444444443 on epoch=165
05/30/2022 19:30:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=166
05/30/2022 19:30:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=166
05/30/2022 19:30:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/30/2022 19:30:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=168
05/30/2022 19:30:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=168
05/30/2022 19:30:44 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.2710296239708005 on epoch=168
05/30/2022 19:30:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=169
05/30/2022 19:30:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/30/2022 19:30:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=170
05/30/2022 19:31:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=171
05/30/2022 19:31:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=171
05/30/2022 19:31:18 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.25147270252045245 on epoch=171
05/30/2022 19:31:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=172
05/30/2022 19:31:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=173
05/30/2022 19:31:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=173
05/30/2022 19:31:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=174
05/30/2022 19:31:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=174
05/30/2022 19:31:52 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.18077397471477027 on epoch=174
05/30/2022 19:31:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/30/2022 19:32:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=176
05/30/2022 19:32:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=176
05/30/2022 19:32:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=177
05/30/2022 19:32:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=178
05/30/2022 19:32:26 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.154046881910659 on epoch=178
05/30/2022 19:32:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=178
05/30/2022 19:32:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=179
05/30/2022 19:32:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/30/2022 19:32:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=180
05/30/2022 19:32:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=181
05/30/2022 19:33:00 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.26322339011719226 on epoch=181
05/30/2022 19:33:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=181
05/30/2022 19:33:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=182
05/30/2022 19:33:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=183
05/30/2022 19:33:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=183
05/30/2022 19:33:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=184
05/30/2022 19:33:35 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.18769136558481322 on epoch=184
05/30/2022 19:33:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=184
05/30/2022 19:33:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/30/2022 19:33:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=186
05/30/2022 19:33:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=186
05/30/2022 19:33:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.21 on epoch=187
05/30/2022 19:33:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:33:59 - INFO - __main__ - Printing 3 examples
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:33:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:33:59 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 19:33:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:33:59 - INFO - __main__ - Printing 3 examples
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 19:33:59 - INFO - __main__ - ['refuted']
05/30/2022 19:33:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:34:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:34:00 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 19:34:09 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.1914597663916465 on epoch=187
05/30/2022 19:34:09 - INFO - __main__ - save last model!
05/30/2022 19:34:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 19:34:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 19:34:09 - INFO - __main__ - Printing 3 examples
05/30/2022 19:34:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 19:34:09 - INFO - __main__ - ['entailed']
05/30/2022 19:34:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 19:34:09 - INFO - __main__ - ['entailed']
05/30/2022 19:34:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 19:34:09 - INFO - __main__ - ['entailed']
05/30/2022 19:34:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:34:17 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 19:34:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 19:34:17 - INFO - __main__ - Starting training!
05/30/2022 19:34:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:34:47 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 19:44:06 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_42_0.3_8_predictions.txt
05/30/2022 19:44:06 - INFO - __main__ - Classification-F1 on test data: 0.0601
05/30/2022 19:44:07 - INFO - __main__ - prefix=tab_fact_128_42, lr=0.3, bsz=8, dev_performance=0.37922403003754696, test_performance=0.06014280692915068
05/30/2022 19:44:07 - INFO - __main__ - Running ... prefix=tab_fact_128_42, lr=0.2, bsz=8 ...
05/30/2022 19:44:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:44:08 - INFO - __main__ - Printing 3 examples
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:44:08 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:44:08 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 19:44:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 19:44:08 - INFO - __main__ - Printing 3 examples
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: clay regazzoni be not the driver with 1 in grid [SEP] table_caption: 1971 british grand prix [SEP] table_text: driver#constructor#laps#time / retired#grid [n] jackie stewart#tyrrell - ford#68#1:31:31.5#2 [n] ronnie peterson#march - ford#68#+ 36.1#5 [n] emerson fittipaldi#lotus - ford#68#+ 50.5#4 [n] henri pescarolo#march - ford#67#+ 1 lap#17 [n] rolf stommelen#surtees - ford#67#+ 1 lap#12 [n] john surtees#surtees - ford#67#+ 1 lap#18 [n] jean - pierre beltoise#matra#66#+ 2 laps#15 [n] howden ganley#brm#66#+ 2 laps#11 [n] jo siffert#brm#66#+ 2 laps#3 [n] franã§ois cevert#tyrrell - ford#65#+ 3 laps#10 [n] nanni galli#march - ford#65#+ 3 laps#21 [n] tim schenken#brabham - ford#63#gearbox#7 [n] reine wisell#lotus - pratt & whitney#57#not classified#19 [n] andrea de adamich#march - alfa romeo#56#not classified#24 [n] peter gethin#mclaren - ford#53#engine#14 [n] jacky ickx#ferrari#51#engine#6 [n] clay regazzoni#ferrari#48#oil pressure#1 [n] chris amon#matra#35#engine#9 [n] denny hulme#mclaren - ford#32#engine#8 [n] derek bell#surtees - ford#23#suspension#23 [n] mike beuttler#march - ford#21#oil pressure#20 [n] dave charlton#lotus - ford#1#engine#13 [n] graham hill#brabham - ford#0#accident#22 [n] jackie oliver#mclaren - ford#0#accident#16 [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: the episode correspond to silver in segment d of episode 205 and series ep 16 - 07 be tequila of segment a [SEP] table_caption: list of how it 's made episodes [SEP] table_text: series ep#episode#netflix#segment a#segment b#segment c#segment d [n] 16 - 01#196#s08e14#millefiori glass paperweights#road salt#s nutcracker#car doors [n] 16 - 02#197#s08e15#straight razors#black pudding#steering wheels#inorganic pigments [n] 16 - 03#198#s08e16#cast iron cookware#biodiesel#clothes hangers#stone wool insulation [n] 16 - 04#199#s08e17#needles & pins#architectural mouldings#s locomotive#s clothespin [n] 16 - 05#200#s08e18#filigree glass#fish food#s motor home (part 1)#s motor home (part 2) [n] 16 - 06#201#s08e19#surgical instruments#ketchup#double - decker buses#walking sticks [n] 16 - 07#202#s08e20#audio vacuum tubes#light bars#wood model aircraft#metal s snare drum [n] 16 - 08#203#s08e21#kitchen accessories#central vacuums#papier - mché animals#hydraulic cylinders [n] 16 - 09#204#s08e22#clay liquor jugs#poultry deli meats#nascar engines (part 1)#nascar engines (part 2) [n] 16 - 10#205#s08e23#digital dentistry#s nail clipper#poster restoration#canola oil [n] 16 - 11#206#s08e24#dial thermometers#hummus#spent fuel containers#straw s sombrero [n] 16 - 12#207#s08e25#tequila#s waterbed#s flip flop#silver [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ -  [tab_fact] statement: class 157 be on 1879 - 81 with builder abarclay [SEP] table_caption: locomotives of the glasgow and south western railway [SEP] table_text: class#date#builder#no built#1919 nos#lms class#lms nos [n] 157#1879 - 81#g&swr kilmarnock#12#720 - 5#1p#14001 - 2 [n] 119#1882 - 5#g&swr kilmarnock#24#467 - 8 , 700 - 719#1p#14116 - 37 [n] 153#1886 - 9#g&swr kilmarnock#20#448 - 466#1p#14138 - 56 [n] 1#1879 - 81#g&swr kilmarnock#4#728 - 31#1p#15241 - 4 [n] 291#1883#abarclay#1#734#u#16042 [n] 218#1881#andrews , barr & co#2#658 - 9#u#16040 - 1 [n] 224#1881 - 92#g&swr kilmarnock#44#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1883#neilson#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 224#1889#dã¼bs#10#135 - 9 , 560 - 616 with gaps#1f#17112 - 64 [n] 
05/30/2022 19:44:08 - INFO - __main__ - ['refuted']
05/30/2022 19:44:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 19:44:09 - INFO - __main__ - Tokenizing Output ...
05/30/2022 19:44:09 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 19:44:24 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 19:44:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 19:44:25 - INFO - __main__ - Starting training!
05/30/2022 19:44:30 - INFO - __main__ - Step 10 Global step 10 Train loss 5.48 on epoch=0
05/30/2022 19:44:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.77 on epoch=1
05/30/2022 19:44:39 - INFO - __main__ - Step 30 Global step 30 Train loss 2.95 on epoch=1
05/30/2022 19:44:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.64 on epoch=2
05/30/2022 19:44:48 - INFO - __main__ - Step 50 Global step 50 Train loss 2.24 on epoch=3
05/30/2022 19:44:59 - INFO - __main__ - Global step 50 Train loss 3.41 Classification-F1 0.0 on epoch=3
05/30/2022 19:44:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/30/2022 19:45:04 - INFO - __main__ - Step 60 Global step 60 Train loss 1.79 on epoch=3
05/30/2022 19:45:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.62 on epoch=4
05/30/2022 19:45:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.27 on epoch=4
05/30/2022 19:45:17 - INFO - __main__ - Step 90 Global step 90 Train loss 1.04 on epoch=5
05/30/2022 19:45:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=6
05/30/2022 19:45:33 - INFO - __main__ - Global step 100 Train loss 1.31 Classification-F1 0.5154574132492113 on epoch=6
05/30/2022 19:45:33 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.5154574132492113 on epoch=6, global_step=100
05/30/2022 19:45:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=6
05/30/2022 19:45:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=7
05/30/2022 19:45:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/30/2022 19:45:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=8
05/30/2022 19:45:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=9
05/30/2022 19:46:06 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 19:46:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=9
05/30/2022 19:46:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
05/30/2022 19:46:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=11
05/30/2022 19:46:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=11
05/30/2022 19:46:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=12
05/30/2022 19:46:39 - INFO - __main__ - Global step 200 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 19:46:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=13
05/30/2022 19:46:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=13
05/30/2022 19:46:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=14
05/30/2022 19:46:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=14
05/30/2022 19:47:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=15
05/30/2022 19:47:11 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 19:47:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=16
05/30/2022 19:47:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=16
05/30/2022 19:47:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=17
05/30/2022 19:47:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=18
05/30/2022 19:47:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=18
05/30/2022 19:47:43 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 19:47:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=19
05/30/2022 19:47:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=19
05/30/2022 19:47:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=20
05/30/2022 19:48:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=21
05/30/2022 19:48:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=21
05/30/2022 19:48:16 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 19:48:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=22
05/30/2022 19:48:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=23
05/30/2022 19:48:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=23
05/30/2022 19:48:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=24
05/30/2022 19:48:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=24
05/30/2022 19:48:48 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 19:48:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=25
05/30/2022 19:48:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=26
05/30/2022 19:49:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=26
05/30/2022 19:49:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=27
05/30/2022 19:49:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=28
05/30/2022 19:49:20 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 19:49:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=28
05/30/2022 19:49:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=29
05/30/2022 19:49:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=29
05/30/2022 19:49:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=30
05/30/2022 19:49:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=31
05/30/2022 19:49:53 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 19:49:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 19:50:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=32
05/30/2022 19:50:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 19:50:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=33
05/30/2022 19:50:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=34
05/30/2022 19:50:26 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 19:50:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=34
05/30/2022 19:50:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=35
05/30/2022 19:50:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=36
05/30/2022 19:50:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=36
05/30/2022 19:50:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=37
05/30/2022 19:51:00 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 19:51:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=38
05/30/2022 19:51:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=38
05/30/2022 19:51:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=39
05/30/2022 19:51:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=39
05/30/2022 19:51:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/30/2022 19:51:33 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 19:51:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=41
05/30/2022 19:51:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=41
05/30/2022 19:51:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=42
05/30/2022 19:51:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/30/2022 19:51:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=43
05/30/2022 19:52:07 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 19:52:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=44
05/30/2022 19:52:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=44
05/30/2022 19:52:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=45
05/30/2022 19:52:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=46
05/30/2022 19:52:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=46
05/30/2022 19:52:40 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 19:52:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/30/2022 19:52:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/30/2022 19:52:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=48
05/30/2022 19:52:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=49
05/30/2022 19:53:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=49
05/30/2022 19:53:14 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 19:53:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=50
05/30/2022 19:53:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=51
05/30/2022 19:53:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 19:53:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=52
05/30/2022 19:53:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=53
05/30/2022 19:53:48 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 19:53:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/30/2022 19:53:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/30/2022 19:54:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=54
05/30/2022 19:54:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/30/2022 19:54:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/30/2022 19:54:21 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 19:54:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/30/2022 19:54:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=57
05/30/2022 19:54:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=58
05/30/2022 19:54:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
05/30/2022 19:54:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=59
05/30/2022 19:54:55 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 19:54:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/30/2022 19:55:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=60
05/30/2022 19:55:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 19:55:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=61
05/30/2022 19:55:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=62
05/30/2022 19:55:28 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 19:55:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/30/2022 19:55:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/30/2022 19:55:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/30/2022 19:55:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=64
05/30/2022 19:55:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/30/2022 19:56:02 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 19:56:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/30/2022 19:56:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/30/2022 19:56:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
05/30/2022 19:56:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 19:56:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 19:56:36 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 19:56:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/30/2022 19:56:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 19:56:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=70
05/30/2022 19:56:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/30/2022 19:56:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/30/2022 19:57:09 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 19:57:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=72
05/30/2022 19:57:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 19:57:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/30/2022 19:57:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/30/2022 19:57:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
05/30/2022 19:57:43 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 19:57:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=75
05/30/2022 19:57:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 19:57:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/30/2022 19:58:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
05/30/2022 19:58:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/30/2022 19:58:16 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 19:58:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/30/2022 19:58:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 19:58:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/30/2022 19:58:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=80
05/30/2022 19:58:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
05/30/2022 19:58:50 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 19:58:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/30/2022 19:58:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 19:59:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=83
05/30/2022 19:59:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=83
05/30/2022 19:59:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=84
05/30/2022 19:59:23 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 19:59:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=84
05/30/2022 19:59:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=85
05/30/2022 19:59:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/30/2022 19:59:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
05/30/2022 19:59:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=87
05/30/2022 19:59:57 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 20:00:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/30/2022 20:00:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
05/30/2022 20:00:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=89
05/30/2022 20:00:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
05/30/2022 20:00:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/30/2022 20:00:30 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 20:00:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 20:00:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/30/2022 20:00:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/30/2022 20:00:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
05/30/2022 20:00:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/30/2022 20:01:04 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 20:01:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=94
05/30/2022 20:01:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 20:01:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/30/2022 20:01:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=96
05/30/2022 20:01:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
05/30/2022 20:01:38 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 20:01:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/30/2022 20:01:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/30/2022 20:01:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/30/2022 20:01:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/30/2022 20:02:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/30/2022 20:02:12 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 20:02:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
05/30/2022 20:02:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/30/2022 20:02:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=101
05/30/2022 20:02:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=102
05/30/2022 20:02:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/30/2022 20:02:46 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 20:02:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=103
05/30/2022 20:02:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/30/2022 20:02:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=104
05/30/2022 20:03:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 20:03:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=106
05/30/2022 20:03:19 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 20:03:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 20:03:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/30/2022 20:03:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 20:03:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=108
05/30/2022 20:03:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=109
05/30/2022 20:03:53 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 20:03:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/30/2022 20:04:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
05/30/2022 20:04:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 20:04:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/30/2022 20:04:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=112
05/30/2022 20:04:26 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.34195559333697656 on epoch=112
05/30/2022 20:04:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/30/2022 20:04:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/30/2022 20:04:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/30/2022 20:04:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
05/30/2022 20:04:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/30/2022 20:05:00 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.3401530406766009 on epoch=115
05/30/2022 20:05:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=116
05/30/2022 20:05:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/30/2022 20:05:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/30/2022 20:05:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=118
05/30/2022 20:05:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/30/2022 20:05:34 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.3401530406766009 on epoch=118
05/30/2022 20:05:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/30/2022 20:05:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=119
05/30/2022 20:05:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=120
05/30/2022 20:05:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/30/2022 20:05:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=121
05/30/2022 20:06:08 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3486005089058525 on epoch=121
05/30/2022 20:06:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=122
05/30/2022 20:06:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
05/30/2022 20:06:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/30/2022 20:06:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/30/2022 20:06:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=124
05/30/2022 20:06:42 - INFO - __main__ - Global step 2000 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=124
05/30/2022 20:06:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=125
05/30/2022 20:06:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=126
05/30/2022 20:06:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=126
05/30/2022 20:07:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
05/30/2022 20:07:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
05/30/2022 20:07:16 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.37922403003754696 on epoch=128
05/30/2022 20:07:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 20:07:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
05/30/2022 20:07:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=129
05/30/2022 20:07:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=130
05/30/2022 20:07:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 20:07:51 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.3486005089058525 on epoch=131
05/30/2022 20:07:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=131
05/30/2022 20:08:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=132
05/30/2022 20:08:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=133
05/30/2022 20:08:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=133
05/30/2022 20:08:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=134
05/30/2022 20:08:24 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.3401530406766009 on epoch=134
05/30/2022 20:08:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/30/2022 20:08:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.20 on epoch=135
05/30/2022 20:08:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/30/2022 20:08:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=136
05/30/2022 20:08:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=137
05/30/2022 20:08:58 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.36318407960199 on epoch=137
05/30/2022 20:09:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=138
05/30/2022 20:09:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/30/2022 20:09:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/30/2022 20:09:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=139
05/30/2022 20:09:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
05/30/2022 20:09:32 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.35693779904306216 on epoch=140
05/30/2022 20:09:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=141
05/30/2022 20:09:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
05/30/2022 20:09:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
05/30/2022 20:09:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
05/30/2022 20:09:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
05/30/2022 20:10:06 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.3712545436683367 on epoch=143
05/30/2022 20:10:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/30/2022 20:10:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=144
05/30/2022 20:10:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=145
05/30/2022 20:10:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=146
05/30/2022 20:10:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=146
05/30/2022 20:10:40 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.37922403003754696 on epoch=146
05/30/2022 20:10:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=147
05/30/2022 20:10:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/30/2022 20:10:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 20:10:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
05/30/2022 20:11:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/30/2022 20:11:13 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.3712545436683367 on epoch=149
05/30/2022 20:11:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=150
05/30/2022 20:11:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
05/30/2022 20:11:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
05/30/2022 20:11:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
05/30/2022 20:11:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=153
05/30/2022 20:11:47 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.37922403003754696 on epoch=153
05/30/2022 20:11:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=153
05/30/2022 20:11:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=154
05/30/2022 20:12:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=154
05/30/2022 20:12:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/30/2022 20:12:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=156
05/30/2022 20:12:21 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=156
05/30/2022 20:12:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
05/30/2022 20:12:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
05/30/2022 20:12:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=158
05/30/2022 20:12:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=158
05/30/2022 20:12:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=159
05/30/2022 20:12:55 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=159
05/30/2022 20:12:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/30/2022 20:13:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=160
05/30/2022 20:13:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/30/2022 20:13:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
05/30/2022 20:13:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
05/30/2022 20:13:29 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.35693779904306216 on epoch=162
05/30/2022 20:13:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.22 on epoch=163
05/30/2022 20:13:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
05/30/2022 20:13:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=164
05/30/2022 20:13:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=164
05/30/2022 20:13:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=165
05/30/2022 20:14:02 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.3712545436683367 on epoch=165
05/30/2022 20:14:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=166
05/30/2022 20:14:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/30/2022 20:14:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=167
05/30/2022 20:14:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=168
05/30/2022 20:14:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/30/2022 20:14:36 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=168
05/30/2022 20:14:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=169
05/30/2022 20:14:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
05/30/2022 20:14:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=170
05/30/2022 20:14:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
05/30/2022 20:14:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=171
05/30/2022 20:15:10 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=171
05/30/2022 20:15:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=172
05/30/2022 20:15:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=173
05/30/2022 20:15:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.22 on epoch=173
05/30/2022 20:15:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=174
05/30/2022 20:15:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=174
05/30/2022 20:15:44 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=174
05/30/2022 20:15:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/30/2022 20:15:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
05/30/2022 20:15:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=176
05/30/2022 20:16:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=177
05/30/2022 20:16:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=178
05/30/2022 20:16:18 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.37922403003754696 on epoch=178
05/30/2022 20:16:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=178
05/30/2022 20:16:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/30/2022 20:16:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=179
05/30/2022 20:16:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=180
05/30/2022 20:16:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=181
05/30/2022 20:16:52 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=181
05/30/2022 20:16:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.21 on epoch=181
05/30/2022 20:17:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=182
05/30/2022 20:17:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=183
05/30/2022 20:17:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=183
05/30/2022 20:17:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=184
05/30/2022 20:17:26 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.3771289537712896 on epoch=184
05/30/2022 20:17:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=184
05/30/2022 20:17:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=185
05/30/2022 20:17:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=186
05/30/2022 20:17:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
05/30/2022 20:17:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=187
05/30/2022 20:17:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 20:17:50 - INFO - __main__ - Printing 3 examples
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:17:50 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:17:50 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 20:17:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 20:17:50 - INFO - __main__ - Printing 3 examples
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 20:17:50 - INFO - __main__ - ['entailed']
05/30/2022 20:17:50 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:17:51 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:17:51 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 20:18:00 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=187
05/30/2022 20:18:00 - INFO - __main__ - save last model!
05/30/2022 20:18:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 20:18:00 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 20:18:00 - INFO - __main__ - Printing 3 examples
05/30/2022 20:18:00 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 20:18:00 - INFO - __main__ - ['entailed']
05/30/2022 20:18:00 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 20:18:00 - INFO - __main__ - ['entailed']
05/30/2022 20:18:00 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 20:18:00 - INFO - __main__ - ['entailed']
05/30/2022 20:18:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:18:07 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 20:18:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 20:18:08 - INFO - __main__ - Starting training!
05/30/2022 20:18:24 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:18:36 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 20:27:55 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_42_0.2_8_predictions.txt
05/30/2022 20:27:55 - INFO - __main__ - Classification-F1 on test data: 0.1792
05/30/2022 20:27:56 - INFO - __main__ - prefix=tab_fact_128_42, lr=0.2, bsz=8, dev_performance=0.5154574132492113, test_performance=0.17922669858169435
05/30/2022 20:27:56 - INFO - __main__ - Running ... prefix=tab_fact_128_87, lr=0.5, bsz=8 ...
05/30/2022 20:27:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 20:27:57 - INFO - __main__ - Printing 3 examples
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:27:57 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:27:57 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 20:27:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 20:27:57 - INFO - __main__ - Printing 3 examples
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 20:27:57 - INFO - __main__ - ['entailed']
05/30/2022 20:27:57 - INFO - __main__ - Tokenizing Input ...
05/30/2022 20:27:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 20:27:58 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 20:28:17 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 20:28:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 20:28:18 - INFO - __main__ - Starting training!
05/30/2022 20:28:23 - INFO - __main__ - Step 10 Global step 10 Train loss 4.64 on epoch=0
05/30/2022 20:28:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.80 on epoch=1
05/30/2022 20:28:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=1
05/30/2022 20:28:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.24 on epoch=2
05/30/2022 20:28:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=3
05/30/2022 20:28:52 - INFO - __main__ - Global step 50 Train loss 2.29 Classification-F1 0.41013824884792627 on epoch=3
05/30/2022 20:28:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.41013824884792627 on epoch=3, global_step=50
05/30/2022 20:28:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/30/2022 20:29:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
05/30/2022 20:29:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.34 on epoch=4
05/30/2022 20:29:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=5
05/30/2022 20:29:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=6
05/30/2022 20:29:25 - INFO - __main__ - Global step 100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 20:29:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=6
05/30/2022 20:29:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=7
05/30/2022 20:29:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=8
05/30/2022 20:29:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=8
05/30/2022 20:29:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=9
05/30/2022 20:29:59 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 20:30:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=9
05/30/2022 20:30:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=10
05/30/2022 20:30:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=11
05/30/2022 20:30:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=11
05/30/2022 20:30:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=12
05/30/2022 20:30:33 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 20:30:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=13
05/30/2022 20:30:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=13
05/30/2022 20:30:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=14
05/30/2022 20:30:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=14
05/30/2022 20:30:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=15
05/30/2022 20:31:06 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 20:31:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=16
05/30/2022 20:31:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=16
05/30/2022 20:31:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=17
05/30/2022 20:31:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.20 on epoch=18
05/30/2022 20:31:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=18
05/30/2022 20:31:40 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 20:31:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=19
05/30/2022 20:31:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=19
05/30/2022 20:31:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=20
05/30/2022 20:31:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=21
05/30/2022 20:32:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=21
05/30/2022 20:32:14 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 20:32:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 20:32:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=23
05/30/2022 20:32:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=23
05/30/2022 20:32:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=24
05/30/2022 20:32:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=24
05/30/2022 20:32:48 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 20:32:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=25
05/30/2022 20:32:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 20:33:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=26
05/30/2022 20:33:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=27
05/30/2022 20:33:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=28
05/30/2022 20:33:22 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 20:33:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=28
05/30/2022 20:33:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=29
05/30/2022 20:33:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=29
05/30/2022 20:33:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=30
05/30/2022 20:33:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.19 on epoch=31
05/30/2022 20:33:57 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 20:34:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=31
05/30/2022 20:34:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=32
05/30/2022 20:34:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 20:34:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=33
05/30/2022 20:34:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 20:34:31 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 20:34:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=34
05/30/2022 20:34:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=35
05/30/2022 20:34:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=36
05/30/2022 20:34:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=36
05/30/2022 20:34:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=37
05/30/2022 20:35:04 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 20:35:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=38
05/30/2022 20:35:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=38
05/30/2022 20:35:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=39
05/30/2022 20:35:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=39
05/30/2022 20:35:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=40
05/30/2022 20:35:38 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 20:35:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=41
05/30/2022 20:35:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=41
05/30/2022 20:35:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 20:35:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/30/2022 20:36:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=43
05/30/2022 20:36:12 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 20:36:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=44
05/30/2022 20:36:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=44
05/30/2022 20:36:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=45
05/30/2022 20:36:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=46
05/30/2022 20:36:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=46
05/30/2022 20:36:46 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 20:36:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=47
05/30/2022 20:36:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=48
05/30/2022 20:37:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=48
05/30/2022 20:37:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=49
05/30/2022 20:37:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=49
05/30/2022 20:37:20 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 20:37:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=50
05/30/2022 20:37:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=51
05/30/2022 20:37:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=51
05/30/2022 20:37:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=52
05/30/2022 20:37:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=53
05/30/2022 20:37:54 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 20:37:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 20:38:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/30/2022 20:38:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/30/2022 20:38:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/30/2022 20:38:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 20:38:28 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 20:38:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/30/2022 20:38:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/30/2022 20:38:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=58
05/30/2022 20:38:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=58
05/30/2022 20:38:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=59
05/30/2022 20:39:01 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 20:39:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=59
05/30/2022 20:39:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/30/2022 20:39:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=61
05/30/2022 20:39:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 20:39:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/30/2022 20:39:35 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 20:39:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/30/2022 20:39:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=63
05/30/2022 20:39:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/30/2022 20:39:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=64
05/30/2022 20:39:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/30/2022 20:40:09 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 20:40:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/30/2022 20:40:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/30/2022 20:40:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 20:40:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=68
05/30/2022 20:40:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=68
05/30/2022 20:40:43 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 20:40:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/30/2022 20:40:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/30/2022 20:40:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/30/2022 20:41:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/30/2022 20:41:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/30/2022 20:41:17 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 20:41:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 20:41:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=73
05/30/2022 20:41:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=73
05/30/2022 20:41:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/30/2022 20:41:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/30/2022 20:41:51 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 20:41:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=75
05/30/2022 20:42:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 20:42:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=76
05/30/2022 20:42:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=77
05/30/2022 20:42:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=78
05/30/2022 20:42:25 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 20:42:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/30/2022 20:42:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
05/30/2022 20:42:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/30/2022 20:42:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=80
05/30/2022 20:42:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=81
05/30/2022 20:42:59 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 20:43:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=81
05/30/2022 20:43:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=82
05/30/2022 20:43:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
05/30/2022 20:43:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/30/2022 20:43:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=84
05/30/2022 20:43:33 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 20:43:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=84
05/30/2022 20:43:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/30/2022 20:43:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/30/2022 20:43:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/30/2022 20:43:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
05/30/2022 20:44:07 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 20:44:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/30/2022 20:44:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=88
05/30/2022 20:44:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
05/30/2022 20:44:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/30/2022 20:44:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/30/2022 20:44:41 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 20:44:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
05/30/2022 20:44:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/30/2022 20:44:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
05/30/2022 20:44:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/30/2022 20:45:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
05/30/2022 20:45:15 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 20:45:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=94
05/30/2022 20:45:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 20:45:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/30/2022 20:45:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/30/2022 20:45:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
05/30/2022 20:45:49 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 20:45:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/30/2022 20:45:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/30/2022 20:46:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/30/2022 20:46:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
05/30/2022 20:46:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
05/30/2022 20:46:23 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.4152823920265781 on epoch=99
05/30/2022 20:46:23 - INFO - __main__ - Saving model with best Classification-F1: 0.41013824884792627 -> 0.4152823920265781 on epoch=99, global_step=1600
05/30/2022 20:46:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
05/30/2022 20:46:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=101
05/30/2022 20:46:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/30/2022 20:46:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/30/2022 20:46:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 20:46:57 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.3383422492035824 on epoch=103
05/30/2022 20:47:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/30/2022 20:47:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
05/30/2022 20:47:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/30/2022 20:47:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=105
05/30/2022 20:47:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
05/30/2022 20:47:31 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.3591989987484355 on epoch=106
05/30/2022 20:47:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/30/2022 20:47:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/30/2022 20:47:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 20:47:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/30/2022 20:47:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/30/2022 20:48:04 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.47033358798064684 on epoch=109
05/30/2022 20:48:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4152823920265781 -> 0.47033358798064684 on epoch=109, global_step=1750
05/30/2022 20:48:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=109
05/30/2022 20:48:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
05/30/2022 20:48:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/30/2022 20:48:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
05/30/2022 20:48:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
05/30/2022 20:48:38 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.4941980806623596 on epoch=112
05/30/2022 20:48:38 - INFO - __main__ - Saving model with best Classification-F1: 0.47033358798064684 -> 0.4941980806623596 on epoch=112, global_step=1800
05/30/2022 20:48:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
05/30/2022 20:48:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/30/2022 20:48:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/30/2022 20:48:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=114
05/30/2022 20:49:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/30/2022 20:49:11 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.4734247177602965 on epoch=115
05/30/2022 20:49:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=116
05/30/2022 20:49:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
05/30/2022 20:49:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
05/30/2022 20:49:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/30/2022 20:49:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
05/30/2022 20:49:44 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.34132104454685097 on epoch=118
05/30/2022 20:49:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=119
05/30/2022 20:49:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 20:49:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/30/2022 20:50:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=121
05/30/2022 20:50:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 20:50:18 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.46938457462558636 on epoch=121
05/30/2022 20:50:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
05/30/2022 20:50:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/30/2022 20:50:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/30/2022 20:50:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/30/2022 20:50:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=124
05/30/2022 20:50:52 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.4513254371122391 on epoch=124
05/30/2022 20:50:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/30/2022 20:51:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=126
05/30/2022 20:51:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 20:51:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=127
05/30/2022 20:51:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/30/2022 20:51:26 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.30556614967370116 on epoch=128
05/30/2022 20:51:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=128
05/30/2022 20:51:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=129
05/30/2022 20:51:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
05/30/2022 20:51:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/30/2022 20:51:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 20:52:00 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.49435776515441504 on epoch=131
05/30/2022 20:52:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4941980806623596 -> 0.49435776515441504 on epoch=131, global_step=2100
05/30/2022 20:52:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/30/2022 20:52:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
05/30/2022 20:52:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
05/30/2022 20:52:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/30/2022 20:52:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
05/30/2022 20:52:34 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.48424908424908425 on epoch=134
05/30/2022 20:52:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/30/2022 20:52:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/30/2022 20:52:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/30/2022 20:52:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/30/2022 20:52:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
05/30/2022 20:53:08 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.24790188714239345 on epoch=137
05/30/2022 20:53:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=138
05/30/2022 20:53:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=138
05/30/2022 20:53:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/30/2022 20:53:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/30/2022 20:53:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/30/2022 20:53:41 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.3384805877898529 on epoch=140
05/30/2022 20:53:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
05/30/2022 20:53:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
05/30/2022 20:53:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/30/2022 20:53:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=143
05/30/2022 20:54:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
05/30/2022 20:54:15 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.1155383967883968 on epoch=143
05/30/2022 20:54:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=144
05/30/2022 20:54:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/30/2022 20:54:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=145
05/30/2022 20:54:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=146
05/30/2022 20:54:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/30/2022 20:54:49 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.11432957021192314 on epoch=146
05/30/2022 20:54:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/30/2022 20:54:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
05/30/2022 20:55:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 20:55:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/30/2022 20:55:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/30/2022 20:55:23 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.1228219696969697 on epoch=149
05/30/2022 20:55:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=150
05/30/2022 20:55:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/30/2022 20:55:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/30/2022 20:55:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/30/2022 20:55:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/30/2022 20:55:57 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.11570793972109764 on epoch=153
05/30/2022 20:56:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/30/2022 20:56:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=154
05/30/2022 20:56:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/30/2022 20:56:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
05/30/2022 20:56:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/30/2022 20:56:31 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.11423530121807911 on epoch=156
05/30/2022 20:56:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
05/30/2022 20:56:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=157
05/30/2022 20:56:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
05/30/2022 20:56:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/30/2022 20:56:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/30/2022 20:57:05 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.1432076904754414 on epoch=159
05/30/2022 20:57:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/30/2022 20:57:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
05/30/2022 20:57:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=161
05/30/2022 20:57:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=161
05/30/2022 20:57:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/30/2022 20:57:38 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.12847864220357463 on epoch=162
05/30/2022 20:57:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/30/2022 20:57:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/30/2022 20:57:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
05/30/2022 20:57:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/30/2022 20:58:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=165
05/30/2022 20:58:12 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.14546194546194544 on epoch=165
05/30/2022 20:58:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=166
05/30/2022 20:58:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
05/30/2022 20:58:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
05/30/2022 20:58:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=168
05/30/2022 20:58:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/30/2022 20:58:46 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.09004017984709498 on epoch=168
05/30/2022 20:58:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
05/30/2022 20:58:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=169
05/30/2022 20:59:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/30/2022 20:59:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=171
05/30/2022 20:59:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
05/30/2022 20:59:20 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.11988416988416986 on epoch=171
05/30/2022 20:59:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
05/30/2022 20:59:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=173
05/30/2022 20:59:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/30/2022 20:59:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/30/2022 20:59:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/30/2022 20:59:54 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.09134125636672326 on epoch=174
05/30/2022 20:59:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=175
05/30/2022 21:00:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=176
05/30/2022 21:00:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/30/2022 21:00:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=177
05/30/2022 21:00:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
05/30/2022 21:00:28 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.07932357043235704 on epoch=178
05/30/2022 21:00:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
05/30/2022 21:00:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=179
05/30/2022 21:00:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
05/30/2022 21:00:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=180
05/30/2022 21:00:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/30/2022 21:01:02 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.10697162404359656 on epoch=181
05/30/2022 21:01:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
05/30/2022 21:01:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/30/2022 21:01:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/30/2022 21:01:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
05/30/2022 21:01:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=184
05/30/2022 21:01:35 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.10688939313474627 on epoch=184
05/30/2022 21:01:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=184
05/30/2022 21:01:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
05/30/2022 21:01:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/30/2022 21:01:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=186
05/30/2022 21:01:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
05/30/2022 21:01:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:01:59 - INFO - __main__ - Printing 3 examples
05/30/2022 21:01:59 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 21:01:59 - INFO - __main__ - ['entailed']
05/30/2022 21:01:59 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 21:01:59 - INFO - __main__ - ['entailed']
05/30/2022 21:01:59 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 21:01:59 - INFO - __main__ - ['entailed']
05/30/2022 21:01:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:01:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:02:00 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 21:02:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:02:00 - INFO - __main__ - Printing 3 examples
05/30/2022 21:02:00 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 21:02:00 - INFO - __main__ - ['entailed']
05/30/2022 21:02:00 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 21:02:00 - INFO - __main__ - ['entailed']
05/30/2022 21:02:00 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 21:02:00 - INFO - __main__ - ['entailed']
05/30/2022 21:02:00 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:02:00 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:02:00 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 21:02:09 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.09983645983645982 on epoch=187
05/30/2022 21:02:09 - INFO - __main__ - save last model!
05/30/2022 21:02:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:02:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 21:02:09 - INFO - __main__ - Printing 3 examples
05/30/2022 21:02:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:02:09 - INFO - __main__ - ['entailed']
05/30/2022 21:02:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:02:09 - INFO - __main__ - ['entailed']
05/30/2022 21:02:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:02:09 - INFO - __main__ - ['entailed']
05/30/2022 21:02:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:02:18 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 21:02:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 21:02:19 - INFO - __main__ - Starting training!
05/30/2022 21:02:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:02:47 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 21:11:59 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_87_0.5_8_predictions.txt
05/30/2022 21:11:59 - INFO - __main__ - Classification-F1 on test data: 0.0079
05/30/2022 21:12:00 - INFO - __main__ - prefix=tab_fact_128_87, lr=0.5, bsz=8, dev_performance=0.49435776515441504, test_performance=0.007883576692224687
05/30/2022 21:12:00 - INFO - __main__ - Running ... prefix=tab_fact_128_87, lr=0.4, bsz=8 ...
05/30/2022 21:12:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:12:01 - INFO - __main__ - Printing 3 examples
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:12:01 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:12:01 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 21:12:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:12:01 - INFO - __main__ - Printing 3 examples
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 21:12:01 - INFO - __main__ - ['entailed']
05/30/2022 21:12:01 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:12:02 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:12:02 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 21:12:18 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 21:12:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 21:12:18 - INFO - __main__ - Starting training!
05/30/2022 21:12:24 - INFO - __main__ - Step 10 Global step 10 Train loss 4.76 on epoch=0
05/30/2022 21:12:28 - INFO - __main__ - Step 20 Global step 20 Train loss 2.96 on epoch=1
05/30/2022 21:12:33 - INFO - __main__ - Step 30 Global step 30 Train loss 2.31 on epoch=1
05/30/2022 21:12:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.64 on epoch=2
05/30/2022 21:12:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.07 on epoch=3
05/30/2022 21:12:52 - INFO - __main__ - Global step 50 Train loss 2.55 Classification-F1 0.22338568935427575 on epoch=3
05/30/2022 21:12:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22338568935427575 on epoch=3, global_step=50
05/30/2022 21:12:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.71 on epoch=3
05/30/2022 21:13:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=4
05/30/2022 21:13:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=4
05/30/2022 21:13:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=5
05/30/2022 21:13:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/30/2022 21:13:25 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 21:13:26 - INFO - __main__ - Saving model with best Classification-F1: 0.22338568935427575 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 21:13:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.33 on epoch=6
05/30/2022 21:13:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=7
05/30/2022 21:13:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=8
05/30/2022 21:13:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=8
05/30/2022 21:13:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=9
05/30/2022 21:13:59 - INFO - __main__ - Global step 150 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 21:14:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=9
05/30/2022 21:14:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=10
05/30/2022 21:14:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=11
05/30/2022 21:14:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=11
05/30/2022 21:14:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=12
05/30/2022 21:14:33 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 21:14:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=13
05/30/2022 21:14:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=13
05/30/2022 21:14:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=14
05/30/2022 21:14:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=14
05/30/2022 21:14:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=15
05/30/2022 21:15:07 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 21:15:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=16
05/30/2022 21:15:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=16
05/30/2022 21:15:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=17
05/30/2022 21:15:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=18
05/30/2022 21:15:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=18
05/30/2022 21:15:41 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 21:15:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=19
05/30/2022 21:15:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=19
05/30/2022 21:15:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=20
05/30/2022 21:15:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=21
05/30/2022 21:16:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=21
05/30/2022 21:16:15 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 21:16:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 21:16:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=23
05/30/2022 21:16:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=23
05/30/2022 21:16:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=24
05/30/2022 21:16:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=24
05/30/2022 21:16:49 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 21:16:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=25
05/30/2022 21:16:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=26
05/30/2022 21:17:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=26
05/30/2022 21:17:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=27
05/30/2022 21:17:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=28
05/30/2022 21:17:23 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 21:17:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=28
05/30/2022 21:17:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=29
05/30/2022 21:17:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=29
05/30/2022 21:17:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=30
05/30/2022 21:17:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=31
05/30/2022 21:17:56 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 21:18:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 21:18:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=32
05/30/2022 21:18:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=33
05/30/2022 21:18:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=33
05/30/2022 21:18:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=34
05/30/2022 21:18:30 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 21:18:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 21:18:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=35
05/30/2022 21:18:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=36
05/30/2022 21:18:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=36
05/30/2022 21:18:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=37
05/30/2022 21:19:04 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 21:19:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=38
05/30/2022 21:19:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=38
05/30/2022 21:19:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=39
05/30/2022 21:19:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=39
05/30/2022 21:19:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=40
05/30/2022 21:19:38 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 21:19:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=41
05/30/2022 21:19:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=41
05/30/2022 21:19:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=42
05/30/2022 21:19:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=43
05/30/2022 21:20:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=43
05/30/2022 21:20:11 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 21:20:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=44
05/30/2022 21:20:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=44
05/30/2022 21:20:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=45
05/30/2022 21:20:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=46
05/30/2022 21:20:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=46
05/30/2022 21:20:45 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 21:20:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=47
05/30/2022 21:20:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=48
05/30/2022 21:20:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=48
05/30/2022 21:21:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=49
05/30/2022 21:21:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/30/2022 21:21:19 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 21:21:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=50
05/30/2022 21:21:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/30/2022 21:21:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/30/2022 21:21:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=52
05/30/2022 21:21:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=53
05/30/2022 21:21:53 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 21:21:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/30/2022 21:22:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=54
05/30/2022 21:22:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=54
05/30/2022 21:22:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/30/2022 21:22:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/30/2022 21:22:26 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 21:22:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/30/2022 21:22:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=57
05/30/2022 21:22:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/30/2022 21:22:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=58
05/30/2022 21:22:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/30/2022 21:23:00 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 21:23:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/30/2022 21:23:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/30/2022 21:23:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 21:23:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/30/2022 21:23:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
05/30/2022 21:23:34 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 21:23:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/30/2022 21:23:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=63
05/30/2022 21:23:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=64
05/30/2022 21:23:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=64
05/30/2022 21:23:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/30/2022 21:24:08 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 21:24:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=66
05/30/2022 21:24:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/30/2022 21:24:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/30/2022 21:24:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/30/2022 21:24:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/30/2022 21:24:42 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 21:24:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=69
05/30/2022 21:24:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=69
05/30/2022 21:24:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=70
05/30/2022 21:25:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/30/2022 21:25:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=71
05/30/2022 21:25:16 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 21:25:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
05/30/2022 21:25:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 21:25:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 21:25:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/30/2022 21:25:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/30/2022 21:25:50 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 21:25:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=75
05/30/2022 21:25:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
05/30/2022 21:26:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=76
05/30/2022 21:26:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/30/2022 21:26:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/30/2022 21:26:24 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 21:26:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/30/2022 21:26:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/30/2022 21:26:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/30/2022 21:26:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=80
05/30/2022 21:26:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=81
05/30/2022 21:26:58 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 21:27:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=81
05/30/2022 21:27:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/30/2022 21:27:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=83
05/30/2022 21:27:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=83
05/30/2022 21:27:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
05/30/2022 21:27:32 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 21:27:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=84
05/30/2022 21:27:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/30/2022 21:27:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
05/30/2022 21:27:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
05/30/2022 21:27:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 21:28:06 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 21:28:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/30/2022 21:28:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=88
05/30/2022 21:28:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/30/2022 21:28:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
05/30/2022 21:28:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/30/2022 21:28:40 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 21:28:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/30/2022 21:28:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/30/2022 21:28:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/30/2022 21:28:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/30/2022 21:29:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/30/2022 21:29:14 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 21:29:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
05/30/2022 21:29:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/30/2022 21:29:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/30/2022 21:29:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=96
05/30/2022 21:29:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
05/30/2022 21:29:47 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 21:29:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=97
05/30/2022 21:29:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/30/2022 21:30:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/30/2022 21:30:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/30/2022 21:30:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
05/30/2022 21:30:21 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 21:30:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
05/30/2022 21:30:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/30/2022 21:30:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/30/2022 21:30:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=102
05/30/2022 21:30:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 21:30:55 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 21:31:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/30/2022 21:31:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=104
05/30/2022 21:31:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=104
05/30/2022 21:31:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=105
05/30/2022 21:31:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=106
05/30/2022 21:31:29 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.34485289741504155 on epoch=106
05/30/2022 21:31:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34485289741504155 on epoch=106, global_step=1700
05/30/2022 21:31:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
05/30/2022 21:31:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/30/2022 21:31:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/30/2022 21:31:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
05/30/2022 21:31:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/30/2022 21:32:03 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.5138231631382316 on epoch=109
05/30/2022 21:32:03 - INFO - __main__ - Saving model with best Classification-F1: 0.34485289741504155 -> 0.5138231631382316 on epoch=109, global_step=1750
05/30/2022 21:32:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/30/2022 21:32:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=110
05/30/2022 21:32:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 21:32:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=111
05/30/2022 21:32:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
05/30/2022 21:32:37 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.5234375 on epoch=112
05/30/2022 21:32:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5138231631382316 -> 0.5234375 on epoch=112, global_step=1800
05/30/2022 21:32:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
05/30/2022 21:32:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 21:32:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/30/2022 21:32:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=114
05/30/2022 21:32:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/30/2022 21:33:10 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.4931496000633563 on epoch=115
05/30/2022 21:33:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=116
05/30/2022 21:33:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/30/2022 21:33:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
05/30/2022 21:33:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
05/30/2022 21:33:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=118
05/30/2022 21:33:44 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.36304897101085887 on epoch=118
05/30/2022 21:33:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/30/2022 21:33:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 21:33:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=120
05/30/2022 21:34:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
05/30/2022 21:34:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/30/2022 21:34:18 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.4996742671009772 on epoch=121
05/30/2022 21:34:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=122
05/30/2022 21:34:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/30/2022 21:34:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/30/2022 21:34:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=124
05/30/2022 21:34:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
05/30/2022 21:34:51 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.5012592064623426 on epoch=124
05/30/2022 21:34:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/30/2022 21:35:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=126
05/30/2022 21:35:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=126
05/30/2022 21:35:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
05/30/2022 21:35:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
05/30/2022 21:35:25 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.5089308176100629 on epoch=128
05/30/2022 21:35:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 21:35:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=129
05/30/2022 21:35:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=129
05/30/2022 21:35:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/30/2022 21:35:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 21:35:58 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.512273464439231 on epoch=131
05/30/2022 21:36:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=131
05/30/2022 21:36:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=132
05/30/2022 21:36:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/30/2022 21:36:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/30/2022 21:36:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/30/2022 21:36:32 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.48550832959622636 on epoch=134
05/30/2022 21:36:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/30/2022 21:36:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=135
05/30/2022 21:36:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
05/30/2022 21:36:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/30/2022 21:36:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/30/2022 21:37:06 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.5153587786259541 on epoch=137
05/30/2022 21:37:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/30/2022 21:37:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/30/2022 21:37:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/30/2022 21:37:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=139
05/30/2022 21:37:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/30/2022 21:37:40 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.47848230201171377 on epoch=140
05/30/2022 21:37:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
05/30/2022 21:37:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=141
05/30/2022 21:37:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
05/30/2022 21:37:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/30/2022 21:38:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/30/2022 21:38:13 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.4895432413633203 on epoch=143
05/30/2022 21:38:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/30/2022 21:38:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/30/2022 21:38:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
05/30/2022 21:38:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=146
05/30/2022 21:38:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=146
05/30/2022 21:38:47 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.515625 on epoch=146
05/30/2022 21:38:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/30/2022 21:38:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/30/2022 21:39:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/30/2022 21:39:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=149
05/30/2022 21:39:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/30/2022 21:39:21 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.26915985687723626 on epoch=149
05/30/2022 21:39:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
05/30/2022 21:39:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/30/2022 21:39:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/30/2022 21:39:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
05/30/2022 21:39:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
05/30/2022 21:39:55 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.3282491944146079 on epoch=153
05/30/2022 21:40:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/30/2022 21:40:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
05/30/2022 21:40:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=154
05/30/2022 21:40:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
05/30/2022 21:40:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=156
05/30/2022 21:40:29 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.14442454097626514 on epoch=156
05/30/2022 21:40:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/30/2022 21:40:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
05/30/2022 21:40:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/30/2022 21:40:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/30/2022 21:40:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
05/30/2022 21:41:03 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.17528406074121272 on epoch=159
05/30/2022 21:41:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/30/2022 21:41:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/30/2022 21:41:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
05/30/2022 21:41:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
05/30/2022 21:41:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
05/30/2022 21:41:37 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.14304436188761896 on epoch=162
05/30/2022 21:41:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/30/2022 21:41:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/30/2022 21:41:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=164
05/30/2022 21:41:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
05/30/2022 21:42:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
05/30/2022 21:42:11 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.11523159988633136 on epoch=165
05/30/2022 21:42:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=166
05/30/2022 21:42:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
05/30/2022 21:42:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/30/2022 21:42:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=168
05/30/2022 21:42:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/30/2022 21:42:45 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.11413435049798684 on epoch=168
05/30/2022 21:42:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
05/30/2022 21:42:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/30/2022 21:42:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
05/30/2022 21:43:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
05/30/2022 21:43:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
05/30/2022 21:43:19 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.11705060771785997 on epoch=171
05/30/2022 21:43:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
05/30/2022 21:43:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=173
05/30/2022 21:43:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/30/2022 21:43:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=174
05/30/2022 21:43:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=174
05/30/2022 21:43:53 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.09673023708918783 on epoch=174
05/30/2022 21:43:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/30/2022 21:44:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
05/30/2022 21:44:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/30/2022 21:44:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
05/30/2022 21:44:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
05/30/2022 21:44:27 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.0923664040133851 on epoch=178
05/30/2022 21:44:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=178
05/30/2022 21:44:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=179
05/30/2022 21:44:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=179
05/30/2022 21:44:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=180
05/30/2022 21:44:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=181
05/30/2022 21:45:01 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.07913797865919157 on epoch=181
05/30/2022 21:45:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=181
05/30/2022 21:45:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=182
05/30/2022 21:45:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
05/30/2022 21:45:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
05/30/2022 21:45:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=184
05/30/2022 21:45:34 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.07480017618092141 on epoch=184
05/30/2022 21:45:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=184
05/30/2022 21:45:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
05/30/2022 21:45:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/30/2022 21:45:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=186
05/30/2022 21:45:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=187
05/30/2022 21:45:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:45:58 - INFO - __main__ - Printing 3 examples
05/30/2022 21:45:58 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 21:45:58 - INFO - __main__ - ['entailed']
05/30/2022 21:45:58 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 21:45:58 - INFO - __main__ - ['entailed']
05/30/2022 21:45:58 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 21:45:58 - INFO - __main__ - ['entailed']
05/30/2022 21:45:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:45:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:45:59 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 21:45:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:45:59 - INFO - __main__ - Printing 3 examples
05/30/2022 21:45:59 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 21:45:59 - INFO - __main__ - ['entailed']
05/30/2022 21:45:59 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 21:45:59 - INFO - __main__ - ['entailed']
05/30/2022 21:45:59 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 21:45:59 - INFO - __main__ - ['entailed']
05/30/2022 21:45:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:45:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:46:00 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 21:46:08 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.08151219437699485 on epoch=187
05/30/2022 21:46:08 - INFO - __main__ - save last model!
05/30/2022 21:46:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 21:46:08 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 21:46:08 - INFO - __main__ - Printing 3 examples
05/30/2022 21:46:08 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:46:08 - INFO - __main__ - ['entailed']
05/30/2022 21:46:08 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:46:08 - INFO - __main__ - ['entailed']
05/30/2022 21:46:08 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 21:46:08 - INFO - __main__ - ['entailed']
05/30/2022 21:46:08 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:46:16 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 21:46:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 21:46:17 - INFO - __main__ - Starting training!
05/30/2022 21:46:33 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:46:45 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 21:55:52 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_87_0.4_8_predictions.txt
05/30/2022 21:55:53 - INFO - __main__ - Classification-F1 on test data: 0.0085
05/30/2022 21:55:53 - INFO - __main__ - prefix=tab_fact_128_87, lr=0.4, bsz=8, dev_performance=0.5234375, test_performance=0.008464849537005364
05/30/2022 21:55:53 - INFO - __main__ - Running ... prefix=tab_fact_128_87, lr=0.3, bsz=8 ...
05/30/2022 21:55:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:55:54 - INFO - __main__ - Printing 3 examples
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:55:54 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:55:54 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 21:55:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 21:55:54 - INFO - __main__ - Printing 3 examples
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 21:55:54 - INFO - __main__ - ['entailed']
05/30/2022 21:55:54 - INFO - __main__ - Tokenizing Input ...
05/30/2022 21:55:55 - INFO - __main__ - Tokenizing Output ...
05/30/2022 21:55:55 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 21:56:12 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 21:56:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 21:56:13 - INFO - __main__ - Starting training!
05/30/2022 21:56:18 - INFO - __main__ - Step 10 Global step 10 Train loss 5.34 on epoch=0
05/30/2022 21:56:23 - INFO - __main__ - Step 20 Global step 20 Train loss 3.48 on epoch=1
05/30/2022 21:56:27 - INFO - __main__ - Step 30 Global step 30 Train loss 2.64 on epoch=1
05/30/2022 21:56:32 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=2
05/30/2022 21:56:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.61 on epoch=3
05/30/2022 21:56:47 - INFO - __main__ - Global step 50 Train loss 3.03 Classification-F1 0.05276742185259535 on epoch=3
05/30/2022 21:56:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.05276742185259535 on epoch=3, global_step=50
05/30/2022 21:56:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.35 on epoch=3
05/30/2022 21:56:56 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=4
05/30/2022 21:57:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=4
05/30/2022 21:57:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=5
05/30/2022 21:57:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/30/2022 21:57:21 - INFO - __main__ - Global step 100 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=6
05/30/2022 21:57:21 - INFO - __main__ - Saving model with best Classification-F1: 0.05276742185259535 -> 0.3333333333333333 on epoch=6, global_step=100
05/30/2022 21:57:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=6
05/30/2022 21:57:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=7
05/30/2022 21:57:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=8
05/30/2022 21:57:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=8
05/30/2022 21:57:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=9
05/30/2022 21:57:53 - INFO - __main__ - Global step 150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 21:57:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=9
05/30/2022 21:58:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=10
05/30/2022 21:58:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=11
05/30/2022 21:58:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=11
05/30/2022 21:58:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=12
05/30/2022 21:58:26 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 21:58:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=13
05/30/2022 21:58:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=13
05/30/2022 21:58:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=14
05/30/2022 21:58:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=14
05/30/2022 21:58:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=15
05/30/2022 21:59:00 - INFO - __main__ - Global step 250 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 21:59:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=16
05/30/2022 21:59:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=16
05/30/2022 21:59:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=17
05/30/2022 21:59:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=18
05/30/2022 21:59:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=18
05/30/2022 21:59:32 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 21:59:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=19
05/30/2022 21:59:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=19
05/30/2022 21:59:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=20
05/30/2022 21:59:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=21
05/30/2022 21:59:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=21
05/30/2022 22:00:06 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 22:00:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=22
05/30/2022 22:00:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=23
05/30/2022 22:00:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=23
05/30/2022 22:00:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=24
05/30/2022 22:00:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=24
05/30/2022 22:00:40 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 22:00:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=25
05/30/2022 22:00:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=26
05/30/2022 22:00:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.29 on epoch=26
05/30/2022 22:00:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=27
05/30/2022 22:01:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 22:01:14 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 22:01:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=28
05/30/2022 22:01:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=29
05/30/2022 22:01:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=29
05/30/2022 22:01:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=30
05/30/2022 22:01:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=31
05/30/2022 22:01:47 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 22:01:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 22:01:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=32
05/30/2022 22:02:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=33
05/30/2022 22:02:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=33
05/30/2022 22:02:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=34
05/30/2022 22:02:21 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 22:02:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=34
05/30/2022 22:02:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=35
05/30/2022 22:02:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=36
05/30/2022 22:02:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=36
05/30/2022 22:02:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=37
05/30/2022 22:02:55 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 22:03:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=38
05/30/2022 22:03:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=38
05/30/2022 22:03:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=39
05/30/2022 22:03:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=39
05/30/2022 22:03:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=40
05/30/2022 22:03:29 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 22:03:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=41
05/30/2022 22:03:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=41
05/30/2022 22:03:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=42
05/30/2022 22:03:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=43
05/30/2022 22:03:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=43
05/30/2022 22:04:02 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 22:04:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/30/2022 22:04:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=44
05/30/2022 22:04:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=45
05/30/2022 22:04:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=46
05/30/2022 22:04:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/30/2022 22:04:36 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 22:04:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=47
05/30/2022 22:04:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=48
05/30/2022 22:04:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/30/2022 22:04:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=49
05/30/2022 22:04:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=49
05/30/2022 22:05:10 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 22:05:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=50
05/30/2022 22:05:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=51
05/30/2022 22:05:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=51
05/30/2022 22:05:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/30/2022 22:05:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=53
05/30/2022 22:05:44 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 22:05:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=53
05/30/2022 22:05:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=54
05/30/2022 22:05:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/30/2022 22:06:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/30/2022 22:06:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/30/2022 22:06:18 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 22:06:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=56
05/30/2022 22:06:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/30/2022 22:06:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=58
05/30/2022 22:06:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=58
05/30/2022 22:06:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/30/2022 22:06:52 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 22:06:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/30/2022 22:07:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=60
05/30/2022 22:07:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/30/2022 22:07:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=61
05/30/2022 22:07:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
05/30/2022 22:07:27 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 22:07:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=63
05/30/2022 22:07:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/30/2022 22:07:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/30/2022 22:07:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=64
05/30/2022 22:07:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/30/2022 22:08:01 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 22:08:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=66
05/30/2022 22:08:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=66
05/30/2022 22:08:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/30/2022 22:08:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/30/2022 22:08:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/30/2022 22:08:35 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 22:08:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
05/30/2022 22:08:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=69
05/30/2022 22:08:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/30/2022 22:08:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=71
05/30/2022 22:08:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/30/2022 22:09:09 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 22:09:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
05/30/2022 22:09:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/30/2022 22:09:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/30/2022 22:09:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=74
05/30/2022 22:09:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/30/2022 22:09:43 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 22:09:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/30/2022 22:09:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/30/2022 22:09:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=76
05/30/2022 22:10:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/30/2022 22:10:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
05/30/2022 22:10:17 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 22:10:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/30/2022 22:10:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
05/30/2022 22:10:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/30/2022 22:10:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=80
05/30/2022 22:10:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=81
05/30/2022 22:10:50 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 22:10:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/30/2022 22:10:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=82
05/30/2022 22:11:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/30/2022 22:11:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=83
05/30/2022 22:11:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=84
05/30/2022 22:11:24 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 22:11:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/30/2022 22:11:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
05/30/2022 22:11:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/30/2022 22:11:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=86
05/30/2022 22:11:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=87
05/30/2022 22:11:58 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 22:12:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/30/2022 22:12:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=88
05/30/2022 22:12:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=89
05/30/2022 22:12:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
05/30/2022 22:12:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/30/2022 22:12:32 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 22:12:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/30/2022 22:12:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/30/2022 22:12:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/30/2022 22:12:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
05/30/2022 22:12:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/30/2022 22:13:05 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 22:13:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=94
05/30/2022 22:13:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 22:13:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/30/2022 22:13:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/30/2022 22:13:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/30/2022 22:13:39 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 22:13:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/30/2022 22:13:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/30/2022 22:13:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
05/30/2022 22:13:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=99
05/30/2022 22:14:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/30/2022 22:14:13 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 22:14:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/30/2022 22:14:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/30/2022 22:14:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/30/2022 22:14:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=102
05/30/2022 22:14:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 22:14:46 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 22:14:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/30/2022 22:14:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
05/30/2022 22:15:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=104
05/30/2022 22:15:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 22:15:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
05/30/2022 22:15:20 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 22:15:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
05/30/2022 22:15:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/30/2022 22:15:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 22:15:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=108
05/30/2022 22:15:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
05/30/2022 22:15:54 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 22:15:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/30/2022 22:16:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
05/30/2022 22:16:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=111
05/30/2022 22:16:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=111
05/30/2022 22:16:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
05/30/2022 22:16:27 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 22:16:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/30/2022 22:16:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=113
05/30/2022 22:16:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
05/30/2022 22:16:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=114
05/30/2022 22:16:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=115
05/30/2022 22:17:01 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=115
05/30/2022 22:17:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=116
05/30/2022 22:17:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
05/30/2022 22:17:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/30/2022 22:17:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=118
05/30/2022 22:17:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
05/30/2022 22:17:35 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=118
05/30/2022 22:17:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=119
05/30/2022 22:17:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 22:17:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=120
05/30/2022 22:17:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=121
05/30/2022 22:17:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/30/2022 22:18:09 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=121
05/30/2022 22:18:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=122
05/30/2022 22:18:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
05/30/2022 22:18:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/30/2022 22:18:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=124
05/30/2022 22:18:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=124
05/30/2022 22:18:42 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 22:18:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
05/30/2022 22:18:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=126
05/30/2022 22:18:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/30/2022 22:19:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
05/30/2022 22:19:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=128
05/30/2022 22:19:16 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=128
05/30/2022 22:19:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 22:19:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/30/2022 22:19:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/30/2022 22:19:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/30/2022 22:19:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
05/30/2022 22:19:50 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=131
05/30/2022 22:19:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=131
05/30/2022 22:19:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=132
05/30/2022 22:20:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=133
05/30/2022 22:20:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=133
05/30/2022 22:20:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.20 on epoch=134
05/30/2022 22:20:23 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=134
05/30/2022 22:20:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/30/2022 22:20:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/30/2022 22:20:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.23 on epoch=136
05/30/2022 22:20:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
05/30/2022 22:20:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/30/2022 22:20:57 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=137
05/30/2022 22:21:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=138
05/30/2022 22:21:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/30/2022 22:21:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=139
05/30/2022 22:21:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=139
05/30/2022 22:21:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/30/2022 22:21:30 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=140
05/30/2022 22:21:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=141
05/30/2022 22:21:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=141
05/30/2022 22:21:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=142
05/30/2022 22:21:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
05/30/2022 22:21:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
05/30/2022 22:22:04 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=143
05/30/2022 22:22:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
05/30/2022 22:22:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=144
05/30/2022 22:22:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=145
05/30/2022 22:22:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/30/2022 22:22:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
05/30/2022 22:22:37 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=146
05/30/2022 22:22:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/30/2022 22:22:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=148
05/30/2022 22:22:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=148
05/30/2022 22:22:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=149
05/30/2022 22:23:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/30/2022 22:23:11 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.3280839895013123 on epoch=149
05/30/2022 22:23:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/30/2022 22:23:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
05/30/2022 22:23:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=151
05/30/2022 22:23:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=152
05/30/2022 22:23:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/30/2022 22:23:45 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.3328595119639896 on epoch=153
05/30/2022 22:23:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/30/2022 22:23:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=154
05/30/2022 22:23:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=154
05/30/2022 22:24:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=155
05/30/2022 22:24:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.20 on epoch=156
05/30/2022 22:24:18 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.34723114355231144 on epoch=156
05/30/2022 22:24:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34723114355231144 on epoch=156, global_step=2500
05/30/2022 22:24:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/30/2022 22:24:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
05/30/2022 22:24:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
05/30/2022 22:24:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/30/2022 22:24:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=159
05/30/2022 22:24:52 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.46476939399613054 on epoch=159
05/30/2022 22:24:52 - INFO - __main__ - Saving model with best Classification-F1: 0.34723114355231144 -> 0.46476939399613054 on epoch=159, global_step=2550
05/30/2022 22:24:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/30/2022 22:25:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/30/2022 22:25:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/30/2022 22:25:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/30/2022 22:25:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=162
05/30/2022 22:25:26 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.4375889597128535 on epoch=162
05/30/2022 22:25:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/30/2022 22:25:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
05/30/2022 22:25:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=164
05/30/2022 22:25:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=164
05/30/2022 22:25:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=165
05/30/2022 22:25:59 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.46218487394957986 on epoch=165
05/30/2022 22:26:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.22 on epoch=166
05/30/2022 22:26:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=166
05/30/2022 22:26:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/30/2022 22:26:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=168
05/30/2022 22:26:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
05/30/2022 22:26:33 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.475 on epoch=168
05/30/2022 22:26:33 - INFO - __main__ - Saving model with best Classification-F1: 0.46476939399613054 -> 0.475 on epoch=168, global_step=2700
05/30/2022 22:26:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=169
05/30/2022 22:26:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=169
05/30/2022 22:26:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=170
05/30/2022 22:26:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=171
05/30/2022 22:26:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=171
05/30/2022 22:27:07 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.4116200758992997 on epoch=171
05/30/2022 22:27:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
05/30/2022 22:27:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/30/2022 22:27:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=173
05/30/2022 22:27:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
05/30/2022 22:27:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=174
05/30/2022 22:27:41 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.4722616702551652 on epoch=174
05/30/2022 22:27:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=175
05/30/2022 22:27:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=176
05/30/2022 22:27:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=176
05/30/2022 22:27:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=177
05/30/2022 22:28:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=178
05/30/2022 22:28:15 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.45257861635220126 on epoch=178
05/30/2022 22:28:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
05/30/2022 22:28:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=179
05/30/2022 22:28:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/30/2022 22:28:33 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=180
05/30/2022 22:28:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=181
05/30/2022 22:28:50 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.4544106745737583 on epoch=181
05/30/2022 22:28:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=181
05/30/2022 22:28:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=182
05/30/2022 22:29:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=183
05/30/2022 22:29:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=183
05/30/2022 22:29:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=184
05/30/2022 22:29:24 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.5090141925584963 on epoch=184
05/30/2022 22:29:24 - INFO - __main__ - Saving model with best Classification-F1: 0.475 -> 0.5090141925584963 on epoch=184, global_step=2950
05/30/2022 22:29:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=184
05/30/2022 22:29:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=185
05/30/2022 22:29:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=186
05/30/2022 22:29:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
05/30/2022 22:29:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=187
05/30/2022 22:29:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 22:29:48 - INFO - __main__ - Printing 3 examples
05/30/2022 22:29:48 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 22:29:48 - INFO - __main__ - ['entailed']
05/30/2022 22:29:48 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 22:29:48 - INFO - __main__ - ['entailed']
05/30/2022 22:29:48 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 22:29:48 - INFO - __main__ - ['entailed']
05/30/2022 22:29:48 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:29:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:29:49 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 22:29:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 22:29:49 - INFO - __main__ - Printing 3 examples
05/30/2022 22:29:49 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 22:29:49 - INFO - __main__ - ['entailed']
05/30/2022 22:29:49 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 22:29:49 - INFO - __main__ - ['entailed']
05/30/2022 22:29:49 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 22:29:49 - INFO - __main__ - ['entailed']
05/30/2022 22:29:49 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:29:49 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:29:50 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 22:29:58 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.46973532796317596 on epoch=187
05/30/2022 22:29:58 - INFO - __main__ - save last model!
05/30/2022 22:29:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 22:29:58 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 22:29:58 - INFO - __main__ - Printing 3 examples
05/30/2022 22:29:58 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 22:29:58 - INFO - __main__ - ['entailed']
05/30/2022 22:29:58 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 22:29:58 - INFO - __main__ - ['entailed']
05/30/2022 22:29:58 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 22:29:58 - INFO - __main__ - ['entailed']
05/30/2022 22:29:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:30:08 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 22:30:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 22:30:09 - INFO - __main__ - Starting training!
05/30/2022 22:30:23 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:30:35 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 22:39:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_87_0.3_8_predictions.txt
05/30/2022 22:39:57 - INFO - __main__ - Classification-F1 on test data: 0.1259
05/30/2022 22:39:57 - INFO - __main__ - prefix=tab_fact_128_87, lr=0.3, bsz=8, dev_performance=0.5090141925584963, test_performance=0.12590451317945855
05/30/2022 22:39:57 - INFO - __main__ - Running ... prefix=tab_fact_128_87, lr=0.2, bsz=8 ...
05/30/2022 22:39:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 22:39:58 - INFO - __main__ - Printing 3 examples
05/30/2022 22:39:58 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
05/30/2022 22:39:58 - INFO - __main__ - ['entailed']
05/30/2022 22:39:58 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
05/30/2022 22:39:58 - INFO - __main__ - ['entailed']
05/30/2022 22:39:58 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
05/30/2022 22:39:58 - INFO - __main__ - ['entailed']
05/30/2022 22:39:58 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:39:58 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:39:59 - INFO - __main__ - Loaded 256 examples from train data
05/30/2022 22:39:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/30/2022 22:39:59 - INFO - __main__ - Printing 3 examples
05/30/2022 22:39:59 - INFO - __main__ -  [tab_fact] statement: florian trimpl , of position 6 (12pts) , score a 5 (4 in 32.66s) in the last 6 atlas stone event [SEP] table_caption: 2009 world 's strongest man [SEP] table_text: position#name#nationality#event 1 medley#event 2 truck pull#event 3 dead lift#event 4 fingals fingers#event 5 keg toss#event 6 atlas stones [n] 3 (26pts)#andrus murumets#estonia#1 (34.38s)#4 (47.07s)#2 (6 in 30.89s)#4 (4 in 45.45s)#2 = (6 in 1 m15s)#2 (5 in 30.03s) [n] 4 (18pts)#christian savoie#canada#3 (37.78s)#2 (45.91s)#4 (6 in 38.79s)#6 (3 in 22.83s)#6 (4 in 1 m15s)#3 (5 in 32.85s) [n] 2 (26pts)#dave ostlund#united states#5 (42.15s)#3 (46.28s)#3 (6 in 30.96s)#1 (5 in 33.84s)#2 = (6 in 1 m15s)#1 (5 in 26.66s) [n] 6 (12pts)#florian trimpl#germany#4 (41.98s)#5 (57.43s)#6 (5 in 32.74s)#5 (4 in 49.85s)#5 (5 in 1 m15s)#5 (4 in 32.66s) [n] 5 (15pts)#jarek dymek#poland#6 (46.40s)#6 (16.2 m)#5 (5 in 24.30s)#3 (4 in 32.47s)#1 (7 in 1 m15s)#6 (4 in 34.49s) [n] 
05/30/2022 22:39:59 - INFO - __main__ - ['entailed']
05/30/2022 22:39:59 - INFO - __main__ -  [tab_fact] statement: the ferrari 048 be drive by michael schumacher , mika salo , and eddie irvine [SEP] table_caption: 1999 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#mika häkkinen#all [n] west mclaren mercedes#mclaren - mercedes#mp4 / 14#mercedes fo110h#b#david coulthard#all [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#michael schumacher#1 - 8 , 15 - 16 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#mika salo#9 - 14 [n] scuderia ferrari marlboro#ferrari#f399#ferrari 048#b#eddie irvine#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#alessandro zanardi#all [n] winfield williams#williams - supertec#fw21#supertec fb01#b#ralf schumacher#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#damon hill#all [n] benson and hedges jordan#jordan - mugen - honda#199#mugen - honda mf - 301 hd#b#heinz - harald frentzen#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#giancarlo fisichella#all [n] mild seven benetton playlife#benetton - playlife#b199#playlife fb01#b#alexander wurz#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#jean alesi#all [n] red bull sauber petronas#sauber - petronas#c18#petronas spe - 03a#b#pedro diniz#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#pedro de la rosa#all [n] repsol arrows#arrows#a20#arrows t2 - f1#b#toranosuke takagi#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#rubens barrichello#all [n] stewart ford#stewart - ford#sf3#ford cr - 1#b#johnny herbert#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#olivier panis#all [n] gauloises prost peugeot#prost - peugeot#ap02#peugeot a18#b#jarno trulli#all [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#luca badoer#1 , 3 - 16 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#stéphane sarrazin#2 [n] fondmetal minardi team#minardi - ford#m01#ford vjm1 zetec - r ford vjm2 zetec - r#b#marc gené#all [n] british american racing#bar - supertec#01#supertec fb01#b#jacques villeneuve#all [n] british american racing#bar - supertec#01#supertec fb01#b#ricardo zonta#1 - 2 , 6 - 16 [n] british american racing#bar - supertec#01#supertec fb01#b#mika salo#3 - 5 [n] 
05/30/2022 22:39:59 - INFO - __main__ - ['entailed']
05/30/2022 22:39:59 - INFO - __main__ -  [tab_fact] statement: jai waetford do worse than samantha jade in their respective season [SEP] table_caption: the x factor (australian tv series) [SEP] table_text: series#start#finish#winner#runner - up#third place#winning mentor#main host [n] one#6 february 2005#15 may 2005#random#russell gooley#vince harder#mark holden#daniel macpherson [n] two#30 august 2010#22 november 2010#altiyan childs#sally chatfield#andrew lawson#ronan keating#luke jacobz [n] three#29 august 2011#22 november 2011#reece mastin#andrew wishart#johnny ruffo#guy sebastian#luke jacobz [n] four#20 august 2012#20 november 2012#samantha jade#jason owen#the collective#guy sebastian#luke jacobz [n] five#29 july 2013#28 october 2013#dami im#taylor henderson#jai waetford#dannii minogue#luke jacobz [n] six#2014#2014#tba#tba#tba#tba#luke jacobz [n] 
05/30/2022 22:39:59 - INFO - __main__ - ['entailed']
05/30/2022 22:39:59 - INFO - __main__ - Tokenizing Input ...
05/30/2022 22:39:59 - INFO - __main__ - Tokenizing Output ...
05/30/2022 22:39:59 - INFO - __main__ - Loaded 256 examples from dev data
05/30/2022 22:40:18 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 22:40:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 22:40:19 - INFO - __main__ - Starting training!
05/30/2022 22:40:24 - INFO - __main__ - Step 10 Global step 10 Train loss 5.40 on epoch=0
05/30/2022 22:40:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.80 on epoch=1
05/30/2022 22:40:33 - INFO - __main__ - Step 30 Global step 30 Train loss 3.07 on epoch=1
05/30/2022 22:40:37 - INFO - __main__ - Step 40 Global step 40 Train loss 2.64 on epoch=2
05/30/2022 22:40:42 - INFO - __main__ - Step 50 Global step 50 Train loss 2.21 on epoch=3
05/30/2022 22:40:53 - INFO - __main__ - Global step 50 Train loss 3.42 Classification-F1 0.0 on epoch=3
05/30/2022 22:40:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/30/2022 22:40:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.88 on epoch=3
05/30/2022 22:41:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.67 on epoch=4
05/30/2022 22:41:06 - INFO - __main__ - Step 80 Global step 80 Train loss 1.37 on epoch=4
05/30/2022 22:41:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.12 on epoch=5
05/30/2022 22:41:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=6
05/30/2022 22:41:27 - INFO - __main__ - Global step 100 Train loss 1.40 Classification-F1 0.1332102564102564 on epoch=6
05/30/2022 22:41:27 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.1332102564102564 on epoch=6, global_step=100
05/30/2022 22:41:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=6
05/30/2022 22:41:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=7
05/30/2022 22:41:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=8
05/30/2022 22:41:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
05/30/2022 22:41:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
05/30/2022 22:42:00 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=9
05/30/2022 22:42:00 - INFO - __main__ - Saving model with best Classification-F1: 0.1332102564102564 -> 0.3333333333333333 on epoch=9, global_step=150
05/30/2022 22:42:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=9
05/30/2022 22:42:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
05/30/2022 22:42:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=11
05/30/2022 22:42:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=11
05/30/2022 22:42:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=12
05/30/2022 22:42:33 - INFO - __main__ - Global step 200 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=12
05/30/2022 22:42:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=13
05/30/2022 22:42:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=13
05/30/2022 22:42:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=14
05/30/2022 22:42:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=14
05/30/2022 22:42:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=15
05/30/2022 22:43:05 - INFO - __main__ - Global step 250 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=15
05/30/2022 22:43:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
05/30/2022 22:43:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=16
05/30/2022 22:43:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=17
05/30/2022 22:43:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=18
05/30/2022 22:43:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=18
05/30/2022 22:43:39 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=18
05/30/2022 22:43:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=19
05/30/2022 22:43:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=19
05/30/2022 22:43:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=20
05/30/2022 22:43:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=21
05/30/2022 22:44:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=21
05/30/2022 22:44:12 - INFO - __main__ - Global step 350 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=21
05/30/2022 22:44:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=22
05/30/2022 22:44:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=23
05/30/2022 22:44:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=23
05/30/2022 22:44:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=24
05/30/2022 22:44:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=24
05/30/2022 22:44:46 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
05/30/2022 22:44:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=25
05/30/2022 22:44:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=26
05/30/2022 22:44:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=26
05/30/2022 22:45:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=27
05/30/2022 22:45:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=28
05/30/2022 22:45:19 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=28
05/30/2022 22:45:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=28
05/30/2022 22:45:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=29
05/30/2022 22:45:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=29
05/30/2022 22:45:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=30
05/30/2022 22:45:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=31
05/30/2022 22:45:53 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
05/30/2022 22:45:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=31
05/30/2022 22:46:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=32
05/30/2022 22:46:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=33
05/30/2022 22:46:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=33
05/30/2022 22:46:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=34
05/30/2022 22:46:26 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=34
05/30/2022 22:46:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=34
05/30/2022 22:46:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=35
05/30/2022 22:46:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=36
05/30/2022 22:46:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=36
05/30/2022 22:46:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=37
05/30/2022 22:47:00 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
05/30/2022 22:47:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=38
05/30/2022 22:47:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=38
05/30/2022 22:47:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=39
05/30/2022 22:47:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=39
05/30/2022 22:47:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=40
05/30/2022 22:47:33 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=40
05/30/2022 22:47:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=41
05/30/2022 22:47:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=41
05/30/2022 22:47:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=42
05/30/2022 22:47:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/30/2022 22:47:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=43
05/30/2022 22:48:07 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
05/30/2022 22:48:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=44
05/30/2022 22:48:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=44
05/30/2022 22:48:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=45
05/30/2022 22:48:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=46
05/30/2022 22:48:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=46
05/30/2022 22:48:40 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=46
05/30/2022 22:48:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=47
05/30/2022 22:48:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/30/2022 22:48:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/30/2022 22:48:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=49
05/30/2022 22:49:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=49
05/30/2022 22:49:14 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
05/30/2022 22:49:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=50
05/30/2022 22:49:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=51
05/30/2022 22:49:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/30/2022 22:49:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=52
05/30/2022 22:49:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=53
05/30/2022 22:49:47 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=53
05/30/2022 22:49:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/30/2022 22:49:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/30/2022 22:50:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/30/2022 22:50:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/30/2022 22:50:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/30/2022 22:50:21 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
05/30/2022 22:50:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=56
05/30/2022 22:50:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=57
05/30/2022 22:50:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/30/2022 22:50:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/30/2022 22:50:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/30/2022 22:50:55 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=59
05/30/2022 22:50:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/30/2022 22:51:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/30/2022 22:51:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=61
05/30/2022 22:51:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=61
05/30/2022 22:51:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=62
05/30/2022 22:51:28 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
05/30/2022 22:51:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/30/2022 22:51:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/30/2022 22:51:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=64
05/30/2022 22:51:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
05/30/2022 22:51:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=65
05/30/2022 22:52:02 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=65
05/30/2022 22:52:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=66
05/30/2022 22:52:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
05/30/2022 22:52:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=67
05/30/2022 22:52:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=68
05/30/2022 22:52:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=68
05/30/2022 22:52:36 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
05/30/2022 22:52:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=69
05/30/2022 22:52:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/30/2022 22:52:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=70
05/30/2022 22:52:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=71
05/30/2022 22:52:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=71
05/30/2022 22:53:09 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=71
05/30/2022 22:53:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/30/2022 22:53:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/30/2022 22:53:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=73
05/30/2022 22:53:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/30/2022 22:53:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/30/2022 22:53:43 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
05/30/2022 22:53:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/30/2022 22:53:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=76
05/30/2022 22:53:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/30/2022 22:54:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=77
05/30/2022 22:54:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=78
05/30/2022 22:54:17 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=78
05/30/2022 22:54:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=78
05/30/2022 22:54:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/30/2022 22:54:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/30/2022 22:54:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/30/2022 22:54:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=81
05/30/2022 22:54:50 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
05/30/2022 22:54:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/30/2022 22:54:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/30/2022 22:55:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
05/30/2022 22:55:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=83
05/30/2022 22:55:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/30/2022 22:55:24 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=84
05/30/2022 22:55:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/30/2022 22:55:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/30/2022 22:55:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
05/30/2022 22:55:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=86
05/30/2022 22:55:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/30/2022 22:55:58 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
05/30/2022 22:56:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=88
05/30/2022 22:56:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
05/30/2022 22:56:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/30/2022 22:56:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/30/2022 22:56:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/30/2022 22:56:32 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=90
05/30/2022 22:56:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=91
05/30/2022 22:56:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/30/2022 22:56:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
05/30/2022 22:56:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=93
05/30/2022 22:56:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/30/2022 22:57:06 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
05/30/2022 22:57:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
05/30/2022 22:57:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
05/30/2022 22:57:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/30/2022 22:57:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
05/30/2022 22:57:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
05/30/2022 22:57:39 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=96
05/30/2022 22:57:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=97
05/30/2022 22:57:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=98
05/30/2022 22:57:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/30/2022 22:57:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=99
05/30/2022 22:58:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/30/2022 22:58:13 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
05/30/2022 22:58:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=100
05/30/2022 22:58:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/30/2022 22:58:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=101
05/30/2022 22:58:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/30/2022 22:58:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/30/2022 22:58:47 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=103
05/30/2022 22:58:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/30/2022 22:58:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
05/30/2022 22:59:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=104
05/30/2022 22:59:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/30/2022 22:59:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/30/2022 22:59:22 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
05/30/2022 22:59:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=106
05/30/2022 22:59:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=107
05/30/2022 22:59:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/30/2022 22:59:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/30/2022 22:59:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/30/2022 22:59:56 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=109
05/30/2022 23:00:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=109
05/30/2022 23:00:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=110
05/30/2022 23:00:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/30/2022 23:00:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/30/2022 23:00:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
05/30/2022 23:00:30 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
05/30/2022 23:00:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
05/30/2022 23:00:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/30/2022 23:00:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/30/2022 23:00:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/30/2022 23:00:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=115
05/30/2022 23:01:04 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=115
05/30/2022 23:01:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=116
05/30/2022 23:01:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
05/30/2022 23:01:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/30/2022 23:01:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
05/30/2022 23:01:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=118
05/30/2022 23:01:38 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=118
05/30/2022 23:01:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=119
05/30/2022 23:01:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/30/2022 23:01:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=120
05/30/2022 23:01:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=121
05/30/2022 23:02:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=121
05/30/2022 23:02:11 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=121
05/30/2022 23:02:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
05/30/2022 23:02:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/30/2022 23:02:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=123
05/30/2022 23:02:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/30/2022 23:02:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=124
05/30/2022 23:02:45 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=124
05/30/2022 23:02:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
05/30/2022 23:02:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=126
05/30/2022 23:02:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=126
05/30/2022 23:03:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
05/30/2022 23:03:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
05/30/2022 23:03:19 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=128
05/30/2022 23:03:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/30/2022 23:03:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
05/30/2022 23:03:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=129
05/30/2022 23:03:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/30/2022 23:03:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/30/2022 23:03:53 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=131
05/30/2022 23:03:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/30/2022 23:04:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
05/30/2022 23:04:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=133
05/30/2022 23:04:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=133
05/30/2022 23:04:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=134
05/30/2022 23:04:26 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=134
05/30/2022 23:04:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/30/2022 23:04:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/30/2022 23:04:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/30/2022 23:04:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
05/30/2022 23:04:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=137
05/30/2022 23:05:00 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=137
05/30/2022 23:05:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/30/2022 23:05:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/30/2022 23:05:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/30/2022 23:05:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=139
05/30/2022 23:05:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/30/2022 23:05:34 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=140
05/30/2022 23:05:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=141
05/30/2022 23:05:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
05/30/2022 23:05:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
05/30/2022 23:05:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=143
05/30/2022 23:05:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
05/30/2022 23:06:08 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=143
05/30/2022 23:06:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=144
05/30/2022 23:06:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/30/2022 23:06:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
05/30/2022 23:06:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=146
05/30/2022 23:06:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/30/2022 23:06:43 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=146
05/30/2022 23:06:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=147
05/30/2022 23:06:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/30/2022 23:06:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=148
05/30/2022 23:07:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
05/30/2022 23:07:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
05/30/2022 23:07:17 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=149
05/30/2022 23:07:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/30/2022 23:07:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=151
05/30/2022 23:07:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
05/30/2022 23:07:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.22 on epoch=152
05/30/2022 23:07:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=153
05/30/2022 23:07:51 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=153
05/30/2022 23:07:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=153
05/30/2022 23:08:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/30/2022 23:08:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=154
05/30/2022 23:08:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=155
05/30/2022 23:08:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=156
05/30/2022 23:08:25 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=156
05/30/2022 23:08:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/30/2022 23:08:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
05/30/2022 23:08:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
05/30/2022 23:08:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.20 on epoch=158
05/30/2022 23:08:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=159
05/30/2022 23:09:00 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=159
05/30/2022 23:09:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=159
05/30/2022 23:09:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=160
05/30/2022 23:09:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=161
05/30/2022 23:09:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=161
05/30/2022 23:09:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
05/30/2022 23:09:34 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=162
05/30/2022 23:09:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/30/2022 23:09:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=163
05/30/2022 23:09:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
05/30/2022 23:09:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=164
05/30/2022 23:09:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=165
05/30/2022 23:10:08 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=165
05/30/2022 23:10:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=166
05/30/2022 23:10:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/30/2022 23:10:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=167
05/30/2022 23:10:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=168
05/30/2022 23:10:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=168
05/30/2022 23:10:43 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=168
05/30/2022 23:10:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=169
05/30/2022 23:10:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
05/30/2022 23:10:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.23 on epoch=170
05/30/2022 23:11:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
05/30/2022 23:11:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=171
05/30/2022 23:11:17 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=171
05/30/2022 23:11:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=172
05/30/2022 23:11:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=173
05/30/2022 23:11:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=173
05/30/2022 23:11:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=174
05/30/2022 23:11:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=174
05/30/2022 23:11:51 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=174
05/30/2022 23:11:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/30/2022 23:12:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=176
05/30/2022 23:12:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=176
05/30/2022 23:12:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
05/30/2022 23:12:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=178
05/30/2022 23:12:25 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=178
05/30/2022 23:12:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=178
05/30/2022 23:12:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=179
05/30/2022 23:12:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.20 on epoch=179
05/30/2022 23:12:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=180
05/30/2022 23:12:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=181
05/30/2022 23:13:00 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=181
05/30/2022 23:13:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=181
05/30/2022 23:13:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=182
05/30/2022 23:13:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=183
05/30/2022 23:13:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=183
05/30/2022 23:13:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=184
05/30/2022 23:13:34 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.34673046251993617 on epoch=184
05/30/2022 23:13:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34673046251993617 on epoch=184, global_step=2950
05/30/2022 23:13:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=184
05/30/2022 23:13:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=185
05/30/2022 23:13:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=186
05/30/2022 23:13:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=186
05/30/2022 23:13:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=187
05/30/2022 23:14:09 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.34485289741504155 on epoch=187
05/30/2022 23:14:09 - INFO - __main__ - save last model!
05/30/2022 23:14:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:14:09 - INFO - __main__ - Start tokenizing ... 12792 instances
05/30/2022 23:14:09 - INFO - __main__ - Printing 3 examples
05/30/2022 23:14:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 23:14:09 - INFO - __main__ - ['entailed']
05/30/2022 23:14:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 23:14:09 - INFO - __main__ - ['entailed']
05/30/2022 23:14:09 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
05/30/2022 23:14:09 - INFO - __main__ - ['entailed']
05/30/2022 23:14:09 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:14:34 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:14:47 - INFO - __main__ - Loaded 12792 examples from test data
05/30/2022 23:24:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-tab_fact/tab_fact_128_87_0.2_8_predictions.txt
05/30/2022 23:24:22 - INFO - __main__ - Classification-F1 on test data: 0.3485
05/30/2022 23:24:22 - INFO - __main__ - prefix=tab_fact_128_87, lr=0.2, bsz=8, dev_performance=0.34673046251993617, test_performance=0.3485045467348144
