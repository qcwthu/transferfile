05/29/2022 01:39:26 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/29/2022 01:39:26 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo
05/29/2022 01:39:26 - INFO - __main__ - Namespace(task_dir='data_128/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/29/2022 01:39:26 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo
05/29/2022 01:39:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/29/2022 01:39:27 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/29/2022 01:39:27 - INFO - __main__ - args.device: cuda:0
05/29/2022 01:39:27 - INFO - __main__ - Using 2 gpus
05/29/2022 01:39:27 - INFO - __main__ - args.device: cuda:1
05/29/2022 01:39:27 - INFO - __main__ - Using 2 gpus
05/29/2022 01:39:27 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/29/2022 01:39:27 - INFO - __main__ - Fine-tuning the following samples: ['emo_128_100', 'emo_128_13', 'emo_128_21', 'emo_128_42', 'emo_128_87']
05/29/2022 01:39:31 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.5, bsz=8 ...
05/29/2022 01:39:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:39:32 - INFO - __main__ - Printing 3 examples
05/29/2022 01:39:32 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:39:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:39:32 - INFO - __main__ - Printing 3 examples
05/29/2022 01:39:32 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 01:39:32 - INFO - __main__ - ['others']
05/29/2022 01:39:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:39:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:39:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:39:33 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 01:39:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:39:33 - INFO - __main__ - Printing 3 examples
05/29/2022 01:39:33 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:39:33 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 01:39:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:39:33 - INFO - __main__ - Printing 3 examples
05/29/2022 01:39:33 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 01:39:33 - INFO - __main__ - ['others']
05/29/2022 01:39:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:39:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:39:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:39:34 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 01:39:34 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 01:39:52 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:39:52 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:39:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:39:52 - INFO - __main__ - Starting training!
05/29/2022 01:39:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:39:58 - INFO - __main__ - Starting training!
05/29/2022 01:40:01 - INFO - __main__ - Step 10 Global step 10 Train loss 3.72 on epoch=0
05/29/2022 01:40:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.77 on epoch=0
05/29/2022 01:40:06 - INFO - __main__ - Step 30 Global step 30 Train loss 2.32 on epoch=0
05/29/2022 01:40:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.66 on epoch=1
05/29/2022 01:40:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.44 on epoch=1
05/29/2022 01:40:18 - INFO - __main__ - Global step 50 Train loss 2.38 Classification-F1 0.27422563377152587 on epoch=1
05/29/2022 01:40:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27422563377152587 on epoch=1, global_step=50
05/29/2022 01:40:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.32 on epoch=1
05/29/2022 01:40:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=2
05/29/2022 01:40:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.84 on epoch=2
05/29/2022 01:40:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=2
05/29/2022 01:40:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=3
05/29/2022 01:40:38 - INFO - __main__ - Global step 100 Train loss 0.93 Classification-F1 0.4821862288269644 on epoch=3
05/29/2022 01:40:38 - INFO - __main__ - Saving model with best Classification-F1: 0.27422563377152587 -> 0.4821862288269644 on epoch=3, global_step=100
05/29/2022 01:40:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.96 on epoch=3
05/29/2022 01:40:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=3
05/29/2022 01:40:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.67 on epoch=4
05/29/2022 01:40:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=4
05/29/2022 01:40:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=4
05/29/2022 01:40:57 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.6008968145219357 on epoch=4
05/29/2022 01:40:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4821862288269644 -> 0.6008968145219357 on epoch=4, global_step=150
05/29/2022 01:41:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=4
05/29/2022 01:41:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=5
05/29/2022 01:41:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=5
05/29/2022 01:41:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=5
05/29/2022 01:41:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=6
05/29/2022 01:41:17 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.6221584129409077 on epoch=6
05/29/2022 01:41:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6008968145219357 -> 0.6221584129409077 on epoch=6, global_step=200
05/29/2022 01:41:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=6
05/29/2022 01:41:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=6
05/29/2022 01:41:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=7
05/29/2022 01:41:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.64 on epoch=7
05/29/2022 01:41:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=7
05/29/2022 01:41:36 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.6309506467968294 on epoch=7
05/29/2022 01:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6221584129409077 -> 0.6309506467968294 on epoch=7, global_step=250
05/29/2022 01:41:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=8
05/29/2022 01:41:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=8
05/29/2022 01:41:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=8
05/29/2022 01:41:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=9
05/29/2022 01:41:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=9
05/29/2022 01:41:56 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.6854049765438415 on epoch=9
05/29/2022 01:41:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6309506467968294 -> 0.6854049765438415 on epoch=9, global_step=300
05/29/2022 01:41:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=9
05/29/2022 01:42:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=9
05/29/2022 01:42:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=10
05/29/2022 01:42:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.61 on epoch=10
05/29/2022 01:42:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=10
05/29/2022 01:42:15 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.7289240991614527 on epoch=10
05/29/2022 01:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6854049765438415 -> 0.7289240991614527 on epoch=10, global_step=350
05/29/2022 01:42:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=11
05/29/2022 01:42:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=11
05/29/2022 01:42:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=11
05/29/2022 01:42:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=12
05/29/2022 01:42:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=12
05/29/2022 01:42:34 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.6993167816380329 on epoch=12
05/29/2022 01:42:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=12
05/29/2022 01:42:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=13
05/29/2022 01:42:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=13
05/29/2022 01:42:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=13
05/29/2022 01:42:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=14
05/29/2022 01:42:54 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.7370993739054559 on epoch=14
05/29/2022 01:42:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7289240991614527 -> 0.7370993739054559 on epoch=14, global_step=450
05/29/2022 01:42:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=14
05/29/2022 01:42:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=14
05/29/2022 01:43:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=14
05/29/2022 01:43:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=15
05/29/2022 01:43:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=15
05/29/2022 01:43:13 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.726493008484988 on epoch=15
05/29/2022 01:43:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=15
05/29/2022 01:43:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=16
05/29/2022 01:43:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=16
05/29/2022 01:43:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=16
05/29/2022 01:43:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=17
05/29/2022 01:43:33 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.7273462706176028 on epoch=17
05/29/2022 01:43:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=17
05/29/2022 01:43:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=17
05/29/2022 01:43:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=18
05/29/2022 01:43:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=18
05/29/2022 01:43:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=18
05/29/2022 01:43:52 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.7858554461033861 on epoch=18
05/29/2022 01:43:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7370993739054559 -> 0.7858554461033861 on epoch=18, global_step=600
05/29/2022 01:43:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=19
05/29/2022 01:43:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=19
05/29/2022 01:44:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=19
05/29/2022 01:44:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=19
05/29/2022 01:44:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=20
05/29/2022 01:44:12 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.7512738943382382 on epoch=20
05/29/2022 01:44:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=20
05/29/2022 01:44:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=20
05/29/2022 01:44:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=21
05/29/2022 01:44:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=21
05/29/2022 01:44:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=21
05/29/2022 01:44:31 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.7738133436880124 on epoch=21
05/29/2022 01:44:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=22
05/29/2022 01:44:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=22
05/29/2022 01:44:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=22
05/29/2022 01:44:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=23
05/29/2022 01:44:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=23
05/29/2022 01:44:51 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7813350368043508 on epoch=23
05/29/2022 01:44:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=23
05/29/2022 01:44:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=24
05/29/2022 01:44:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=24
05/29/2022 01:45:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=24
05/29/2022 01:45:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=24
05/29/2022 01:45:10 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.7842563358899008 on epoch=24
05/29/2022 01:45:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=25
05/29/2022 01:45:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=25
05/29/2022 01:45:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=25
05/29/2022 01:45:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=26
05/29/2022 01:45:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=26
05/29/2022 01:45:30 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.7659827632563789 on epoch=26
05/29/2022 01:45:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=26
05/29/2022 01:45:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=27
05/29/2022 01:45:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=27
05/29/2022 01:45:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=27
05/29/2022 01:45:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=28
05/29/2022 01:45:49 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.746239156620668 on epoch=28
05/29/2022 01:45:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=28
05/29/2022 01:45:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=28
05/29/2022 01:45:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=29
05/29/2022 01:45:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=29
05/29/2022 01:46:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=29
05/29/2022 01:46:09 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.7955877546061316 on epoch=29
05/29/2022 01:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7858554461033861 -> 0.7955877546061316 on epoch=29, global_step=950
05/29/2022 01:46:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=29
05/29/2022 01:46:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=30
05/29/2022 01:46:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=30
05/29/2022 01:46:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=30
05/29/2022 01:46:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=31
05/29/2022 01:46:29 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.7856693596580148 on epoch=31
05/29/2022 01:46:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=31
05/29/2022 01:46:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=31
05/29/2022 01:46:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=32
05/29/2022 01:46:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=32
05/29/2022 01:46:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=32
05/29/2022 01:46:48 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.7989596951863696 on epoch=32
05/29/2022 01:46:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7955877546061316 -> 0.7989596951863696 on epoch=32, global_step=1050
05/29/2022 01:46:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=33
05/29/2022 01:46:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=33
05/29/2022 01:46:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=33
05/29/2022 01:46:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=34
05/29/2022 01:47:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=34
05/29/2022 01:47:08 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.7860983102918586 on epoch=34
05/29/2022 01:47:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=34
05/29/2022 01:47:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=34
05/29/2022 01:47:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=35
05/29/2022 01:47:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=35
05/29/2022 01:47:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=35
05/29/2022 01:47:28 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.7960747085509894 on epoch=35
05/29/2022 01:47:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=36
05/29/2022 01:47:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=36
05/29/2022 01:47:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=36
05/29/2022 01:47:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=37
05/29/2022 01:47:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=37
05/29/2022 01:47:48 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.7760185644359956 on epoch=37
05/29/2022 01:47:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=37
05/29/2022 01:47:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=38
05/29/2022 01:47:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=38
05/29/2022 01:47:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
05/29/2022 01:48:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=39
05/29/2022 01:48:07 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.7935466889699239 on epoch=39
05/29/2022 01:48:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=39
05/29/2022 01:48:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=39
05/29/2022 01:48:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=39
05/29/2022 01:48:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=40
05/29/2022 01:48:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=40
05/29/2022 01:48:27 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.7879706636134318 on epoch=40
05/29/2022 01:48:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=40
05/29/2022 01:48:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=41
05/29/2022 01:48:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=41
05/29/2022 01:48:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=41
05/29/2022 01:48:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=42
05/29/2022 01:48:47 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.7756942612046387 on epoch=42
05/29/2022 01:48:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=42
05/29/2022 01:48:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=42
05/29/2022 01:48:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=43
05/29/2022 01:48:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=43
05/29/2022 01:48:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=43
05/29/2022 01:49:06 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.8186339743163218 on epoch=43
05/29/2022 01:49:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7989596951863696 -> 0.8186339743163218 on epoch=43, global_step=1400
05/29/2022 01:49:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=44
05/29/2022 01:49:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=44
05/29/2022 01:49:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=44
05/29/2022 01:49:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=44
05/29/2022 01:49:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=45
05/29/2022 01:49:26 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.809959204273878 on epoch=45
05/29/2022 01:49:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=45
05/29/2022 01:49:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=45
05/29/2022 01:49:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=46
05/29/2022 01:49:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=46
05/29/2022 01:49:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=46
05/29/2022 01:49:45 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.8191203995616765 on epoch=46
05/29/2022 01:49:45 - INFO - __main__ - Saving model with best Classification-F1: 0.8186339743163218 -> 0.8191203995616765 on epoch=46, global_step=1500
05/29/2022 01:49:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=47
05/29/2022 01:49:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=47
05/29/2022 01:49:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=47
05/29/2022 01:49:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
05/29/2022 01:49:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=48
05/29/2022 01:50:05 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.8090099332000376 on epoch=48
05/29/2022 01:50:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=48
05/29/2022 01:50:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=49
05/29/2022 01:50:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=49
05/29/2022 01:50:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=49
05/29/2022 01:50:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=49
05/29/2022 01:50:24 - INFO - __main__ - Global step 1600 Train loss 0.17 Classification-F1 0.822276853537941 on epoch=49
05/29/2022 01:50:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8191203995616765 -> 0.822276853537941 on epoch=49, global_step=1600
05/29/2022 01:50:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=50
05/29/2022 01:50:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=50
05/29/2022 01:50:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=50
05/29/2022 01:50:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=51
05/29/2022 01:50:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=51
05/29/2022 01:50:44 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.770313785099668 on epoch=51
05/29/2022 01:50:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=51
05/29/2022 01:50:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=52
05/29/2022 01:50:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=52
05/29/2022 01:50:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=52
05/29/2022 01:50:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=53
05/29/2022 01:51:04 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.8208647312093147 on epoch=53
05/29/2022 01:51:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=53
05/29/2022 01:51:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=53
05/29/2022 01:51:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=54
05/29/2022 01:51:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=54
05/29/2022 01:51:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=54
05/29/2022 01:51:23 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.7869089233835711 on epoch=54
05/29/2022 01:51:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=54
05/29/2022 01:51:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
05/29/2022 01:51:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=55
05/29/2022 01:51:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=55
05/29/2022 01:51:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=56
05/29/2022 01:51:43 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.8112305533045703 on epoch=56
05/29/2022 01:51:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=56
05/29/2022 01:51:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=56
05/29/2022 01:51:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=57
05/29/2022 01:51:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=57
05/29/2022 01:51:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=57
05/29/2022 01:52:03 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.7861394790033069 on epoch=57
05/29/2022 01:52:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=58
05/29/2022 01:52:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=58
05/29/2022 01:52:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=58
05/29/2022 01:52:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=59
05/29/2022 01:52:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=59
05/29/2022 01:52:23 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.7850473246887194 on epoch=59
05/29/2022 01:52:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=59
05/29/2022 01:52:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=59
05/29/2022 01:52:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=60
05/29/2022 01:52:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=60
05/29/2022 01:52:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=60
05/29/2022 01:52:43 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.8135899979649981 on epoch=60
05/29/2022 01:52:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=61
05/29/2022 01:52:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=61
05/29/2022 01:52:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=61
05/29/2022 01:52:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=62
05/29/2022 01:52:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=62
05/29/2022 01:53:03 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7916045050068878 on epoch=62
05/29/2022 01:53:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=62
05/29/2022 01:53:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=63
05/29/2022 01:53:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=63
05/29/2022 01:53:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=63
05/29/2022 01:53:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=64
05/29/2022 01:53:23 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.8173791164799102 on epoch=64
05/29/2022 01:53:26 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=64
05/29/2022 01:53:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=64
05/29/2022 01:53:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=64
05/29/2022 01:53:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=65
05/29/2022 01:53:36 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=65
05/29/2022 01:53:43 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7975648178487807 on epoch=65
05/29/2022 01:53:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=65
05/29/2022 01:53:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=66
05/29/2022 01:53:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=66
05/29/2022 01:53:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=66
05/29/2022 01:53:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=67
05/29/2022 01:54:03 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.785892036036619 on epoch=67
05/29/2022 01:54:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=67
05/29/2022 01:54:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=67
05/29/2022 01:54:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=68
05/29/2022 01:54:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=68
05/29/2022 01:54:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=68
05/29/2022 01:54:22 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.799731182795699 on epoch=68
05/29/2022 01:54:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=69
05/29/2022 01:54:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=69
05/29/2022 01:54:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=69
05/29/2022 01:54:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=69
05/29/2022 01:54:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/29/2022 01:54:42 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.8213911664858706 on epoch=70
05/29/2022 01:54:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=70
05/29/2022 01:54:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 01:54:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/29/2022 01:54:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=71
05/29/2022 01:54:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=71
05/29/2022 01:55:02 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.8064180902026703 on epoch=71
05/29/2022 01:55:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=72
05/29/2022 01:55:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=72
05/29/2022 01:55:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=72
05/29/2022 01:55:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=73
05/29/2022 01:55:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/29/2022 01:55:22 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.8067887924968605 on epoch=73
05/29/2022 01:55:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=73
05/29/2022 01:55:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=74
05/29/2022 01:55:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=74
05/29/2022 01:55:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=74
05/29/2022 01:55:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=74
05/29/2022 01:55:42 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7928005437492357 on epoch=74
05/29/2022 01:55:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=75
05/29/2022 01:55:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=75
05/29/2022 01:55:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=75
05/29/2022 01:55:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=76
05/29/2022 01:55:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=76
05/29/2022 01:56:02 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.7634759720427454 on epoch=76
05/29/2022 01:56:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=76
05/29/2022 01:56:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=77
05/29/2022 01:56:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=77
05/29/2022 01:56:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=77
05/29/2022 01:56:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=78
05/29/2022 01:56:22 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.8034313238758919 on epoch=78
05/29/2022 01:56:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=78
05/29/2022 01:56:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=78
05/29/2022 01:56:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=79
05/29/2022 01:56:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=79
05/29/2022 01:56:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=79
05/29/2022 01:56:42 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7874940457234914 on epoch=79
05/29/2022 01:56:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=79
05/29/2022 01:56:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=80
05/29/2022 01:56:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=80
05/29/2022 01:56:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=80
05/29/2022 01:56:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=81
05/29/2022 01:57:02 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.8052229929388829 on epoch=81
05/29/2022 01:57:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
05/29/2022 01:57:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=81
05/29/2022 01:57:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/29/2022 01:57:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=82
05/29/2022 01:57:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=82
05/29/2022 01:57:22 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.7983620767514014 on epoch=82
05/29/2022 01:57:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=83
05/29/2022 01:57:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.24 on epoch=83
05/29/2022 01:57:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=83
05/29/2022 01:57:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=84
05/29/2022 01:57:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=84
05/29/2022 01:57:41 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8061069538581748 on epoch=84
05/29/2022 01:57:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=84
05/29/2022 01:57:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=84
05/29/2022 01:57:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=85
05/29/2022 01:57:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=85
05/29/2022 01:57:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=85
05/29/2022 01:58:01 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.7966590629298789 on epoch=85
05/29/2022 01:58:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=86
05/29/2022 01:58:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=86
05/29/2022 01:58:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=86
05/29/2022 01:58:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/29/2022 01:58:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=87
05/29/2022 01:58:21 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.8060653167089934 on epoch=87
05/29/2022 01:58:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=87
05/29/2022 01:58:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=88
05/29/2022 01:58:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=88
05/29/2022 01:58:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/29/2022 01:58:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=89
05/29/2022 01:58:41 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.8279109887761489 on epoch=89
05/29/2022 01:58:41 - INFO - __main__ - Saving model with best Classification-F1: 0.822276853537941 -> 0.8279109887761489 on epoch=89, global_step=2850
05/29/2022 01:58:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
05/29/2022 01:58:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
05/29/2022 01:58:48 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=89
05/29/2022 01:58:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=90
05/29/2022 01:58:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=90
05/29/2022 01:59:00 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.7892145052062649 on epoch=90
05/29/2022 01:59:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=90
05/29/2022 01:59:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=91
05/29/2022 01:59:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=91
05/29/2022 01:59:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=91
05/29/2022 01:59:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=92
05/29/2022 01:59:20 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.8056049929947805 on epoch=92
05/29/2022 01:59:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=92
05/29/2022 01:59:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=92
05/29/2022 01:59:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
05/29/2022 01:59:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=93
05/29/2022 01:59:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=93
05/29/2022 01:59:34 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:59:34 - INFO - __main__ - Printing 3 examples
05/29/2022 01:59:34 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 01:59:34 - INFO - __main__ - ['others']
05/29/2022 01:59:34 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 01:59:34 - INFO - __main__ - ['others']
05/29/2022 01:59:34 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 01:59:34 - INFO - __main__ - ['others']
05/29/2022 01:59:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:59:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:59:35 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 01:59:35 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 01:59:35 - INFO - __main__ - Printing 3 examples
05/29/2022 01:59:35 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 01:59:35 - INFO - __main__ - ['others']
05/29/2022 01:59:35 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 01:59:35 - INFO - __main__ - ['others']
05/29/2022 01:59:35 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 01:59:35 - INFO - __main__ - ['others']
05/29/2022 01:59:35 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:59:35 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:59:36 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 01:59:40 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.7969102412417614 on epoch=93
05/29/2022 01:59:40 - INFO - __main__ - save last model!
05/29/2022 01:59:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 01:59:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 01:59:40 - INFO - __main__ - Printing 3 examples
05/29/2022 01:59:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 01:59:40 - INFO - __main__ - ['others']
05/29/2022 01:59:40 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 01:59:40 - INFO - __main__ - ['others']
05/29/2022 01:59:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 01:59:40 - INFO - __main__ - ['others']
05/29/2022 01:59:40 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:59:42 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:59:47 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 01:59:54 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:59:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:59:55 - INFO - __main__ - Starting training!
05/29/2022 02:01:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_100_0.5_8_predictions.txt
05/29/2022 02:01:03 - INFO - __main__ - Classification-F1 on test data: 0.5011
05/29/2022 02:01:03 - INFO - __main__ - prefix=emo_128_100, lr=0.5, bsz=8, dev_performance=0.8279109887761489, test_performance=0.5010874728782588
05/29/2022 02:01:03 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.4, bsz=8 ...
05/29/2022 02:01:04 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:01:04 - INFO - __main__ - Printing 3 examples
05/29/2022 02:01:04 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 02:01:04 - INFO - __main__ - ['others']
05/29/2022 02:01:04 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 02:01:04 - INFO - __main__ - ['others']
05/29/2022 02:01:04 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 02:01:04 - INFO - __main__ - ['others']
05/29/2022 02:01:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:01:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:01:05 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 02:01:05 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:01:05 - INFO - __main__ - Printing 3 examples
05/29/2022 02:01:05 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 02:01:05 - INFO - __main__ - ['others']
05/29/2022 02:01:05 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 02:01:05 - INFO - __main__ - ['others']
05/29/2022 02:01:05 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 02:01:05 - INFO - __main__ - ['others']
05/29/2022 02:01:05 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:01:05 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:01:06 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 02:01:23 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 02:01:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 02:01:23 - INFO - __main__ - Starting training!
05/29/2022 02:01:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.66 on epoch=0
05/29/2022 02:01:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.86 on epoch=0
05/29/2022 02:01:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.47 on epoch=0
05/29/2022 02:01:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.79 on epoch=1
05/29/2022 02:01:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=1
05/29/2022 02:01:44 - INFO - __main__ - Global step 50 Train loss 2.52 Classification-F1 0.170267048184327 on epoch=1
05/29/2022 02:01:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.170267048184327 on epoch=1, global_step=50
05/29/2022 02:01:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=1
05/29/2022 02:01:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=2
05/29/2022 02:01:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=2
05/29/2022 02:01:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.89 on epoch=2
05/29/2022 02:01:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=3
05/29/2022 02:02:03 - INFO - __main__ - Global step 100 Train loss 1.06 Classification-F1 0.5326115136018182 on epoch=3
05/29/2022 02:02:03 - INFO - __main__ - Saving model with best Classification-F1: 0.170267048184327 -> 0.5326115136018182 on epoch=3, global_step=100
05/29/2022 02:02:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.82 on epoch=3
05/29/2022 02:02:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=3
05/29/2022 02:02:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=4
05/29/2022 02:02:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=4
05/29/2022 02:02:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.69 on epoch=4
05/29/2022 02:02:23 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.5831960327549326 on epoch=4
05/29/2022 02:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5326115136018182 -> 0.5831960327549326 on epoch=4, global_step=150
05/29/2022 02:02:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=4
05/29/2022 02:02:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=5
05/29/2022 02:02:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=5
05/29/2022 02:02:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=5
05/29/2022 02:02:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=6
05/29/2022 02:02:42 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.6531558569665435 on epoch=6
05/29/2022 02:02:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5831960327549326 -> 0.6531558569665435 on epoch=6, global_step=200
05/29/2022 02:02:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=6
05/29/2022 02:02:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=6
05/29/2022 02:02:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=7
05/29/2022 02:02:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=7
05/29/2022 02:02:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=7
05/29/2022 02:03:01 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.5969049356717244 on epoch=7
05/29/2022 02:03:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=8
05/29/2022 02:03:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=8
05/29/2022 02:03:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=8
05/29/2022 02:03:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=9
05/29/2022 02:03:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=9
05/29/2022 02:03:21 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.6843137592662818 on epoch=9
05/29/2022 02:03:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6531558569665435 -> 0.6843137592662818 on epoch=9, global_step=300
05/29/2022 02:03:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=9
05/29/2022 02:03:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=9
05/29/2022 02:03:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=10
05/29/2022 02:03:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=10
05/29/2022 02:03:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=10
05/29/2022 02:03:40 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.6423440563390593 on epoch=10
05/29/2022 02:03:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=11
05/29/2022 02:03:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=11
05/29/2022 02:03:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=11
05/29/2022 02:03:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=12
05/29/2022 02:03:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=12
05/29/2022 02:03:59 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.6866436503277944 on epoch=12
05/29/2022 02:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6843137592662818 -> 0.6866436503277944 on epoch=12, global_step=400
05/29/2022 02:04:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=12
05/29/2022 02:04:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=13
05/29/2022 02:04:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=13
05/29/2022 02:04:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=13
05/29/2022 02:04:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=14
05/29/2022 02:04:19 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.696499190571616 on epoch=14
05/29/2022 02:04:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6866436503277944 -> 0.696499190571616 on epoch=14, global_step=450
05/29/2022 02:04:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=14
05/29/2022 02:04:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=14
05/29/2022 02:04:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=14
05/29/2022 02:04:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=15
05/29/2022 02:04:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=15
05/29/2022 02:04:38 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.7090082150574248 on epoch=15
05/29/2022 02:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.696499190571616 -> 0.7090082150574248 on epoch=15, global_step=500
05/29/2022 02:04:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=15
05/29/2022 02:04:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=16
05/29/2022 02:04:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=16
05/29/2022 02:04:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=16
05/29/2022 02:04:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=17
05/29/2022 02:04:57 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.7441211486811925 on epoch=17
05/29/2022 02:04:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7090082150574248 -> 0.7441211486811925 on epoch=17, global_step=550
05/29/2022 02:05:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=17
05/29/2022 02:05:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=17
05/29/2022 02:05:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=18
05/29/2022 02:05:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=18
05/29/2022 02:05:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=18
05/29/2022 02:05:16 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.7597024679458111 on epoch=18
05/29/2022 02:05:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7441211486811925 -> 0.7597024679458111 on epoch=18, global_step=600
05/29/2022 02:05:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=19
05/29/2022 02:05:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=19
05/29/2022 02:05:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=19
05/29/2022 02:05:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=19
05/29/2022 02:05:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=20
05/29/2022 02:05:36 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.7627143885361829 on epoch=20
05/29/2022 02:05:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7597024679458111 -> 0.7627143885361829 on epoch=20, global_step=650
05/29/2022 02:05:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=20
05/29/2022 02:05:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=20
05/29/2022 02:05:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=21
05/29/2022 02:05:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=21
05/29/2022 02:05:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=21
05/29/2022 02:05:55 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.7550486822537913 on epoch=21
05/29/2022 02:05:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=22
05/29/2022 02:06:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=22
05/29/2022 02:06:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=22
05/29/2022 02:06:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=23
05/29/2022 02:06:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=23
05/29/2022 02:06:14 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.7856820142267903 on epoch=23
05/29/2022 02:06:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7627143885361829 -> 0.7856820142267903 on epoch=23, global_step=750
05/29/2022 02:06:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=23
05/29/2022 02:06:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=24
05/29/2022 02:06:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=24
05/29/2022 02:06:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=24
05/29/2022 02:06:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=24
05/29/2022 02:06:34 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.7853925196667133 on epoch=24
05/29/2022 02:06:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=25
05/29/2022 02:06:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=25
05/29/2022 02:06:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=25
05/29/2022 02:06:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=26
05/29/2022 02:06:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=26
05/29/2022 02:06:53 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.7782071508339434 on epoch=26
05/29/2022 02:06:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=26
05/29/2022 02:06:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=27
05/29/2022 02:07:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=27
05/29/2022 02:07:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=27
05/29/2022 02:07:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=28
05/29/2022 02:07:12 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.7730982330038935 on epoch=28
05/29/2022 02:07:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=28
05/29/2022 02:07:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=28
05/29/2022 02:07:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=29
05/29/2022 02:07:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=29
05/29/2022 02:07:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=29
05/29/2022 02:07:31 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.7943374425881444 on epoch=29
05/29/2022 02:07:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7856820142267903 -> 0.7943374425881444 on epoch=29, global_step=950
05/29/2022 02:07:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=29
05/29/2022 02:07:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=30
05/29/2022 02:07:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=30
05/29/2022 02:07:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=30
05/29/2022 02:07:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=31
05/29/2022 02:07:51 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.7904723458807319 on epoch=31
05/29/2022 02:07:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=31
05/29/2022 02:07:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=31
05/29/2022 02:07:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=32
05/29/2022 02:08:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=32
05/29/2022 02:08:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=32
05/29/2022 02:08:10 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.7992081229526673 on epoch=32
05/29/2022 02:08:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7943374425881444 -> 0.7992081229526673 on epoch=32, global_step=1050
05/29/2022 02:08:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=33
05/29/2022 02:08:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=33
05/29/2022 02:08:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=33
05/29/2022 02:08:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=34
05/29/2022 02:08:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=34
05/29/2022 02:08:29 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.7916899696020405 on epoch=34
05/29/2022 02:08:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=34
05/29/2022 02:08:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=34
05/29/2022 02:08:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=35
05/29/2022 02:08:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=35
05/29/2022 02:08:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=35
05/29/2022 02:08:48 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.7987455646172567 on epoch=35
05/29/2022 02:08:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=36
05/29/2022 02:08:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=36
05/29/2022 02:08:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=36
05/29/2022 02:08:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=37
05/29/2022 02:09:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=37
05/29/2022 02:09:08 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.7896679038339431 on epoch=37
05/29/2022 02:09:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=37
05/29/2022 02:09:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=38
05/29/2022 02:09:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=38
05/29/2022 02:09:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=38
05/29/2022 02:09:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=39
05/29/2022 02:09:27 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.8024791875135397 on epoch=39
05/29/2022 02:09:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7992081229526673 -> 0.8024791875135397 on epoch=39, global_step=1250
05/29/2022 02:09:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=39
05/29/2022 02:09:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=39
05/29/2022 02:09:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=39
05/29/2022 02:09:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=40
05/29/2022 02:09:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=40
05/29/2022 02:09:46 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7980923491742501 on epoch=40
05/29/2022 02:09:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=40
05/29/2022 02:09:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=41
05/29/2022 02:09:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=41
05/29/2022 02:09:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=41
05/29/2022 02:09:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=42
05/29/2022 02:10:05 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.8158298412188577 on epoch=42
05/29/2022 02:10:05 - INFO - __main__ - Saving model with best Classification-F1: 0.8024791875135397 -> 0.8158298412188577 on epoch=42, global_step=1350
05/29/2022 02:10:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=42
05/29/2022 02:10:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=42
05/29/2022 02:10:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=43
05/29/2022 02:10:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=43
05/29/2022 02:10:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=43
05/29/2022 02:10:25 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.8108766408727894 on epoch=43
05/29/2022 02:10:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=44
05/29/2022 02:10:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=44
05/29/2022 02:10:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=44
05/29/2022 02:10:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=44
05/29/2022 02:10:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=45
05/29/2022 02:10:44 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.8163778136583448 on epoch=45
05/29/2022 02:10:44 - INFO - __main__ - Saving model with best Classification-F1: 0.8158298412188577 -> 0.8163778136583448 on epoch=45, global_step=1450
05/29/2022 02:10:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=45
05/29/2022 02:10:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=45
05/29/2022 02:10:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=46
05/29/2022 02:10:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=46
05/29/2022 02:10:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=46
05/29/2022 02:11:03 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.8032337288695444 on epoch=46
05/29/2022 02:11:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=47
05/29/2022 02:11:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=47
05/29/2022 02:11:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=47
05/29/2022 02:11:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=48
05/29/2022 02:11:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=48
05/29/2022 02:11:22 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.8064739920466174 on epoch=48
05/29/2022 02:11:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=48
05/29/2022 02:11:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=49
05/29/2022 02:11:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=49
05/29/2022 02:11:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=49
05/29/2022 02:11:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=49
05/29/2022 02:11:42 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.8174745931490348 on epoch=49
05/29/2022 02:11:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8163778136583448 -> 0.8174745931490348 on epoch=49, global_step=1600
05/29/2022 02:11:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=50
05/29/2022 02:11:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=50
05/29/2022 02:11:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=50
05/29/2022 02:11:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=51
05/29/2022 02:11:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=51
05/29/2022 02:12:01 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.8203312831615105 on epoch=51
05/29/2022 02:12:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8174745931490348 -> 0.8203312831615105 on epoch=51, global_step=1650
05/29/2022 02:12:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=51
05/29/2022 02:12:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=52
05/29/2022 02:12:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=52
05/29/2022 02:12:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=52
05/29/2022 02:12:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=53
05/29/2022 02:12:20 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.8016348956159238 on epoch=53
05/29/2022 02:12:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=53
05/29/2022 02:12:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=53
05/29/2022 02:12:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=54
05/29/2022 02:12:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=54
05/29/2022 02:12:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=54
05/29/2022 02:12:40 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.8065549938975258 on epoch=54
05/29/2022 02:12:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=54
05/29/2022 02:12:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=55
05/29/2022 02:12:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=55
05/29/2022 02:12:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=55
05/29/2022 02:12:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=56
05/29/2022 02:12:59 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.7975177648701763 on epoch=56
05/29/2022 02:13:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=56
05/29/2022 02:13:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
05/29/2022 02:13:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=57
05/29/2022 02:13:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=57
05/29/2022 02:13:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=57
05/29/2022 02:13:18 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7850926355767154 on epoch=57
05/29/2022 02:13:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=58
05/29/2022 02:13:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=58
05/29/2022 02:13:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=58
05/29/2022 02:13:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=59
05/29/2022 02:13:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=59
05/29/2022 02:13:38 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7989741676224356 on epoch=59
05/29/2022 02:13:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=59
05/29/2022 02:13:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=59
05/29/2022 02:13:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
05/29/2022 02:13:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=60
05/29/2022 02:13:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=60
05/29/2022 02:13:57 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.823959321382219 on epoch=60
05/29/2022 02:13:57 - INFO - __main__ - Saving model with best Classification-F1: 0.8203312831615105 -> 0.823959321382219 on epoch=60, global_step=1950
05/29/2022 02:13:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=61
05/29/2022 02:14:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
05/29/2022 02:14:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=61
05/29/2022 02:14:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=62
05/29/2022 02:14:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=62
05/29/2022 02:14:16 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.8015450424567452 on epoch=62
05/29/2022 02:14:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=62
05/29/2022 02:14:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=63
05/29/2022 02:14:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=63
05/29/2022 02:14:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/29/2022 02:14:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.15 on epoch=64
05/29/2022 02:14:36 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.8376488296903009 on epoch=64
05/29/2022 02:14:36 - INFO - __main__ - Saving model with best Classification-F1: 0.823959321382219 -> 0.8376488296903009 on epoch=64, global_step=2050
05/29/2022 02:14:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=64
05/29/2022 02:14:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=64
05/29/2022 02:14:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=64
05/29/2022 02:14:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
05/29/2022 02:14:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=65
05/29/2022 02:14:55 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.8304618757898208 on epoch=65
05/29/2022 02:14:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=65
05/29/2022 02:15:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=66
05/29/2022 02:15:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=66
05/29/2022 02:15:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=66
05/29/2022 02:15:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=67
05/29/2022 02:15:14 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.8173428910653424 on epoch=67
05/29/2022 02:15:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=67
05/29/2022 02:15:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=67
05/29/2022 02:15:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=68
05/29/2022 02:15:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=68
05/29/2022 02:15:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=68
05/29/2022 02:15:34 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.8079537312669475 on epoch=68
05/29/2022 02:15:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=69
05/29/2022 02:15:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=69
05/29/2022 02:15:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=69
05/29/2022 02:15:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=69
05/29/2022 02:15:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=70
05/29/2022 02:15:53 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.7884681083131788 on epoch=70
05/29/2022 02:15:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=70
05/29/2022 02:15:58 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=70
05/29/2022 02:16:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=71
05/29/2022 02:16:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=71
05/29/2022 02:16:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=71
05/29/2022 02:16:13 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.8079768597034478 on epoch=71
05/29/2022 02:16:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=72
05/29/2022 02:16:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=72
05/29/2022 02:16:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=72
05/29/2022 02:16:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=73
05/29/2022 02:16:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/29/2022 02:16:32 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.8139753486808665 on epoch=73
05/29/2022 02:16:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=73
05/29/2022 02:16:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=74
05/29/2022 02:16:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=74
05/29/2022 02:16:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=74
05/29/2022 02:16:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.25 on epoch=74
05/29/2022 02:16:51 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.8267731194250356 on epoch=74
05/29/2022 02:16:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=75
05/29/2022 02:16:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=75
05/29/2022 02:16:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=75
05/29/2022 02:17:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
05/29/2022 02:17:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.27 on epoch=76
05/29/2022 02:17:11 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.8163402384295187 on epoch=76
05/29/2022 02:17:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=76
05/29/2022 02:17:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=77
05/29/2022 02:17:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=77
05/29/2022 02:17:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=77
05/29/2022 02:17:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=78
05/29/2022 02:17:30 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.8120290476031543 on epoch=78
05/29/2022 02:17:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=78
05/29/2022 02:17:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=78
05/29/2022 02:17:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=79
05/29/2022 02:17:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.30 on epoch=79
05/29/2022 02:17:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=79
05/29/2022 02:17:49 - INFO - __main__ - Global step 2550 Train loss 0.25 Classification-F1 0.8138460079720214 on epoch=79
05/29/2022 02:17:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=79
05/29/2022 02:17:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=80
05/29/2022 02:17:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=80
05/29/2022 02:17:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=80
05/29/2022 02:18:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=81
05/29/2022 02:18:08 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.8057818966019701 on epoch=81
05/29/2022 02:18:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=81
05/29/2022 02:18:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=81
05/29/2022 02:18:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=82
05/29/2022 02:18:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=82
05/29/2022 02:18:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=82
05/29/2022 02:18:28 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.8081804036151862 on epoch=82
05/29/2022 02:18:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=83
05/29/2022 02:18:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/29/2022 02:18:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/29/2022 02:18:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=84
05/29/2022 02:18:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=84
05/29/2022 02:18:47 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.7781518476045236 on epoch=84
05/29/2022 02:18:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=84
05/29/2022 02:18:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=84
05/29/2022 02:18:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=85
05/29/2022 02:18:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=85
05/29/2022 02:18:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=85
05/29/2022 02:19:06 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.8077336386371428 on epoch=85
05/29/2022 02:19:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=86
05/29/2022 02:19:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=86
05/29/2022 02:19:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=86
05/29/2022 02:19:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=87
05/29/2022 02:19:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=87
05/29/2022 02:19:25 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7914135271545804 on epoch=87
05/29/2022 02:19:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=87
05/29/2022 02:19:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
05/29/2022 02:19:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=88
05/29/2022 02:19:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=88
05/29/2022 02:19:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=89
05/29/2022 02:19:45 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.8190680383939413 on epoch=89
05/29/2022 02:19:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=89
05/29/2022 02:19:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=89
05/29/2022 02:19:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
05/29/2022 02:19:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=90
05/29/2022 02:19:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=90
05/29/2022 02:20:04 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.8112104483448734 on epoch=90
05/29/2022 02:20:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=90
05/29/2022 02:20:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=91
05/29/2022 02:20:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=91
05/29/2022 02:20:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=91
05/29/2022 02:20:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=92
05/29/2022 02:20:23 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.8124771508181057 on epoch=92
05/29/2022 02:20:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=92
05/29/2022 02:20:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=92
05/29/2022 02:20:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=93
05/29/2022 02:20:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
05/29/2022 02:20:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=93
05/29/2022 02:20:37 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:20:37 - INFO - __main__ - Printing 3 examples
05/29/2022 02:20:37 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 02:20:37 - INFO - __main__ - ['others']
05/29/2022 02:20:37 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 02:20:37 - INFO - __main__ - ['others']
05/29/2022 02:20:37 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 02:20:37 - INFO - __main__ - ['others']
05/29/2022 02:20:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:20:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:20:38 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 02:20:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:20:38 - INFO - __main__ - Printing 3 examples
05/29/2022 02:20:38 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 02:20:38 - INFO - __main__ - ['others']
05/29/2022 02:20:38 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 02:20:38 - INFO - __main__ - ['others']
05/29/2022 02:20:38 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 02:20:38 - INFO - __main__ - ['others']
05/29/2022 02:20:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:20:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:20:38 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 02:20:43 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.8106833068502965 on epoch=93
05/29/2022 02:20:43 - INFO - __main__ - save last model!
05/29/2022 02:20:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 02:20:43 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 02:20:43 - INFO - __main__ - Printing 3 examples
05/29/2022 02:20:43 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 02:20:43 - INFO - __main__ - ['others']
05/29/2022 02:20:43 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 02:20:43 - INFO - __main__ - ['others']
05/29/2022 02:20:43 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 02:20:43 - INFO - __main__ - ['others']
05/29/2022 02:20:43 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:20:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:20:51 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 02:20:55 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 02:20:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 02:20:56 - INFO - __main__ - Starting training!
05/29/2022 02:22:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_100_0.4_8_predictions.txt
05/29/2022 02:22:13 - INFO - __main__ - Classification-F1 on test data: 0.5723
05/29/2022 02:22:14 - INFO - __main__ - prefix=emo_128_100, lr=0.4, bsz=8, dev_performance=0.8376488296903009, test_performance=0.5723030567251561
05/29/2022 02:22:14 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.3, bsz=8 ...
05/29/2022 02:22:14 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:22:14 - INFO - __main__ - Printing 3 examples
05/29/2022 02:22:14 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 02:22:14 - INFO - __main__ - ['others']
05/29/2022 02:22:14 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 02:22:14 - INFO - __main__ - ['others']
05/29/2022 02:22:14 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 02:22:14 - INFO - __main__ - ['others']
05/29/2022 02:22:14 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:22:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:22:15 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 02:22:15 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:22:15 - INFO - __main__ - Printing 3 examples
05/29/2022 02:22:15 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 02:22:15 - INFO - __main__ - ['others']
05/29/2022 02:22:15 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 02:22:15 - INFO - __main__ - ['others']
05/29/2022 02:22:15 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 02:22:15 - INFO - __main__ - ['others']
05/29/2022 02:22:15 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:22:15 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:22:16 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 02:22:35 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 02:22:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 02:22:36 - INFO - __main__ - Starting training!
05/29/2022 02:22:39 - INFO - __main__ - Step 10 Global step 10 Train loss 3.94 on epoch=0
05/29/2022 02:22:41 - INFO - __main__ - Step 20 Global step 20 Train loss 3.20 on epoch=0
05/29/2022 02:22:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.80 on epoch=0
05/29/2022 02:22:46 - INFO - __main__ - Step 40 Global step 40 Train loss 2.11 on epoch=1
05/29/2022 02:22:49 - INFO - __main__ - Step 50 Global step 50 Train loss 2.14 on epoch=1
05/29/2022 02:22:56 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.0685303397167804 on epoch=1
05/29/2022 02:22:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0685303397167804 on epoch=1, global_step=50
05/29/2022 02:22:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.96 on epoch=1
05/29/2022 02:23:00 - INFO - __main__ - Step 70 Global step 70 Train loss 1.55 on epoch=2
05/29/2022 02:23:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.49 on epoch=2
05/29/2022 02:23:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.10 on epoch=2
05/29/2022 02:23:08 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=3
05/29/2022 02:23:15 - INFO - __main__ - Global step 100 Train loss 1.43 Classification-F1 0.3302169504159051 on epoch=3
05/29/2022 02:23:15 - INFO - __main__ - Saving model with best Classification-F1: 0.0685303397167804 -> 0.3302169504159051 on epoch=3, global_step=100
05/29/2022 02:23:17 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=3
05/29/2022 02:23:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=3
05/29/2022 02:23:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.76 on epoch=4
05/29/2022 02:23:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=4
05/29/2022 02:23:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=4
05/29/2022 02:23:34 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.5260133425773514 on epoch=4
05/29/2022 02:23:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3302169504159051 -> 0.5260133425773514 on epoch=4, global_step=150
05/29/2022 02:23:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=4
05/29/2022 02:23:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.72 on epoch=5
05/29/2022 02:23:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=5
05/29/2022 02:23:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=5
05/29/2022 02:23:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=6
05/29/2022 02:23:53 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.5590900979038101 on epoch=6
05/29/2022 02:23:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5260133425773514 -> 0.5590900979038101 on epoch=6, global_step=200
05/29/2022 02:23:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=6
05/29/2022 02:23:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=6
05/29/2022 02:24:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=7
05/29/2022 02:24:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=7
05/29/2022 02:24:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=7
05/29/2022 02:24:12 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5541265978425688 on epoch=7
05/29/2022 02:24:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=8
05/29/2022 02:24:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.68 on epoch=8
05/29/2022 02:24:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=8
05/29/2022 02:24:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=9
05/29/2022 02:24:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=9
05/29/2022 02:24:31 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6157281483853569 on epoch=9
05/29/2022 02:24:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5590900979038101 -> 0.6157281483853569 on epoch=9, global_step=300
05/29/2022 02:24:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.73 on epoch=9
05/29/2022 02:24:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=9
05/29/2022 02:24:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=10
05/29/2022 02:24:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
05/29/2022 02:24:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=10
05/29/2022 02:24:50 - INFO - __main__ - Global step 350 Train loss 0.63 Classification-F1 0.629266495530127 on epoch=10
05/29/2022 02:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6157281483853569 -> 0.629266495530127 on epoch=10, global_step=350
05/29/2022 02:24:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=11
05/29/2022 02:24:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.70 on epoch=11
05/29/2022 02:24:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=11
05/29/2022 02:25:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=12
05/29/2022 02:25:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=12
05/29/2022 02:25:09 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.68270478642777 on epoch=12
05/29/2022 02:25:09 - INFO - __main__ - Saving model with best Classification-F1: 0.629266495530127 -> 0.68270478642777 on epoch=12, global_step=400
05/29/2022 02:25:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=12
05/29/2022 02:25:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=13
05/29/2022 02:25:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=13
05/29/2022 02:25:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=13
05/29/2022 02:25:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=14
05/29/2022 02:25:28 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.6364050528591679 on epoch=14
05/29/2022 02:25:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=14
05/29/2022 02:25:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=14
05/29/2022 02:25:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=14
05/29/2022 02:25:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=15
05/29/2022 02:25:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=15
05/29/2022 02:25:47 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.6877328244850772 on epoch=15
05/29/2022 02:25:48 - INFO - __main__ - Saving model with best Classification-F1: 0.68270478642777 -> 0.6877328244850772 on epoch=15, global_step=500
05/29/2022 02:25:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=15
05/29/2022 02:25:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=16
05/29/2022 02:25:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=16
05/29/2022 02:25:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=16
05/29/2022 02:26:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=17
05/29/2022 02:26:07 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.6780366192253798 on epoch=17
05/29/2022 02:26:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=17
05/29/2022 02:26:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=17
05/29/2022 02:26:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=18
05/29/2022 02:26:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=18
05/29/2022 02:26:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=18
05/29/2022 02:26:26 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.707341576051985 on epoch=18
05/29/2022 02:26:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6877328244850772 -> 0.707341576051985 on epoch=18, global_step=600
05/29/2022 02:26:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=19
05/29/2022 02:26:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=19
05/29/2022 02:26:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=19
05/29/2022 02:26:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=19
05/29/2022 02:26:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=20
05/29/2022 02:26:45 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.7205499498031003 on epoch=20
05/29/2022 02:26:45 - INFO - __main__ - Saving model with best Classification-F1: 0.707341576051985 -> 0.7205499498031003 on epoch=20, global_step=650
05/29/2022 02:26:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=20
05/29/2022 02:26:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=20
05/29/2022 02:26:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=21
05/29/2022 02:26:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=21
05/29/2022 02:26:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=21
05/29/2022 02:27:04 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.7152243463537807 on epoch=21
05/29/2022 02:27:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=22
05/29/2022 02:27:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=22
05/29/2022 02:27:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=22
05/29/2022 02:27:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=23
05/29/2022 02:27:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=23
05/29/2022 02:27:23 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.7360515969065866 on epoch=23
05/29/2022 02:27:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7205499498031003 -> 0.7360515969065866 on epoch=23, global_step=750
05/29/2022 02:27:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=23
05/29/2022 02:27:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=24
05/29/2022 02:27:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=24
05/29/2022 02:27:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=24
05/29/2022 02:27:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=24
05/29/2022 02:27:43 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.7402378464722339 on epoch=24
05/29/2022 02:27:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7360515969065866 -> 0.7402378464722339 on epoch=24, global_step=800
05/29/2022 02:27:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=25
05/29/2022 02:27:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=25
05/29/2022 02:27:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
05/29/2022 02:27:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=26
05/29/2022 02:27:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=26
05/29/2022 02:28:02 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.7423454524315432 on epoch=26
05/29/2022 02:28:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7402378464722339 -> 0.7423454524315432 on epoch=26, global_step=850
05/29/2022 02:28:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=26
05/29/2022 02:28:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=27
05/29/2022 02:28:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=27
05/29/2022 02:28:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=27
05/29/2022 02:28:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=28
05/29/2022 02:28:21 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.7528249726672523 on epoch=28
05/29/2022 02:28:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7423454524315432 -> 0.7528249726672523 on epoch=28, global_step=900
05/29/2022 02:28:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=28
05/29/2022 02:28:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=28
05/29/2022 02:28:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=29
05/29/2022 02:28:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=29
05/29/2022 02:28:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=29
05/29/2022 02:28:41 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.7538509752542794 on epoch=29
05/29/2022 02:28:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7528249726672523 -> 0.7538509752542794 on epoch=29, global_step=950
05/29/2022 02:28:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=29
05/29/2022 02:28:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=30
05/29/2022 02:28:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=30
05/29/2022 02:28:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=30
05/29/2022 02:28:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=31
05/29/2022 02:29:00 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.7686069392809264 on epoch=31
05/29/2022 02:29:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7538509752542794 -> 0.7686069392809264 on epoch=31, global_step=1000
05/29/2022 02:29:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=31
05/29/2022 02:29:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=31
05/29/2022 02:29:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=32
05/29/2022 02:29:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=32
05/29/2022 02:29:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=32
05/29/2022 02:29:19 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.7602480006065123 on epoch=32
05/29/2022 02:29:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
05/29/2022 02:29:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=33
05/29/2022 02:29:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=33
05/29/2022 02:29:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=34
05/29/2022 02:29:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=34
05/29/2022 02:29:39 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.7684247609779525 on epoch=34
05/29/2022 02:29:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=34
05/29/2022 02:29:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=34
05/29/2022 02:29:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=35
05/29/2022 02:29:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=35
05/29/2022 02:29:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=35
05/29/2022 02:29:58 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.7742871608453235 on epoch=35
05/29/2022 02:29:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7686069392809264 -> 0.7742871608453235 on epoch=35, global_step=1150
05/29/2022 02:30:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=36
05/29/2022 02:30:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=36
05/29/2022 02:30:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=36
05/29/2022 02:30:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=37
05/29/2022 02:30:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=37
05/29/2022 02:30:17 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.777520654969154 on epoch=37
05/29/2022 02:30:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7742871608453235 -> 0.777520654969154 on epoch=37, global_step=1200
05/29/2022 02:30:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=37
05/29/2022 02:30:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=38
05/29/2022 02:30:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=38
05/29/2022 02:30:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
05/29/2022 02:30:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=39
05/29/2022 02:30:37 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.7856003869747945 on epoch=39
05/29/2022 02:30:37 - INFO - __main__ - Saving model with best Classification-F1: 0.777520654969154 -> 0.7856003869747945 on epoch=39, global_step=1250
05/29/2022 02:30:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=39
05/29/2022 02:30:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
05/29/2022 02:30:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=39
05/29/2022 02:30:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=40
05/29/2022 02:30:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=40
05/29/2022 02:30:56 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7916680507657674 on epoch=40
05/29/2022 02:30:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7856003869747945 -> 0.7916680507657674 on epoch=40, global_step=1300
05/29/2022 02:30:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=40
05/29/2022 02:31:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=41
05/29/2022 02:31:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=41
05/29/2022 02:31:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=41
05/29/2022 02:31:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=42
05/29/2022 02:31:16 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7774094751149212 on epoch=42
05/29/2022 02:31:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=42
05/29/2022 02:31:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=42
05/29/2022 02:31:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=43
05/29/2022 02:31:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=43
05/29/2022 02:31:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=43
05/29/2022 02:31:35 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.8172341571826711 on epoch=43
05/29/2022 02:31:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7916680507657674 -> 0.8172341571826711 on epoch=43, global_step=1400
05/29/2022 02:31:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=44
05/29/2022 02:31:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=44
05/29/2022 02:31:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=44
05/29/2022 02:31:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=44
05/29/2022 02:31:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=45
05/29/2022 02:31:54 - INFO - __main__ - Global step 1450 Train loss 0.28 Classification-F1 0.8172229351881164 on epoch=45
05/29/2022 02:31:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
05/29/2022 02:31:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=45
05/29/2022 02:32:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
05/29/2022 02:32:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=46
05/29/2022 02:32:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=46
05/29/2022 02:32:14 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.8167383054210632 on epoch=46
05/29/2022 02:32:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=47
05/29/2022 02:32:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=47
05/29/2022 02:32:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=47
05/29/2022 02:32:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=48
05/29/2022 02:32:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=48
05/29/2022 02:32:33 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.8088087266279853 on epoch=48
05/29/2022 02:32:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=48
05/29/2022 02:32:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=49
05/29/2022 02:32:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=49
05/29/2022 02:32:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=49
05/29/2022 02:32:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=49
05/29/2022 02:32:52 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.827051439704608 on epoch=49
05/29/2022 02:32:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8172341571826711 -> 0.827051439704608 on epoch=49, global_step=1600
05/29/2022 02:32:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=50
05/29/2022 02:32:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=50
05/29/2022 02:33:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=50
05/29/2022 02:33:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=51
05/29/2022 02:33:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=51
05/29/2022 02:33:11 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.7948542324374264 on epoch=51
05/29/2022 02:33:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=51
05/29/2022 02:33:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=52
05/29/2022 02:33:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=52
05/29/2022 02:33:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=52
05/29/2022 02:33:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=53
05/29/2022 02:33:31 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.799847654568615 on epoch=53
05/29/2022 02:33:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=53
05/29/2022 02:33:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=53
05/29/2022 02:33:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=54
05/29/2022 02:33:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=54
05/29/2022 02:33:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=54
05/29/2022 02:33:50 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.8114592736020503 on epoch=54
05/29/2022 02:33:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=54
05/29/2022 02:33:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=55
05/29/2022 02:33:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=55
05/29/2022 02:34:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=55
05/29/2022 02:34:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=56
05/29/2022 02:34:10 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.8057382735359471 on epoch=56
05/29/2022 02:34:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=56
05/29/2022 02:34:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=56
05/29/2022 02:34:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=57
05/29/2022 02:34:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=57
05/29/2022 02:34:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=57
05/29/2022 02:34:29 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.8395306558410531 on epoch=57
05/29/2022 02:34:29 - INFO - __main__ - Saving model with best Classification-F1: 0.827051439704608 -> 0.8395306558410531 on epoch=57, global_step=1850
05/29/2022 02:34:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=58
05/29/2022 02:34:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=58
05/29/2022 02:34:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=58
05/29/2022 02:34:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=59
05/29/2022 02:34:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=59
05/29/2022 02:34:49 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.7986930798353702 on epoch=59
05/29/2022 02:34:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=59
05/29/2022 02:34:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
05/29/2022 02:34:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=60
05/29/2022 02:34:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=60
05/29/2022 02:35:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=60
05/29/2022 02:35:08 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.8349771470311024 on epoch=60
05/29/2022 02:35:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=61
05/29/2022 02:35:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
05/29/2022 02:35:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=61
05/29/2022 02:35:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=62
05/29/2022 02:35:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=62
05/29/2022 02:35:28 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.7878752944714551 on epoch=62
05/29/2022 02:35:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=62
05/29/2022 02:35:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=63
05/29/2022 02:35:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=63
05/29/2022 02:35:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=63
05/29/2022 02:35:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=64
05/29/2022 02:35:47 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.8308139719736678 on epoch=64
05/29/2022 02:35:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=64
05/29/2022 02:35:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=64
05/29/2022 02:35:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=64
05/29/2022 02:35:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=65
05/29/2022 02:35:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=65
05/29/2022 02:36:06 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.8086158857993235 on epoch=65
05/29/2022 02:36:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=65
05/29/2022 02:36:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=66
05/29/2022 02:36:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=66
05/29/2022 02:36:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=66
05/29/2022 02:36:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=67
05/29/2022 02:36:26 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.806397192678502 on epoch=67
05/29/2022 02:36:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=67
05/29/2022 02:36:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=67
05/29/2022 02:36:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=68
05/29/2022 02:36:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=68
05/29/2022 02:36:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
05/29/2022 02:36:45 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.8205061771657359 on epoch=68
05/29/2022 02:36:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=69
05/29/2022 02:36:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=69
05/29/2022 02:36:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=69
05/29/2022 02:36:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=69
05/29/2022 02:36:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=70
05/29/2022 02:37:05 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.8081222085183136 on epoch=70
05/29/2022 02:37:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.27 on epoch=70
05/29/2022 02:37:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.28 on epoch=70
05/29/2022 02:37:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=71
05/29/2022 02:37:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=71
05/29/2022 02:37:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=71
05/29/2022 02:37:24 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.8280244650345164 on epoch=71
05/29/2022 02:37:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=72
05/29/2022 02:37:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=72
05/29/2022 02:37:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=72
05/29/2022 02:37:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=73
05/29/2022 02:37:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=73
05/29/2022 02:37:44 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.8243409476016685 on epoch=73
05/29/2022 02:37:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=73
05/29/2022 02:37:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=74
05/29/2022 02:37:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=74
05/29/2022 02:37:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
05/29/2022 02:37:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/29/2022 02:38:03 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.8164093586868884 on epoch=74
05/29/2022 02:38:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=75
05/29/2022 02:38:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=75
05/29/2022 02:38:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=75
05/29/2022 02:38:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
05/29/2022 02:38:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=76
05/29/2022 02:38:23 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.8008620051819845 on epoch=76
05/29/2022 02:38:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=76
05/29/2022 02:38:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=77
05/29/2022 02:38:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=77
05/29/2022 02:38:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=77
05/29/2022 02:38:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=78
05/29/2022 02:38:42 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.8333103788640426 on epoch=78
05/29/2022 02:38:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=78
05/29/2022 02:38:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=78
05/29/2022 02:38:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=79
05/29/2022 02:38:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=79
05/29/2022 02:38:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=79
05/29/2022 02:39:02 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.8121268521286336 on epoch=79
05/29/2022 02:39:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=79
05/29/2022 02:39:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=80
05/29/2022 02:39:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
05/29/2022 02:39:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
05/29/2022 02:39:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/29/2022 02:39:21 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.7985449657398465 on epoch=81
05/29/2022 02:39:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=81
05/29/2022 02:39:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
05/29/2022 02:39:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=82
05/29/2022 02:39:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=82
05/29/2022 02:39:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=82
05/29/2022 02:39:41 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.797381080993735 on epoch=82
05/29/2022 02:39:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=83
05/29/2022 02:39:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=83
05/29/2022 02:39:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/29/2022 02:39:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=84
05/29/2022 02:39:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=84
05/29/2022 02:40:01 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.8073166361200723 on epoch=84
05/29/2022 02:40:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=84
05/29/2022 02:40:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=84
05/29/2022 02:40:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=85
05/29/2022 02:40:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=85
05/29/2022 02:40:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=85
05/29/2022 02:40:20 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.8192453848803223 on epoch=85
05/29/2022 02:40:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=86
05/29/2022 02:40:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=86
05/29/2022 02:40:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=86
05/29/2022 02:40:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=87
05/29/2022 02:40:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=87
05/29/2022 02:40:39 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.8052334888092346 on epoch=87
05/29/2022 02:40:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=87
05/29/2022 02:40:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=88
05/29/2022 02:40:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.25 on epoch=88
05/29/2022 02:40:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=88
05/29/2022 02:40:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=89
05/29/2022 02:40:59 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.8133872046694061 on epoch=89
05/29/2022 02:41:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=89
05/29/2022 02:41:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=89
05/29/2022 02:41:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=89
05/29/2022 02:41:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.26 on epoch=90
05/29/2022 02:41:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
05/29/2022 02:41:18 - INFO - __main__ - Global step 2900 Train loss 0.21 Classification-F1 0.7898114304761761 on epoch=90
05/29/2022 02:41:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=90
05/29/2022 02:41:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=91
05/29/2022 02:41:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.24 on epoch=91
05/29/2022 02:41:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=91
05/29/2022 02:41:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=92
05/29/2022 02:41:38 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.7933862279821288 on epoch=92
05/29/2022 02:41:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.27 on epoch=92
05/29/2022 02:41:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=92
05/29/2022 02:41:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/29/2022 02:41:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=93
05/29/2022 02:41:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=93
05/29/2022 02:41:52 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:41:52 - INFO - __main__ - Printing 3 examples
05/29/2022 02:41:52 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 02:41:52 - INFO - __main__ - ['others']
05/29/2022 02:41:52 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 02:41:52 - INFO - __main__ - ['others']
05/29/2022 02:41:52 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 02:41:52 - INFO - __main__ - ['others']
05/29/2022 02:41:52 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:41:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:41:53 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 02:41:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:41:53 - INFO - __main__ - Printing 3 examples
05/29/2022 02:41:53 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 02:41:53 - INFO - __main__ - ['others']
05/29/2022 02:41:53 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 02:41:53 - INFO - __main__ - ['others']
05/29/2022 02:41:53 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 02:41:53 - INFO - __main__ - ['others']
05/29/2022 02:41:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:41:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:41:53 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 02:41:57 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.8121884237303081 on epoch=93
05/29/2022 02:41:57 - INFO - __main__ - save last model!
05/29/2022 02:41:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 02:41:57 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 02:41:57 - INFO - __main__ - Printing 3 examples
05/29/2022 02:41:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 02:41:57 - INFO - __main__ - ['others']
05/29/2022 02:41:57 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 02:41:57 - INFO - __main__ - ['others']
05/29/2022 02:41:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 02:41:57 - INFO - __main__ - ['others']
05/29/2022 02:41:57 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:42:00 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:42:05 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 02:42:09 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 02:42:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 02:42:10 - INFO - __main__ - Starting training!
05/29/2022 02:43:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_100_0.3_8_predictions.txt
05/29/2022 02:43:21 - INFO - __main__ - Classification-F1 on test data: 0.5420
05/29/2022 02:43:21 - INFO - __main__ - prefix=emo_128_100, lr=0.3, bsz=8, dev_performance=0.8395306558410531, test_performance=0.5419824776104694
05/29/2022 02:43:21 - INFO - __main__ - Running ... prefix=emo_128_100, lr=0.2, bsz=8 ...
05/29/2022 02:43:22 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:43:22 - INFO - __main__ - Printing 3 examples
05/29/2022 02:43:22 - INFO - __main__ -  [emo] how cause yes am listening
05/29/2022 02:43:22 - INFO - __main__ - ['others']
05/29/2022 02:43:22 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/29/2022 02:43:22 - INFO - __main__ - ['others']
05/29/2022 02:43:22 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/29/2022 02:43:22 - INFO - __main__ - ['others']
05/29/2022 02:43:22 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:43:22 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:43:23 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 02:43:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 02:43:23 - INFO - __main__ - Printing 3 examples
05/29/2022 02:43:23 - INFO - __main__ -  [emo] when when it comes to you never why
05/29/2022 02:43:23 - INFO - __main__ - ['others']
05/29/2022 02:43:23 - INFO - __main__ -  [emo] whats your fun fact here's a fun fact did you know that i hate fun facts where do you live
05/29/2022 02:43:23 - INFO - __main__ - ['others']
05/29/2022 02:43:23 - INFO - __main__ -  [emo] yes i am okay ok but why are you still awake  and you
05/29/2022 02:43:23 - INFO - __main__ - ['others']
05/29/2022 02:43:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 02:43:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 02:43:24 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 02:43:39 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 02:43:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 02:43:40 - INFO - __main__ - Starting training!
05/29/2022 02:43:43 - INFO - __main__ - Step 10 Global step 10 Train loss 4.10 on epoch=0
05/29/2022 02:43:45 - INFO - __main__ - Step 20 Global step 20 Train loss 3.60 on epoch=0
05/29/2022 02:43:48 - INFO - __main__ - Step 30 Global step 30 Train loss 3.01 on epoch=0
05/29/2022 02:43:51 - INFO - __main__ - Step 40 Global step 40 Train loss 2.53 on epoch=1
05/29/2022 02:43:53 - INFO - __main__ - Step 50 Global step 50 Train loss 2.52 on epoch=1
05/29/2022 02:44:00 - INFO - __main__ - Global step 50 Train loss 3.15 Classification-F1 0.014710040080493605 on epoch=1
05/29/2022 02:44:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.014710040080493605 on epoch=1, global_step=50
05/29/2022 02:44:03 - INFO - __main__ - Step 60 Global step 60 Train loss 2.38 on epoch=1
05/29/2022 02:44:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.92 on epoch=2
05/29/2022 02:44:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.96 on epoch=2
05/29/2022 02:44:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.59 on epoch=2
05/29/2022 02:44:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.44 on epoch=3
05/29/2022 02:44:20 - INFO - __main__ - Global step 100 Train loss 1.86 Classification-F1 0.222957596607003 on epoch=3
05/29/2022 02:44:20 - INFO - __main__ - Saving model with best Classification-F1: 0.014710040080493605 -> 0.222957596607003 on epoch=3, global_step=100
05/29/2022 02:44:22 - INFO - __main__ - Step 110 Global step 110 Train loss 1.46 on epoch=3
05/29/2022 02:44:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.21 on epoch=3
05/29/2022 02:44:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=4
05/29/2022 02:44:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.94 on epoch=4
05/29/2022 02:44:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.07 on epoch=4
05/29/2022 02:44:39 - INFO - __main__ - Global step 150 Train loss 1.14 Classification-F1 0.4013245249164016 on epoch=4
05/29/2022 02:44:39 - INFO - __main__ - Saving model with best Classification-F1: 0.222957596607003 -> 0.4013245249164016 on epoch=4, global_step=150
05/29/2022 02:44:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.03 on epoch=4
05/29/2022 02:44:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.92 on epoch=5
05/29/2022 02:44:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=5
05/29/2022 02:44:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=5
05/29/2022 02:44:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.76 on epoch=6
05/29/2022 02:44:59 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.5237789721634313 on epoch=6
05/29/2022 02:44:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4013245249164016 -> 0.5237789721634313 on epoch=6, global_step=200
05/29/2022 02:45:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=6
05/29/2022 02:45:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=6
05/29/2022 02:45:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=7
05/29/2022 02:45:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=7
05/29/2022 02:45:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.76 on epoch=7
05/29/2022 02:45:18 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5427473999482992 on epoch=7
05/29/2022 02:45:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5237789721634313 -> 0.5427473999482992 on epoch=7, global_step=250
05/29/2022 02:45:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=8
05/29/2022 02:45:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=8
05/29/2022 02:45:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=8
05/29/2022 02:45:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=9
05/29/2022 02:45:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=9
05/29/2022 02:45:37 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.574769699907747 on epoch=9
05/29/2022 02:45:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5427473999482992 -> 0.574769699907747 on epoch=9, global_step=300
05/29/2022 02:45:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=9
05/29/2022 02:45:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=9
05/29/2022 02:45:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=10
05/29/2022 02:45:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.74 on epoch=10
05/29/2022 02:45:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=10
05/29/2022 02:45:57 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.5813313586057931 on epoch=10
05/29/2022 02:45:57 - INFO - __main__ - Saving model with best Classification-F1: 0.574769699907747 -> 0.5813313586057931 on epoch=10, global_step=350
05/29/2022 02:45:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=11
05/29/2022 02:46:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=11
05/29/2022 02:46:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=11
05/29/2022 02:46:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=12
05/29/2022 02:46:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=12
05/29/2022 02:46:16 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.6182351621048869 on epoch=12
05/29/2022 02:46:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5813313586057931 -> 0.6182351621048869 on epoch=12, global_step=400
05/29/2022 02:46:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=12
05/29/2022 02:46:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=13
05/29/2022 02:46:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=13
05/29/2022 02:46:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=13
05/29/2022 02:46:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=14
05/29/2022 02:46:36 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.6113130616106477 on epoch=14
05/29/2022 02:46:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.59 on epoch=14
05/29/2022 02:46:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=14
05/29/2022 02:46:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=14
05/29/2022 02:46:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=15
05/29/2022 02:46:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.68 on epoch=15
05/29/2022 02:46:55 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.6335940781684732 on epoch=15
05/29/2022 02:46:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6182351621048869 -> 0.6335940781684732 on epoch=15, global_step=500
05/29/2022 02:46:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.73 on epoch=15
05/29/2022 02:47:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=16
05/29/2022 02:47:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.62 on epoch=16
05/29/2022 02:47:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=16
05/29/2022 02:47:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=17
05/29/2022 02:47:14 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.6446596396694475 on epoch=17
05/29/2022 02:47:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6335940781684732 -> 0.6446596396694475 on epoch=17, global_step=550
05/29/2022 02:47:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=17
05/29/2022 02:47:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=17
05/29/2022 02:47:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=18
05/29/2022 02:47:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=18
05/29/2022 02:47:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=18
05/29/2022 02:47:33 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.6797225448929081 on epoch=18
05/29/2022 02:47:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6446596396694475 -> 0.6797225448929081 on epoch=18, global_step=600
05/29/2022 02:47:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=19
05/29/2022 02:47:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=19
05/29/2022 02:47:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=19
05/29/2022 02:47:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=19
05/29/2022 02:47:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=20
05/29/2022 02:47:53 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.6836045770168642 on epoch=20
05/29/2022 02:47:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6797225448929081 -> 0.6836045770168642 on epoch=20, global_step=650
05/29/2022 02:47:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.58 on epoch=20
05/29/2022 02:47:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=20
05/29/2022 02:48:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=21
05/29/2022 02:48:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=21
05/29/2022 02:48:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=21
05/29/2022 02:48:12 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.7022959245708441 on epoch=21
05/29/2022 02:48:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6836045770168642 -> 0.7022959245708441 on epoch=21, global_step=700
05/29/2022 02:48:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=22
05/29/2022 02:48:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.57 on epoch=22
05/29/2022 02:48:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.55 on epoch=22
05/29/2022 02:48:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=23
05/29/2022 02:48:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.54 on epoch=23
05/29/2022 02:48:32 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.7150548230332241 on epoch=23
05/29/2022 02:48:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7022959245708441 -> 0.7150548230332241 on epoch=23, global_step=750
05/29/2022 02:48:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=23
05/29/2022 02:48:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=24
05/29/2022 02:48:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=24
05/29/2022 02:48:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.57 on epoch=24
05/29/2022 02:48:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=24
05/29/2022 02:48:51 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.7156473871948446 on epoch=24
05/29/2022 02:48:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7150548230332241 -> 0.7156473871948446 on epoch=24, global_step=800
05/29/2022 02:48:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=25
05/29/2022 02:48:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=25
05/29/2022 02:48:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.63 on epoch=25
05/29/2022 02:49:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=26
05/29/2022 02:49:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=26
05/29/2022 02:49:10 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.7143885507646373 on epoch=26
05/29/2022 02:49:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=26
05/29/2022 02:49:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=27
05/29/2022 02:49:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=27
05/29/2022 02:49:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=27
05/29/2022 02:49:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=28
05/29/2022 02:49:30 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.711439695772089 on epoch=28
05/29/2022 02:49:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=28
05/29/2022 02:49:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=28
05/29/2022 02:49:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=29
05/29/2022 02:49:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.53 on epoch=29
05/29/2022 02:49:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
05/29/2022 02:49:49 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.7240021318811212 on epoch=29
05/29/2022 02:49:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7156473871948446 -> 0.7240021318811212 on epoch=29, global_step=950
05/29/2022 02:49:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=29
05/29/2022 02:49:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=30
05/29/2022 02:49:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=30
05/29/2022 02:49:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=30
05/29/2022 02:50:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=31
05/29/2022 02:50:09 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.7292923695674574 on epoch=31
05/29/2022 02:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7240021318811212 -> 0.7292923695674574 on epoch=31, global_step=1000
05/29/2022 02:50:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=31
05/29/2022 02:50:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=31
05/29/2022 02:50:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=32
05/29/2022 02:50:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=32
05/29/2022 02:50:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=32
05/29/2022 02:50:28 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.7487281630885394 on epoch=32
05/29/2022 02:50:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7292923695674574 -> 0.7487281630885394 on epoch=32, global_step=1050
05/29/2022 02:50:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=33
05/29/2022 02:50:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=33
05/29/2022 02:50:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=33
05/29/2022 02:50:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=34
05/29/2022 02:50:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=34
05/29/2022 02:50:48 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.722371776413636 on epoch=34
05/29/2022 02:50:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=34
05/29/2022 02:50:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=34
05/29/2022 02:50:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=35
05/29/2022 02:50:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=35
05/29/2022 02:51:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=35
05/29/2022 02:51:08 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.7454549359032281 on epoch=35
05/29/2022 02:51:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=36
05/29/2022 02:51:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=36
05/29/2022 02:51:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=36
05/29/2022 02:51:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=37
05/29/2022 02:51:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.52 on epoch=37
05/29/2022 02:51:27 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.7485714063746077 on epoch=37
05/29/2022 02:51:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=37
05/29/2022 02:51:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=38
05/29/2022 02:51:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=38
05/29/2022 02:51:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=38
05/29/2022 02:51:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=39
05/29/2022 02:51:46 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.7684641556995805 on epoch=39
05/29/2022 02:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7487281630885394 -> 0.7684641556995805 on epoch=39, global_step=1250
05/29/2022 02:51:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=39
05/29/2022 02:51:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=39
05/29/2022 02:51:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=39
05/29/2022 02:51:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=40
05/29/2022 02:51:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=40
05/29/2022 02:52:06 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.7485451417734754 on epoch=40
05/29/2022 02:52:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.53 on epoch=40
05/29/2022 02:52:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=41
05/29/2022 02:52:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=41
05/29/2022 02:52:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=41
05/29/2022 02:52:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=42
05/29/2022 02:52:25 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.7354891654710762 on epoch=42
05/29/2022 02:52:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=42
05/29/2022 02:52:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=42
05/29/2022 02:52:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=43
05/29/2022 02:52:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=43
05/29/2022 02:52:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=43
05/29/2022 02:52:44 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.7646238464598571 on epoch=43
05/29/2022 02:52:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
05/29/2022 02:52:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=44
05/29/2022 02:52:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=44
05/29/2022 02:52:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=44
05/29/2022 02:52:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=45
05/29/2022 02:53:04 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.76139704173439 on epoch=45
05/29/2022 02:53:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
05/29/2022 02:53:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=45
05/29/2022 02:53:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=46
05/29/2022 02:53:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=46
05/29/2022 02:53:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=46
05/29/2022 02:53:23 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.778603862190935 on epoch=46
05/29/2022 02:53:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7684641556995805 -> 0.778603862190935 on epoch=46, global_step=1500
05/29/2022 02:53:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=47
05/29/2022 02:53:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.49 on epoch=47
05/29/2022 02:53:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=47
05/29/2022 02:53:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=48
05/29/2022 02:53:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=48
05/29/2022 02:53:43 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.7751302055496276 on epoch=48
05/29/2022 02:53:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=48
05/29/2022 02:53:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=49
05/29/2022 02:53:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=49
05/29/2022 02:53:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=49
05/29/2022 02:53:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=49
05/29/2022 02:54:02 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.7823059818837301 on epoch=49
05/29/2022 02:54:02 - INFO - __main__ - Saving model with best Classification-F1: 0.778603862190935 -> 0.7823059818837301 on epoch=49, global_step=1600
05/29/2022 02:54:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=50
05/29/2022 02:54:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=50
05/29/2022 02:54:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=50
05/29/2022 02:54:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=51
05/29/2022 02:54:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=51
05/29/2022 02:54:21 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.779255027536671 on epoch=51
05/29/2022 02:54:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=51
05/29/2022 02:54:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=52
05/29/2022 02:54:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=52
05/29/2022 02:54:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=52
05/29/2022 02:54:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=53
05/29/2022 02:54:41 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.7902266887969144 on epoch=53
05/29/2022 02:54:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7823059818837301 -> 0.7902266887969144 on epoch=53, global_step=1700
05/29/2022 02:54:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=53
05/29/2022 02:54:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
05/29/2022 02:54:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=54
05/29/2022 02:54:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=54
05/29/2022 02:54:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=54
05/29/2022 02:55:00 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.7857381234859463 on epoch=54
05/29/2022 02:55:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=54
05/29/2022 02:55:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
05/29/2022 02:55:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=55
05/29/2022 02:55:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=55
05/29/2022 02:55:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
05/29/2022 02:55:19 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.7777027189902107 on epoch=56
05/29/2022 02:55:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=56
05/29/2022 02:55:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=56
05/29/2022 02:55:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=57
05/29/2022 02:55:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=57
05/29/2022 02:55:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=57
05/29/2022 02:55:39 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.7958953930448365 on epoch=57
05/29/2022 02:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7902266887969144 -> 0.7958953930448365 on epoch=57, global_step=1850
05/29/2022 02:55:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=58
05/29/2022 02:55:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=58
05/29/2022 02:55:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=58
05/29/2022 02:55:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
05/29/2022 02:55:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=59
05/29/2022 02:55:58 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.7888355287775418 on epoch=59
05/29/2022 02:56:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=59
05/29/2022 02:56:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=59
05/29/2022 02:56:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
05/29/2022 02:56:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=60
05/29/2022 02:56:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=60
05/29/2022 02:56:18 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.7972538858128763 on epoch=60
05/29/2022 02:56:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7958953930448365 -> 0.7972538858128763 on epoch=60, global_step=1950
05/29/2022 02:56:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
05/29/2022 02:56:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=61
05/29/2022 02:56:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=61
05/29/2022 02:56:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=62
05/29/2022 02:56:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=62
05/29/2022 02:56:37 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.7921980254979376 on epoch=62
05/29/2022 02:56:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=62
05/29/2022 02:56:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=63
05/29/2022 02:56:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=63
05/29/2022 02:56:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.30 on epoch=63
05/29/2022 02:56:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=64
05/29/2022 02:56:57 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.8045556566717897 on epoch=64
05/29/2022 02:56:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7972538858128763 -> 0.8045556566717897 on epoch=64, global_step=2050
05/29/2022 02:56:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=64
05/29/2022 02:57:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=64
05/29/2022 02:57:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
05/29/2022 02:57:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=65
05/29/2022 02:57:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=65
05/29/2022 02:57:16 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.8050224444117791 on epoch=65
05/29/2022 02:57:16 - INFO - __main__ - Saving model with best Classification-F1: 0.8045556566717897 -> 0.8050224444117791 on epoch=65, global_step=2100
05/29/2022 02:57:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=65
05/29/2022 02:57:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=66
05/29/2022 02:57:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.47 on epoch=66
05/29/2022 02:57:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=66
05/29/2022 02:57:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=67
05/29/2022 02:57:36 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.7948460050289603 on epoch=67
05/29/2022 02:57:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=67
05/29/2022 02:57:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=67
05/29/2022 02:57:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=68
05/29/2022 02:57:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.28 on epoch=68
05/29/2022 02:57:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=68
05/29/2022 02:57:55 - INFO - __main__ - Global step 2200 Train loss 0.31 Classification-F1 0.808697780518499 on epoch=68
05/29/2022 02:57:55 - INFO - __main__ - Saving model with best Classification-F1: 0.8050224444117791 -> 0.808697780518499 on epoch=68, global_step=2200
05/29/2022 02:57:58 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=69
05/29/2022 02:58:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.31 on epoch=69
05/29/2022 02:58:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.33 on epoch=69
05/29/2022 02:58:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=69
05/29/2022 02:58:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.34 on epoch=70
05/29/2022 02:58:15 - INFO - __main__ - Global step 2250 Train loss 0.31 Classification-F1 0.7931199247972296 on epoch=70
05/29/2022 02:58:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=70
05/29/2022 02:58:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.32 on epoch=70
05/29/2022 02:58:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=71
05/29/2022 02:58:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.37 on epoch=71
05/29/2022 02:58:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=71
05/29/2022 02:58:34 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.8037827964027225 on epoch=71
05/29/2022 02:58:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.22 on epoch=72
05/29/2022 02:58:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.33 on epoch=72
05/29/2022 02:58:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.30 on epoch=72
05/29/2022 02:58:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=73
05/29/2022 02:58:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=73
05/29/2022 02:58:54 - INFO - __main__ - Global step 2350 Train loss 0.28 Classification-F1 0.8021805770720174 on epoch=73
05/29/2022 02:58:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=73
05/29/2022 02:58:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=74
05/29/2022 02:59:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.33 on epoch=74
05/29/2022 02:59:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=74
05/29/2022 02:59:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/29/2022 02:59:13 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.8088829173839014 on epoch=74
05/29/2022 02:59:13 - INFO - __main__ - Saving model with best Classification-F1: 0.808697780518499 -> 0.8088829173839014 on epoch=74, global_step=2400
05/29/2022 02:59:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=75
05/29/2022 02:59:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=75
05/29/2022 02:59:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.31 on epoch=75
05/29/2022 02:59:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=76
05/29/2022 02:59:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.33 on epoch=76
05/29/2022 02:59:33 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.8018686823422725 on epoch=76
05/29/2022 02:59:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=76
05/29/2022 02:59:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=77
05/29/2022 02:59:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=77
05/29/2022 02:59:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=77
05/29/2022 02:59:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=78
05/29/2022 02:59:52 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.8068881361858035 on epoch=78
05/29/2022 02:59:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.28 on epoch=78
05/29/2022 02:59:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=78
05/29/2022 03:00:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=79
05/29/2022 03:00:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=79
05/29/2022 03:00:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
05/29/2022 03:00:12 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.8100174097416745 on epoch=79
05/29/2022 03:00:12 - INFO - __main__ - Saving model with best Classification-F1: 0.8088829173839014 -> 0.8100174097416745 on epoch=79, global_step=2550
05/29/2022 03:00:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.23 on epoch=79
05/29/2022 03:00:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=80
05/29/2022 03:00:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=80
05/29/2022 03:00:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=80
05/29/2022 03:00:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=81
05/29/2022 03:00:31 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.7962891858919896 on epoch=81
05/29/2022 03:00:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=81
05/29/2022 03:00:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=81
05/29/2022 03:00:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=82
05/29/2022 03:00:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=82
05/29/2022 03:00:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=82
05/29/2022 03:00:51 - INFO - __main__ - Global step 2650 Train loss 0.31 Classification-F1 0.8190809073708702 on epoch=82
05/29/2022 03:00:51 - INFO - __main__ - Saving model with best Classification-F1: 0.8100174097416745 -> 0.8190809073708702 on epoch=82, global_step=2650
05/29/2022 03:00:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=83
05/29/2022 03:00:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=83
05/29/2022 03:00:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=83
05/29/2022 03:01:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.26 on epoch=84
05/29/2022 03:01:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.33 on epoch=84
05/29/2022 03:01:10 - INFO - __main__ - Global step 2700 Train loss 0.30 Classification-F1 0.7924801013321523 on epoch=84
05/29/2022 03:01:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.28 on epoch=84
05/29/2022 03:01:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=84
05/29/2022 03:01:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=85
05/29/2022 03:01:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.23 on epoch=85
05/29/2022 03:01:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=85
05/29/2022 03:01:30 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.8059148314128217 on epoch=85
05/29/2022 03:01:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=86
05/29/2022 03:01:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.36 on epoch=86
05/29/2022 03:01:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=86
05/29/2022 03:01:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=87
05/29/2022 03:01:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=87
05/29/2022 03:01:49 - INFO - __main__ - Global step 2800 Train loss 0.25 Classification-F1 0.7945965944191351 on epoch=87
05/29/2022 03:01:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=87
05/29/2022 03:01:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=88
05/29/2022 03:01:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=88
05/29/2022 03:01:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
05/29/2022 03:02:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=89
05/29/2022 03:02:09 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.8086402070823049 on epoch=89
05/29/2022 03:02:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=89
05/29/2022 03:02:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.30 on epoch=89
05/29/2022 03:02:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
05/29/2022 03:02:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=90
05/29/2022 03:02:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.31 on epoch=90
05/29/2022 03:02:29 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.8032625348887319 on epoch=90
05/29/2022 03:02:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.33 on epoch=90
05/29/2022 03:02:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.29 on epoch=91
05/29/2022 03:02:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=91
05/29/2022 03:02:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=91
05/29/2022 03:02:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=92
05/29/2022 03:02:48 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.8035827000528613 on epoch=92
05/29/2022 03:02:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.29 on epoch=92
05/29/2022 03:02:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=92
05/29/2022 03:02:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=93
05/29/2022 03:02:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.30 on epoch=93
05/29/2022 03:03:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=93
05/29/2022 03:03:02 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:03:02 - INFO - __main__ - Printing 3 examples
05/29/2022 03:03:02 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:03:02 - INFO - __main__ - ['others']
05/29/2022 03:03:02 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:03:02 - INFO - __main__ - ['others']
05/29/2022 03:03:02 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:03:02 - INFO - __main__ - ['others']
05/29/2022 03:03:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:03:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:03:03 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:03:03 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:03:03 - INFO - __main__ - Printing 3 examples
05/29/2022 03:03:03 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:03:03 - INFO - __main__ - ['others']
05/29/2022 03:03:03 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:03:03 - INFO - __main__ - ['others']
05/29/2022 03:03:03 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:03:03 - INFO - __main__ - ['others']
05/29/2022 03:03:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:03:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:03:04 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:03:08 - INFO - __main__ - Global step 3000 Train loss 0.23 Classification-F1 0.8196182827526111 on epoch=93
05/29/2022 03:03:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8190809073708702 -> 0.8196182827526111 on epoch=93, global_step=3000
05/29/2022 03:03:08 - INFO - __main__ - save last model!
05/29/2022 03:03:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 03:03:08 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 03:03:08 - INFO - __main__ - Printing 3 examples
05/29/2022 03:03:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 03:03:08 - INFO - __main__ - ['others']
05/29/2022 03:03:08 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 03:03:08 - INFO - __main__ - ['others']
05/29/2022 03:03:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 03:03:08 - INFO - __main__ - ['others']
05/29/2022 03:03:08 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:03:10 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:03:15 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 03:03:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:03:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:03:20 - INFO - __main__ - Starting training!
05/29/2022 03:04:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_100_0.2_8_predictions.txt
05/29/2022 03:04:32 - INFO - __main__ - Classification-F1 on test data: 0.5305
05/29/2022 03:04:32 - INFO - __main__ - prefix=emo_128_100, lr=0.2, bsz=8, dev_performance=0.8196182827526111, test_performance=0.5305255302019225
05/29/2022 03:04:32 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.5, bsz=8 ...
05/29/2022 03:04:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:04:33 - INFO - __main__ - Printing 3 examples
05/29/2022 03:04:33 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:04:33 - INFO - __main__ - ['others']
05/29/2022 03:04:33 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:04:33 - INFO - __main__ - ['others']
05/29/2022 03:04:33 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:04:33 - INFO - __main__ - ['others']
05/29/2022 03:04:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:04:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:04:34 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:04:34 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:04:34 - INFO - __main__ - Printing 3 examples
05/29/2022 03:04:34 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:04:34 - INFO - __main__ - ['others']
05/29/2022 03:04:34 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:04:34 - INFO - __main__ - ['others']
05/29/2022 03:04:34 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:04:34 - INFO - __main__ - ['others']
05/29/2022 03:04:34 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:04:34 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:04:35 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:04:53 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:04:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:04:54 - INFO - __main__ - Starting training!
05/29/2022 03:04:57 - INFO - __main__ - Step 10 Global step 10 Train loss 3.94 on epoch=0
05/29/2022 03:05:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.87 on epoch=0
05/29/2022 03:05:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.33 on epoch=0
05/29/2022 03:05:05 - INFO - __main__ - Step 40 Global step 40 Train loss 1.52 on epoch=1
05/29/2022 03:05:07 - INFO - __main__ - Step 50 Global step 50 Train loss 1.63 on epoch=1
05/29/2022 03:05:14 - INFO - __main__ - Global step 50 Train loss 2.46 Classification-F1 0.272510032086069 on epoch=1
05/29/2022 03:05:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.272510032086069 on epoch=1, global_step=50
05/29/2022 03:05:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.22 on epoch=1
05/29/2022 03:05:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=2
05/29/2022 03:05:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=2
05/29/2022 03:05:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
05/29/2022 03:05:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=3
05/29/2022 03:05:33 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.5389354391756938 on epoch=3
05/29/2022 03:05:33 - INFO - __main__ - Saving model with best Classification-F1: 0.272510032086069 -> 0.5389354391756938 on epoch=3, global_step=100
05/29/2022 03:05:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=3
05/29/2022 03:05:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=3
05/29/2022 03:05:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.75 on epoch=4
05/29/2022 03:05:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=4
05/29/2022 03:05:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=4
05/29/2022 03:05:53 - INFO - __main__ - Global step 150 Train loss 0.72 Classification-F1 0.5606206042527454 on epoch=4
05/29/2022 03:05:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5389354391756938 -> 0.5606206042527454 on epoch=4, global_step=150
05/29/2022 03:05:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=4
05/29/2022 03:05:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=5
05/29/2022 03:06:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.65 on epoch=5
05/29/2022 03:06:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.67 on epoch=5
05/29/2022 03:06:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=6
05/29/2022 03:06:12 - INFO - __main__ - Global step 200 Train loss 0.64 Classification-F1 0.5988340718242952 on epoch=6
05/29/2022 03:06:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5606206042527454 -> 0.5988340718242952 on epoch=6, global_step=200
05/29/2022 03:06:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=6
05/29/2022 03:06:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.61 on epoch=6
05/29/2022 03:06:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.72 on epoch=7
05/29/2022 03:06:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=7
05/29/2022 03:06:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=7
05/29/2022 03:06:31 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.542635374978859 on epoch=7
05/29/2022 03:06:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=8
05/29/2022 03:06:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=8
05/29/2022 03:06:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.68 on epoch=8
05/29/2022 03:06:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=9
05/29/2022 03:06:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=9
05/29/2022 03:06:50 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.6495806947832095 on epoch=9
05/29/2022 03:06:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5988340718242952 -> 0.6495806947832095 on epoch=9, global_step=300
05/29/2022 03:06:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=9
05/29/2022 03:06:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=9
05/29/2022 03:06:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=10
05/29/2022 03:07:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=10
05/29/2022 03:07:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=10
05/29/2022 03:07:09 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.6713465504842084 on epoch=10
05/29/2022 03:07:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6495806947832095 -> 0.6713465504842084 on epoch=10, global_step=350
05/29/2022 03:07:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=11
05/29/2022 03:07:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=11
05/29/2022 03:07:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=11
05/29/2022 03:07:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=12
05/29/2022 03:07:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=12
05/29/2022 03:07:28 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.7286998075596027 on epoch=12
05/29/2022 03:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6713465504842084 -> 0.7286998075596027 on epoch=12, global_step=400
05/29/2022 03:07:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=12
05/29/2022 03:07:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=13
05/29/2022 03:07:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=13
05/29/2022 03:07:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=13
05/29/2022 03:07:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=14
05/29/2022 03:07:47 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.7039469728758118 on epoch=14
05/29/2022 03:07:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=14
05/29/2022 03:07:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=14
05/29/2022 03:07:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=14
05/29/2022 03:07:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=15
05/29/2022 03:08:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=15
05/29/2022 03:08:07 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.7312352900588195 on epoch=15
05/29/2022 03:08:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7286998075596027 -> 0.7312352900588195 on epoch=15, global_step=500
05/29/2022 03:08:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=15
05/29/2022 03:08:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=16
05/29/2022 03:08:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=16
05/29/2022 03:08:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=16
05/29/2022 03:08:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=17
05/29/2022 03:08:26 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.6979101564762158 on epoch=17
05/29/2022 03:08:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=17
05/29/2022 03:08:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=17
05/29/2022 03:08:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=18
05/29/2022 03:08:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=18
05/29/2022 03:08:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=18
05/29/2022 03:08:45 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.7678772934002508 on epoch=18
05/29/2022 03:08:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7312352900588195 -> 0.7678772934002508 on epoch=18, global_step=600
05/29/2022 03:08:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=19
05/29/2022 03:08:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=19
05/29/2022 03:08:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=19
05/29/2022 03:08:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=19
05/29/2022 03:08:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=20
05/29/2022 03:09:04 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.7774078211802183 on epoch=20
05/29/2022 03:09:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7678772934002508 -> 0.7774078211802183 on epoch=20, global_step=650
05/29/2022 03:09:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=20
05/29/2022 03:09:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=20
05/29/2022 03:09:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=21
05/29/2022 03:09:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=21
05/29/2022 03:09:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=21
05/29/2022 03:09:23 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.7876857011556133 on epoch=21
05/29/2022 03:09:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7774078211802183 -> 0.7876857011556133 on epoch=21, global_step=700
05/29/2022 03:09:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=22
05/29/2022 03:09:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=22
05/29/2022 03:09:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=22
05/29/2022 03:09:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=23
05/29/2022 03:09:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=23
05/29/2022 03:09:42 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7919011627792816 on epoch=23
05/29/2022 03:09:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7876857011556133 -> 0.7919011627792816 on epoch=23, global_step=750
05/29/2022 03:09:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=23
05/29/2022 03:09:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=24
05/29/2022 03:09:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=24
05/29/2022 03:09:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=24
05/29/2022 03:09:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=24
05/29/2022 03:10:01 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.7773927328743968 on epoch=24
05/29/2022 03:10:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=25
05/29/2022 03:10:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=25
05/29/2022 03:10:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=25
05/29/2022 03:10:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=26
05/29/2022 03:10:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=26
05/29/2022 03:10:21 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.779112179073796 on epoch=26
05/29/2022 03:10:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=26
05/29/2022 03:10:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=27
05/29/2022 03:10:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=27
05/29/2022 03:10:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=27
05/29/2022 03:10:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=28
05/29/2022 03:10:40 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.7285337406641315 on epoch=28
05/29/2022 03:10:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=28
05/29/2022 03:10:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=28
05/29/2022 03:10:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=29
05/29/2022 03:10:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=29
05/29/2022 03:10:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=29
05/29/2022 03:10:59 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.7965963380411111 on epoch=29
05/29/2022 03:10:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7919011627792816 -> 0.7965963380411111 on epoch=29, global_step=950
05/29/2022 03:11:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=29
05/29/2022 03:11:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=30
05/29/2022 03:11:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=30
05/29/2022 03:11:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=30
05/29/2022 03:11:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=31
05/29/2022 03:11:18 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.7947404397363417 on epoch=31
05/29/2022 03:11:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=31
05/29/2022 03:11:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
05/29/2022 03:11:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=32
05/29/2022 03:11:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=32
05/29/2022 03:11:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=32
05/29/2022 03:11:37 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.7875318777766254 on epoch=32
05/29/2022 03:11:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=33
05/29/2022 03:11:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/29/2022 03:11:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=33
05/29/2022 03:11:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=34
05/29/2022 03:11:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=34
05/29/2022 03:11:56 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.8183324700509902 on epoch=34
05/29/2022 03:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7965963380411111 -> 0.8183324700509902 on epoch=34, global_step=1100
05/29/2022 03:11:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=34
05/29/2022 03:12:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=34
05/29/2022 03:12:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=35
05/29/2022 03:12:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=35
05/29/2022 03:12:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/29/2022 03:12:16 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.7971675254569992 on epoch=35
05/29/2022 03:12:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=36
05/29/2022 03:12:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=36
05/29/2022 03:12:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=36
05/29/2022 03:12:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=37
05/29/2022 03:12:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=37
05/29/2022 03:12:35 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.8175355387755511 on epoch=37
05/29/2022 03:12:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=37
05/29/2022 03:12:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=38
05/29/2022 03:12:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=38
05/29/2022 03:12:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=38
05/29/2022 03:12:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=39
05/29/2022 03:12:54 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.7980521505697509 on epoch=39
05/29/2022 03:12:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=39
05/29/2022 03:12:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=39
05/29/2022 03:13:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=39
05/29/2022 03:13:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=40
05/29/2022 03:13:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=40
05/29/2022 03:13:13 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.7896333202735741 on epoch=40
05/29/2022 03:13:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=40
05/29/2022 03:13:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=41
05/29/2022 03:13:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=41
05/29/2022 03:13:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=41
05/29/2022 03:13:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=42
05/29/2022 03:13:32 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.8110708398639769 on epoch=42
05/29/2022 03:13:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=42
05/29/2022 03:13:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=42
05/29/2022 03:13:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=43
05/29/2022 03:13:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=43
05/29/2022 03:13:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=43
05/29/2022 03:13:52 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.7920241861279396 on epoch=43
05/29/2022 03:13:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=44
05/29/2022 03:13:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=44
05/29/2022 03:13:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=44
05/29/2022 03:14:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=44
05/29/2022 03:14:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=45
05/29/2022 03:14:11 - INFO - __main__ - Global step 1450 Train loss 0.24 Classification-F1 0.808406497055588 on epoch=45
05/29/2022 03:14:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=45
05/29/2022 03:14:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/29/2022 03:14:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=46
05/29/2022 03:14:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=46
05/29/2022 03:14:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.27 on epoch=46
05/29/2022 03:14:30 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.8012972273887529 on epoch=46
05/29/2022 03:14:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=47
05/29/2022 03:14:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=47
05/29/2022 03:14:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=47
05/29/2022 03:14:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=48
05/29/2022 03:14:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=48
05/29/2022 03:14:49 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.8098098787907664 on epoch=48
05/29/2022 03:14:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=48
05/29/2022 03:14:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=49
05/29/2022 03:14:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=49
05/29/2022 03:14:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=49
05/29/2022 03:15:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=49
05/29/2022 03:15:08 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.8050214429663384 on epoch=49
05/29/2022 03:15:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=50
05/29/2022 03:15:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=50
05/29/2022 03:15:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=50
05/29/2022 03:15:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=51
05/29/2022 03:15:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=51
05/29/2022 03:15:27 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.8104694304694304 on epoch=51
05/29/2022 03:15:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=51
05/29/2022 03:15:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=52
05/29/2022 03:15:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=52
05/29/2022 03:15:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=52
05/29/2022 03:15:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=53
05/29/2022 03:15:46 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.8090053222317966 on epoch=53
05/29/2022 03:15:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=53
05/29/2022 03:15:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=53
05/29/2022 03:15:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=54
05/29/2022 03:15:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=54
05/29/2022 03:15:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=54
05/29/2022 03:16:06 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.8130968643912948 on epoch=54
05/29/2022 03:16:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=54
05/29/2022 03:16:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
05/29/2022 03:16:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=55
05/29/2022 03:16:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=55
05/29/2022 03:16:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=56
05/29/2022 03:16:25 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.8186540277173089 on epoch=56
05/29/2022 03:16:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8183324700509902 -> 0.8186540277173089 on epoch=56, global_step=1800
05/29/2022 03:16:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=56
05/29/2022 03:16:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=56
05/29/2022 03:16:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=57
05/29/2022 03:16:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=57
05/29/2022 03:16:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=57
05/29/2022 03:16:44 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.8107735807639217 on epoch=57
05/29/2022 03:16:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=58
05/29/2022 03:16:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=58
05/29/2022 03:16:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=58
05/29/2022 03:16:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=59
05/29/2022 03:16:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=59
05/29/2022 03:17:03 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.8252916206428779 on epoch=59
05/29/2022 03:17:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8186540277173089 -> 0.8252916206428779 on epoch=59, global_step=1900
05/29/2022 03:17:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=59
05/29/2022 03:17:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=59
05/29/2022 03:17:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=60
05/29/2022 03:17:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=60
05/29/2022 03:17:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=60
05/29/2022 03:17:23 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.819514729922579 on epoch=60
05/29/2022 03:17:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=61
05/29/2022 03:17:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/29/2022 03:17:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=61
05/29/2022 03:17:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=62
05/29/2022 03:17:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=62
05/29/2022 03:17:42 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.8189036812856805 on epoch=62
05/29/2022 03:17:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=62
05/29/2022 03:17:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=63
05/29/2022 03:17:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=63
05/29/2022 03:17:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=63
05/29/2022 03:17:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=64
05/29/2022 03:18:01 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.8165390121689334 on epoch=64
05/29/2022 03:18:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=64
05/29/2022 03:18:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=64
05/29/2022 03:18:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=64
05/29/2022 03:18:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=65
05/29/2022 03:18:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=65
05/29/2022 03:18:20 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.8004162745162877 on epoch=65
05/29/2022 03:18:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=65
05/29/2022 03:18:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=66
05/29/2022 03:18:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=66
05/29/2022 03:18:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/29/2022 03:18:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=67
05/29/2022 03:18:39 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.8040605846497104 on epoch=67
05/29/2022 03:18:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/29/2022 03:18:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=67
05/29/2022 03:18:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=68
05/29/2022 03:18:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=68
05/29/2022 03:18:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=68
05/29/2022 03:18:58 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.8156538269409771 on epoch=68
05/29/2022 03:19:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=69
05/29/2022 03:19:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=69
05/29/2022 03:19:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=69
05/29/2022 03:19:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=69
05/29/2022 03:19:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/29/2022 03:19:18 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.8203972269968857 on epoch=70
05/29/2022 03:19:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=70
05/29/2022 03:19:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 03:19:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=71
05/29/2022 03:19:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=71
05/29/2022 03:19:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=71
05/29/2022 03:19:37 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.794218137474958 on epoch=71
05/29/2022 03:19:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=72
05/29/2022 03:19:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=72
05/29/2022 03:19:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=72
05/29/2022 03:19:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=73
05/29/2022 03:19:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=73
05/29/2022 03:19:56 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.8237921181626557 on epoch=73
05/29/2022 03:19:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
05/29/2022 03:20:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=74
05/29/2022 03:20:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=74
05/29/2022 03:20:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=74
05/29/2022 03:20:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=74
05/29/2022 03:20:15 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.8108401165367207 on epoch=74
05/29/2022 03:20:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=75
05/29/2022 03:20:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=75
05/29/2022 03:20:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=75
05/29/2022 03:20:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=76
05/29/2022 03:20:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=76
05/29/2022 03:20:34 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.8117388191682938 on epoch=76
05/29/2022 03:20:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
05/29/2022 03:20:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=77
05/29/2022 03:20:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=77
05/29/2022 03:20:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=77
05/29/2022 03:20:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=78
05/29/2022 03:20:54 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.8014778432313471 on epoch=78
05/29/2022 03:20:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=78
05/29/2022 03:20:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=78
05/29/2022 03:21:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=79
05/29/2022 03:21:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=79
05/29/2022 03:21:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=79
05/29/2022 03:21:13 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.8064794127225878 on epoch=79
05/29/2022 03:21:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=79
05/29/2022 03:21:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=80
05/29/2022 03:21:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
05/29/2022 03:21:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=80
05/29/2022 03:21:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=81
05/29/2022 03:21:32 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.8003240760432633 on epoch=81
05/29/2022 03:21:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=81
05/29/2022 03:21:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=81
05/29/2022 03:21:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=82
05/29/2022 03:21:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=82
05/29/2022 03:21:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=82
05/29/2022 03:21:51 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.8136986438773872 on epoch=82
05/29/2022 03:21:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=83
05/29/2022 03:21:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/29/2022 03:21:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/29/2022 03:22:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=84
05/29/2022 03:22:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=84
05/29/2022 03:22:11 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.8097195540127993 on epoch=84
05/29/2022 03:22:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=84
05/29/2022 03:22:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=84
05/29/2022 03:22:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=85
05/29/2022 03:22:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=85
05/29/2022 03:22:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=85
05/29/2022 03:22:30 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.8176699032065605 on epoch=85
05/29/2022 03:22:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=86
05/29/2022 03:22:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=86
05/29/2022 03:22:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=86
05/29/2022 03:22:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
05/29/2022 03:22:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=87
05/29/2022 03:22:49 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.8143965683439368 on epoch=87
05/29/2022 03:22:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=87
05/29/2022 03:22:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=88
05/29/2022 03:22:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=88
05/29/2022 03:22:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/29/2022 03:23:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=89
05/29/2022 03:23:08 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.8150624603610394 on epoch=89
05/29/2022 03:23:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
05/29/2022 03:23:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=89
05/29/2022 03:23:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=89
05/29/2022 03:23:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=90
05/29/2022 03:23:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=90
05/29/2022 03:23:28 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.8154221539105415 on epoch=90
05/29/2022 03:23:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=90
05/29/2022 03:23:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=91
05/29/2022 03:23:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=91
05/29/2022 03:23:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=91
05/29/2022 03:23:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=92
05/29/2022 03:23:47 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.8057227318197901 on epoch=92
05/29/2022 03:23:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=92
05/29/2022 03:23:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=92
05/29/2022 03:23:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=93
05/29/2022 03:23:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=93
05/29/2022 03:23:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=93
05/29/2022 03:24:01 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:24:01 - INFO - __main__ - Printing 3 examples
05/29/2022 03:24:01 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:24:01 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:24:01 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:24:01 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:24:01 - INFO - __main__ - Printing 3 examples
05/29/2022 03:24:01 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:24:01 - INFO - __main__ - ['others']
05/29/2022 03:24:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:24:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:24:02 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:24:06 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.808306217817603 on epoch=93
05/29/2022 03:24:06 - INFO - __main__ - save last model!
05/29/2022 03:24:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 03:24:06 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 03:24:06 - INFO - __main__ - Printing 3 examples
05/29/2022 03:24:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 03:24:06 - INFO - __main__ - ['others']
05/29/2022 03:24:06 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 03:24:06 - INFO - __main__ - ['others']
05/29/2022 03:24:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 03:24:06 - INFO - __main__ - ['others']
05/29/2022 03:24:06 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:24:08 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:24:14 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 03:24:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:24:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:24:21 - INFO - __main__ - Starting training!
05/29/2022 03:25:26 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_13_0.5_8_predictions.txt
05/29/2022 03:25:26 - INFO - __main__ - Classification-F1 on test data: 0.5379
05/29/2022 03:25:26 - INFO - __main__ - prefix=emo_128_13, lr=0.5, bsz=8, dev_performance=0.8252916206428779, test_performance=0.5379385876283808
05/29/2022 03:25:26 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.4, bsz=8 ...
05/29/2022 03:25:27 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:25:27 - INFO - __main__ - Printing 3 examples
05/29/2022 03:25:27 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:25:27 - INFO - __main__ - ['others']
05/29/2022 03:25:27 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:25:27 - INFO - __main__ - ['others']
05/29/2022 03:25:27 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:25:27 - INFO - __main__ - ['others']
05/29/2022 03:25:27 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:25:27 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:25:28 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:25:28 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:25:28 - INFO - __main__ - Printing 3 examples
05/29/2022 03:25:28 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:25:28 - INFO - __main__ - ['others']
05/29/2022 03:25:28 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:25:28 - INFO - __main__ - ['others']
05/29/2022 03:25:28 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:25:28 - INFO - __main__ - ['others']
05/29/2022 03:25:28 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:25:28 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:25:29 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:25:44 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:25:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:25:45 - INFO - __main__ - Starting training!
05/29/2022 03:25:49 - INFO - __main__ - Step 10 Global step 10 Train loss 4.16 on epoch=0
05/29/2022 03:25:51 - INFO - __main__ - Step 20 Global step 20 Train loss 3.15 on epoch=0
05/29/2022 03:25:54 - INFO - __main__ - Step 30 Global step 30 Train loss 2.68 on epoch=0
05/29/2022 03:25:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.86 on epoch=1
05/29/2022 03:25:59 - INFO - __main__ - Step 50 Global step 50 Train loss 1.87 on epoch=1
05/29/2022 03:26:06 - INFO - __main__ - Global step 50 Train loss 2.74 Classification-F1 0.05972288152204974 on epoch=1
05/29/2022 03:26:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.05972288152204974 on epoch=1, global_step=50
05/29/2022 03:26:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.65 on epoch=1
05/29/2022 03:26:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=2
05/29/2022 03:26:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=2
05/29/2022 03:26:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
05/29/2022 03:26:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=3
05/29/2022 03:26:25 - INFO - __main__ - Global step 100 Train loss 1.14 Classification-F1 0.5053357483257654 on epoch=3
05/29/2022 03:26:25 - INFO - __main__ - Saving model with best Classification-F1: 0.05972288152204974 -> 0.5053357483257654 on epoch=3, global_step=100
05/29/2022 03:26:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.82 on epoch=3
05/29/2022 03:26:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=3
05/29/2022 03:26:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=4
05/29/2022 03:26:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=4
05/29/2022 03:26:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=4
05/29/2022 03:26:44 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.5007036842424899 on epoch=4
05/29/2022 03:26:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=4
05/29/2022 03:26:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=5
05/29/2022 03:26:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=5
05/29/2022 03:26:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=5
05/29/2022 03:26:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=6
05/29/2022 03:27:03 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5628962100964957 on epoch=6
05/29/2022 03:27:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5053357483257654 -> 0.5628962100964957 on epoch=6, global_step=200
05/29/2022 03:27:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=6
05/29/2022 03:27:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.68 on epoch=6
05/29/2022 03:27:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=7
05/29/2022 03:27:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=7
05/29/2022 03:27:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=7
05/29/2022 03:27:22 - INFO - __main__ - Global step 250 Train loss 0.68 Classification-F1 0.5777751814146335 on epoch=7
05/29/2022 03:27:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5628962100964957 -> 0.5777751814146335 on epoch=7, global_step=250
05/29/2022 03:27:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=8
05/29/2022 03:27:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=8
05/29/2022 03:27:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=8
05/29/2022 03:27:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=9
05/29/2022 03:27:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=9
05/29/2022 03:27:41 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6451752270717788 on epoch=9
05/29/2022 03:27:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5777751814146335 -> 0.6451752270717788 on epoch=9, global_step=300
05/29/2022 03:27:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=9
05/29/2022 03:27:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=9
05/29/2022 03:27:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=10
05/29/2022 03:27:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.61 on epoch=10
05/29/2022 03:27:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=10
05/29/2022 03:28:00 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.657324600255657 on epoch=10
05/29/2022 03:28:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6451752270717788 -> 0.657324600255657 on epoch=10, global_step=350
05/29/2022 03:28:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=11
05/29/2022 03:28:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.61 on epoch=11
05/29/2022 03:28:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=11
05/29/2022 03:28:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=12
05/29/2022 03:28:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.61 on epoch=12
05/29/2022 03:28:20 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.7120659610560562 on epoch=12
05/29/2022 03:28:20 - INFO - __main__ - Saving model with best Classification-F1: 0.657324600255657 -> 0.7120659610560562 on epoch=12, global_step=400
05/29/2022 03:28:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=12
05/29/2022 03:28:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=13
05/29/2022 03:28:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=13
05/29/2022 03:28:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=13
05/29/2022 03:28:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=14
05/29/2022 03:28:39 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.7169754768060372 on epoch=14
05/29/2022 03:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7120659610560562 -> 0.7169754768060372 on epoch=14, global_step=450
05/29/2022 03:28:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=14
05/29/2022 03:28:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=14
05/29/2022 03:28:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=14
05/29/2022 03:28:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=15
05/29/2022 03:28:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=15
05/29/2022 03:28:58 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.7060785347209203 on epoch=15
05/29/2022 03:29:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=15
05/29/2022 03:29:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=16
05/29/2022 03:29:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=16
05/29/2022 03:29:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=16
05/29/2022 03:29:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=17
05/29/2022 03:29:17 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.695619027210733 on epoch=17
05/29/2022 03:29:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=17
05/29/2022 03:29:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=17
05/29/2022 03:29:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=18
05/29/2022 03:29:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=18
05/29/2022 03:29:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=18
05/29/2022 03:29:36 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.75405042443925 on epoch=18
05/29/2022 03:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7169754768060372 -> 0.75405042443925 on epoch=18, global_step=600
05/29/2022 03:29:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=19
05/29/2022 03:29:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=19
05/29/2022 03:29:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=19
05/29/2022 03:29:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=19
05/29/2022 03:29:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=20
05/29/2022 03:29:55 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.7331128675859444 on epoch=20
05/29/2022 03:29:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=20
05/29/2022 03:30:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=20
05/29/2022 03:30:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=21
05/29/2022 03:30:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=21
05/29/2022 03:30:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=21
05/29/2022 03:30:14 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.750373337394483 on epoch=21
05/29/2022 03:30:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=22
05/29/2022 03:30:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=22
05/29/2022 03:30:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=22
05/29/2022 03:30:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=23
05/29/2022 03:30:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=23
05/29/2022 03:30:34 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.7674216902916341 on epoch=23
05/29/2022 03:30:34 - INFO - __main__ - Saving model with best Classification-F1: 0.75405042443925 -> 0.7674216902916341 on epoch=23, global_step=750
05/29/2022 03:30:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=23
05/29/2022 03:30:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=24
05/29/2022 03:30:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=24
05/29/2022 03:30:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=24
05/29/2022 03:30:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=24
05/29/2022 03:30:53 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.7758614862114235 on epoch=24
05/29/2022 03:30:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7674216902916341 -> 0.7758614862114235 on epoch=24, global_step=800
05/29/2022 03:30:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=25
05/29/2022 03:30:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=25
05/29/2022 03:31:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=25
05/29/2022 03:31:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=26
05/29/2022 03:31:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=26
05/29/2022 03:31:12 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.7477276216733387 on epoch=26
05/29/2022 03:31:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=26
05/29/2022 03:31:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=27
05/29/2022 03:31:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=27
05/29/2022 03:31:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=27
05/29/2022 03:31:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=28
05/29/2022 03:31:31 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.7576801032098133 on epoch=28
05/29/2022 03:31:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=28
05/29/2022 03:31:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=28
05/29/2022 03:31:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=29
05/29/2022 03:31:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=29
05/29/2022 03:31:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=29
05/29/2022 03:31:51 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.7725686232672943 on epoch=29
05/29/2022 03:31:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=29
05/29/2022 03:31:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=30
05/29/2022 03:31:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=30
05/29/2022 03:32:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=30
05/29/2022 03:32:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=31
05/29/2022 03:32:10 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.7771895802191039 on epoch=31
05/29/2022 03:32:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7758614862114235 -> 0.7771895802191039 on epoch=31, global_step=1000
05/29/2022 03:32:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=31
05/29/2022 03:32:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=31
05/29/2022 03:32:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=32
05/29/2022 03:32:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=32
05/29/2022 03:32:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=32
05/29/2022 03:32:29 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.7937175823490242 on epoch=32
05/29/2022 03:32:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7771895802191039 -> 0.7937175823490242 on epoch=32, global_step=1050
05/29/2022 03:32:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=33
05/29/2022 03:32:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=33
05/29/2022 03:32:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=33
05/29/2022 03:32:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=34
05/29/2022 03:32:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=34
05/29/2022 03:32:49 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.7991641076091964 on epoch=34
05/29/2022 03:32:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7937175823490242 -> 0.7991641076091964 on epoch=34, global_step=1100
05/29/2022 03:32:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=34
05/29/2022 03:32:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=34
05/29/2022 03:32:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=35
05/29/2022 03:32:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=35
05/29/2022 03:33:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/29/2022 03:33:08 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.7850864855027654 on epoch=35
05/29/2022 03:33:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=36
05/29/2022 03:33:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=36
05/29/2022 03:33:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=36
05/29/2022 03:33:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=37
05/29/2022 03:33:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=37
05/29/2022 03:33:27 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.7987411535166896 on epoch=37
05/29/2022 03:33:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=37
05/29/2022 03:33:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=38
05/29/2022 03:33:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=38
05/29/2022 03:33:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=38
05/29/2022 03:33:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=39
05/29/2022 03:33:46 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.7780006417821158 on epoch=39
05/29/2022 03:33:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=39
05/29/2022 03:33:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=39
05/29/2022 03:33:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=39
05/29/2022 03:33:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=40
05/29/2022 03:33:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=40
05/29/2022 03:34:06 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.7932904339843305 on epoch=40
05/29/2022 03:34:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=40
05/29/2022 03:34:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=41
05/29/2022 03:34:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=41
05/29/2022 03:34:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=41
05/29/2022 03:34:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=42
05/29/2022 03:34:25 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.7958713637081959 on epoch=42
05/29/2022 03:34:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=42
05/29/2022 03:34:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=42
05/29/2022 03:34:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=43
05/29/2022 03:34:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=43
05/29/2022 03:34:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=43
05/29/2022 03:34:44 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.8037864005886677 on epoch=43
05/29/2022 03:34:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7991641076091964 -> 0.8037864005886677 on epoch=43, global_step=1400
05/29/2022 03:34:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=44
05/29/2022 03:34:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=44
05/29/2022 03:34:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=44
05/29/2022 03:34:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=44
05/29/2022 03:34:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=45
05/29/2022 03:35:04 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.7918548655933546 on epoch=45
05/29/2022 03:35:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=45
05/29/2022 03:35:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=45
05/29/2022 03:35:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=46
05/29/2022 03:35:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=46
05/29/2022 03:35:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=46
05/29/2022 03:35:23 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.79588126767248 on epoch=46
05/29/2022 03:35:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=47
05/29/2022 03:35:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=47
05/29/2022 03:35:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=47
05/29/2022 03:35:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=48
05/29/2022 03:35:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=48
05/29/2022 03:35:42 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.8222672080211967 on epoch=48
05/29/2022 03:35:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8037864005886677 -> 0.8222672080211967 on epoch=48, global_step=1550
05/29/2022 03:35:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=48
05/29/2022 03:35:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=49
05/29/2022 03:35:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=49
05/29/2022 03:35:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=49
05/29/2022 03:35:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=49
05/29/2022 03:36:01 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.7992526204852464 on epoch=49
05/29/2022 03:36:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=50
05/29/2022 03:36:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=50
05/29/2022 03:36:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=50
05/29/2022 03:36:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=51
05/29/2022 03:36:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=51
05/29/2022 03:36:21 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.7920567913945222 on epoch=51
05/29/2022 03:36:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=51
05/29/2022 03:36:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=52
05/29/2022 03:36:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=52
05/29/2022 03:36:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=52
05/29/2022 03:36:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=53
05/29/2022 03:36:40 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.7803098171316526 on epoch=53
05/29/2022 03:36:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=53
05/29/2022 03:36:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=53
05/29/2022 03:36:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=54
05/29/2022 03:36:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=54
05/29/2022 03:36:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=54
05/29/2022 03:36:59 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.7906461006281063 on epoch=54
05/29/2022 03:37:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=54
05/29/2022 03:37:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=55
05/29/2022 03:37:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=55
05/29/2022 03:37:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=55
05/29/2022 03:37:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=56
05/29/2022 03:37:19 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.8032033341058126 on epoch=56
05/29/2022 03:37:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/29/2022 03:37:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=56
05/29/2022 03:37:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=57
05/29/2022 03:37:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=57
05/29/2022 03:37:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=57
05/29/2022 03:37:38 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7906027829080752 on epoch=57
05/29/2022 03:37:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=58
05/29/2022 03:37:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=58
05/29/2022 03:37:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=58
05/29/2022 03:37:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=59
05/29/2022 03:37:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=59
05/29/2022 03:37:57 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.7968790675796585 on epoch=59
05/29/2022 03:37:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=59
05/29/2022 03:38:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/29/2022 03:38:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=60
05/29/2022 03:38:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=60
05/29/2022 03:38:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=60
05/29/2022 03:38:16 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.7811055493758498 on epoch=60
05/29/2022 03:38:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=61
05/29/2022 03:38:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=61
05/29/2022 03:38:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=61
05/29/2022 03:38:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=62
05/29/2022 03:38:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=62
05/29/2022 03:38:35 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.8195439692334223 on epoch=62
05/29/2022 03:38:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=62
05/29/2022 03:38:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=63
05/29/2022 03:38:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=63
05/29/2022 03:38:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=63
05/29/2022 03:38:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=64
05/29/2022 03:38:54 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.7924806300806956 on epoch=64
05/29/2022 03:38:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=64
05/29/2022 03:38:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=64
05/29/2022 03:39:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=64
05/29/2022 03:39:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
05/29/2022 03:39:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=65
05/29/2022 03:39:14 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.7866916447079491 on epoch=65
05/29/2022 03:39:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=65
05/29/2022 03:39:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=66
05/29/2022 03:39:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=66
05/29/2022 03:39:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
05/29/2022 03:39:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=67
05/29/2022 03:39:33 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7827616544435481 on epoch=67
05/29/2022 03:39:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=67
05/29/2022 03:39:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=67
05/29/2022 03:39:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=68
05/29/2022 03:39:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=68
05/29/2022 03:39:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=68
05/29/2022 03:39:52 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.7935185770903311 on epoch=68
05/29/2022 03:39:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=69
05/29/2022 03:39:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=69
05/29/2022 03:40:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=69
05/29/2022 03:40:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
05/29/2022 03:40:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=70
05/29/2022 03:40:11 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.8026237503267856 on epoch=70
05/29/2022 03:40:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
05/29/2022 03:40:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=70
05/29/2022 03:40:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=71
05/29/2022 03:40:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=71
05/29/2022 03:40:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=71
05/29/2022 03:40:31 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7933518431800408 on epoch=71
05/29/2022 03:40:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=72
05/29/2022 03:40:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=72
05/29/2022 03:40:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=72
05/29/2022 03:40:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=73
05/29/2022 03:40:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=73
05/29/2022 03:40:50 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.8133015128438469 on epoch=73
05/29/2022 03:40:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=73
05/29/2022 03:40:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=74
05/29/2022 03:40:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=74
05/29/2022 03:41:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=74
05/29/2022 03:41:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=74
05/29/2022 03:41:09 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.7795918749205564 on epoch=74
05/29/2022 03:41:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=75
05/29/2022 03:41:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=75
05/29/2022 03:41:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=75
05/29/2022 03:41:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=76
05/29/2022 03:41:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=76
05/29/2022 03:41:28 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.7989760088243404 on epoch=76
05/29/2022 03:41:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=76
05/29/2022 03:41:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=77
05/29/2022 03:41:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=77
05/29/2022 03:41:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=77
05/29/2022 03:41:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=78
05/29/2022 03:41:47 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7896432967880477 on epoch=78
05/29/2022 03:41:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=78
05/29/2022 03:41:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=78
05/29/2022 03:41:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=79
05/29/2022 03:41:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/29/2022 03:42:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=79
05/29/2022 03:42:07 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.8073708691075104 on epoch=79
05/29/2022 03:42:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=79
05/29/2022 03:42:12 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=80
05/29/2022 03:42:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=80
05/29/2022 03:42:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
05/29/2022 03:42:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=81
05/29/2022 03:42:26 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.7901506666584744 on epoch=81
05/29/2022 03:42:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.22 on epoch=81
05/29/2022 03:42:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=81
05/29/2022 03:42:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/29/2022 03:42:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=82
05/29/2022 03:42:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
05/29/2022 03:42:46 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.8092290840258229 on epoch=82
05/29/2022 03:42:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=83
05/29/2022 03:42:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=83
05/29/2022 03:42:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/29/2022 03:42:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=84
05/29/2022 03:42:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=84
05/29/2022 03:43:05 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.7967485136676208 on epoch=84
05/29/2022 03:43:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.23 on epoch=84
05/29/2022 03:43:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=84
05/29/2022 03:43:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=85
05/29/2022 03:43:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=85
05/29/2022 03:43:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=85
05/29/2022 03:43:24 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.7824021900359539 on epoch=85
05/29/2022 03:43:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=86
05/29/2022 03:43:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=86
05/29/2022 03:43:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
05/29/2022 03:43:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=87
05/29/2022 03:43:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=87
05/29/2022 03:43:44 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7972067853161908 on epoch=87
05/29/2022 03:43:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.21 on epoch=87
05/29/2022 03:43:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=88
05/29/2022 03:43:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=88
05/29/2022 03:43:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=88
05/29/2022 03:43:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=89
05/29/2022 03:44:03 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.7942547711212031 on epoch=89
05/29/2022 03:44:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
05/29/2022 03:44:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=89
05/29/2022 03:44:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=89
05/29/2022 03:44:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=90
05/29/2022 03:44:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=90
05/29/2022 03:44:22 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.7827570402164057 on epoch=90
05/29/2022 03:44:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=90
05/29/2022 03:44:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=91
05/29/2022 03:44:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=91
05/29/2022 03:44:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.23 on epoch=91
05/29/2022 03:44:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=92
05/29/2022 03:44:42 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.7806033563386504 on epoch=92
05/29/2022 03:44:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=92
05/29/2022 03:44:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=92
05/29/2022 03:44:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=93
05/29/2022 03:44:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=93
05/29/2022 03:44:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
05/29/2022 03:44:55 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:44:55 - INFO - __main__ - Printing 3 examples
05/29/2022 03:44:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:44:55 - INFO - __main__ - ['others']
05/29/2022 03:44:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:44:55 - INFO - __main__ - ['others']
05/29/2022 03:44:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:44:55 - INFO - __main__ - ['others']
05/29/2022 03:44:55 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:44:56 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:44:56 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:44:56 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:44:56 - INFO - __main__ - Printing 3 examples
05/29/2022 03:44:56 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:44:56 - INFO - __main__ - ['others']
05/29/2022 03:44:56 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:44:56 - INFO - __main__ - ['others']
05/29/2022 03:44:56 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:44:56 - INFO - __main__ - ['others']
05/29/2022 03:44:56 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:44:56 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:44:57 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:45:01 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.8020565064592186 on epoch=93
05/29/2022 03:45:01 - INFO - __main__ - save last model!
05/29/2022 03:45:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 03:45:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 03:45:01 - INFO - __main__ - Printing 3 examples
05/29/2022 03:45:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 03:45:01 - INFO - __main__ - ['others']
05/29/2022 03:45:01 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 03:45:01 - INFO - __main__ - ['others']
05/29/2022 03:45:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 03:45:01 - INFO - __main__ - ['others']
05/29/2022 03:45:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:45:03 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:45:08 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 03:45:14 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:45:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:45:15 - INFO - __main__ - Starting training!
05/29/2022 03:46:22 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_13_0.4_8_predictions.txt
05/29/2022 03:46:22 - INFO - __main__ - Classification-F1 on test data: 0.5192
05/29/2022 03:46:23 - INFO - __main__ - prefix=emo_128_13, lr=0.4, bsz=8, dev_performance=0.8222672080211967, test_performance=0.5192184666434065
05/29/2022 03:46:23 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.3, bsz=8 ...
05/29/2022 03:46:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:46:23 - INFO - __main__ - Printing 3 examples
05/29/2022 03:46:23 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 03:46:23 - INFO - __main__ - ['others']
05/29/2022 03:46:23 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 03:46:23 - INFO - __main__ - ['others']
05/29/2022 03:46:23 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 03:46:23 - INFO - __main__ - ['others']
05/29/2022 03:46:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:46:24 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:46:24 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 03:46:24 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 03:46:24 - INFO - __main__ - Printing 3 examples
05/29/2022 03:46:24 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 03:46:24 - INFO - __main__ - ['others']
05/29/2022 03:46:24 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 03:46:24 - INFO - __main__ - ['others']
05/29/2022 03:46:24 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 03:46:24 - INFO - __main__ - ['others']
05/29/2022 03:46:24 - INFO - __main__ - Tokenizing Input ...
05/29/2022 03:46:24 - INFO - __main__ - Tokenizing Output ...
05/29/2022 03:46:25 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 03:46:42 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 03:46:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 03:46:43 - INFO - __main__ - Starting training!
05/29/2022 03:46:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.96 on epoch=0
05/29/2022 03:46:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.16 on epoch=0
05/29/2022 03:46:50 - INFO - __main__ - Step 30 Global step 30 Train loss 2.74 on epoch=0
05/29/2022 03:46:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.08 on epoch=1
05/29/2022 03:46:55 - INFO - __main__ - Step 50 Global step 50 Train loss 2.14 on epoch=1
05/29/2022 03:47:02 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.04823544089780221 on epoch=1
05/29/2022 03:47:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.04823544089780221 on epoch=1, global_step=50
05/29/2022 03:47:05 - INFO - __main__ - Step 60 Global step 60 Train loss 1.97 on epoch=1
05/29/2022 03:47:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.51 on epoch=2
05/29/2022 03:47:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.47 on epoch=2
05/29/2022 03:47:12 - INFO - __main__ - Step 90 Global step 90 Train loss 1.14 on epoch=2
05/29/2022 03:47:15 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=3
05/29/2022 03:47:22 - INFO - __main__ - Global step 100 Train loss 1.43 Classification-F1 0.49616166735865996 on epoch=3
05/29/2022 03:47:22 - INFO - __main__ - Saving model with best Classification-F1: 0.04823544089780221 -> 0.49616166735865996 on epoch=3, global_step=100
05/29/2022 03:47:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=3
05/29/2022 03:47:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=3
05/29/2022 03:47:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=4
05/29/2022 03:47:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=4
05/29/2022 03:47:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=4
05/29/2022 03:47:41 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.5283530871340663 on epoch=4
05/29/2022 03:47:41 - INFO - __main__ - Saving model with best Classification-F1: 0.49616166735865996 -> 0.5283530871340663 on epoch=4, global_step=150
05/29/2022 03:47:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.81 on epoch=4
05/29/2022 03:47:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=5
05/29/2022 03:47:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=5
05/29/2022 03:47:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=5
05/29/2022 03:47:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=6
05/29/2022 03:48:00 - INFO - __main__ - Global step 200 Train loss 0.77 Classification-F1 0.5640550066907021 on epoch=6
05/29/2022 03:48:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5283530871340663 -> 0.5640550066907021 on epoch=6, global_step=200
05/29/2022 03:48:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=6
05/29/2022 03:48:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=6
05/29/2022 03:48:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=7
05/29/2022 03:48:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=7
05/29/2022 03:48:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=7
05/29/2022 03:48:19 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.5339755434654413 on epoch=7
05/29/2022 03:48:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=8
05/29/2022 03:48:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=8
05/29/2022 03:48:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=8
05/29/2022 03:48:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=9
05/29/2022 03:48:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=9
05/29/2022 03:48:38 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.6033581179848112 on epoch=9
05/29/2022 03:48:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5640550066907021 -> 0.6033581179848112 on epoch=9, global_step=300
05/29/2022 03:48:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=9
05/29/2022 03:48:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=9
05/29/2022 03:48:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=10
05/29/2022 03:48:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=10
05/29/2022 03:48:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=10
05/29/2022 03:48:57 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.632460492302553 on epoch=10
05/29/2022 03:48:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6033581179848112 -> 0.632460492302553 on epoch=10, global_step=350
05/29/2022 03:49:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=11
05/29/2022 03:49:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.68 on epoch=11
05/29/2022 03:49:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.61 on epoch=11
05/29/2022 03:49:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=12
05/29/2022 03:49:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=12
05/29/2022 03:49:16 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.6780182663207955 on epoch=12
05/29/2022 03:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.632460492302553 -> 0.6780182663207955 on epoch=12, global_step=400
05/29/2022 03:49:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=12
05/29/2022 03:49:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=13
05/29/2022 03:49:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.71 on epoch=13
05/29/2022 03:49:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=13
05/29/2022 03:49:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=14
05/29/2022 03:49:36 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.6899969692429238 on epoch=14
05/29/2022 03:49:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6780182663207955 -> 0.6899969692429238 on epoch=14, global_step=450
05/29/2022 03:49:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=14
05/29/2022 03:49:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=14
05/29/2022 03:49:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=14
05/29/2022 03:49:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=15
05/29/2022 03:49:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=15
05/29/2022 03:49:55 - INFO - __main__ - Global step 500 Train loss 0.53 Classification-F1 0.6820703285181446 on epoch=15
05/29/2022 03:49:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=15
05/29/2022 03:50:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=16
05/29/2022 03:50:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=16
05/29/2022 03:50:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=16
05/29/2022 03:50:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=17
05/29/2022 03:50:14 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.6720037484117233 on epoch=17
05/29/2022 03:50:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=17
05/29/2022 03:50:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=17
05/29/2022 03:50:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=18
05/29/2022 03:50:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=18
05/29/2022 03:50:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=18
05/29/2022 03:50:33 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.7067108702675713 on epoch=18
05/29/2022 03:50:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6899969692429238 -> 0.7067108702675713 on epoch=18, global_step=600
05/29/2022 03:50:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=19
05/29/2022 03:50:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=19
05/29/2022 03:50:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=19
05/29/2022 03:50:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=19
05/29/2022 03:50:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=20
05/29/2022 03:50:52 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.7138180827608521 on epoch=20
05/29/2022 03:50:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7067108702675713 -> 0.7138180827608521 on epoch=20, global_step=650
05/29/2022 03:50:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=20
05/29/2022 03:50:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=20
05/29/2022 03:50:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=21
05/29/2022 03:51:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=21
05/29/2022 03:51:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=21
05/29/2022 03:51:11 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.707012023369838 on epoch=21
05/29/2022 03:51:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=22
05/29/2022 03:51:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=22
05/29/2022 03:51:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=22
05/29/2022 03:51:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=23
05/29/2022 03:51:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=23
05/29/2022 03:51:30 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.7408580233650418 on epoch=23
05/29/2022 03:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7138180827608521 -> 0.7408580233650418 on epoch=23, global_step=750
05/29/2022 03:51:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=23
05/29/2022 03:51:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=24
05/29/2022 03:51:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=24
05/29/2022 03:51:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=24
05/29/2022 03:51:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=24
05/29/2022 03:51:49 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.750879451210969 on epoch=24
05/29/2022 03:51:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7408580233650418 -> 0.750879451210969 on epoch=24, global_step=800
05/29/2022 03:51:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=25
05/29/2022 03:51:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=25
05/29/2022 03:51:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=25
05/29/2022 03:51:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=26
05/29/2022 03:52:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=26
05/29/2022 03:52:08 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.745831990966051 on epoch=26
05/29/2022 03:52:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=26
05/29/2022 03:52:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=27
05/29/2022 03:52:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=27
05/29/2022 03:52:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=27
05/29/2022 03:52:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=28
05/29/2022 03:52:27 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.753600990645944 on epoch=28
05/29/2022 03:52:27 - INFO - __main__ - Saving model with best Classification-F1: 0.750879451210969 -> 0.753600990645944 on epoch=28, global_step=900
05/29/2022 03:52:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=28
05/29/2022 03:52:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=28
05/29/2022 03:52:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=29
05/29/2022 03:52:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=29
05/29/2022 03:52:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=29
05/29/2022 03:52:46 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.7587537665699708 on epoch=29
05/29/2022 03:52:46 - INFO - __main__ - Saving model with best Classification-F1: 0.753600990645944 -> 0.7587537665699708 on epoch=29, global_step=950
05/29/2022 03:52:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.31 on epoch=29
05/29/2022 03:52:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=30
05/29/2022 03:52:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=30
05/29/2022 03:52:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=30
05/29/2022 03:52:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=31
05/29/2022 03:53:05 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.7550546546049899 on epoch=31
05/29/2022 03:53:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=31
05/29/2022 03:53:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=31
05/29/2022 03:53:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=32
05/29/2022 03:53:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=32
05/29/2022 03:53:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=32
05/29/2022 03:53:24 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.7732041723570304 on epoch=32
05/29/2022 03:53:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7587537665699708 -> 0.7732041723570304 on epoch=32, global_step=1050
05/29/2022 03:53:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=33
05/29/2022 03:53:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=33
05/29/2022 03:53:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=33
05/29/2022 03:53:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=34
05/29/2022 03:53:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=34
05/29/2022 03:53:43 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.7810831921895666 on epoch=34
05/29/2022 03:53:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7732041723570304 -> 0.7810831921895666 on epoch=34, global_step=1100
05/29/2022 03:53:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=34
05/29/2022 03:53:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=34
05/29/2022 03:53:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=35
05/29/2022 03:53:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=35
05/29/2022 03:53:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=35
05/29/2022 03:54:02 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.7722224056597259 on epoch=35
05/29/2022 03:54:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=36
05/29/2022 03:54:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=36
05/29/2022 03:54:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=36
05/29/2022 03:54:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=37
05/29/2022 03:54:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=37
05/29/2022 03:54:21 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7988327336839507 on epoch=37
05/29/2022 03:54:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7810831921895666 -> 0.7988327336839507 on epoch=37, global_step=1200
05/29/2022 03:54:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=37
05/29/2022 03:54:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=38
05/29/2022 03:54:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=38
05/29/2022 03:54:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=38
05/29/2022 03:54:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=39
05/29/2022 03:54:40 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.7760699561558805 on epoch=39
05/29/2022 03:54:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=39
05/29/2022 03:54:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=39
05/29/2022 03:54:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=39
05/29/2022 03:54:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=40
05/29/2022 03:54:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=40
05/29/2022 03:54:59 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7698710738666841 on epoch=40
05/29/2022 03:55:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=40
05/29/2022 03:55:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=41
05/29/2022 03:55:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=41
05/29/2022 03:55:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=41
05/29/2022 03:55:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=42
05/29/2022 03:55:18 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7830779974118837 on epoch=42
05/29/2022 03:55:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=42
05/29/2022 03:55:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=42
05/29/2022 03:55:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=43
05/29/2022 03:55:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=43
05/29/2022 03:55:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=43
05/29/2022 03:55:37 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.7834500475980857 on epoch=43
05/29/2022 03:55:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=44
05/29/2022 03:55:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=44
05/29/2022 03:55:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=44
05/29/2022 03:55:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=44
05/29/2022 03:55:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=45
05/29/2022 03:55:56 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.7717285272207036 on epoch=45
05/29/2022 03:55:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=45
05/29/2022 03:56:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=45
05/29/2022 03:56:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=46
05/29/2022 03:56:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=46
05/29/2022 03:56:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=46
05/29/2022 03:56:15 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.7861422526410811 on epoch=46
05/29/2022 03:56:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=47
05/29/2022 03:56:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=47
05/29/2022 03:56:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=47
05/29/2022 03:56:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=48
05/29/2022 03:56:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=48
05/29/2022 03:56:34 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7931289414374576 on epoch=48
05/29/2022 03:56:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=48
05/29/2022 03:56:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=49
05/29/2022 03:56:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=49
05/29/2022 03:56:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=49
05/29/2022 03:56:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=49
05/29/2022 03:56:53 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.7870459781193702 on epoch=49
05/29/2022 03:56:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=50
05/29/2022 03:56:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=50
05/29/2022 03:57:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=50
05/29/2022 03:57:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=51
05/29/2022 03:57:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=51
05/29/2022 03:57:12 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.7952139891971421 on epoch=51
05/29/2022 03:57:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=51
05/29/2022 03:57:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=52
05/29/2022 03:57:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=52
05/29/2022 03:57:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=52
05/29/2022 03:57:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=53
05/29/2022 03:57:31 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.8009988258599778 on epoch=53
05/29/2022 03:57:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7988327336839507 -> 0.8009988258599778 on epoch=53, global_step=1700
05/29/2022 03:57:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=53
05/29/2022 03:57:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=53
05/29/2022 03:57:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=54
05/29/2022 03:57:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=54
05/29/2022 03:57:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=54
05/29/2022 03:57:50 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7909330712413424 on epoch=54
05/29/2022 03:57:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=54
05/29/2022 03:57:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=55
05/29/2022 03:57:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=55
05/29/2022 03:58:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=55
05/29/2022 03:58:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=56
05/29/2022 03:58:09 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.787900827624877 on epoch=56
05/29/2022 03:58:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=56
05/29/2022 03:58:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=56
05/29/2022 03:58:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=57
05/29/2022 03:58:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=57
05/29/2022 03:58:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=57
05/29/2022 03:58:28 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.7947916601481214 on epoch=57
05/29/2022 03:58:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=58
05/29/2022 03:58:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=58
05/29/2022 03:58:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=58
05/29/2022 03:58:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=59
05/29/2022 03:58:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=59
05/29/2022 03:58:47 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.8078667741802841 on epoch=59
05/29/2022 03:58:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8009988258599778 -> 0.8078667741802841 on epoch=59, global_step=1900
05/29/2022 03:58:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=59
05/29/2022 03:58:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=59
05/29/2022 03:58:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=60
05/29/2022 03:58:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=60
05/29/2022 03:58:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
05/29/2022 03:59:06 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.7990828171242046 on epoch=60
05/29/2022 03:59:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
05/29/2022 03:59:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=61
05/29/2022 03:59:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
05/29/2022 03:59:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=62
05/29/2022 03:59:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.29 on epoch=62
05/29/2022 03:59:25 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.8101002212661322 on epoch=62
05/29/2022 03:59:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8078667741802841 -> 0.8101002212661322 on epoch=62, global_step=2000
05/29/2022 03:59:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=62
05/29/2022 03:59:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=63
05/29/2022 03:59:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=63
05/29/2022 03:59:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=63
05/29/2022 03:59:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=64
05/29/2022 03:59:44 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.7886535523825855 on epoch=64
05/29/2022 03:59:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=64
05/29/2022 03:59:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=64
05/29/2022 03:59:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=64
05/29/2022 03:59:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=65
05/29/2022 03:59:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=65
05/29/2022 04:00:04 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.8022401168828268 on epoch=65
05/29/2022 04:00:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=65
05/29/2022 04:00:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=66
05/29/2022 04:00:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=66
05/29/2022 04:00:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.29 on epoch=66
05/29/2022 04:00:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=67
05/29/2022 04:00:23 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.7870306870846022 on epoch=67
05/29/2022 04:00:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=67
05/29/2022 04:00:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=67
05/29/2022 04:00:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=68
05/29/2022 04:00:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=68
05/29/2022 04:00:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=68
05/29/2022 04:00:42 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.8073280020383747 on epoch=68
05/29/2022 04:00:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=69
05/29/2022 04:00:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.30 on epoch=69
05/29/2022 04:00:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=69
05/29/2022 04:00:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=69
05/29/2022 04:00:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=70
05/29/2022 04:01:01 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.8016122826928088 on epoch=70
05/29/2022 04:01:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=70
05/29/2022 04:01:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=70
05/29/2022 04:01:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=71
05/29/2022 04:01:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=71
05/29/2022 04:01:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=71
05/29/2022 04:01:21 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.7883873707032645 on epoch=71
05/29/2022 04:01:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=72
05/29/2022 04:01:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=72
05/29/2022 04:01:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=72
05/29/2022 04:01:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=73
05/29/2022 04:01:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/29/2022 04:01:40 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.8206530090013132 on epoch=73
05/29/2022 04:01:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8101002212661322 -> 0.8206530090013132 on epoch=73, global_step=2350
05/29/2022 04:01:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
05/29/2022 04:01:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=74
05/29/2022 04:01:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=74
05/29/2022 04:01:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=74
05/29/2022 04:01:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/29/2022 04:01:59 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.7994711899892611 on epoch=74
05/29/2022 04:02:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=75
05/29/2022 04:02:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.30 on epoch=75
05/29/2022 04:02:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=75
05/29/2022 04:02:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=76
05/29/2022 04:02:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.23 on epoch=76
05/29/2022 04:02:18 - INFO - __main__ - Global step 2450 Train loss 0.23 Classification-F1 0.7899698960043787 on epoch=76
05/29/2022 04:02:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=76
05/29/2022 04:02:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=77
05/29/2022 04:02:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.23 on epoch=77
05/29/2022 04:02:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.28 on epoch=77
05/29/2022 04:02:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=78
05/29/2022 04:02:38 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.790454368906535 on epoch=78
05/29/2022 04:02:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=78
05/29/2022 04:02:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=78
05/29/2022 04:02:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=79
05/29/2022 04:02:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=79
05/29/2022 04:02:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.24 on epoch=79
05/29/2022 04:02:57 - INFO - __main__ - Global step 2550 Train loss 0.23 Classification-F1 0.7950798828270904 on epoch=79
05/29/2022 04:03:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=79
05/29/2022 04:03:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=80
05/29/2022 04:03:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=80
05/29/2022 04:03:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=80
05/29/2022 04:03:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=81
05/29/2022 04:03:16 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.7998252086281971 on epoch=81
05/29/2022 04:03:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=81
05/29/2022 04:03:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=81
05/29/2022 04:03:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/29/2022 04:03:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=82
05/29/2022 04:03:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=82
05/29/2022 04:03:36 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.8084407483649328 on epoch=82
05/29/2022 04:03:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=83
05/29/2022 04:03:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=83
05/29/2022 04:03:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.23 on epoch=83
05/29/2022 04:03:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=84
05/29/2022 04:03:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=84
05/29/2022 04:03:55 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.8194851369979298 on epoch=84
05/29/2022 04:03:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=84
05/29/2022 04:04:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=84
05/29/2022 04:04:03 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=85
05/29/2022 04:04:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=85
05/29/2022 04:04:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=85
05/29/2022 04:04:14 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.8045743523862388 on epoch=85
05/29/2022 04:04:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=86
05/29/2022 04:04:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=86
05/29/2022 04:04:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=86
05/29/2022 04:04:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=87
05/29/2022 04:04:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=87
05/29/2022 04:04:34 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.815613015628799 on epoch=87
05/29/2022 04:04:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.28 on epoch=87
05/29/2022 04:04:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=88
05/29/2022 04:04:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=88
05/29/2022 04:04:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=88
05/29/2022 04:04:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=89
05/29/2022 04:04:53 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.7777876953402065 on epoch=89
05/29/2022 04:04:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=89
05/29/2022 04:04:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=89
05/29/2022 04:05:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=89
05/29/2022 04:05:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=90
05/29/2022 04:05:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=90
05/29/2022 04:05:12 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.8053079553164192 on epoch=90
05/29/2022 04:05:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=90
05/29/2022 04:05:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=91
05/29/2022 04:05:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.24 on epoch=91
05/29/2022 04:05:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=91
05/29/2022 04:05:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=92
05/29/2022 04:05:31 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.8076455223501099 on epoch=92
05/29/2022 04:05:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=92
05/29/2022 04:05:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.19 on epoch=92
05/29/2022 04:05:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
05/29/2022 04:05:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=93
05/29/2022 04:05:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=93
05/29/2022 04:05:45 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:05:45 - INFO - __main__ - Printing 3 examples
05/29/2022 04:05:45 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 04:05:45 - INFO - __main__ - ['others']
05/29/2022 04:05:45 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 04:05:45 - INFO - __main__ - ['others']
05/29/2022 04:05:45 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 04:05:45 - INFO - __main__ - ['others']
05/29/2022 04:05:45 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:05:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:05:46 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:05:46 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:05:46 - INFO - __main__ - Printing 3 examples
05/29/2022 04:05:46 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 04:05:46 - INFO - __main__ - ['others']
05/29/2022 04:05:46 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 04:05:46 - INFO - __main__ - ['others']
05/29/2022 04:05:46 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 04:05:46 - INFO - __main__ - ['others']
05/29/2022 04:05:46 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:05:46 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:05:46 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:05:50 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.7898275141385076 on epoch=93
05/29/2022 04:05:50 - INFO - __main__ - save last model!
05/29/2022 04:05:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 04:05:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 04:05:51 - INFO - __main__ - Printing 3 examples
05/29/2022 04:05:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 04:05:51 - INFO - __main__ - ['others']
05/29/2022 04:05:51 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 04:05:51 - INFO - __main__ - ['others']
05/29/2022 04:05:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 04:05:51 - INFO - __main__ - ['others']
05/29/2022 04:05:51 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:05:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:05:58 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 04:06:04 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:06:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:06:05 - INFO - __main__ - Starting training!
05/29/2022 04:07:11 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_13_0.3_8_predictions.txt
05/29/2022 04:07:11 - INFO - __main__ - Classification-F1 on test data: 0.3983
05/29/2022 04:07:11 - INFO - __main__ - prefix=emo_128_13, lr=0.3, bsz=8, dev_performance=0.8206530090013132, test_performance=0.3983133331351907
05/29/2022 04:07:11 - INFO - __main__ - Running ... prefix=emo_128_13, lr=0.2, bsz=8 ...
05/29/2022 04:07:12 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:07:12 - INFO - __main__ - Printing 3 examples
05/29/2022 04:07:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/29/2022 04:07:12 - INFO - __main__ - ['others']
05/29/2022 04:07:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/29/2022 04:07:12 - INFO - __main__ - ['others']
05/29/2022 04:07:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/29/2022 04:07:12 - INFO - __main__ - ['others']
05/29/2022 04:07:12 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:07:12 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:07:13 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:07:13 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:07:13 - INFO - __main__ - Printing 3 examples
05/29/2022 04:07:13 - INFO - __main__ -  [emo] my lovely nature what kind of nature loving kind compassionate
05/29/2022 04:07:13 - INFO - __main__ - ['others']
05/29/2022 04:07:13 - INFO - __main__ -  [emo] indian premier league i think csk because they played last few game like as a champion cricket league
05/29/2022 04:07:13 - INFO - __main__ - ['others']
05/29/2022 04:07:13 - INFO - __main__ -  [emo] they are not respond properly even if they reply that doesn't guarantee them to be your real friends ha sorry
05/29/2022 04:07:13 - INFO - __main__ - ['others']
05/29/2022 04:07:13 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:07:13 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:07:14 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:07:32 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:07:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:07:33 - INFO - __main__ - Starting training!
05/29/2022 04:07:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.20 on epoch=0
05/29/2022 04:07:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.55 on epoch=0
05/29/2022 04:07:41 - INFO - __main__ - Step 30 Global step 30 Train loss 3.20 on epoch=0
05/29/2022 04:07:44 - INFO - __main__ - Step 40 Global step 40 Train loss 2.57 on epoch=1
05/29/2022 04:07:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.65 on epoch=1
05/29/2022 04:07:54 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.0 on epoch=1
05/29/2022 04:07:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=1, global_step=50
05/29/2022 04:07:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.55 on epoch=1
05/29/2022 04:07:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.96 on epoch=2
05/29/2022 04:08:01 - INFO - __main__ - Step 80 Global step 80 Train loss 1.91 on epoch=2
05/29/2022 04:08:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.71 on epoch=2
05/29/2022 04:08:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.52 on epoch=3
05/29/2022 04:08:13 - INFO - __main__ - Global step 100 Train loss 1.93 Classification-F1 0.16238415246861237 on epoch=3
05/29/2022 04:08:13 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.16238415246861237 on epoch=3, global_step=100
05/29/2022 04:08:15 - INFO - __main__ - Step 110 Global step 110 Train loss 1.59 on epoch=3
05/29/2022 04:08:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.33 on epoch=3
05/29/2022 04:08:20 - INFO - __main__ - Step 130 Global step 130 Train loss 1.21 on epoch=4
05/29/2022 04:08:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.32 on epoch=4
05/29/2022 04:08:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=4
05/29/2022 04:08:32 - INFO - __main__ - Global step 150 Train loss 1.32 Classification-F1 0.4856630266708651 on epoch=4
05/29/2022 04:08:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16238415246861237 -> 0.4856630266708651 on epoch=4, global_step=150
05/29/2022 04:08:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=4
05/29/2022 04:08:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=5
05/29/2022 04:08:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=5
05/29/2022 04:08:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=5
05/29/2022 04:08:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
05/29/2022 04:08:52 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.495386963446211 on epoch=6
05/29/2022 04:08:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4856630266708651 -> 0.495386963446211 on epoch=6, global_step=200
05/29/2022 04:08:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=6
05/29/2022 04:08:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.90 on epoch=6
05/29/2022 04:08:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.72 on epoch=7
05/29/2022 04:09:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=7
05/29/2022 04:09:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=7
05/29/2022 04:09:11 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.510748852186123 on epoch=7
05/29/2022 04:09:11 - INFO - __main__ - Saving model with best Classification-F1: 0.495386963446211 -> 0.510748852186123 on epoch=7, global_step=250
05/29/2022 04:09:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.70 on epoch=8
05/29/2022 04:09:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=8
05/29/2022 04:09:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=8
05/29/2022 04:09:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=9
05/29/2022 04:09:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=9
05/29/2022 04:09:31 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.5307818079792908 on epoch=9
05/29/2022 04:09:31 - INFO - __main__ - Saving model with best Classification-F1: 0.510748852186123 -> 0.5307818079792908 on epoch=9, global_step=300
05/29/2022 04:09:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=9
05/29/2022 04:09:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=9
05/29/2022 04:09:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=10
05/29/2022 04:09:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=10
05/29/2022 04:09:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=10
05/29/2022 04:09:50 - INFO - __main__ - Global step 350 Train loss 0.70 Classification-F1 0.5574921366396021 on epoch=10
05/29/2022 04:09:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5307818079792908 -> 0.5574921366396021 on epoch=10, global_step=350
05/29/2022 04:09:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=11
05/29/2022 04:09:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.78 on epoch=11
05/29/2022 04:09:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=11
05/29/2022 04:10:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=12
05/29/2022 04:10:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.67 on epoch=12
05/29/2022 04:10:10 - INFO - __main__ - Global step 400 Train loss 0.69 Classification-F1 0.5824788233994525 on epoch=12
05/29/2022 04:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5574921366396021 -> 0.5824788233994525 on epoch=12, global_step=400
05/29/2022 04:10:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=12
05/29/2022 04:10:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=13
05/29/2022 04:10:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=13
05/29/2022 04:10:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=13
05/29/2022 04:10:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=14
05/29/2022 04:10:29 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.5963196135119848 on epoch=14
05/29/2022 04:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5824788233994525 -> 0.5963196135119848 on epoch=14, global_step=450
05/29/2022 04:10:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=14
05/29/2022 04:10:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.57 on epoch=14
05/29/2022 04:10:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=14
05/29/2022 04:10:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=15
05/29/2022 04:10:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.65 on epoch=15
05/29/2022 04:10:49 - INFO - __main__ - Global step 500 Train loss 0.63 Classification-F1 0.5987991463769702 on epoch=15
05/29/2022 04:10:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5963196135119848 -> 0.5987991463769702 on epoch=15, global_step=500
05/29/2022 04:10:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=15
05/29/2022 04:10:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=16
05/29/2022 04:10:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.65 on epoch=16
05/29/2022 04:10:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=16
05/29/2022 04:11:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=17
05/29/2022 04:11:08 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6469360687108641 on epoch=17
05/29/2022 04:11:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5987991463769702 -> 0.6469360687108641 on epoch=17, global_step=550
05/29/2022 04:11:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=17
05/29/2022 04:11:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.56 on epoch=17
05/29/2022 04:11:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=18
05/29/2022 04:11:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=18
05/29/2022 04:11:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=18
05/29/2022 04:11:28 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.6507833607257546 on epoch=18
05/29/2022 04:11:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6469360687108641 -> 0.6507833607257546 on epoch=18, global_step=600
05/29/2022 04:11:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=19
05/29/2022 04:11:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.63 on epoch=19
05/29/2022 04:11:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
05/29/2022 04:11:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.60 on epoch=19
05/29/2022 04:11:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=20
05/29/2022 04:11:47 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.6574280021588522 on epoch=20
05/29/2022 04:11:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6507833607257546 -> 0.6574280021588522 on epoch=20, global_step=650
05/29/2022 04:11:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=20
05/29/2022 04:11:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=20
05/29/2022 04:11:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=21
05/29/2022 04:11:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=21
05/29/2022 04:11:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.56 on epoch=21
05/29/2022 04:12:06 - INFO - __main__ - Global step 700 Train loss 0.54 Classification-F1 0.6725231798694791 on epoch=21
05/29/2022 04:12:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6574280021588522 -> 0.6725231798694791 on epoch=21, global_step=700
05/29/2022 04:12:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=22
05/29/2022 04:12:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=22
05/29/2022 04:12:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=22
05/29/2022 04:12:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=23
05/29/2022 04:12:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=23
05/29/2022 04:12:26 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.6776016672909873 on epoch=23
05/29/2022 04:12:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6725231798694791 -> 0.6776016672909873 on epoch=23, global_step=750
05/29/2022 04:12:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=23
05/29/2022 04:12:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=24
05/29/2022 04:12:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=24
05/29/2022 04:12:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=24
05/29/2022 04:12:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=24
05/29/2022 04:12:45 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.6910702757475189 on epoch=24
05/29/2022 04:12:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6776016672909873 -> 0.6910702757475189 on epoch=24, global_step=800
05/29/2022 04:12:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=25
05/29/2022 04:12:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=25
05/29/2022 04:12:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.64 on epoch=25
05/29/2022 04:12:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=26
05/29/2022 04:12:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=26
05/29/2022 04:13:04 - INFO - __main__ - Global step 850 Train loss 0.53 Classification-F1 0.6680438392111245 on epoch=26
05/29/2022 04:13:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.55 on epoch=26
05/29/2022 04:13:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=27
05/29/2022 04:13:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=27
05/29/2022 04:13:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=27
05/29/2022 04:13:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=28
05/29/2022 04:13:24 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.680881553871398 on epoch=28
05/29/2022 04:13:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=28
05/29/2022 04:13:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.56 on epoch=28
05/29/2022 04:13:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=29
05/29/2022 04:13:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=29
05/29/2022 04:13:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=29
05/29/2022 04:13:43 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.6937406284787496 on epoch=29
05/29/2022 04:13:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6910702757475189 -> 0.6937406284787496 on epoch=29, global_step=950
05/29/2022 04:13:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=29
05/29/2022 04:13:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=30
05/29/2022 04:13:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=30
05/29/2022 04:13:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=30
05/29/2022 04:13:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=31
05/29/2022 04:14:02 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.7082895369446182 on epoch=31
05/29/2022 04:14:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6937406284787496 -> 0.7082895369446182 on epoch=31, global_step=1000
05/29/2022 04:14:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=31
05/29/2022 04:14:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=31
05/29/2022 04:14:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=32
05/29/2022 04:14:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=32
05/29/2022 04:14:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=32
05/29/2022 04:14:22 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.7045896926431208 on epoch=32
05/29/2022 04:14:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
05/29/2022 04:14:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=33
05/29/2022 04:14:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=33
05/29/2022 04:14:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=34
05/29/2022 04:14:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=34
05/29/2022 04:14:41 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.7294237217141695 on epoch=34
05/29/2022 04:14:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7082895369446182 -> 0.7294237217141695 on epoch=34, global_step=1100
05/29/2022 04:14:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=34
05/29/2022 04:14:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=34
05/29/2022 04:14:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=35
05/29/2022 04:14:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=35
05/29/2022 04:14:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=35
05/29/2022 04:15:00 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.7226057478845582 on epoch=35
05/29/2022 04:15:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=36
05/29/2022 04:15:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=36
05/29/2022 04:15:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=36
05/29/2022 04:15:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=37
05/29/2022 04:15:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=37
05/29/2022 04:15:19 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.7551680457184671 on epoch=37
05/29/2022 04:15:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7294237217141695 -> 0.7551680457184671 on epoch=37, global_step=1200
05/29/2022 04:15:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=37
05/29/2022 04:15:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=38
05/29/2022 04:15:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=38
05/29/2022 04:15:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.62 on epoch=38
05/29/2022 04:15:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=39
05/29/2022 04:15:39 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.7463784688559094 on epoch=39
05/29/2022 04:15:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
05/29/2022 04:15:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=39
05/29/2022 04:15:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=39
05/29/2022 04:15:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=40
05/29/2022 04:15:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=40
05/29/2022 04:15:58 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.7682890904584402 on epoch=40
05/29/2022 04:15:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7551680457184671 -> 0.7682890904584402 on epoch=40, global_step=1300
05/29/2022 04:16:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=40
05/29/2022 04:16:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
05/29/2022 04:16:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=41
05/29/2022 04:16:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=41
05/29/2022 04:16:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=42
05/29/2022 04:16:17 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.7431215889077538 on epoch=42
05/29/2022 04:16:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=42
05/29/2022 04:16:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=42
05/29/2022 04:16:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=43
05/29/2022 04:16:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=43
05/29/2022 04:16:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=43
05/29/2022 04:16:37 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.7684769539296292 on epoch=43
05/29/2022 04:16:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7682890904584402 -> 0.7684769539296292 on epoch=43, global_step=1400
05/29/2022 04:16:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=44
05/29/2022 04:16:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=44
05/29/2022 04:16:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=44
05/29/2022 04:16:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=44
05/29/2022 04:16:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=45
05/29/2022 04:16:56 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.7770560800234524 on epoch=45
05/29/2022 04:16:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7684769539296292 -> 0.7770560800234524 on epoch=45, global_step=1450
05/29/2022 04:16:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=45
05/29/2022 04:17:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=45
05/29/2022 04:17:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=46
05/29/2022 04:17:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=46
05/29/2022 04:17:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=46
05/29/2022 04:17:15 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.7802041026818818 on epoch=46
05/29/2022 04:17:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7770560800234524 -> 0.7802041026818818 on epoch=46, global_step=1500
05/29/2022 04:17:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=47
05/29/2022 04:17:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=47
05/29/2022 04:17:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=47
05/29/2022 04:17:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=48
05/29/2022 04:17:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=48
05/29/2022 04:17:35 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.7886729072182221 on epoch=48
05/29/2022 04:17:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7802041026818818 -> 0.7886729072182221 on epoch=48, global_step=1550
05/29/2022 04:17:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=48
05/29/2022 04:17:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=49
05/29/2022 04:17:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=49
05/29/2022 04:17:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=49
05/29/2022 04:17:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=49
05/29/2022 04:17:54 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.7713086664579202 on epoch=49
05/29/2022 04:17:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=50
05/29/2022 04:17:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=50
05/29/2022 04:18:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=50
05/29/2022 04:18:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
05/29/2022 04:18:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=51
05/29/2022 04:18:14 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.7692302870864345 on epoch=51
05/29/2022 04:18:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=51
05/29/2022 04:18:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
05/29/2022 04:18:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=52
05/29/2022 04:18:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=52
05/29/2022 04:18:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
05/29/2022 04:18:33 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7626326158516583 on epoch=53
05/29/2022 04:18:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=53
05/29/2022 04:18:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=53
05/29/2022 04:18:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=54
05/29/2022 04:18:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=54
05/29/2022 04:18:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=54
05/29/2022 04:18:52 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.7838458772107777 on epoch=54
05/29/2022 04:18:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=54
05/29/2022 04:18:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=55
05/29/2022 04:19:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=55
05/29/2022 04:19:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=55
05/29/2022 04:19:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=56
05/29/2022 04:19:12 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.7812989723379079 on epoch=56
05/29/2022 04:19:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=56
05/29/2022 04:19:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=56
05/29/2022 04:19:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=57
05/29/2022 04:19:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=57
05/29/2022 04:19:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=57
05/29/2022 04:19:31 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.7699632751258683 on epoch=57
05/29/2022 04:19:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=58
05/29/2022 04:19:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=58
05/29/2022 04:19:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=58
05/29/2022 04:19:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=59
05/29/2022 04:19:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=59
05/29/2022 04:19:51 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.8041959773788264 on epoch=59
05/29/2022 04:19:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7886729072182221 -> 0.8041959773788264 on epoch=59, global_step=1900
05/29/2022 04:19:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=59
05/29/2022 04:19:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
05/29/2022 04:19:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=60
05/29/2022 04:20:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=60
05/29/2022 04:20:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
05/29/2022 04:20:10 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.7909954908888687 on epoch=60
05/29/2022 04:20:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=61
05/29/2022 04:20:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=61
05/29/2022 04:20:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=61
05/29/2022 04:20:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=62
05/29/2022 04:20:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
05/29/2022 04:20:30 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.7837810094953265 on epoch=62
05/29/2022 04:20:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=62
05/29/2022 04:20:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=63
05/29/2022 04:20:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=63
05/29/2022 04:20:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.45 on epoch=63
05/29/2022 04:20:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=64
05/29/2022 04:20:49 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.8002616220846904 on epoch=64
05/29/2022 04:20:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=64
05/29/2022 04:20:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=64
05/29/2022 04:20:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=64
05/29/2022 04:20:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=65
05/29/2022 04:21:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=65
05/29/2022 04:21:08 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.7766585194525533 on epoch=65
05/29/2022 04:21:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=65
05/29/2022 04:21:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=66
05/29/2022 04:21:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=66
05/29/2022 04:21:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=66
05/29/2022 04:21:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=67
05/29/2022 04:21:28 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.784370563861048 on epoch=67
05/29/2022 04:21:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=67
05/29/2022 04:21:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=67
05/29/2022 04:21:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=68
05/29/2022 04:21:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.31 on epoch=68
05/29/2022 04:21:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.32 on epoch=68
05/29/2022 04:21:47 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.7960943369034139 on epoch=68
05/29/2022 04:21:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=69
05/29/2022 04:21:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.30 on epoch=69
05/29/2022 04:21:55 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=69
05/29/2022 04:21:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
05/29/2022 04:22:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=70
05/29/2022 04:22:06 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.7815030984162066 on epoch=70
05/29/2022 04:22:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=70
05/29/2022 04:22:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=70
05/29/2022 04:22:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=71
05/29/2022 04:22:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=71
05/29/2022 04:22:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=71
05/29/2022 04:22:26 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.7916168231494952 on epoch=71
05/29/2022 04:22:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=72
05/29/2022 04:22:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=72
05/29/2022 04:22:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=72
05/29/2022 04:22:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=73
05/29/2022 04:22:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=73
05/29/2022 04:22:45 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.7922893457140319 on epoch=73
05/29/2022 04:22:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.33 on epoch=73
05/29/2022 04:22:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=74
05/29/2022 04:22:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.34 on epoch=74
05/29/2022 04:22:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=74
05/29/2022 04:22:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=74
05/29/2022 04:23:04 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.7944277441274193 on epoch=74
05/29/2022 04:23:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=75
05/29/2022 04:23:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=75
05/29/2022 04:23:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=75
05/29/2022 04:23:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=76
05/29/2022 04:23:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=76
05/29/2022 04:23:24 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.7898545539729436 on epoch=76
05/29/2022 04:23:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.29 on epoch=76
05/29/2022 04:23:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=77
05/29/2022 04:23:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=77
05/29/2022 04:23:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=77
05/29/2022 04:23:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=78
05/29/2022 04:23:43 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.7881598812525072 on epoch=78
05/29/2022 04:23:46 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.32 on epoch=78
05/29/2022 04:23:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=78
05/29/2022 04:23:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=79
05/29/2022 04:23:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.32 on epoch=79
05/29/2022 04:23:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=79
05/29/2022 04:24:03 - INFO - __main__ - Global step 2550 Train loss 0.30 Classification-F1 0.8073785654315654 on epoch=79
05/29/2022 04:24:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8041959773788264 -> 0.8073785654315654 on epoch=79, global_step=2550
05/29/2022 04:24:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=79
05/29/2022 04:24:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=80
05/29/2022 04:24:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=80
05/29/2022 04:24:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=80
05/29/2022 04:24:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=81
05/29/2022 04:24:23 - INFO - __main__ - Global step 2600 Train loss 0.26 Classification-F1 0.8002169117147996 on epoch=81
05/29/2022 04:24:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=81
05/29/2022 04:24:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.25 on epoch=81
05/29/2022 04:24:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=82
05/29/2022 04:24:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.34 on epoch=82
05/29/2022 04:24:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.26 on epoch=82
05/29/2022 04:24:42 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.7964482948058851 on epoch=82
05/29/2022 04:24:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=83
05/29/2022 04:24:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.28 on epoch=83
05/29/2022 04:24:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.35 on epoch=83
05/29/2022 04:24:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=84
05/29/2022 04:24:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.27 on epoch=84
05/29/2022 04:25:01 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.794640703536361 on epoch=84
05/29/2022 04:25:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.29 on epoch=84
05/29/2022 04:25:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=84
05/29/2022 04:25:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.31 on epoch=85
05/29/2022 04:25:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=85
05/29/2022 04:25:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=85
05/29/2022 04:25:21 - INFO - __main__ - Global step 2750 Train loss 0.29 Classification-F1 0.8159259158351678 on epoch=85
05/29/2022 04:25:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8073785654315654 -> 0.8159259158351678 on epoch=85, global_step=2750
05/29/2022 04:25:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=86
05/29/2022 04:25:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=86
05/29/2022 04:25:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=86
05/29/2022 04:25:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=87
05/29/2022 04:25:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.23 on epoch=87
05/29/2022 04:25:40 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.8103316217633643 on epoch=87
05/29/2022 04:25:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=87
05/29/2022 04:25:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.18 on epoch=88
05/29/2022 04:25:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=88
05/29/2022 04:25:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=88
05/29/2022 04:25:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=89
05/29/2022 04:25:59 - INFO - __main__ - Global step 2850 Train loss 0.25 Classification-F1 0.8016843668238823 on epoch=89
05/29/2022 04:26:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=89
05/29/2022 04:26:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=89
05/29/2022 04:26:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=89
05/29/2022 04:26:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.22 on epoch=90
05/29/2022 04:26:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.29 on epoch=90
05/29/2022 04:26:19 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.8054113999318854 on epoch=90
05/29/2022 04:26:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.27 on epoch=90
05/29/2022 04:26:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
05/29/2022 04:26:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=91
05/29/2022 04:26:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.26 on epoch=91
05/29/2022 04:26:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=92
05/29/2022 04:26:38 - INFO - __main__ - Global step 2950 Train loss 0.25 Classification-F1 0.8002628294972125 on epoch=92
05/29/2022 04:26:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=92
05/29/2022 04:26:43 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.27 on epoch=92
05/29/2022 04:26:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=93
05/29/2022 04:26:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.33 on epoch=93
05/29/2022 04:26:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.28 on epoch=93
05/29/2022 04:26:52 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:26:52 - INFO - __main__ - Printing 3 examples
05/29/2022 04:26:52 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 04:26:52 - INFO - __main__ - ['sad']
05/29/2022 04:26:52 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 04:26:52 - INFO - __main__ - ['sad']
05/29/2022 04:26:52 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 04:26:52 - INFO - __main__ - ['sad']
05/29/2022 04:26:52 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:26:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:26:53 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:26:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:26:53 - INFO - __main__ - Printing 3 examples
05/29/2022 04:26:53 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 04:26:53 - INFO - __main__ - ['sad']
05/29/2022 04:26:53 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 04:26:53 - INFO - __main__ - ['sad']
05/29/2022 04:26:53 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 04:26:53 - INFO - __main__ - ['sad']
05/29/2022 04:26:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:26:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:26:53 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:26:58 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.7917114224077413 on epoch=93
05/29/2022 04:26:58 - INFO - __main__ - save last model!
05/29/2022 04:26:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 04:26:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 04:26:58 - INFO - __main__ - Printing 3 examples
05/29/2022 04:26:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 04:26:58 - INFO - __main__ - ['others']
05/29/2022 04:26:58 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 04:26:58 - INFO - __main__ - ['others']
05/29/2022 04:26:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 04:26:58 - INFO - __main__ - ['others']
05/29/2022 04:26:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:27:00 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:27:05 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 04:27:10 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:27:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:27:10 - INFO - __main__ - Starting training!
05/29/2022 04:28:18 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_13_0.2_8_predictions.txt
05/29/2022 04:28:18 - INFO - __main__ - Classification-F1 on test data: 0.3932
05/29/2022 04:28:18 - INFO - __main__ - prefix=emo_128_13, lr=0.2, bsz=8, dev_performance=0.8159259158351678, test_performance=0.393188729963987
05/29/2022 04:28:18 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.5, bsz=8 ...
05/29/2022 04:28:19 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:28:19 - INFO - __main__ - Printing 3 examples
05/29/2022 04:28:19 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 04:28:19 - INFO - __main__ - ['sad']
05/29/2022 04:28:19 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 04:28:19 - INFO - __main__ - ['sad']
05/29/2022 04:28:19 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 04:28:19 - INFO - __main__ - ['sad']
05/29/2022 04:28:19 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:28:20 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:28:20 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:28:20 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:28:20 - INFO - __main__ - Printing 3 examples
05/29/2022 04:28:20 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 04:28:20 - INFO - __main__ - ['sad']
05/29/2022 04:28:20 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 04:28:20 - INFO - __main__ - ['sad']
05/29/2022 04:28:20 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 04:28:20 - INFO - __main__ - ['sad']
05/29/2022 04:28:20 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:28:20 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:28:21 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:28:39 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:28:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:28:40 - INFO - __main__ - Starting training!
05/29/2022 04:28:43 - INFO - __main__ - Step 10 Global step 10 Train loss 3.71 on epoch=0
05/29/2022 04:28:46 - INFO - __main__ - Step 20 Global step 20 Train loss 2.70 on epoch=0
05/29/2022 04:28:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.08 on epoch=0
05/29/2022 04:28:51 - INFO - __main__ - Step 40 Global step 40 Train loss 1.75 on epoch=1
05/29/2022 04:28:53 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=1
05/29/2022 04:29:00 - INFO - __main__ - Global step 50 Train loss 2.32 Classification-F1 0.24246506366783033 on epoch=1
05/29/2022 04:29:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24246506366783033 on epoch=1, global_step=50
05/29/2022 04:29:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=1
05/29/2022 04:29:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.03 on epoch=2
05/29/2022 04:29:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=2
05/29/2022 04:29:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=2
05/29/2022 04:29:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=3
05/29/2022 04:29:20 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.5143142878804552 on epoch=3
05/29/2022 04:29:20 - INFO - __main__ - Saving model with best Classification-F1: 0.24246506366783033 -> 0.5143142878804552 on epoch=3, global_step=100
05/29/2022 04:29:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=3
05/29/2022 04:29:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=3
05/29/2022 04:29:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=4
05/29/2022 04:29:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=4
05/29/2022 04:29:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=4
05/29/2022 04:29:39 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.6166461002980904 on epoch=4
05/29/2022 04:29:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5143142878804552 -> 0.6166461002980904 on epoch=4, global_step=150
05/29/2022 04:29:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=4
05/29/2022 04:29:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=5
05/29/2022 04:29:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=5
05/29/2022 04:29:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=5
05/29/2022 04:29:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.65 on epoch=6
05/29/2022 04:29:58 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5818826650222475 on epoch=6
05/29/2022 04:30:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.64 on epoch=6
05/29/2022 04:30:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=6
05/29/2022 04:30:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=7
05/29/2022 04:30:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=7
05/29/2022 04:30:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=7
05/29/2022 04:30:18 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.7026666807325512 on epoch=7
05/29/2022 04:30:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6166461002980904 -> 0.7026666807325512 on epoch=7, global_step=250
05/29/2022 04:30:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=8
05/29/2022 04:30:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=8
05/29/2022 04:30:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=8
05/29/2022 04:30:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=9
05/29/2022 04:30:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=9
05/29/2022 04:30:37 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.6563725296220628 on epoch=9
05/29/2022 04:30:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=9
05/29/2022 04:30:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=9
05/29/2022 04:30:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=10
05/29/2022 04:30:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=10
05/29/2022 04:30:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=10
05/29/2022 04:30:56 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.7226107414624727 on epoch=10
05/29/2022 04:30:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7026666807325512 -> 0.7226107414624727 on epoch=10, global_step=350
05/29/2022 04:30:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=11
05/29/2022 04:31:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=11
05/29/2022 04:31:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=11
05/29/2022 04:31:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=12
05/29/2022 04:31:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=12
05/29/2022 04:31:16 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.6874969518599774 on epoch=12
05/29/2022 04:31:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=12
05/29/2022 04:31:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=13
05/29/2022 04:31:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=13
05/29/2022 04:31:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=13
05/29/2022 04:31:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.63 on epoch=14
05/29/2022 04:31:35 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.71954349436735 on epoch=14
05/29/2022 04:31:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=14
05/29/2022 04:31:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=14
05/29/2022 04:31:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=14
05/29/2022 04:31:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=15
05/29/2022 04:31:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=15
05/29/2022 04:31:54 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.7605009572138498 on epoch=15
05/29/2022 04:31:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7226107414624727 -> 0.7605009572138498 on epoch=15, global_step=500
05/29/2022 04:31:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=15
05/29/2022 04:31:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=16
05/29/2022 04:32:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=16
05/29/2022 04:32:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=16
05/29/2022 04:32:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=17
05/29/2022 04:32:13 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.75074050042643 on epoch=17
05/29/2022 04:32:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=17
05/29/2022 04:32:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=17
05/29/2022 04:32:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=18
05/29/2022 04:32:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=18
05/29/2022 04:32:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=18
05/29/2022 04:32:33 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.7751880601300467 on epoch=18
05/29/2022 04:32:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7605009572138498 -> 0.7751880601300467 on epoch=18, global_step=600
05/29/2022 04:32:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=19
05/29/2022 04:32:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=19
05/29/2022 04:32:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=19
05/29/2022 04:32:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=19
05/29/2022 04:32:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=20
05/29/2022 04:32:52 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.755722885449342 on epoch=20
05/29/2022 04:32:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=20
05/29/2022 04:32:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=20
05/29/2022 04:32:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=21
05/29/2022 04:33:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=21
05/29/2022 04:33:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=21
05/29/2022 04:33:11 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.7760836319162748 on epoch=21
05/29/2022 04:33:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7751880601300467 -> 0.7760836319162748 on epoch=21, global_step=700
05/29/2022 04:33:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=22
05/29/2022 04:33:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=22
05/29/2022 04:33:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=22
05/29/2022 04:33:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=23
05/29/2022 04:33:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=23
05/29/2022 04:33:30 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.7918898240485894 on epoch=23
05/29/2022 04:33:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7760836319162748 -> 0.7918898240485894 on epoch=23, global_step=750
05/29/2022 04:33:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=23
05/29/2022 04:33:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=24
05/29/2022 04:33:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=24
05/29/2022 04:33:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=24
05/29/2022 04:33:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=24
05/29/2022 04:33:50 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.7707393257355584 on epoch=24
05/29/2022 04:33:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=25
05/29/2022 04:33:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=25
05/29/2022 04:33:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=25
05/29/2022 04:34:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=26
05/29/2022 04:34:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=26
05/29/2022 04:34:09 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.7717771419364665 on epoch=26
05/29/2022 04:34:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=26
05/29/2022 04:34:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=27
05/29/2022 04:34:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=27
05/29/2022 04:34:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=27
05/29/2022 04:34:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=28
05/29/2022 04:34:28 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.7998861288195691 on epoch=28
05/29/2022 04:34:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7918898240485894 -> 0.7998861288195691 on epoch=28, global_step=900
05/29/2022 04:34:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=28
05/29/2022 04:34:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=28
05/29/2022 04:34:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=29
05/29/2022 04:34:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=29
05/29/2022 04:34:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=29
05/29/2022 04:34:48 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.7842837436322354 on epoch=29
05/29/2022 04:34:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=29
05/29/2022 04:34:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=30
05/29/2022 04:34:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=30
05/29/2022 04:34:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=30
05/29/2022 04:35:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=31
05/29/2022 04:35:07 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.7766786201589521 on epoch=31
05/29/2022 04:35:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=31
05/29/2022 04:35:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=31
05/29/2022 04:35:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=32
05/29/2022 04:35:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=32
05/29/2022 04:35:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=32
05/29/2022 04:35:26 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.804054614208204 on epoch=32
05/29/2022 04:35:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7998861288195691 -> 0.804054614208204 on epoch=32, global_step=1050
05/29/2022 04:35:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=33
05/29/2022 04:35:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=33
05/29/2022 04:35:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
05/29/2022 04:35:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=34
05/29/2022 04:35:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=34
05/29/2022 04:35:45 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.7771501563299101 on epoch=34
05/29/2022 04:35:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/29/2022 04:35:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=34
05/29/2022 04:35:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=35
05/29/2022 04:35:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=35
05/29/2022 04:35:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=35
05/29/2022 04:36:05 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.7774106939905712 on epoch=35
05/29/2022 04:36:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
05/29/2022 04:36:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=36
05/29/2022 04:36:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=36
05/29/2022 04:36:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=37
05/29/2022 04:36:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=37
05/29/2022 04:36:24 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7678539013004636 on epoch=37
05/29/2022 04:36:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=37
05/29/2022 04:36:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=38
05/29/2022 04:36:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=38
05/29/2022 04:36:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=38
05/29/2022 04:36:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=39
05/29/2022 04:36:43 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.8089483802061472 on epoch=39
05/29/2022 04:36:43 - INFO - __main__ - Saving model with best Classification-F1: 0.804054614208204 -> 0.8089483802061472 on epoch=39, global_step=1250
05/29/2022 04:36:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=39
05/29/2022 04:36:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=39
05/29/2022 04:36:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=39
05/29/2022 04:36:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=40
05/29/2022 04:36:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=40
05/29/2022 04:37:02 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.799198736216666 on epoch=40
05/29/2022 04:37:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=40
05/29/2022 04:37:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=41
05/29/2022 04:37:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=41
05/29/2022 04:37:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=41
05/29/2022 04:37:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=42
05/29/2022 04:37:22 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.8064311521627558 on epoch=42
05/29/2022 04:37:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=42
05/29/2022 04:37:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=42
05/29/2022 04:37:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=43
05/29/2022 04:37:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=43
05/29/2022 04:37:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=43
05/29/2022 04:37:41 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.8015537066770464 on epoch=43
05/29/2022 04:37:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=44
05/29/2022 04:37:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=44
05/29/2022 04:37:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=44
05/29/2022 04:37:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=44
05/29/2022 04:37:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=45
05/29/2022 04:38:00 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.7941810585373601 on epoch=45
05/29/2022 04:38:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=45
05/29/2022 04:38:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=45
05/29/2022 04:38:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=46
05/29/2022 04:38:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=46
05/29/2022 04:38:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=46
05/29/2022 04:38:19 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.8067515789912258 on epoch=46
05/29/2022 04:38:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=47
05/29/2022 04:38:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/29/2022 04:38:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=47
05/29/2022 04:38:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=48
05/29/2022 04:38:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=48
05/29/2022 04:38:39 - INFO - __main__ - Global step 1550 Train loss 0.25 Classification-F1 0.8002598291343893 on epoch=48
05/29/2022 04:38:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=48
05/29/2022 04:38:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=49
05/29/2022 04:38:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=49
05/29/2022 04:38:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=49
05/29/2022 04:38:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=49
05/29/2022 04:38:58 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.7943393971361875 on epoch=49
05/29/2022 04:39:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=50
05/29/2022 04:39:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
05/29/2022 04:39:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=50
05/29/2022 04:39:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=51
05/29/2022 04:39:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=51
05/29/2022 04:39:17 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.7737640757824502 on epoch=51
05/29/2022 04:39:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=51
05/29/2022 04:39:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=52
05/29/2022 04:39:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=52
05/29/2022 04:39:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=52
05/29/2022 04:39:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=53
05/29/2022 04:39:36 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.8202614344256164 on epoch=53
05/29/2022 04:39:36 - INFO - __main__ - Saving model with best Classification-F1: 0.8089483802061472 -> 0.8202614344256164 on epoch=53, global_step=1700
05/29/2022 04:39:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=53
05/29/2022 04:39:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=53
05/29/2022 04:39:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=54
05/29/2022 04:39:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=54
05/29/2022 04:39:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=54
05/29/2022 04:39:56 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.7920675764436246 on epoch=54
05/29/2022 04:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=54
05/29/2022 04:40:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=55
05/29/2022 04:40:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=55
05/29/2022 04:40:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=55
05/29/2022 04:40:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=56
05/29/2022 04:40:15 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.7884508101759882 on epoch=56
05/29/2022 04:40:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=56
05/29/2022 04:40:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=56
05/29/2022 04:40:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=57
05/29/2022 04:40:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=57
05/29/2022 04:40:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=57
05/29/2022 04:40:34 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.8130151488903798 on epoch=57
05/29/2022 04:40:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=58
05/29/2022 04:40:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=58
05/29/2022 04:40:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=58
05/29/2022 04:40:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=59
05/29/2022 04:40:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=59
05/29/2022 04:40:53 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7747505059513343 on epoch=59
05/29/2022 04:40:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=59
05/29/2022 04:40:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=59
05/29/2022 04:41:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=60
05/29/2022 04:41:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
05/29/2022 04:41:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=60
05/29/2022 04:41:13 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.7921541833707594 on epoch=60
05/29/2022 04:41:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=61
05/29/2022 04:41:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=61
05/29/2022 04:41:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=61
05/29/2022 04:41:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=62
05/29/2022 04:41:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=62
05/29/2022 04:41:32 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.7781301884416266 on epoch=62
05/29/2022 04:41:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=62
05/29/2022 04:41:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=63
05/29/2022 04:41:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=63
05/29/2022 04:41:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=63
05/29/2022 04:41:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=64
05/29/2022 04:41:51 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.8188776882680037 on epoch=64
05/29/2022 04:41:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=64
05/29/2022 04:41:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=64
05/29/2022 04:41:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=64
05/29/2022 04:42:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=65
05/29/2022 04:42:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=65
05/29/2022 04:42:10 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.8098062051747029 on epoch=65
05/29/2022 04:42:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=65
05/29/2022 04:42:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=66
05/29/2022 04:42:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=66
05/29/2022 04:42:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=66
05/29/2022 04:42:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=67
05/29/2022 04:42:30 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.8170508695049852 on epoch=67
05/29/2022 04:42:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=67
05/29/2022 04:42:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/29/2022 04:42:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=68
05/29/2022 04:42:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=68
05/29/2022 04:42:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=68
05/29/2022 04:42:49 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.8120804868601488 on epoch=68
05/29/2022 04:42:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=69
05/29/2022 04:42:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=69
05/29/2022 04:42:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=69
05/29/2022 04:42:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=69
05/29/2022 04:43:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=70
05/29/2022 04:43:08 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.7871929814089745 on epoch=70
05/29/2022 04:43:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=70
05/29/2022 04:43:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=70
05/29/2022 04:43:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/29/2022 04:43:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=71
05/29/2022 04:43:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=71
05/29/2022 04:43:28 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.785375594064154 on epoch=71
05/29/2022 04:43:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=72
05/29/2022 04:43:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=72
05/29/2022 04:43:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=72
05/29/2022 04:43:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=73
05/29/2022 04:43:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=73
05/29/2022 04:43:47 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.8011762521138559 on epoch=73
05/29/2022 04:43:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=73
05/29/2022 04:43:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=74
05/29/2022 04:43:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=74
05/29/2022 04:43:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=74
05/29/2022 04:44:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=74
05/29/2022 04:44:07 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7936929445149885 on epoch=74
05/29/2022 04:44:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=75
05/29/2022 04:44:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=75
05/29/2022 04:44:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=75
05/29/2022 04:44:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=76
05/29/2022 04:44:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=76
05/29/2022 04:44:26 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7763353338596651 on epoch=76
05/29/2022 04:44:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=76
05/29/2022 04:44:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=77
05/29/2022 04:44:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=77
05/29/2022 04:44:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=77
05/29/2022 04:44:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=78
05/29/2022 04:44:45 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.813885793298643 on epoch=78
05/29/2022 04:44:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=78
05/29/2022 04:44:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=78
05/29/2022 04:44:53 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=79
05/29/2022 04:44:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=79
05/29/2022 04:44:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=79
05/29/2022 04:45:04 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.7749854430741803 on epoch=79
05/29/2022 04:45:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=79
05/29/2022 04:45:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=80
05/29/2022 04:45:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=80
05/29/2022 04:45:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.26 on epoch=80
05/29/2022 04:45:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=81
05/29/2022 04:45:24 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.792833689242133 on epoch=81
05/29/2022 04:45:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=81
05/29/2022 04:45:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=81
05/29/2022 04:45:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=82
05/29/2022 04:45:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=82
05/29/2022 04:45:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=82
05/29/2022 04:45:43 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.8200575460195619 on epoch=82
05/29/2022 04:45:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=83
05/29/2022 04:45:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=83
05/29/2022 04:45:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=83
05/29/2022 04:45:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
05/29/2022 04:45:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=84
05/29/2022 04:46:02 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8018904810249313 on epoch=84
05/29/2022 04:46:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=84
05/29/2022 04:46:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=84
05/29/2022 04:46:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
05/29/2022 04:46:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=85
05/29/2022 04:46:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=85
05/29/2022 04:46:21 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.8069428368352427 on epoch=85
05/29/2022 04:46:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
05/29/2022 04:46:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=86
05/29/2022 04:46:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=86
05/29/2022 04:46:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=87
05/29/2022 04:46:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=87
05/29/2022 04:46:41 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.8014547785359742 on epoch=87
05/29/2022 04:46:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=87
05/29/2022 04:46:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=88
05/29/2022 04:46:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=88
05/29/2022 04:46:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=88
05/29/2022 04:46:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=89
05/29/2022 04:47:00 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.7958819527644125 on epoch=89
05/29/2022 04:47:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=89
05/29/2022 04:47:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=89
05/29/2022 04:47:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
05/29/2022 04:47:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=90
05/29/2022 04:47:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=90
05/29/2022 04:47:19 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.8138762369153856 on epoch=90
05/29/2022 04:47:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=90
05/29/2022 04:47:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=91
05/29/2022 04:47:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=91
05/29/2022 04:47:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=91
05/29/2022 04:47:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=92
05/29/2022 04:47:39 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.8224981484613276 on epoch=92
05/29/2022 04:47:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8202614344256164 -> 0.8224981484613276 on epoch=92, global_step=2950
05/29/2022 04:47:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=92
05/29/2022 04:47:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=92
05/29/2022 04:47:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
05/29/2022 04:47:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=93
05/29/2022 04:47:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
05/29/2022 04:47:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:47:53 - INFO - __main__ - Printing 3 examples
05/29/2022 04:47:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:47:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:47:53 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:47:53 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:47:53 - INFO - __main__ - Printing 3 examples
05/29/2022 04:47:53 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 04:47:53 - INFO - __main__ - ['sad']
05/29/2022 04:47:53 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:47:53 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:47:54 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:47:58 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.8316234879569766 on epoch=93
05/29/2022 04:47:58 - INFO - __main__ - Saving model with best Classification-F1: 0.8224981484613276 -> 0.8316234879569766 on epoch=93, global_step=3000
05/29/2022 04:47:58 - INFO - __main__ - save last model!
05/29/2022 04:47:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 04:47:58 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 04:47:58 - INFO - __main__ - Printing 3 examples
05/29/2022 04:47:58 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 04:47:58 - INFO - __main__ - ['others']
05/29/2022 04:47:58 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 04:47:58 - INFO - __main__ - ['others']
05/29/2022 04:47:58 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 04:47:58 - INFO - __main__ - ['others']
05/29/2022 04:47:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:48:00 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:48:06 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 04:48:12 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:48:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:48:13 - INFO - __main__ - Starting training!
05/29/2022 04:49:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_21_0.5_8_predictions.txt
05/29/2022 04:49:21 - INFO - __main__ - Classification-F1 on test data: 0.5029
05/29/2022 04:49:22 - INFO - __main__ - prefix=emo_128_21, lr=0.5, bsz=8, dev_performance=0.8316234879569766, test_performance=0.5029089977559735
05/29/2022 04:49:22 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.4, bsz=8 ...
05/29/2022 04:49:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:49:23 - INFO - __main__ - Printing 3 examples
05/29/2022 04:49:23 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:49:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:49:23 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 04:49:23 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 04:49:23 - INFO - __main__ - Printing 3 examples
05/29/2022 04:49:23 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 04:49:23 - INFO - __main__ - ['sad']
05/29/2022 04:49:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 04:49:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 04:49:24 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 04:49:43 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 04:49:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 04:49:44 - INFO - __main__ - Starting training!
05/29/2022 04:49:47 - INFO - __main__ - Step 10 Global step 10 Train loss 3.82 on epoch=0
05/29/2022 04:49:49 - INFO - __main__ - Step 20 Global step 20 Train loss 2.93 on epoch=0
05/29/2022 04:49:52 - INFO - __main__ - Step 30 Global step 30 Train loss 2.12 on epoch=0
05/29/2022 04:49:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.89 on epoch=1
05/29/2022 04:49:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.78 on epoch=1
05/29/2022 04:50:04 - INFO - __main__ - Global step 50 Train loss 2.51 Classification-F1 0.16237104102230795 on epoch=1
05/29/2022 04:50:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16237104102230795 on epoch=1, global_step=50
05/29/2022 04:50:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=1
05/29/2022 04:50:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=2
05/29/2022 04:50:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=2
05/29/2022 04:50:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.05 on epoch=2
05/29/2022 04:50:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=3
05/29/2022 04:50:23 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.5413455208386084 on epoch=3
05/29/2022 04:50:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16237104102230795 -> 0.5413455208386084 on epoch=3, global_step=100
05/29/2022 04:50:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.78 on epoch=3
05/29/2022 04:50:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=3
05/29/2022 04:50:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=4
05/29/2022 04:50:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=4
05/29/2022 04:50:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=4
05/29/2022 04:50:43 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.554857877896704 on epoch=4
05/29/2022 04:50:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5413455208386084 -> 0.554857877896704 on epoch=4, global_step=150
05/29/2022 04:50:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.81 on epoch=4
05/29/2022 04:50:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=5
05/29/2022 04:50:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=5
05/29/2022 04:50:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=5
05/29/2022 04:50:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
05/29/2022 04:51:02 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.5395012429259005 on epoch=6
05/29/2022 04:51:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=6
05/29/2022 04:51:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=6
05/29/2022 04:51:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=7
05/29/2022 04:51:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=7
05/29/2022 04:51:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=7
05/29/2022 04:51:21 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.6616532973946148 on epoch=7
05/29/2022 04:51:21 - INFO - __main__ - Saving model with best Classification-F1: 0.554857877896704 -> 0.6616532973946148 on epoch=7, global_step=250
05/29/2022 04:51:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=8
05/29/2022 04:51:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=8
05/29/2022 04:51:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=8
05/29/2022 04:51:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=9
05/29/2022 04:51:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=9
05/29/2022 04:51:40 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.6517182324237902 on epoch=9
05/29/2022 04:51:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.64 on epoch=9
05/29/2022 04:51:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=9
05/29/2022 04:51:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=10
05/29/2022 04:51:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=10
05/29/2022 04:51:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=10
05/29/2022 04:52:00 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.6391262158686528 on epoch=10
05/29/2022 04:52:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=11
05/29/2022 04:52:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=11
05/29/2022 04:52:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=11
05/29/2022 04:52:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=12
05/29/2022 04:52:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=12
05/29/2022 04:52:19 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.673794727045627 on epoch=12
05/29/2022 04:52:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6616532973946148 -> 0.673794727045627 on epoch=12, global_step=400
05/29/2022 04:52:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=12
05/29/2022 04:52:24 - INFO - __main__ - Step 420 Global step 420 Train loss 0.64 on epoch=13
05/29/2022 04:52:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=13
05/29/2022 04:52:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=13
05/29/2022 04:52:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=14
05/29/2022 04:52:38 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.7233242385454148 on epoch=14
05/29/2022 04:52:38 - INFO - __main__ - Saving model with best Classification-F1: 0.673794727045627 -> 0.7233242385454148 on epoch=14, global_step=450
05/29/2022 04:52:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=14
05/29/2022 04:52:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=14
05/29/2022 04:52:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=14
05/29/2022 04:52:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=15
05/29/2022 04:52:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=15
05/29/2022 04:52:58 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.7122291085020054 on epoch=15
05/29/2022 04:53:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=15
05/29/2022 04:53:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=16
05/29/2022 04:53:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=16
05/29/2022 04:53:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=16
05/29/2022 04:53:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=17
05/29/2022 04:53:17 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.7389902413669143 on epoch=17
05/29/2022 04:53:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7233242385454148 -> 0.7389902413669143 on epoch=17, global_step=550
05/29/2022 04:53:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=17
05/29/2022 04:53:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=17
05/29/2022 04:53:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=18
05/29/2022 04:53:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=18
05/29/2022 04:53:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=18
05/29/2022 04:53:36 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.7534739445307833 on epoch=18
05/29/2022 04:53:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7389902413669143 -> 0.7534739445307833 on epoch=18, global_step=600
05/29/2022 04:53:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=19
05/29/2022 04:53:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=19
05/29/2022 04:53:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=19
05/29/2022 04:53:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=19
05/29/2022 04:53:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=20
05/29/2022 04:53:55 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.7056008540195272 on epoch=20
05/29/2022 04:53:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=20
05/29/2022 04:54:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=20
05/29/2022 04:54:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=21
05/29/2022 04:54:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=21
05/29/2022 04:54:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=21
05/29/2022 04:54:15 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.7483326192077976 on epoch=21
05/29/2022 04:54:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=22
05/29/2022 04:54:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=22
05/29/2022 04:54:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=22
05/29/2022 04:54:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=23
05/29/2022 04:54:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=23
05/29/2022 04:54:34 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.7600579882775343 on epoch=23
05/29/2022 04:54:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7534739445307833 -> 0.7600579882775343 on epoch=23, global_step=750
05/29/2022 04:54:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=23
05/29/2022 04:54:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=24
05/29/2022 04:54:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=24
05/29/2022 04:54:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=24
05/29/2022 04:54:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=24
05/29/2022 04:54:53 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.7646475867469962 on epoch=24
05/29/2022 04:54:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7600579882775343 -> 0.7646475867469962 on epoch=24, global_step=800
05/29/2022 04:54:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=25
05/29/2022 04:54:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=25
05/29/2022 04:55:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=25
05/29/2022 04:55:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
05/29/2022 04:55:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=26
05/29/2022 04:55:13 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.7586939676647075 on epoch=26
05/29/2022 04:55:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=26
05/29/2022 04:55:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=27
05/29/2022 04:55:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=27
05/29/2022 04:55:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=27
05/29/2022 04:55:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=28
05/29/2022 04:55:32 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.7829986163638688 on epoch=28
05/29/2022 04:55:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7646475867469962 -> 0.7829986163638688 on epoch=28, global_step=900
05/29/2022 04:55:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=28
05/29/2022 04:55:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=28
05/29/2022 04:55:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=29
05/29/2022 04:55:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=29
05/29/2022 04:55:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=29
05/29/2022 04:55:51 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.7438544627384521 on epoch=29
05/29/2022 04:55:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
05/29/2022 04:55:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=30
05/29/2022 04:55:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=30
05/29/2022 04:56:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=30
05/29/2022 04:56:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=31
05/29/2022 04:56:10 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.7482414933399264 on epoch=31
05/29/2022 04:56:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=31
05/29/2022 04:56:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
05/29/2022 04:56:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=32
05/29/2022 04:56:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=32
05/29/2022 04:56:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=32
05/29/2022 04:56:30 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.7809581527840471 on epoch=32
05/29/2022 04:56:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
05/29/2022 04:56:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=33
05/29/2022 04:56:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=33
05/29/2022 04:56:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=34
05/29/2022 04:56:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=34
05/29/2022 04:56:49 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.7765467756186835 on epoch=34
05/29/2022 04:56:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/29/2022 04:56:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=34
05/29/2022 04:56:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=35
05/29/2022 04:56:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=35
05/29/2022 04:57:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=35
05/29/2022 04:57:08 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.7815942360779318 on epoch=35
05/29/2022 04:57:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=36
05/29/2022 04:57:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=36
05/29/2022 04:57:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=36
05/29/2022 04:57:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=37
05/29/2022 04:57:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=37
05/29/2022 04:57:27 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7708017614366688 on epoch=37
05/29/2022 04:57:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=37
05/29/2022 04:57:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=38
05/29/2022 04:57:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=38
05/29/2022 04:57:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=38
05/29/2022 04:57:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=39
05/29/2022 04:57:47 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.7887139369765813 on epoch=39
05/29/2022 04:57:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7829986163638688 -> 0.7887139369765813 on epoch=39, global_step=1250
05/29/2022 04:57:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.30 on epoch=39
05/29/2022 04:57:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=39
05/29/2022 04:57:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=39
05/29/2022 04:57:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=40
05/29/2022 04:57:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=40
05/29/2022 04:58:06 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.7997158820026785 on epoch=40
05/29/2022 04:58:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7887139369765813 -> 0.7997158820026785 on epoch=40, global_step=1300
05/29/2022 04:58:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=40
05/29/2022 04:58:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=41
05/29/2022 04:58:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=41
05/29/2022 04:58:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=41
05/29/2022 04:58:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=42
05/29/2022 04:58:25 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.7934100940546204 on epoch=42
05/29/2022 04:58:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=42
05/29/2022 04:58:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=42
05/29/2022 04:58:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=43
05/29/2022 04:58:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=43
05/29/2022 04:58:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=43
05/29/2022 04:58:45 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.7901871721844512 on epoch=43
05/29/2022 04:58:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=44
05/29/2022 04:58:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=44
05/29/2022 04:58:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=44
05/29/2022 04:58:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=44
05/29/2022 04:58:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=45
05/29/2022 04:59:04 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.7769485589684039 on epoch=45
05/29/2022 04:59:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=45
05/29/2022 04:59:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=45
05/29/2022 04:59:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
05/29/2022 04:59:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=46
05/29/2022 04:59:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=46
05/29/2022 04:59:23 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.7974938323237933 on epoch=46
05/29/2022 04:59:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=47
05/29/2022 04:59:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=47
05/29/2022 04:59:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=47
05/29/2022 04:59:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=48
05/29/2022 04:59:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=48
05/29/2022 04:59:43 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.7849276017746258 on epoch=48
05/29/2022 04:59:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=48
05/29/2022 04:59:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=49
05/29/2022 04:59:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=49
05/29/2022 04:59:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=49
05/29/2022 04:59:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=49
05/29/2022 05:00:02 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.7886400229034289 on epoch=49
05/29/2022 05:00:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=50
05/29/2022 05:00:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=50
05/29/2022 05:00:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=50
05/29/2022 05:00:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=51
05/29/2022 05:00:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=51
05/29/2022 05:00:21 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7830483601230895 on epoch=51
05/29/2022 05:00:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=51
05/29/2022 05:00:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=52
05/29/2022 05:00:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=52
05/29/2022 05:00:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=52
05/29/2022 05:00:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=53
05/29/2022 05:00:40 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.8088897801331311 on epoch=53
05/29/2022 05:00:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7997158820026785 -> 0.8088897801331311 on epoch=53, global_step=1700
05/29/2022 05:00:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=53
05/29/2022 05:00:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=53
05/29/2022 05:00:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=54
05/29/2022 05:00:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=54
05/29/2022 05:00:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=54
05/29/2022 05:01:00 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.8166476173546837 on epoch=54
05/29/2022 05:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.8088897801331311 -> 0.8166476173546837 on epoch=54, global_step=1750
05/29/2022 05:01:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/29/2022 05:01:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=55
05/29/2022 05:01:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=55
05/29/2022 05:01:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=55
05/29/2022 05:01:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=56
05/29/2022 05:01:19 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7974233105812054 on epoch=56
05/29/2022 05:01:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=56
05/29/2022 05:01:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=56
05/29/2022 05:01:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=57
05/29/2022 05:01:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=57
05/29/2022 05:01:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=57
05/29/2022 05:01:38 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.8010579671620861 on epoch=57
05/29/2022 05:01:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=58
05/29/2022 05:01:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=58
05/29/2022 05:01:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=58
05/29/2022 05:01:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=59
05/29/2022 05:01:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=59
05/29/2022 05:01:57 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.8038750432985642 on epoch=59
05/29/2022 05:02:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=59
05/29/2022 05:02:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/29/2022 05:02:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=60
05/29/2022 05:02:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=60
05/29/2022 05:02:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=60
05/29/2022 05:02:17 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.7992394393287461 on epoch=60
05/29/2022 05:02:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=61
05/29/2022 05:02:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/29/2022 05:02:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=61
05/29/2022 05:02:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=62
05/29/2022 05:02:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=62
05/29/2022 05:02:36 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7847872071863462 on epoch=62
05/29/2022 05:02:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=62
05/29/2022 05:02:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=63
05/29/2022 05:02:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=63
05/29/2022 05:02:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=63
05/29/2022 05:02:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=64
05/29/2022 05:02:55 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.8078183913918662 on epoch=64
05/29/2022 05:02:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=64
05/29/2022 05:03:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=64
05/29/2022 05:03:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=64
05/29/2022 05:03:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=65
05/29/2022 05:03:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=65
05/29/2022 05:03:15 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.7964622016845657 on epoch=65
05/29/2022 05:03:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=65
05/29/2022 05:03:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=66
05/29/2022 05:03:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=66
05/29/2022 05:03:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
05/29/2022 05:03:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=67
05/29/2022 05:03:34 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.8059475658744726 on epoch=67
05/29/2022 05:03:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=67
05/29/2022 05:03:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=67
05/29/2022 05:03:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=68
05/29/2022 05:03:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=68
05/29/2022 05:03:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
05/29/2022 05:03:54 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.8042967233056811 on epoch=68
05/29/2022 05:03:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=69
05/29/2022 05:03:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=69
05/29/2022 05:04:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=69
05/29/2022 05:04:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=69
05/29/2022 05:04:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.30 on epoch=70
05/29/2022 05:04:13 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.804532357163936 on epoch=70
05/29/2022 05:04:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=70
05/29/2022 05:04:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=70
05/29/2022 05:04:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.29 on epoch=71
05/29/2022 05:04:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=71
05/29/2022 05:04:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=71
05/29/2022 05:04:32 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.8064364399138606 on epoch=71
05/29/2022 05:04:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=72
05/29/2022 05:04:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=72
05/29/2022 05:04:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=72
05/29/2022 05:04:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=73
05/29/2022 05:04:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=73
05/29/2022 05:04:52 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.8208743364250483 on epoch=73
05/29/2022 05:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8166476173546837 -> 0.8208743364250483 on epoch=73, global_step=2350
05/29/2022 05:04:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
05/29/2022 05:04:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=74
05/29/2022 05:04:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=74
05/29/2022 05:05:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=74
05/29/2022 05:05:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=74
05/29/2022 05:05:11 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.7982542849301352 on epoch=74
05/29/2022 05:05:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=75
05/29/2022 05:05:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=75
05/29/2022 05:05:19 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=75
05/29/2022 05:05:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=76
05/29/2022 05:05:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=76
05/29/2022 05:05:31 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.8075723702173996 on epoch=76
05/29/2022 05:05:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=76
05/29/2022 05:05:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.29 on epoch=77
05/29/2022 05:05:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=77
05/29/2022 05:05:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=77
05/29/2022 05:05:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
05/29/2022 05:05:50 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.8184963693112856 on epoch=78
05/29/2022 05:05:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=78
05/29/2022 05:05:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=78
05/29/2022 05:05:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=79
05/29/2022 05:06:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=79
05/29/2022 05:06:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=79
05/29/2022 05:06:10 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.8045382525308311 on epoch=79
05/29/2022 05:06:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=79
05/29/2022 05:06:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=80
05/29/2022 05:06:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=80
05/29/2022 05:06:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=80
05/29/2022 05:06:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=81
05/29/2022 05:06:29 - INFO - __main__ - Global step 2600 Train loss 0.21 Classification-F1 0.8138458549858734 on epoch=81
05/29/2022 05:06:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=81
05/29/2022 05:06:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=81
05/29/2022 05:06:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=82
05/29/2022 05:06:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=82
05/29/2022 05:06:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=82
05/29/2022 05:06:48 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.8091014966129444 on epoch=82
05/29/2022 05:06:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=83
05/29/2022 05:06:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=83
05/29/2022 05:06:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=83
05/29/2022 05:06:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=84
05/29/2022 05:07:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=84
05/29/2022 05:07:08 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.8051375648474367 on epoch=84
05/29/2022 05:07:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.27 on epoch=84
05/29/2022 05:07:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=84
05/29/2022 05:07:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.23 on epoch=85
05/29/2022 05:07:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=85
05/29/2022 05:07:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=85
05/29/2022 05:07:27 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.8137409597255721 on epoch=85
05/29/2022 05:07:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=86
05/29/2022 05:07:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=86
05/29/2022 05:07:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=86
05/29/2022 05:07:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.38 on epoch=87
05/29/2022 05:07:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=87
05/29/2022 05:07:46 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.8044184427894912 on epoch=87
05/29/2022 05:07:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.26 on epoch=87
05/29/2022 05:07:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=88
05/29/2022 05:07:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.29 on epoch=88
05/29/2022 05:07:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.31 on epoch=88
05/29/2022 05:07:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=89
05/29/2022 05:08:06 - INFO - __main__ - Global step 2850 Train loss 0.23 Classification-F1 0.8013108956770928 on epoch=89
05/29/2022 05:08:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=89
05/29/2022 05:08:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=89
05/29/2022 05:08:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=89
05/29/2022 05:08:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
05/29/2022 05:08:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=90
05/29/2022 05:08:25 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.8151530265148605 on epoch=90
05/29/2022 05:08:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=90
05/29/2022 05:08:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.30 on epoch=91
05/29/2022 05:08:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=91
05/29/2022 05:08:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=91
05/29/2022 05:08:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=92
05/29/2022 05:08:44 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.8398720773529971 on epoch=92
05/29/2022 05:08:44 - INFO - __main__ - Saving model with best Classification-F1: 0.8208743364250483 -> 0.8398720773529971 on epoch=92, global_step=2950
05/29/2022 05:08:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=92
05/29/2022 05:08:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.31 on epoch=92
05/29/2022 05:08:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/29/2022 05:08:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=93
05/29/2022 05:08:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
05/29/2022 05:08:58 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:08:58 - INFO - __main__ - Printing 3 examples
05/29/2022 05:08:58 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 05:08:58 - INFO - __main__ - ['sad']
05/29/2022 05:08:58 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 05:08:58 - INFO - __main__ - ['sad']
05/29/2022 05:08:58 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 05:08:58 - INFO - __main__ - ['sad']
05/29/2022 05:08:58 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:08:58 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:08:59 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:08:59 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:08:59 - INFO - __main__ - Printing 3 examples
05/29/2022 05:08:59 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 05:08:59 - INFO - __main__ - ['sad']
05/29/2022 05:08:59 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 05:08:59 - INFO - __main__ - ['sad']
05/29/2022 05:08:59 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 05:08:59 - INFO - __main__ - ['sad']
05/29/2022 05:08:59 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:08:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:09:00 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:09:04 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.82391753938663 on epoch=93
05/29/2022 05:09:04 - INFO - __main__ - save last model!
05/29/2022 05:09:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 05:09:04 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 05:09:04 - INFO - __main__ - Printing 3 examples
05/29/2022 05:09:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 05:09:04 - INFO - __main__ - ['others']
05/29/2022 05:09:04 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 05:09:04 - INFO - __main__ - ['others']
05/29/2022 05:09:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 05:09:04 - INFO - __main__ - ['others']
05/29/2022 05:09:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:09:06 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:09:11 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 05:09:17 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:09:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:09:18 - INFO - __main__ - Starting training!
05/29/2022 05:10:26 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_21_0.4_8_predictions.txt
05/29/2022 05:10:26 - INFO - __main__ - Classification-F1 on test data: 0.4223
05/29/2022 05:10:26 - INFO - __main__ - prefix=emo_128_21, lr=0.4, bsz=8, dev_performance=0.8398720773529971, test_performance=0.42231961611640284
05/29/2022 05:10:26 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.3, bsz=8 ...
05/29/2022 05:10:27 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:10:27 - INFO - __main__ - Printing 3 examples
05/29/2022 05:10:27 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 05:10:27 - INFO - __main__ - ['sad']
05/29/2022 05:10:27 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 05:10:27 - INFO - __main__ - ['sad']
05/29/2022 05:10:27 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 05:10:27 - INFO - __main__ - ['sad']
05/29/2022 05:10:27 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:10:27 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:10:28 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:10:28 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:10:28 - INFO - __main__ - Printing 3 examples
05/29/2022 05:10:28 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 05:10:28 - INFO - __main__ - ['sad']
05/29/2022 05:10:28 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 05:10:28 - INFO - __main__ - ['sad']
05/29/2022 05:10:28 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 05:10:28 - INFO - __main__ - ['sad']
05/29/2022 05:10:28 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:10:28 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:10:28 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:10:47 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:10:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:10:48 - INFO - __main__ - Starting training!
05/29/2022 05:10:51 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=0
05/29/2022 05:10:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.22 on epoch=0
05/29/2022 05:10:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=0
05/29/2022 05:10:59 - INFO - __main__ - Step 40 Global step 40 Train loss 2.34 on epoch=1
05/29/2022 05:11:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.99 on epoch=1
05/29/2022 05:11:08 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.07328907530346378 on epoch=1
05/29/2022 05:11:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07328907530346378 on epoch=1, global_step=50
05/29/2022 05:11:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.80 on epoch=1
05/29/2022 05:11:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.82 on epoch=2
05/29/2022 05:11:16 - INFO - __main__ - Step 80 Global step 80 Train loss 1.29 on epoch=2
05/29/2022 05:11:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.18 on epoch=2
05/29/2022 05:11:21 - INFO - __main__ - Step 100 Global step 100 Train loss 1.18 on epoch=3
05/29/2022 05:11:28 - INFO - __main__ - Global step 100 Train loss 1.45 Classification-F1 0.4130117704003375 on epoch=3
05/29/2022 05:11:28 - INFO - __main__ - Saving model with best Classification-F1: 0.07328907530346378 -> 0.4130117704003375 on epoch=3, global_step=100
05/29/2022 05:11:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=3
05/29/2022 05:11:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.78 on epoch=3
05/29/2022 05:11:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=4
05/29/2022 05:11:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.63 on epoch=4
05/29/2022 05:11:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=4
05/29/2022 05:11:48 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.5466793284476854 on epoch=4
05/29/2022 05:11:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4130117704003375 -> 0.5466793284476854 on epoch=4, global_step=150
05/29/2022 05:11:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=4
05/29/2022 05:11:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.80 on epoch=5
05/29/2022 05:11:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=5
05/29/2022 05:11:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=5
05/29/2022 05:12:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=6
05/29/2022 05:12:07 - INFO - __main__ - Global step 200 Train loss 0.75 Classification-F1 0.549187427017735 on epoch=6
05/29/2022 05:12:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5466793284476854 -> 0.549187427017735 on epoch=6, global_step=200
05/29/2022 05:12:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.79 on epoch=6
05/29/2022 05:12:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.74 on epoch=6
05/29/2022 05:12:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=7
05/29/2022 05:12:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=7
05/29/2022 05:12:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.71 on epoch=7
05/29/2022 05:12:27 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.5885725767040205 on epoch=7
05/29/2022 05:12:27 - INFO - __main__ - Saving model with best Classification-F1: 0.549187427017735 -> 0.5885725767040205 on epoch=7, global_step=250
05/29/2022 05:12:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.77 on epoch=8
05/29/2022 05:12:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=8
05/29/2022 05:12:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=8
05/29/2022 05:12:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=9
05/29/2022 05:12:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=9
05/29/2022 05:12:46 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.594032833292917 on epoch=9
05/29/2022 05:12:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5885725767040205 -> 0.594032833292917 on epoch=9, global_step=300
05/29/2022 05:12:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=9
05/29/2022 05:12:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=9
05/29/2022 05:12:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=10
05/29/2022 05:12:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=10
05/29/2022 05:12:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=10
05/29/2022 05:13:06 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.6400852709142089 on epoch=10
05/29/2022 05:13:06 - INFO - __main__ - Saving model with best Classification-F1: 0.594032833292917 -> 0.6400852709142089 on epoch=10, global_step=350
05/29/2022 05:13:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=11
05/29/2022 05:13:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=11
05/29/2022 05:13:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=11
05/29/2022 05:13:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=12
05/29/2022 05:13:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=12
05/29/2022 05:13:25 - INFO - __main__ - Global step 400 Train loss 0.56 Classification-F1 0.5962937516388214 on epoch=12
05/29/2022 05:13:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=12
05/29/2022 05:13:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.64 on epoch=13
05/29/2022 05:13:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=13
05/29/2022 05:13:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=13
05/29/2022 05:13:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.64 on epoch=14
05/29/2022 05:13:45 - INFO - __main__ - Global step 450 Train loss 0.58 Classification-F1 0.7020681191783854 on epoch=14
05/29/2022 05:13:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6400852709142089 -> 0.7020681191783854 on epoch=14, global_step=450
05/29/2022 05:13:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=14
05/29/2022 05:13:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=14
05/29/2022 05:13:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=14
05/29/2022 05:13:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.60 on epoch=15
05/29/2022 05:13:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=15
05/29/2022 05:14:04 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.7189625994521978 on epoch=15
05/29/2022 05:14:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7020681191783854 -> 0.7189625994521978 on epoch=15, global_step=500
05/29/2022 05:14:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=15
05/29/2022 05:14:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=16
05/29/2022 05:14:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=16
05/29/2022 05:14:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=16
05/29/2022 05:14:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=17
05/29/2022 05:14:24 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.7128430988855344 on epoch=17
05/29/2022 05:14:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=17
05/29/2022 05:14:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=17
05/29/2022 05:14:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.59 on epoch=18
05/29/2022 05:14:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=18
05/29/2022 05:14:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=18
05/29/2022 05:14:43 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.7179175569282069 on epoch=18
05/29/2022 05:14:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.65 on epoch=19
05/29/2022 05:14:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=19
05/29/2022 05:14:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=19
05/29/2022 05:14:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=19
05/29/2022 05:14:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=20
05/29/2022 05:15:03 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.681377591971769 on epoch=20
05/29/2022 05:15:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=20
05/29/2022 05:15:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.61 on epoch=20
05/29/2022 05:15:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=21
05/29/2022 05:15:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=21
05/29/2022 05:15:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=21
05/29/2022 05:15:22 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.7018349719380647 on epoch=21
05/29/2022 05:15:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=22
05/29/2022 05:15:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=22
05/29/2022 05:15:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=22
05/29/2022 05:15:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=23
05/29/2022 05:15:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=23
05/29/2022 05:15:42 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.7150706020690791 on epoch=23
05/29/2022 05:15:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=23
05/29/2022 05:15:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.59 on epoch=24
05/29/2022 05:15:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=24
05/29/2022 05:15:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=24
05/29/2022 05:15:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=24
05/29/2022 05:16:01 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.7183300167857917 on epoch=24
05/29/2022 05:16:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=25
05/29/2022 05:16:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=25
05/29/2022 05:16:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=25
05/29/2022 05:16:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=26
05/29/2022 05:16:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=26
05/29/2022 05:16:21 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.7275142068642979 on epoch=26
05/29/2022 05:16:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7189625994521978 -> 0.7275142068642979 on epoch=26, global_step=850
05/29/2022 05:16:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=26
05/29/2022 05:16:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.59 on epoch=27
05/29/2022 05:16:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=27
05/29/2022 05:16:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=27
05/29/2022 05:16:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=28
05/29/2022 05:16:40 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.7431358920274689 on epoch=28
05/29/2022 05:16:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7275142068642979 -> 0.7431358920274689 on epoch=28, global_step=900
05/29/2022 05:16:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=28
05/29/2022 05:16:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=28
05/29/2022 05:16:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=29
05/29/2022 05:16:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=29
05/29/2022 05:16:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=29
05/29/2022 05:17:00 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.7493451099526185 on epoch=29
05/29/2022 05:17:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7431358920274689 -> 0.7493451099526185 on epoch=29, global_step=950
05/29/2022 05:17:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=29
05/29/2022 05:17:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=30
05/29/2022 05:17:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=30
05/29/2022 05:17:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=30
05/29/2022 05:17:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=31
05/29/2022 05:17:19 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.7489158074784064 on epoch=31
05/29/2022 05:17:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=31
05/29/2022 05:17:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=31
05/29/2022 05:17:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=32
05/29/2022 05:17:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=32
05/29/2022 05:17:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=32
05/29/2022 05:17:39 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.761284590228851 on epoch=32
05/29/2022 05:17:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7493451099526185 -> 0.761284590228851 on epoch=32, global_step=1050
05/29/2022 05:17:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=33
05/29/2022 05:17:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=33
05/29/2022 05:17:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=33
05/29/2022 05:17:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=34
05/29/2022 05:17:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=34
05/29/2022 05:17:58 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.7622169778080012 on epoch=34
05/29/2022 05:17:58 - INFO - __main__ - Saving model with best Classification-F1: 0.761284590228851 -> 0.7622169778080012 on epoch=34, global_step=1100
05/29/2022 05:18:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/29/2022 05:18:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
05/29/2022 05:18:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=35
05/29/2022 05:18:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
05/29/2022 05:18:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=35
05/29/2022 05:18:18 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.7723011011729471 on epoch=35
05/29/2022 05:18:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7622169778080012 -> 0.7723011011729471 on epoch=35, global_step=1150
05/29/2022 05:18:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=36
05/29/2022 05:18:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=36
05/29/2022 05:18:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=36
05/29/2022 05:18:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=37
05/29/2022 05:18:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=37
05/29/2022 05:18:38 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.7341835328862931 on epoch=37
05/29/2022 05:18:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=37
05/29/2022 05:18:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=38
05/29/2022 05:18:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=38
05/29/2022 05:18:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=38
05/29/2022 05:18:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=39
05/29/2022 05:18:58 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.7309104240324007 on epoch=39
05/29/2022 05:19:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=39
05/29/2022 05:19:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=39
05/29/2022 05:19:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=39
05/29/2022 05:19:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=40
05/29/2022 05:19:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=40
05/29/2022 05:19:18 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7811618248199965 on epoch=40
05/29/2022 05:19:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7723011011729471 -> 0.7811618248199965 on epoch=40, global_step=1300
05/29/2022 05:19:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=40
05/29/2022 05:19:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=41
05/29/2022 05:19:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=41
05/29/2022 05:19:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=41
05/29/2022 05:19:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.47 on epoch=42
05/29/2022 05:19:38 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7809656852642998 on epoch=42
05/29/2022 05:19:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=42
05/29/2022 05:19:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=42
05/29/2022 05:19:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=43
05/29/2022 05:19:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=43
05/29/2022 05:19:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=43
05/29/2022 05:19:57 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7821703159040393 on epoch=43
05/29/2022 05:19:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7811618248199965 -> 0.7821703159040393 on epoch=43, global_step=1400
05/29/2022 05:20:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=44
05/29/2022 05:20:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=44
05/29/2022 05:20:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=44
05/29/2022 05:20:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=44
05/29/2022 05:20:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=45
05/29/2022 05:20:17 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.7599542762997606 on epoch=45
05/29/2022 05:20:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=45
05/29/2022 05:20:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=45
05/29/2022 05:20:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=46
05/29/2022 05:20:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=46
05/29/2022 05:20:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=46
05/29/2022 05:20:37 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.7750164798945287 on epoch=46
05/29/2022 05:20:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=47
05/29/2022 05:20:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=47
05/29/2022 05:20:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=47
05/29/2022 05:20:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=48
05/29/2022 05:20:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=48
05/29/2022 05:20:57 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.7810742152203504 on epoch=48
05/29/2022 05:21:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=48
05/29/2022 05:21:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=49
05/29/2022 05:21:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=49
05/29/2022 05:21:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=49
05/29/2022 05:21:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=49
05/29/2022 05:21:17 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.7682249497972025 on epoch=49
05/29/2022 05:21:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=50
05/29/2022 05:21:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=50
05/29/2022 05:21:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=50
05/29/2022 05:21:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=51
05/29/2022 05:21:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=51
05/29/2022 05:21:37 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.7579938115357905 on epoch=51
05/29/2022 05:21:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=51
05/29/2022 05:21:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=52
05/29/2022 05:21:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=52
05/29/2022 05:21:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=52
05/29/2022 05:21:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=53
05/29/2022 05:21:57 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.8040359500202786 on epoch=53
05/29/2022 05:21:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7821703159040393 -> 0.8040359500202786 on epoch=53, global_step=1700
05/29/2022 05:21:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=53
05/29/2022 05:22:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=53
05/29/2022 05:22:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=54
05/29/2022 05:22:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=54
05/29/2022 05:22:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=54
05/29/2022 05:22:17 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.7835631818012887 on epoch=54
05/29/2022 05:22:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=54
05/29/2022 05:22:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=55
05/29/2022 05:22:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=55
05/29/2022 05:22:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=55
05/29/2022 05:22:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=56
05/29/2022 05:22:36 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.7809613048600013 on epoch=56
05/29/2022 05:22:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/29/2022 05:22:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=56
05/29/2022 05:22:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=57
05/29/2022 05:22:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=57
05/29/2022 05:22:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=57
05/29/2022 05:22:56 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.786355316304338 on epoch=57
05/29/2022 05:22:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=58
05/29/2022 05:23:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=58
05/29/2022 05:23:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=58
05/29/2022 05:23:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=59
05/29/2022 05:23:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=59
05/29/2022 05:23:16 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.7695688243375065 on epoch=59
05/29/2022 05:23:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=59
05/29/2022 05:23:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=59
05/29/2022 05:23:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=60
05/29/2022 05:23:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=60
05/29/2022 05:23:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=60
05/29/2022 05:23:36 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.7793668184848064 on epoch=60
05/29/2022 05:23:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=61
05/29/2022 05:23:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=61
05/29/2022 05:23:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
05/29/2022 05:23:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=62
05/29/2022 05:23:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=62
05/29/2022 05:23:56 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.7835088400790848 on epoch=62
05/29/2022 05:23:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=62
05/29/2022 05:24:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=63
05/29/2022 05:24:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=63
05/29/2022 05:24:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/29/2022 05:24:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=64
05/29/2022 05:24:16 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.782321071507631 on epoch=64
05/29/2022 05:24:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=64
05/29/2022 05:24:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=64
05/29/2022 05:24:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=64
05/29/2022 05:24:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
05/29/2022 05:24:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=65
05/29/2022 05:24:36 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.7998579150626132 on epoch=65
05/29/2022 05:24:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.33 on epoch=65
05/29/2022 05:24:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=66
05/29/2022 05:24:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=66
05/29/2022 05:24:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=66
05/29/2022 05:24:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=67
05/29/2022 05:24:56 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.8002986657043376 on epoch=67
05/29/2022 05:24:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=67
05/29/2022 05:25:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=67
05/29/2022 05:25:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=68
05/29/2022 05:25:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=68
05/29/2022 05:25:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=68
05/29/2022 05:25:16 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.8008001786170429 on epoch=68
05/29/2022 05:25:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=69
05/29/2022 05:25:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=69
05/29/2022 05:25:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=69
05/29/2022 05:25:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=69
05/29/2022 05:25:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=70
05/29/2022 05:25:36 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.7649536590592521 on epoch=70
05/29/2022 05:25:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=70
05/29/2022 05:25:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=70
05/29/2022 05:25:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=71
05/29/2022 05:25:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=71
05/29/2022 05:25:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=71
05/29/2022 05:25:56 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.7978204806000884 on epoch=71
05/29/2022 05:25:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=72
05/29/2022 05:26:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=72
05/29/2022 05:26:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=72
05/29/2022 05:26:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=73
05/29/2022 05:26:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=73
05/29/2022 05:26:16 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.7884401310407609 on epoch=73
05/29/2022 05:26:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=73
05/29/2022 05:26:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=74
05/29/2022 05:26:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=74
05/29/2022 05:26:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=74
05/29/2022 05:26:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=74
05/29/2022 05:26:36 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.7960459489313763 on epoch=74
05/29/2022 05:26:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=75
05/29/2022 05:26:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=75
05/29/2022 05:26:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=75
05/29/2022 05:26:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.35 on epoch=76
05/29/2022 05:26:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=76
05/29/2022 05:26:56 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.7777205356945738 on epoch=76
05/29/2022 05:26:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=76
05/29/2022 05:27:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=77
05/29/2022 05:27:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=77
05/29/2022 05:27:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=77
05/29/2022 05:27:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.28 on epoch=78
05/29/2022 05:27:16 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.7963474991512756 on epoch=78
05/29/2022 05:27:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=78
05/29/2022 05:27:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=78
05/29/2022 05:27:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=79
05/29/2022 05:27:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=79
05/29/2022 05:27:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=79
05/29/2022 05:27:36 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.7912498537498538 on epoch=79
05/29/2022 05:27:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.23 on epoch=79
05/29/2022 05:27:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=80
05/29/2022 05:27:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=80
05/29/2022 05:27:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=80
05/29/2022 05:27:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=81
05/29/2022 05:27:56 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.7811372421531529 on epoch=81
05/29/2022 05:27:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
05/29/2022 05:28:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=81
05/29/2022 05:28:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=82
05/29/2022 05:28:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=82
05/29/2022 05:28:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=82
05/29/2022 05:28:16 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.7907909736722106 on epoch=82
05/29/2022 05:28:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=83
05/29/2022 05:28:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/29/2022 05:28:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=83
05/29/2022 05:28:26 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=84
05/29/2022 05:28:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=84
05/29/2022 05:28:36 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.7801950287632105 on epoch=84
05/29/2022 05:28:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=84
05/29/2022 05:28:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.24 on epoch=84
05/29/2022 05:28:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
05/29/2022 05:28:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=85
05/29/2022 05:28:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=85
05/29/2022 05:28:56 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.8102322087866823 on epoch=85
05/29/2022 05:28:56 - INFO - __main__ - Saving model with best Classification-F1: 0.8040359500202786 -> 0.8102322087866823 on epoch=85, global_step=2750
05/29/2022 05:28:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=86
05/29/2022 05:29:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=86
05/29/2022 05:29:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=86
05/29/2022 05:29:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=87
05/29/2022 05:29:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=87
05/29/2022 05:29:16 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.7874072394703462 on epoch=87
05/29/2022 05:29:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=87
05/29/2022 05:29:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=88
05/29/2022 05:29:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=88
05/29/2022 05:29:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=88
05/29/2022 05:29:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=89
05/29/2022 05:29:36 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.7839803936200236 on epoch=89
05/29/2022 05:29:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
05/29/2022 05:29:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=89
05/29/2022 05:29:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=89
05/29/2022 05:29:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=90
05/29/2022 05:29:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.25 on epoch=90
05/29/2022 05:29:56 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.8032095321988614 on epoch=90
05/29/2022 05:29:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=90
05/29/2022 05:30:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=91
05/29/2022 05:30:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=91
05/29/2022 05:30:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=91
05/29/2022 05:30:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=92
05/29/2022 05:30:16 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.7979797855528395 on epoch=92
05/29/2022 05:30:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=92
05/29/2022 05:30:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=92
05/29/2022 05:30:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=93
05/29/2022 05:30:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=93
05/29/2022 05:30:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
05/29/2022 05:30:30 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:30:30 - INFO - __main__ - Printing 3 examples
05/29/2022 05:30:30 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 05:30:30 - INFO - __main__ - ['sad']
05/29/2022 05:30:30 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 05:30:30 - INFO - __main__ - ['sad']
05/29/2022 05:30:30 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 05:30:30 - INFO - __main__ - ['sad']
05/29/2022 05:30:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:30:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:30:31 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:30:31 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:30:31 - INFO - __main__ - Printing 3 examples
05/29/2022 05:30:31 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 05:30:31 - INFO - __main__ - ['sad']
05/29/2022 05:30:31 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 05:30:31 - INFO - __main__ - ['sad']
05/29/2022 05:30:31 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 05:30:31 - INFO - __main__ - ['sad']
05/29/2022 05:30:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:30:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:30:32 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:30:36 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.8075327324254686 on epoch=93
05/29/2022 05:30:36 - INFO - __main__ - save last model!
05/29/2022 05:30:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 05:30:36 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 05:30:36 - INFO - __main__ - Printing 3 examples
05/29/2022 05:30:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 05:30:36 - INFO - __main__ - ['others']
05/29/2022 05:30:36 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 05:30:36 - INFO - __main__ - ['others']
05/29/2022 05:30:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 05:30:36 - INFO - __main__ - ['others']
05/29/2022 05:30:36 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:30:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:30:43 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 05:30:49 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:30:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:30:50 - INFO - __main__ - Starting training!
05/29/2022 05:32:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_21_0.3_8_predictions.txt
05/29/2022 05:32:00 - INFO - __main__ - Classification-F1 on test data: 0.4952
05/29/2022 05:32:00 - INFO - __main__ - prefix=emo_128_21, lr=0.3, bsz=8, dev_performance=0.8102322087866823, test_performance=0.4951859808098729
05/29/2022 05:32:00 - INFO - __main__ - Running ... prefix=emo_128_21, lr=0.2, bsz=8 ...
05/29/2022 05:32:01 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:32:01 - INFO - __main__ - Printing 3 examples
05/29/2022 05:32:01 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/29/2022 05:32:01 - INFO - __main__ - ['sad']
05/29/2022 05:32:01 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/29/2022 05:32:01 - INFO - __main__ - ['sad']
05/29/2022 05:32:01 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/29/2022 05:32:01 - INFO - __main__ - ['sad']
05/29/2022 05:32:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:32:01 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:32:02 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:32:02 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:32:02 - INFO - __main__ - Printing 3 examples
05/29/2022 05:32:02 - INFO - __main__ -  [emo] my head is hurting because of these tears reason no reason
05/29/2022 05:32:02 - INFO - __main__ - ['sad']
05/29/2022 05:32:02 - INFO - __main__ -  [emo] yes i don't wait but why loudlycryingfaceloudlycryingface
05/29/2022 05:32:02 - INFO - __main__ - ['sad']
05/29/2022 05:32:02 - INFO - __main__ -  [emo] i can't say actually i have never tried it lol fair enough actually i am very bad at making friends and travelling alone without making new friends is kind of boring i think
05/29/2022 05:32:02 - INFO - __main__ - ['sad']
05/29/2022 05:32:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:32:02 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:32:02 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:32:18 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:32:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:32:19 - INFO - __main__ - Starting training!
05/29/2022 05:32:22 - INFO - __main__ - Step 10 Global step 10 Train loss 4.63 on epoch=0
05/29/2022 05:32:24 - INFO - __main__ - Step 20 Global step 20 Train loss 3.47 on epoch=0
05/29/2022 05:32:27 - INFO - __main__ - Step 30 Global step 30 Train loss 2.85 on epoch=0
05/29/2022 05:32:29 - INFO - __main__ - Step 40 Global step 40 Train loss 2.52 on epoch=1
05/29/2022 05:32:32 - INFO - __main__ - Step 50 Global step 50 Train loss 2.50 on epoch=1
05/29/2022 05:32:39 - INFO - __main__ - Global step 50 Train loss 3.20 Classification-F1 0.02149621212121212 on epoch=1
05/29/2022 05:32:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.02149621212121212 on epoch=1, global_step=50
05/29/2022 05:32:42 - INFO - __main__ - Step 60 Global step 60 Train loss 2.16 on epoch=1
05/29/2022 05:32:44 - INFO - __main__ - Step 70 Global step 70 Train loss 2.17 on epoch=2
05/29/2022 05:32:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.89 on epoch=2
05/29/2022 05:32:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.82 on epoch=2
05/29/2022 05:32:52 - INFO - __main__ - Step 100 Global step 100 Train loss 1.68 on epoch=3
05/29/2022 05:32:59 - INFO - __main__ - Global step 100 Train loss 1.94 Classification-F1 0.14109318008847407 on epoch=3
05/29/2022 05:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.02149621212121212 -> 0.14109318008847407 on epoch=3, global_step=100
05/29/2022 05:33:01 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=3
05/29/2022 05:33:04 - INFO - __main__ - Step 120 Global step 120 Train loss 1.18 on epoch=3
05/29/2022 05:33:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.26 on epoch=4
05/29/2022 05:33:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=4
05/29/2022 05:33:11 - INFO - __main__ - Step 150 Global step 150 Train loss 1.05 on epoch=4
05/29/2022 05:33:18 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.4287033439833001 on epoch=4
05/29/2022 05:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.14109318008847407 -> 0.4287033439833001 on epoch=4, global_step=150
05/29/2022 05:33:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=4
05/29/2022 05:33:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.98 on epoch=5
05/29/2022 05:33:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.93 on epoch=5
05/29/2022 05:33:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=5
05/29/2022 05:33:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.80 on epoch=6
05/29/2022 05:33:37 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.5422728917842925 on epoch=6
05/29/2022 05:33:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4287033439833001 -> 0.5422728917842925 on epoch=6, global_step=200
05/29/2022 05:33:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=6
05/29/2022 05:33:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.89 on epoch=6
05/29/2022 05:33:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=7
05/29/2022 05:33:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.84 on epoch=7
05/29/2022 05:33:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.85 on epoch=7
05/29/2022 05:33:57 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.6019217397153638 on epoch=7
05/29/2022 05:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5422728917842925 -> 0.6019217397153638 on epoch=7, global_step=250
05/29/2022 05:33:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=8
05/29/2022 05:34:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=8
05/29/2022 05:34:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.74 on epoch=8
05/29/2022 05:34:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.88 on epoch=9
05/29/2022 05:34:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=9
05/29/2022 05:34:16 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.562696296032839 on epoch=9
05/29/2022 05:34:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.66 on epoch=9
05/29/2022 05:34:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=9
05/29/2022 05:34:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=10
05/29/2022 05:34:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=10
05/29/2022 05:34:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=10
05/29/2022 05:34:35 - INFO - __main__ - Global step 350 Train loss 0.69 Classification-F1 0.592155880106911 on epoch=10
05/29/2022 05:34:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=11
05/29/2022 05:34:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=11
05/29/2022 05:34:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=11
05/29/2022 05:34:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.72 on epoch=12
05/29/2022 05:34:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=12
05/29/2022 05:34:54 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.5866504698163695 on epoch=12
05/29/2022 05:34:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=12
05/29/2022 05:34:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=13
05/29/2022 05:35:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.65 on epoch=13
05/29/2022 05:35:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=13
05/29/2022 05:35:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=14
05/29/2022 05:35:14 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.6394634760753164 on epoch=14
05/29/2022 05:35:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6019217397153638 -> 0.6394634760753164 on epoch=14, global_step=450
05/29/2022 05:35:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=14
05/29/2022 05:35:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=14
05/29/2022 05:35:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=14
05/29/2022 05:35:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.65 on epoch=15
05/29/2022 05:35:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.57 on epoch=15
05/29/2022 05:35:33 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.6639406172345728 on epoch=15
05/29/2022 05:35:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6394634760753164 -> 0.6639406172345728 on epoch=15, global_step=500
05/29/2022 05:35:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.60 on epoch=15
05/29/2022 05:35:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=16
05/29/2022 05:35:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=16
05/29/2022 05:35:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=16
05/29/2022 05:35:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.60 on epoch=17
05/29/2022 05:35:52 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.6828634890944921 on epoch=17
05/29/2022 05:35:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6639406172345728 -> 0.6828634890944921 on epoch=17, global_step=550
05/29/2022 05:35:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=17
05/29/2022 05:35:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=17
05/29/2022 05:36:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.61 on epoch=18
05/29/2022 05:36:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=18
05/29/2022 05:36:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=18
05/29/2022 05:36:12 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.7077741547637802 on epoch=18
05/29/2022 05:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6828634890944921 -> 0.7077741547637802 on epoch=18, global_step=600
05/29/2022 05:36:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.59 on epoch=19
05/29/2022 05:36:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=19
05/29/2022 05:36:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=19
05/29/2022 05:36:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=19
05/29/2022 05:36:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=20
05/29/2022 05:36:31 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.6607122536119272 on epoch=20
05/29/2022 05:36:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=20
05/29/2022 05:36:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=20
05/29/2022 05:36:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.59 on epoch=21
05/29/2022 05:36:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=21
05/29/2022 05:36:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=21
05/29/2022 05:36:50 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.6616997558161033 on epoch=21
05/29/2022 05:36:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.66 on epoch=22
05/29/2022 05:36:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=22
05/29/2022 05:36:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=22
05/29/2022 05:37:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=23
05/29/2022 05:37:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=23
05/29/2022 05:37:09 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.6847135538483301 on epoch=23
05/29/2022 05:37:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=23
05/29/2022 05:37:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.58 on epoch=24
05/29/2022 05:37:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=24
05/29/2022 05:37:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=24
05/29/2022 05:37:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=24
05/29/2022 05:37:29 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.6717803400304403 on epoch=24
05/29/2022 05:37:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=25
05/29/2022 05:37:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=25
05/29/2022 05:37:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=25
05/29/2022 05:37:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=26
05/29/2022 05:37:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=26
05/29/2022 05:37:48 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.7040610754114861 on epoch=26
05/29/2022 05:37:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=26
05/29/2022 05:37:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=27
05/29/2022 05:37:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=27
05/29/2022 05:37:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=27
05/29/2022 05:38:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=28
05/29/2022 05:38:07 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.7331487119038866 on epoch=28
05/29/2022 05:38:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7077741547637802 -> 0.7331487119038866 on epoch=28, global_step=900
05/29/2022 05:38:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=28
05/29/2022 05:38:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=28
05/29/2022 05:38:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.55 on epoch=29
05/29/2022 05:38:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=29
05/29/2022 05:38:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=29
05/29/2022 05:38:27 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.7342719978711653 on epoch=29
05/29/2022 05:38:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7331487119038866 -> 0.7342719978711653 on epoch=29, global_step=950
05/29/2022 05:38:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=29
05/29/2022 05:38:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=30
05/29/2022 05:38:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=30
05/29/2022 05:38:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=30
05/29/2022 05:38:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=31
05/29/2022 05:38:46 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.719000420604802 on epoch=31
05/29/2022 05:38:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=31
05/29/2022 05:38:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=31
05/29/2022 05:38:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.59 on epoch=32
05/29/2022 05:38:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=32
05/29/2022 05:38:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.50 on epoch=32
05/29/2022 05:39:05 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.7147874489223167 on epoch=32
05/29/2022 05:39:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=33
05/29/2022 05:39:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=33
05/29/2022 05:39:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=33
05/29/2022 05:39:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=34
05/29/2022 05:39:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=34
05/29/2022 05:39:25 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.7293923569239731 on epoch=34
05/29/2022 05:39:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=34
05/29/2022 05:39:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=34
05/29/2022 05:39:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=35
05/29/2022 05:39:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=35
05/29/2022 05:39:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=35
05/29/2022 05:39:44 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.7417955327698383 on epoch=35
05/29/2022 05:39:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7342719978711653 -> 0.7417955327698383 on epoch=35, global_step=1150
05/29/2022 05:39:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=36
05/29/2022 05:39:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=36
05/29/2022 05:39:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=36
05/29/2022 05:39:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=37
05/29/2022 05:39:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=37
05/29/2022 05:40:03 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.7236915530456167 on epoch=37
05/29/2022 05:40:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=37
05/29/2022 05:40:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=38
05/29/2022 05:40:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=38
05/29/2022 05:40:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=38
05/29/2022 05:40:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.60 on epoch=39
05/29/2022 05:40:23 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.7530026881442561 on epoch=39
05/29/2022 05:40:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7417955327698383 -> 0.7530026881442561 on epoch=39, global_step=1250
05/29/2022 05:40:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
05/29/2022 05:40:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=39
05/29/2022 05:40:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=39
05/29/2022 05:40:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=40
05/29/2022 05:40:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=40
05/29/2022 05:40:42 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.7538145331878932 on epoch=40
05/29/2022 05:40:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7530026881442561 -> 0.7538145331878932 on epoch=40, global_step=1300
05/29/2022 05:40:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=40
05/29/2022 05:40:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=41
05/29/2022 05:40:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=41
05/29/2022 05:40:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=41
05/29/2022 05:40:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=42
05/29/2022 05:41:01 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.7550068746675571 on epoch=42
05/29/2022 05:41:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7538145331878932 -> 0.7550068746675571 on epoch=42, global_step=1350
05/29/2022 05:41:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=42
05/29/2022 05:41:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=42
05/29/2022 05:41:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=43
05/29/2022 05:41:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=43
05/29/2022 05:41:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=43
05/29/2022 05:41:21 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.7668800958739493 on epoch=43
05/29/2022 05:41:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7550068746675571 -> 0.7668800958739493 on epoch=43, global_step=1400
05/29/2022 05:41:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=44
05/29/2022 05:41:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=44
05/29/2022 05:41:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=44
05/29/2022 05:41:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=44
05/29/2022 05:41:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=45
05/29/2022 05:41:40 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.7311477238882373 on epoch=45
05/29/2022 05:41:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=45
05/29/2022 05:41:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=45
05/29/2022 05:41:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=46
05/29/2022 05:41:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=46
05/29/2022 05:41:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=46
05/29/2022 05:41:59 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.7647140052251619 on epoch=46
05/29/2022 05:42:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=47
05/29/2022 05:42:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=47
05/29/2022 05:42:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=47
05/29/2022 05:42:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=48
05/29/2022 05:42:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=48
05/29/2022 05:42:18 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.7563200208205658 on epoch=48
05/29/2022 05:42:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=48
05/29/2022 05:42:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=49
05/29/2022 05:42:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=49
05/29/2022 05:42:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=49
05/29/2022 05:42:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=49
05/29/2022 05:42:37 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.7430400544744391 on epoch=49
05/29/2022 05:42:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=50
05/29/2022 05:42:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=50
05/29/2022 05:42:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=50
05/29/2022 05:42:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=51
05/29/2022 05:42:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=51
05/29/2022 05:42:57 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.7497323287399622 on epoch=51
05/29/2022 05:42:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=51
05/29/2022 05:43:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=52
05/29/2022 05:43:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=52
05/29/2022 05:43:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=52
05/29/2022 05:43:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=53
05/29/2022 05:43:16 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.781415861654432 on epoch=53
05/29/2022 05:43:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7668800958739493 -> 0.781415861654432 on epoch=53, global_step=1700
05/29/2022 05:43:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=53
05/29/2022 05:43:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=53
05/29/2022 05:43:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=54
05/29/2022 05:43:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=54
05/29/2022 05:43:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=54
05/29/2022 05:43:35 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.7620454197331394 on epoch=54
05/29/2022 05:43:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=54
05/29/2022 05:43:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=55
05/29/2022 05:43:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=55
05/29/2022 05:43:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=55
05/29/2022 05:43:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=56
05/29/2022 05:43:54 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.7640850480339643 on epoch=56
05/29/2022 05:43:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=56
05/29/2022 05:43:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=56
05/29/2022 05:44:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=57
05/29/2022 05:44:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=57
05/29/2022 05:44:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=57
05/29/2022 05:44:13 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.7721256482165204 on epoch=57
05/29/2022 05:44:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=58
05/29/2022 05:44:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=58
05/29/2022 05:44:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=58
05/29/2022 05:44:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=59
05/29/2022 05:44:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=59
05/29/2022 05:44:33 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.7584906400969108 on epoch=59
05/29/2022 05:44:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=59
05/29/2022 05:44:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=59
05/29/2022 05:44:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=60
05/29/2022 05:44:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=60
05/29/2022 05:44:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=60
05/29/2022 05:44:52 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.7762971820556437 on epoch=60
05/29/2022 05:44:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=61
05/29/2022 05:44:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=61
05/29/2022 05:44:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=61
05/29/2022 05:45:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=62
05/29/2022 05:45:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=62
05/29/2022 05:45:11 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.7647478877542238 on epoch=62
05/29/2022 05:45:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=62
05/29/2022 05:45:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=63
05/29/2022 05:45:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=63
05/29/2022 05:45:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.30 on epoch=63
05/29/2022 05:45:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=64
05/29/2022 05:45:30 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.7719293343319849 on epoch=64
05/29/2022 05:45:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=64
05/29/2022 05:45:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=64
05/29/2022 05:45:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.38 on epoch=64
05/29/2022 05:45:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=65
05/29/2022 05:45:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=65
05/29/2022 05:45:49 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.7719582548973997 on epoch=65
05/29/2022 05:45:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=65
05/29/2022 05:45:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.38 on epoch=66
05/29/2022 05:45:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=66
05/29/2022 05:45:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=66
05/29/2022 05:46:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=67
05/29/2022 05:46:09 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.7771988239188427 on epoch=67
05/29/2022 05:46:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=67
05/29/2022 05:46:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=67
05/29/2022 05:46:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=68
05/29/2022 05:46:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=68
05/29/2022 05:46:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=68
05/29/2022 05:46:28 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.7718891666342644 on epoch=68
05/29/2022 05:46:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.33 on epoch=69
05/29/2022 05:46:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=69
05/29/2022 05:46:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=69
05/29/2022 05:46:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=69
05/29/2022 05:46:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.36 on epoch=70
05/29/2022 05:46:47 - INFO - __main__ - Global step 2250 Train loss 0.29 Classification-F1 0.7659929720668553 on epoch=70
05/29/2022 05:46:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.28 on epoch=70
05/29/2022 05:46:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=70
05/29/2022 05:46:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=71
05/29/2022 05:46:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=71
05/29/2022 05:47:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=71
05/29/2022 05:47:06 - INFO - __main__ - Global step 2300 Train loss 0.32 Classification-F1 0.7809076616665594 on epoch=71
05/29/2022 05:47:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=72
05/29/2022 05:47:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.30 on epoch=72
05/29/2022 05:47:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.30 on epoch=72
05/29/2022 05:47:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.31 on epoch=73
05/29/2022 05:47:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=73
05/29/2022 05:47:26 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.7912081514622678 on epoch=73
05/29/2022 05:47:26 - INFO - __main__ - Saving model with best Classification-F1: 0.781415861654432 -> 0.7912081514622678 on epoch=73, global_step=2350
05/29/2022 05:47:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=73
05/29/2022 05:47:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=74
05/29/2022 05:47:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.25 on epoch=74
05/29/2022 05:47:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=74
05/29/2022 05:47:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=74
05/29/2022 05:47:45 - INFO - __main__ - Global step 2400 Train loss 0.30 Classification-F1 0.7744476315094089 on epoch=74
05/29/2022 05:47:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=75
05/29/2022 05:47:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=75
05/29/2022 05:47:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=75
05/29/2022 05:47:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.26 on epoch=76
05/29/2022 05:47:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=76
05/29/2022 05:48:04 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.7763342172973058 on epoch=76
05/29/2022 05:48:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=76
05/29/2022 05:48:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=77
05/29/2022 05:48:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.25 on epoch=77
05/29/2022 05:48:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=77
05/29/2022 05:48:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=78
05/29/2022 05:48:23 - INFO - __main__ - Global step 2500 Train loss 0.31 Classification-F1 0.7990763282525297 on epoch=78
05/29/2022 05:48:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7912081514622678 -> 0.7990763282525297 on epoch=78, global_step=2500
05/29/2022 05:48:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.26 on epoch=78
05/29/2022 05:48:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.25 on epoch=78
05/29/2022 05:48:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=79
05/29/2022 05:48:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.30 on epoch=79
05/29/2022 05:48:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.26 on epoch=79
05/29/2022 05:48:42 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.7799623037843644 on epoch=79
05/29/2022 05:48:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.30 on epoch=79
05/29/2022 05:48:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.28 on epoch=80
05/29/2022 05:48:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=80
05/29/2022 05:48:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.30 on epoch=80
05/29/2022 05:48:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=81
05/29/2022 05:49:02 - INFO - __main__ - Global step 2600 Train loss 0.31 Classification-F1 0.773827684237482 on epoch=81
05/29/2022 05:49:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
05/29/2022 05:49:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=81
05/29/2022 05:49:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=82
05/29/2022 05:49:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=82
05/29/2022 05:49:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=82
05/29/2022 05:49:21 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.7846344678792974 on epoch=82
05/29/2022 05:49:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.31 on epoch=83
05/29/2022 05:49:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.25 on epoch=83
05/29/2022 05:49:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=83
05/29/2022 05:49:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=84
05/29/2022 05:49:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=84
05/29/2022 05:49:40 - INFO - __main__ - Global step 2700 Train loss 0.27 Classification-F1 0.7934566458572874 on epoch=84
05/29/2022 05:49:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=84
05/29/2022 05:49:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.28 on epoch=84
05/29/2022 05:49:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.30 on epoch=85
05/29/2022 05:49:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=85
05/29/2022 05:49:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.30 on epoch=85
05/29/2022 05:49:59 - INFO - __main__ - Global step 2750 Train loss 0.27 Classification-F1 0.7762546837761086 on epoch=85
05/29/2022 05:50:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.33 on epoch=86
05/29/2022 05:50:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.25 on epoch=86
05/29/2022 05:50:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.31 on epoch=86
05/29/2022 05:50:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.37 on epoch=87
05/29/2022 05:50:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=87
05/29/2022 05:50:18 - INFO - __main__ - Global step 2800 Train loss 0.29 Classification-F1 0.7911362273165183 on epoch=87
05/29/2022 05:50:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=87
05/29/2022 05:50:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.33 on epoch=88
05/29/2022 05:50:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=88
05/29/2022 05:50:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.25 on epoch=88
05/29/2022 05:50:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.37 on epoch=89
05/29/2022 05:50:38 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.795695401244694 on epoch=89
05/29/2022 05:50:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=89
05/29/2022 05:50:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.28 on epoch=89
05/29/2022 05:50:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.33 on epoch=89
05/29/2022 05:50:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.26 on epoch=90
05/29/2022 05:50:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=90
05/29/2022 05:50:57 - INFO - __main__ - Global step 2900 Train loss 0.29 Classification-F1 0.7791576919751211 on epoch=90
05/29/2022 05:50:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=90
05/29/2022 05:51:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.34 on epoch=91
05/29/2022 05:51:04 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.30 on epoch=91
05/29/2022 05:51:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=91
05/29/2022 05:51:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.34 on epoch=92
05/29/2022 05:51:16 - INFO - __main__ - Global step 2950 Train loss 0.31 Classification-F1 0.787013996333229 on epoch=92
05/29/2022 05:51:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=92
05/29/2022 05:51:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.28 on epoch=92
05/29/2022 05:51:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.29 on epoch=93
05/29/2022 05:51:26 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.32 on epoch=93
05/29/2022 05:51:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=93
05/29/2022 05:51:30 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:51:30 - INFO - __main__ - Printing 3 examples
05/29/2022 05:51:30 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 05:51:30 - INFO - __main__ - ['happy']
05/29/2022 05:51:30 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 05:51:30 - INFO - __main__ - ['happy']
05/29/2022 05:51:30 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 05:51:30 - INFO - __main__ - ['happy']
05/29/2022 05:51:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:51:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:51:31 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:51:31 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:51:31 - INFO - __main__ - Printing 3 examples
05/29/2022 05:51:31 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 05:51:31 - INFO - __main__ - ['happy']
05/29/2022 05:51:31 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 05:51:31 - INFO - __main__ - ['happy']
05/29/2022 05:51:31 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 05:51:31 - INFO - __main__ - ['happy']
05/29/2022 05:51:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:51:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:51:32 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:51:36 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.7995644024093078 on epoch=93
05/29/2022 05:51:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7990763282525297 -> 0.7995644024093078 on epoch=93, global_step=3000
05/29/2022 05:51:36 - INFO - __main__ - save last model!
05/29/2022 05:51:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 05:51:36 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 05:51:36 - INFO - __main__ - Printing 3 examples
05/29/2022 05:51:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 05:51:36 - INFO - __main__ - ['others']
05/29/2022 05:51:36 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 05:51:36 - INFO - __main__ - ['others']
05/29/2022 05:51:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 05:51:36 - INFO - __main__ - ['others']
05/29/2022 05:51:36 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:51:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:51:43 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 05:51:48 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:51:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:51:48 - INFO - __main__ - Starting training!
05/29/2022 05:52:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_21_0.2_8_predictions.txt
05/29/2022 05:52:58 - INFO - __main__ - Classification-F1 on test data: 0.3945
05/29/2022 05:52:58 - INFO - __main__ - prefix=emo_128_21, lr=0.2, bsz=8, dev_performance=0.7995644024093078, test_performance=0.39452591864445424
05/29/2022 05:52:58 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.5, bsz=8 ...
05/29/2022 05:52:59 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:52:59 - INFO - __main__ - Printing 3 examples
05/29/2022 05:52:59 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 05:52:59 - INFO - __main__ - ['happy']
05/29/2022 05:52:59 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 05:52:59 - INFO - __main__ - ['happy']
05/29/2022 05:52:59 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 05:52:59 - INFO - __main__ - ['happy']
05/29/2022 05:52:59 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:52:59 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:53:00 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 05:53:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 05:53:00 - INFO - __main__ - Printing 3 examples
05/29/2022 05:53:00 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 05:53:00 - INFO - __main__ - ['happy']
05/29/2022 05:53:00 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 05:53:00 - INFO - __main__ - ['happy']
05/29/2022 05:53:00 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 05:53:00 - INFO - __main__ - ['happy']
05/29/2022 05:53:00 - INFO - __main__ - Tokenizing Input ...
05/29/2022 05:53:00 - INFO - __main__ - Tokenizing Output ...
05/29/2022 05:53:01 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 05:53:16 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 05:53:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 05:53:17 - INFO - __main__ - Starting training!
05/29/2022 05:53:20 - INFO - __main__ - Step 10 Global step 10 Train loss 4.27 on epoch=0
05/29/2022 05:53:22 - INFO - __main__ - Step 20 Global step 20 Train loss 2.80 on epoch=0
05/29/2022 05:53:25 - INFO - __main__ - Step 30 Global step 30 Train loss 2.27 on epoch=0
05/29/2022 05:53:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.89 on epoch=1
05/29/2022 05:53:30 - INFO - __main__ - Step 50 Global step 50 Train loss 1.31 on epoch=1
05/29/2022 05:53:37 - INFO - __main__ - Global step 50 Train loss 2.51 Classification-F1 0.23024766487812398 on epoch=1
05/29/2022 05:53:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23024766487812398 on epoch=1, global_step=50
05/29/2022 05:53:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.14 on epoch=1
05/29/2022 05:53:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.07 on epoch=2
05/29/2022 05:53:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.79 on epoch=2
05/29/2022 05:53:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=2
05/29/2022 05:53:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.04 on epoch=3
05/29/2022 05:53:56 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.5369855299874837 on epoch=3
05/29/2022 05:53:56 - INFO - __main__ - Saving model with best Classification-F1: 0.23024766487812398 -> 0.5369855299874837 on epoch=3, global_step=100
05/29/2022 05:53:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=3
05/29/2022 05:54:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=3
05/29/2022 05:54:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=4
05/29/2022 05:54:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=4
05/29/2022 05:54:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=4
05/29/2022 05:54:15 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.4436036101711053 on epoch=4
05/29/2022 05:54:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=4
05/29/2022 05:54:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=5
05/29/2022 05:54:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.75 on epoch=5
05/29/2022 05:54:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=5
05/29/2022 05:54:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=6
05/29/2022 05:54:35 - INFO - __main__ - Global step 200 Train loss 0.71 Classification-F1 0.46893109966863883 on epoch=6
05/29/2022 05:54:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.64 on epoch=6
05/29/2022 05:54:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=6
05/29/2022 05:54:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.64 on epoch=7
05/29/2022 05:54:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=7
05/29/2022 05:54:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=7
05/29/2022 05:54:54 - INFO - __main__ - Global step 250 Train loss 0.61 Classification-F1 0.47774214437533613 on epoch=7
05/29/2022 05:54:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=8
05/29/2022 05:54:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=8
05/29/2022 05:55:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=8
05/29/2022 05:55:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=9
05/29/2022 05:55:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=9
05/29/2022 05:55:13 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.6971787843531646 on epoch=9
05/29/2022 05:55:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5369855299874837 -> 0.6971787843531646 on epoch=9, global_step=300
05/29/2022 05:55:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=9
05/29/2022 05:55:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=9
05/29/2022 05:55:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=10
05/29/2022 05:55:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=10
05/29/2022 05:55:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=10
05/29/2022 05:55:32 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.7360151049667178 on epoch=10
05/29/2022 05:55:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6971787843531646 -> 0.7360151049667178 on epoch=10, global_step=350
05/29/2022 05:55:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=11
05/29/2022 05:55:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=11
05/29/2022 05:55:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=11
05/29/2022 05:55:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=12
05/29/2022 05:55:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=12
05/29/2022 05:55:52 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.6984614899950363 on epoch=12
05/29/2022 05:55:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=12
05/29/2022 05:55:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=13
05/29/2022 05:55:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=13
05/29/2022 05:56:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=13
05/29/2022 05:56:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=14
05/29/2022 05:56:11 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.8102221289607117 on epoch=14
05/29/2022 05:56:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7360151049667178 -> 0.8102221289607117 on epoch=14, global_step=450
05/29/2022 05:56:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=14
05/29/2022 05:56:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=14
05/29/2022 05:56:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=14
05/29/2022 05:56:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=15
05/29/2022 05:56:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=15
05/29/2022 05:56:31 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.7829770053216963 on epoch=15
05/29/2022 05:56:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=15
05/29/2022 05:56:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=16
05/29/2022 05:56:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=16
05/29/2022 05:56:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=16
05/29/2022 05:56:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=17
05/29/2022 05:56:50 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.7714514054420645 on epoch=17
05/29/2022 05:56:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=17
05/29/2022 05:56:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=17
05/29/2022 05:56:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=18
05/29/2022 05:57:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=18
05/29/2022 05:57:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=18
05/29/2022 05:57:09 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.7316908121667208 on epoch=18
05/29/2022 05:57:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=19
05/29/2022 05:57:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=19
05/29/2022 05:57:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=19
05/29/2022 05:57:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=19
05/29/2022 05:57:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=20
05/29/2022 05:57:28 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.8046166293823389 on epoch=20
05/29/2022 05:57:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=20
05/29/2022 05:57:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=20
05/29/2022 05:57:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=21
05/29/2022 05:57:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=21
05/29/2022 05:57:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=21
05/29/2022 05:57:48 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.8206385727878354 on epoch=21
05/29/2022 05:57:48 - INFO - __main__ - Saving model with best Classification-F1: 0.8102221289607117 -> 0.8206385727878354 on epoch=21, global_step=700
05/29/2022 05:57:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=22
05/29/2022 05:57:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=22
05/29/2022 05:57:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=22
05/29/2022 05:57:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=23
05/29/2022 05:58:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=23
05/29/2022 05:58:07 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.823902298814385 on epoch=23
05/29/2022 05:58:07 - INFO - __main__ - Saving model with best Classification-F1: 0.8206385727878354 -> 0.823902298814385 on epoch=23, global_step=750
05/29/2022 05:58:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=23
05/29/2022 05:58:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=24
05/29/2022 05:58:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=24
05/29/2022 05:58:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=24
05/29/2022 05:58:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=24
05/29/2022 05:58:26 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.8317208565531543 on epoch=24
05/29/2022 05:58:26 - INFO - __main__ - Saving model with best Classification-F1: 0.823902298814385 -> 0.8317208565531543 on epoch=24, global_step=800
05/29/2022 05:58:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=25
05/29/2022 05:58:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=25
05/29/2022 05:58:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=25
05/29/2022 05:58:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=26
05/29/2022 05:58:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=26
05/29/2022 05:58:46 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.8202764579689832 on epoch=26
05/29/2022 05:58:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=26
05/29/2022 05:58:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=27
05/29/2022 05:58:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=27
05/29/2022 05:58:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=27
05/29/2022 05:58:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=28
05/29/2022 05:59:05 - INFO - __main__ - Global step 900 Train loss 0.32 Classification-F1 0.8472069010035703 on epoch=28
05/29/2022 05:59:05 - INFO - __main__ - Saving model with best Classification-F1: 0.8317208565531543 -> 0.8472069010035703 on epoch=28, global_step=900
05/29/2022 05:59:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=28
05/29/2022 05:59:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=28
05/29/2022 05:59:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=29
05/29/2022 05:59:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=29
05/29/2022 05:59:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=29
05/29/2022 05:59:24 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.8204666746920268 on epoch=29
05/29/2022 05:59:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=29
05/29/2022 05:59:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=30
05/29/2022 05:59:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=30
05/29/2022 05:59:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=30
05/29/2022 05:59:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=31
05/29/2022 05:59:43 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.8408173883674932 on epoch=31
05/29/2022 05:59:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=31
05/29/2022 05:59:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
05/29/2022 05:59:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=32
05/29/2022 05:59:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=32
05/29/2022 05:59:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=32
05/29/2022 06:00:03 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.8248166807697168 on epoch=32
05/29/2022 06:00:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=33
05/29/2022 06:00:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=33
05/29/2022 06:00:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=33
05/29/2022 06:00:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=34
05/29/2022 06:00:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=34
05/29/2022 06:00:22 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.8445600047891129 on epoch=34
05/29/2022 06:00:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=34
05/29/2022 06:00:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=34
05/29/2022 06:00:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=35
05/29/2022 06:00:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
05/29/2022 06:00:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/29/2022 06:00:41 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.8313760480665432 on epoch=35
05/29/2022 06:00:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=36
05/29/2022 06:00:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=36
05/29/2022 06:00:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=36
05/29/2022 06:00:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=37
05/29/2022 06:00:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=37
05/29/2022 06:01:00 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.8404998404998405 on epoch=37
05/29/2022 06:01:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=37
05/29/2022 06:01:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=38
05/29/2022 06:01:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=38
05/29/2022 06:01:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=38
05/29/2022 06:01:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=39
05/29/2022 06:01:20 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.8493580453687226 on epoch=39
05/29/2022 06:01:20 - INFO - __main__ - Saving model with best Classification-F1: 0.8472069010035703 -> 0.8493580453687226 on epoch=39, global_step=1250
05/29/2022 06:01:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.30 on epoch=39
05/29/2022 06:01:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=39
05/29/2022 06:01:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=39
05/29/2022 06:01:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=40
05/29/2022 06:01:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=40
05/29/2022 06:01:39 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.8246362301121061 on epoch=40
05/29/2022 06:01:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=40
05/29/2022 06:01:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=41
05/29/2022 06:01:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=41
05/29/2022 06:01:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=41
05/29/2022 06:01:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=42
05/29/2022 06:01:59 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.8430720146450876 on epoch=42
05/29/2022 06:02:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=42
05/29/2022 06:02:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=42
05/29/2022 06:02:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=43
05/29/2022 06:02:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=43
05/29/2022 06:02:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=43
05/29/2022 06:02:18 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.8281370634831647 on epoch=43
05/29/2022 06:02:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=44
05/29/2022 06:02:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=44
05/29/2022 06:02:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=44
05/29/2022 06:02:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=44
05/29/2022 06:02:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=45
05/29/2022 06:02:38 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.8219590199572406 on epoch=45
05/29/2022 06:02:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=45
05/29/2022 06:02:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=45
05/29/2022 06:02:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=46
05/29/2022 06:02:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=46
05/29/2022 06:02:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=46
05/29/2022 06:02:57 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.8137254054676922 on epoch=46
05/29/2022 06:02:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=47
05/29/2022 06:03:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=47
05/29/2022 06:03:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=47
05/29/2022 06:03:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=48
05/29/2022 06:03:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=48
05/29/2022 06:03:16 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.8330787504645905 on epoch=48
05/29/2022 06:03:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=48
05/29/2022 06:03:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=49
05/29/2022 06:03:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=49
05/29/2022 06:03:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=49
05/29/2022 06:03:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=49
05/29/2022 06:03:36 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.843323433371396 on epoch=49
05/29/2022 06:03:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=50
05/29/2022 06:03:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=50
05/29/2022 06:03:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=50
05/29/2022 06:03:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=51
05/29/2022 06:03:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=51
05/29/2022 06:03:55 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.835488510459311 on epoch=51
05/29/2022 06:03:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=51
05/29/2022 06:04:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=52
05/29/2022 06:04:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=52
05/29/2022 06:04:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=52
05/29/2022 06:04:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=53
05/29/2022 06:04:14 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.8475039764862451 on epoch=53
05/29/2022 06:04:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=53
05/29/2022 06:04:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=53
05/29/2022 06:04:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=54
05/29/2022 06:04:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=54
05/29/2022 06:04:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=54
05/29/2022 06:04:34 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.8300119874338625 on epoch=54
05/29/2022 06:04:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=54
05/29/2022 06:04:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=55
05/29/2022 06:04:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=55
05/29/2022 06:04:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=55
05/29/2022 06:04:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=56
05/29/2022 06:04:53 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.8466028079004657 on epoch=56
05/29/2022 06:04:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=56
05/29/2022 06:04:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=56
05/29/2022 06:05:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=57
05/29/2022 06:05:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=57
05/29/2022 06:05:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=57
05/29/2022 06:05:12 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.8258854391915394 on epoch=57
05/29/2022 06:05:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=58
05/29/2022 06:05:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=58
05/29/2022 06:05:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=58
05/29/2022 06:05:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=59
05/29/2022 06:05:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=59
05/29/2022 06:05:31 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.8428124443240895 on epoch=59
05/29/2022 06:05:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=59
05/29/2022 06:05:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/29/2022 06:05:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=60
05/29/2022 06:05:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
05/29/2022 06:05:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=60
05/29/2022 06:05:51 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.8383835314026471 on epoch=60
05/29/2022 06:05:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=61
05/29/2022 06:05:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=61
05/29/2022 06:05:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=61
05/29/2022 06:06:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=62
05/29/2022 06:06:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=62
05/29/2022 06:06:10 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.8482882197127715 on epoch=62
05/29/2022 06:06:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=62
05/29/2022 06:06:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=63
05/29/2022 06:06:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=63
05/29/2022 06:06:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=63
05/29/2022 06:06:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=64
05/29/2022 06:06:29 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.8586845966674869 on epoch=64
05/29/2022 06:06:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8493580453687226 -> 0.8586845966674869 on epoch=64, global_step=2050
05/29/2022 06:06:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=64
05/29/2022 06:06:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=64
05/29/2022 06:06:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=64
05/29/2022 06:06:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=65
05/29/2022 06:06:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=65
05/29/2022 06:06:49 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.860797196962215 on epoch=65
05/29/2022 06:06:49 - INFO - __main__ - Saving model with best Classification-F1: 0.8586845966674869 -> 0.860797196962215 on epoch=65, global_step=2100
05/29/2022 06:06:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=65
05/29/2022 06:06:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=66
05/29/2022 06:06:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=66
05/29/2022 06:06:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=66
05/29/2022 06:07:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=67
05/29/2022 06:07:08 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.8554798687767141 on epoch=67
05/29/2022 06:07:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=67
05/29/2022 06:07:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.15 on epoch=67
05/29/2022 06:07:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=68
05/29/2022 06:07:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=68
05/29/2022 06:07:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=68
05/29/2022 06:07:28 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.850731659249073 on epoch=68
05/29/2022 06:07:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=69
05/29/2022 06:07:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=69
05/29/2022 06:07:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=69
05/29/2022 06:07:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=69
05/29/2022 06:07:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=70
05/29/2022 06:07:47 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.8472152927996031 on epoch=70
05/29/2022 06:07:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=70
05/29/2022 06:07:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 06:07:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=71
05/29/2022 06:07:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=71
05/29/2022 06:07:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=71
05/29/2022 06:08:06 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.8511815294756476 on epoch=71
05/29/2022 06:08:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=72
05/29/2022 06:08:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=72
05/29/2022 06:08:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=72
05/29/2022 06:08:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=73
05/29/2022 06:08:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=73
05/29/2022 06:08:25 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.866126436597751 on epoch=73
05/29/2022 06:08:25 - INFO - __main__ - Saving model with best Classification-F1: 0.860797196962215 -> 0.866126436597751 on epoch=73, global_step=2350
05/29/2022 06:08:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=73
05/29/2022 06:08:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=74
05/29/2022 06:08:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=74
05/29/2022 06:08:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=74
05/29/2022 06:08:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=74
05/29/2022 06:08:45 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.8479392198014499 on epoch=74
05/29/2022 06:08:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=75
05/29/2022 06:08:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=75
05/29/2022 06:08:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=75
05/29/2022 06:08:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=76
05/29/2022 06:08:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=76
05/29/2022 06:09:04 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.8675296624570006 on epoch=76
05/29/2022 06:09:04 - INFO - __main__ - Saving model with best Classification-F1: 0.866126436597751 -> 0.8675296624570006 on epoch=76, global_step=2450
05/29/2022 06:09:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=76
05/29/2022 06:09:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=77
05/29/2022 06:09:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=77
05/29/2022 06:09:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=77
05/29/2022 06:09:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
05/29/2022 06:09:23 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.857587435075303 on epoch=78
05/29/2022 06:09:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=78
05/29/2022 06:09:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=78
05/29/2022 06:09:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=79
05/29/2022 06:09:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=79
05/29/2022 06:09:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=79
05/29/2022 06:09:43 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.8739279884162385 on epoch=79
05/29/2022 06:09:43 - INFO - __main__ - Saving model with best Classification-F1: 0.8675296624570006 -> 0.8739279884162385 on epoch=79, global_step=2550
05/29/2022 06:09:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=79
05/29/2022 06:09:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=80
05/29/2022 06:09:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=80
05/29/2022 06:09:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=80
05/29/2022 06:09:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=81
05/29/2022 06:10:02 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.868485340438175 on epoch=81
05/29/2022 06:10:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=81
05/29/2022 06:10:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=81
05/29/2022 06:10:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=82
05/29/2022 06:10:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=82
05/29/2022 06:10:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=82
05/29/2022 06:10:21 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.8459550457585949 on epoch=82
05/29/2022 06:10:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=83
05/29/2022 06:10:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=83
05/29/2022 06:10:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=83
05/29/2022 06:10:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=84
05/29/2022 06:10:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=84
05/29/2022 06:10:41 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8537869696521381 on epoch=84
05/29/2022 06:10:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=84
05/29/2022 06:10:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=84
05/29/2022 06:10:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=85
05/29/2022 06:10:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=85
05/29/2022 06:10:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=85
05/29/2022 06:11:01 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.8528158134136394 on epoch=85
05/29/2022 06:11:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=86
05/29/2022 06:11:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=86
05/29/2022 06:11:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=86
05/29/2022 06:11:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/29/2022 06:11:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=87
05/29/2022 06:11:20 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.8643789431523174 on epoch=87
05/29/2022 06:11:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=87
05/29/2022 06:11:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=88
05/29/2022 06:11:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=88
05/29/2022 06:11:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=88
05/29/2022 06:11:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=89
05/29/2022 06:11:39 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.8623378591321625 on epoch=89
05/29/2022 06:11:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=89
05/29/2022 06:11:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=89
05/29/2022 06:11:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=89
05/29/2022 06:11:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=90
05/29/2022 06:11:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=90
05/29/2022 06:11:59 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.8714174814170652 on epoch=90
05/29/2022 06:12:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=90
05/29/2022 06:12:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=91
05/29/2022 06:12:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=91
05/29/2022 06:12:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=91
05/29/2022 06:12:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=92
05/29/2022 06:12:18 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.8561981941862329 on epoch=92
05/29/2022 06:12:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=92
05/29/2022 06:12:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=92
05/29/2022 06:12:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=93
05/29/2022 06:12:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=93
05/29/2022 06:12:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=93
05/29/2022 06:12:32 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:12:32 - INFO - __main__ - Printing 3 examples
05/29/2022 06:12:32 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:12:32 - INFO - __main__ - ['happy']
05/29/2022 06:12:32 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:12:32 - INFO - __main__ - ['happy']
05/29/2022 06:12:32 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:12:32 - INFO - __main__ - ['happy']
05/29/2022 06:12:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:12:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:12:33 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:12:33 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:12:33 - INFO - __main__ - Printing 3 examples
05/29/2022 06:12:33 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:12:33 - INFO - __main__ - ['happy']
05/29/2022 06:12:33 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:12:33 - INFO - __main__ - ['happy']
05/29/2022 06:12:33 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:12:33 - INFO - __main__ - ['happy']
05/29/2022 06:12:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:12:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:12:33 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:12:38 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.8487430682166766 on epoch=93
05/29/2022 06:12:38 - INFO - __main__ - save last model!
05/29/2022 06:12:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 06:12:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 06:12:38 - INFO - __main__ - Printing 3 examples
05/29/2022 06:12:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 06:12:38 - INFO - __main__ - ['others']
05/29/2022 06:12:38 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 06:12:38 - INFO - __main__ - ['others']
05/29/2022 06:12:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 06:12:38 - INFO - __main__ - ['others']
05/29/2022 06:12:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:12:40 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:12:45 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 06:12:49 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:12:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:12:50 - INFO - __main__ - Starting training!
05/29/2022 06:13:59 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_42_0.5_8_predictions.txt
05/29/2022 06:13:59 - INFO - __main__ - Classification-F1 on test data: 0.4933
05/29/2022 06:13:59 - INFO - __main__ - prefix=emo_128_42, lr=0.5, bsz=8, dev_performance=0.8739279884162385, test_performance=0.49328116544509915
05/29/2022 06:13:59 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.4, bsz=8 ...
05/29/2022 06:14:00 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:14:00 - INFO - __main__ - Printing 3 examples
05/29/2022 06:14:00 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:14:00 - INFO - __main__ - ['happy']
05/29/2022 06:14:00 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:14:00 - INFO - __main__ - ['happy']
05/29/2022 06:14:00 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:14:00 - INFO - __main__ - ['happy']
05/29/2022 06:14:00 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:14:01 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:14:01 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:14:01 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:14:01 - INFO - __main__ - Printing 3 examples
05/29/2022 06:14:01 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:14:01 - INFO - __main__ - ['happy']
05/29/2022 06:14:01 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:14:01 - INFO - __main__ - ['happy']
05/29/2022 06:14:01 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:14:01 - INFO - __main__ - ['happy']
05/29/2022 06:14:01 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:14:01 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:14:02 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:14:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:14:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:14:21 - INFO - __main__ - Starting training!
05/29/2022 06:14:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.27 on epoch=0
05/29/2022 06:14:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.79 on epoch=0
05/29/2022 06:14:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.23 on epoch=0
05/29/2022 06:14:32 - INFO - __main__ - Step 40 Global step 40 Train loss 2.32 on epoch=1
05/29/2022 06:14:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.51 on epoch=1
05/29/2022 06:14:42 - INFO - __main__ - Global step 50 Train loss 2.62 Classification-F1 0.12991088261440353 on epoch=1
05/29/2022 06:14:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12991088261440353 on epoch=1, global_step=50
05/29/2022 06:14:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.43 on epoch=1
05/29/2022 06:14:47 - INFO - __main__ - Step 70 Global step 70 Train loss 1.23 on epoch=2
05/29/2022 06:14:49 - INFO - __main__ - Step 80 Global step 80 Train loss 1.15 on epoch=2
05/29/2022 06:14:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=2
05/29/2022 06:14:54 - INFO - __main__ - Step 100 Global step 100 Train loss 1.03 on epoch=3
05/29/2022 06:15:01 - INFO - __main__ - Global step 100 Train loss 1.18 Classification-F1 0.5884022223839446 on epoch=3
05/29/2022 06:15:01 - INFO - __main__ - Saving model with best Classification-F1: 0.12991088261440353 -> 0.5884022223839446 on epoch=3, global_step=100
05/29/2022 06:15:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=3
05/29/2022 06:15:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=3
05/29/2022 06:15:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=4
05/29/2022 06:15:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=4
05/29/2022 06:15:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=4
05/29/2022 06:15:21 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.5454937583795979 on epoch=4
05/29/2022 06:15:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=4
05/29/2022 06:15:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=5
05/29/2022 06:15:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=5
05/29/2022 06:15:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=5
05/29/2022 06:15:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
05/29/2022 06:15:40 - INFO - __main__ - Global step 200 Train loss 0.72 Classification-F1 0.5787513822495258 on epoch=6
05/29/2022 06:15:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.75 on epoch=6
05/29/2022 06:15:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=6
05/29/2022 06:15:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=7
05/29/2022 06:15:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=7
05/29/2022 06:15:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=7
05/29/2022 06:16:00 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.6263838981196396 on epoch=7
05/29/2022 06:16:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5884022223839446 -> 0.6263838981196396 on epoch=7, global_step=250
05/29/2022 06:16:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.69 on epoch=8
05/29/2022 06:16:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=8
05/29/2022 06:16:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=8
05/29/2022 06:16:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=9
05/29/2022 06:16:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=9
05/29/2022 06:16:19 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.6694240196078431 on epoch=9
05/29/2022 06:16:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6263838981196396 -> 0.6694240196078431 on epoch=9, global_step=300
05/29/2022 06:16:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=9
05/29/2022 06:16:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=9
05/29/2022 06:16:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=10
05/29/2022 06:16:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=10
05/29/2022 06:16:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.63 on epoch=10
05/29/2022 06:16:39 - INFO - __main__ - Global step 350 Train loss 0.57 Classification-F1 0.7211753474990585 on epoch=10
05/29/2022 06:16:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6694240196078431 -> 0.7211753474990585 on epoch=10, global_step=350
05/29/2022 06:16:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=11
05/29/2022 06:16:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=11
05/29/2022 06:16:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=11
05/29/2022 06:16:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=12
05/29/2022 06:16:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=12
05/29/2022 06:16:58 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.7308635249700723 on epoch=12
05/29/2022 06:16:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7211753474990585 -> 0.7308635249700723 on epoch=12, global_step=400
05/29/2022 06:17:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.58 on epoch=12
05/29/2022 06:17:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=13
05/29/2022 06:17:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=13
05/29/2022 06:17:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=13
05/29/2022 06:17:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.60 on epoch=14
05/29/2022 06:17:18 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.7921690979565529 on epoch=14
05/29/2022 06:17:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7308635249700723 -> 0.7921690979565529 on epoch=14, global_step=450
05/29/2022 06:17:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=14
05/29/2022 06:17:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=14
05/29/2022 06:17:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=14
05/29/2022 06:17:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=15
05/29/2022 06:17:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=15
05/29/2022 06:17:38 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.8069187084056972 on epoch=15
05/29/2022 06:17:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7921690979565529 -> 0.8069187084056972 on epoch=15, global_step=500
05/29/2022 06:17:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=15
05/29/2022 06:17:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=16
05/29/2022 06:17:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=16
05/29/2022 06:17:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=16
05/29/2022 06:17:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=17
05/29/2022 06:17:58 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.7815331003942937 on epoch=17
05/29/2022 06:18:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=17
05/29/2022 06:18:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=17
05/29/2022 06:18:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=18
05/29/2022 06:18:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=18
05/29/2022 06:18:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=18
05/29/2022 06:18:17 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.7782284177597278 on epoch=18
05/29/2022 06:18:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=19
05/29/2022 06:18:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=19
05/29/2022 06:18:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=19
05/29/2022 06:18:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=19
05/29/2022 06:18:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=20
05/29/2022 06:18:37 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.8248450546772572 on epoch=20
05/29/2022 06:18:37 - INFO - __main__ - Saving model with best Classification-F1: 0.8069187084056972 -> 0.8248450546772572 on epoch=20, global_step=650
05/29/2022 06:18:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=20
05/29/2022 06:18:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=20
05/29/2022 06:18:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=21
05/29/2022 06:18:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=21
05/29/2022 06:18:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=21
05/29/2022 06:18:57 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.7908610090738072 on epoch=21
05/29/2022 06:19:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=22
05/29/2022 06:19:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=22
05/29/2022 06:19:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=22
05/29/2022 06:19:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=23
05/29/2022 06:19:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=23
05/29/2022 06:19:17 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.829487940204452 on epoch=23
05/29/2022 06:19:17 - INFO - __main__ - Saving model with best Classification-F1: 0.8248450546772572 -> 0.829487940204452 on epoch=23, global_step=750
05/29/2022 06:19:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=23
05/29/2022 06:19:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=24
05/29/2022 06:19:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=24
05/29/2022 06:19:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=24
05/29/2022 06:19:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=24
05/29/2022 06:19:37 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.8079571421427472 on epoch=24
05/29/2022 06:19:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=25
05/29/2022 06:19:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=25
05/29/2022 06:19:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=25
05/29/2022 06:19:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=26
05/29/2022 06:19:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=26
05/29/2022 06:19:57 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.7997939343371133 on epoch=26
05/29/2022 06:19:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=26
05/29/2022 06:20:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=27
05/29/2022 06:20:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=27
05/29/2022 06:20:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=27
05/29/2022 06:20:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=28
05/29/2022 06:20:17 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.8320020597426179 on epoch=28
05/29/2022 06:20:17 - INFO - __main__ - Saving model with best Classification-F1: 0.829487940204452 -> 0.8320020597426179 on epoch=28, global_step=900
05/29/2022 06:20:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=28
05/29/2022 06:20:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=28
05/29/2022 06:20:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=29
05/29/2022 06:20:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=29
05/29/2022 06:20:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=29
05/29/2022 06:20:36 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.8145194941648027 on epoch=29
05/29/2022 06:20:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=29
05/29/2022 06:20:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=30
05/29/2022 06:20:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=30
05/29/2022 06:20:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=30
05/29/2022 06:20:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=31
05/29/2022 06:20:56 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.8473920665512001 on epoch=31
05/29/2022 06:20:56 - INFO - __main__ - Saving model with best Classification-F1: 0.8320020597426179 -> 0.8473920665512001 on epoch=31, global_step=1000
05/29/2022 06:20:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=31
05/29/2022 06:21:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=31
05/29/2022 06:21:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=32
05/29/2022 06:21:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=32
05/29/2022 06:21:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=32
05/29/2022 06:21:16 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.7745331357123163 on epoch=32
05/29/2022 06:21:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=33
05/29/2022 06:21:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=33
05/29/2022 06:21:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=33
05/29/2022 06:21:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=34
05/29/2022 06:21:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=34
05/29/2022 06:21:36 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.830732617027101 on epoch=34
05/29/2022 06:21:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=34
05/29/2022 06:21:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=34
05/29/2022 06:21:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=35
05/29/2022 06:21:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=35
05/29/2022 06:21:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/29/2022 06:21:56 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.8095870299050513 on epoch=35
05/29/2022 06:21:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=36
05/29/2022 06:22:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=36
05/29/2022 06:22:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=36
05/29/2022 06:22:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=37
05/29/2022 06:22:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=37
05/29/2022 06:22:16 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.8198345059088848 on epoch=37
05/29/2022 06:22:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=37
05/29/2022 06:22:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=38
05/29/2022 06:22:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=38
05/29/2022 06:22:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
05/29/2022 06:22:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=39
05/29/2022 06:22:35 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.8378272058368994 on epoch=39
05/29/2022 06:22:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=39
05/29/2022 06:22:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=39
05/29/2022 06:22:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=39
05/29/2022 06:22:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=40
05/29/2022 06:22:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=40
05/29/2022 06:22:55 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.8338223020473128 on epoch=40
05/29/2022 06:22:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=40
05/29/2022 06:23:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=41
05/29/2022 06:23:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=41
05/29/2022 06:23:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=41
05/29/2022 06:23:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=42
05/29/2022 06:23:15 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.843676363973409 on epoch=42
05/29/2022 06:23:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=42
05/29/2022 06:23:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=42
05/29/2022 06:23:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=43
05/29/2022 06:23:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=43
05/29/2022 06:23:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=43
05/29/2022 06:23:35 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.8185459168066556 on epoch=43
05/29/2022 06:23:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=44
05/29/2022 06:23:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=44
05/29/2022 06:23:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=44
05/29/2022 06:23:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=44
05/29/2022 06:23:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=45
05/29/2022 06:23:55 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.8358952603235442 on epoch=45
05/29/2022 06:23:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=45
05/29/2022 06:24:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=45
05/29/2022 06:24:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=46
05/29/2022 06:24:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=46
05/29/2022 06:24:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=46
05/29/2022 06:24:15 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.8126759881758578 on epoch=46
05/29/2022 06:24:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=47
05/29/2022 06:24:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=47
05/29/2022 06:24:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=47
05/29/2022 06:24:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=48
05/29/2022 06:24:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=48
05/29/2022 06:24:35 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.8246419019759079 on epoch=48
05/29/2022 06:24:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=48
05/29/2022 06:24:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=49
05/29/2022 06:24:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=49
05/29/2022 06:24:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=49
05/29/2022 06:24:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=49
05/29/2022 06:24:54 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.8188486668287471 on epoch=49
05/29/2022 06:24:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=50
05/29/2022 06:25:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=50
05/29/2022 06:25:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=50
05/29/2022 06:25:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=51
05/29/2022 06:25:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=51
05/29/2022 06:25:14 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.8322128364182161 on epoch=51
05/29/2022 06:25:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=51
05/29/2022 06:25:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=52
05/29/2022 06:25:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=52
05/29/2022 06:25:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=52
05/29/2022 06:25:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=53
05/29/2022 06:25:34 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.8441800540611983 on epoch=53
05/29/2022 06:25:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=53
05/29/2022 06:25:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=53
05/29/2022 06:25:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=54
05/29/2022 06:25:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=54
05/29/2022 06:25:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=54
05/29/2022 06:25:54 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.8103141648649098 on epoch=54
05/29/2022 06:25:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/29/2022 06:25:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=55
05/29/2022 06:26:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=55
05/29/2022 06:26:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=55
05/29/2022 06:26:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=56
05/29/2022 06:26:14 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.8489253649829498 on epoch=56
05/29/2022 06:26:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8473920665512001 -> 0.8489253649829498 on epoch=56, global_step=1800
05/29/2022 06:26:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=56
05/29/2022 06:26:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=56
05/29/2022 06:26:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=57
05/29/2022 06:26:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=57
05/29/2022 06:26:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=57
05/29/2022 06:26:34 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.8001055674446848 on epoch=57
05/29/2022 06:26:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=58
05/29/2022 06:26:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=58
05/29/2022 06:26:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=58
05/29/2022 06:26:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=59
05/29/2022 06:26:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=59
05/29/2022 06:26:54 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.8438273546533316 on epoch=59
05/29/2022 06:26:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=59
05/29/2022 06:26:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=59
05/29/2022 06:27:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=60
05/29/2022 06:27:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=60
05/29/2022 06:27:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=60
05/29/2022 06:27:14 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.8477687905251688 on epoch=60
05/29/2022 06:27:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=61
05/29/2022 06:27:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=61
05/29/2022 06:27:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=61
05/29/2022 06:27:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=62
05/29/2022 06:27:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=62
05/29/2022 06:27:34 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.8460628206740748 on epoch=62
05/29/2022 06:27:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=62
05/29/2022 06:27:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=63
05/29/2022 06:27:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=63
05/29/2022 06:27:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=63
05/29/2022 06:27:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.28 on epoch=64
05/29/2022 06:27:54 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.8450946077528543 on epoch=64
05/29/2022 06:27:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=64
05/29/2022 06:27:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=64
05/29/2022 06:28:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=64
05/29/2022 06:28:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=65
05/29/2022 06:28:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=65
05/29/2022 06:28:14 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.8544074313338701 on epoch=65
05/29/2022 06:28:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8489253649829498 -> 0.8544074313338701 on epoch=65, global_step=2100
05/29/2022 06:28:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=65
05/29/2022 06:28:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=66
05/29/2022 06:28:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=66
05/29/2022 06:28:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=66
05/29/2022 06:28:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=67
05/29/2022 06:28:34 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.8609242589984294 on epoch=67
05/29/2022 06:28:34 - INFO - __main__ - Saving model with best Classification-F1: 0.8544074313338701 -> 0.8609242589984294 on epoch=67, global_step=2150
05/29/2022 06:28:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=67
05/29/2022 06:28:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=67
05/29/2022 06:28:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=68
05/29/2022 06:28:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=68
05/29/2022 06:28:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=68
05/29/2022 06:28:54 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.8318036980140188 on epoch=68
05/29/2022 06:28:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=69
05/29/2022 06:28:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=69
05/29/2022 06:29:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=69
05/29/2022 06:29:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=69
05/29/2022 06:29:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=70
05/29/2022 06:29:14 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.8529098624793223 on epoch=70
05/29/2022 06:29:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=70
05/29/2022 06:29:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=70
05/29/2022 06:29:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=71
05/29/2022 06:29:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=71
05/29/2022 06:29:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=71
05/29/2022 06:29:34 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.825710229856208 on epoch=71
05/29/2022 06:29:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=72
05/29/2022 06:29:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=72
05/29/2022 06:29:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=72
05/29/2022 06:29:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=73
05/29/2022 06:29:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=73
05/29/2022 06:29:54 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.8376186055832073 on epoch=73
05/29/2022 06:29:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=73
05/29/2022 06:29:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.25 on epoch=74
05/29/2022 06:30:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=74
05/29/2022 06:30:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=74
05/29/2022 06:30:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=74
05/29/2022 06:30:14 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.8512961749163921 on epoch=74
05/29/2022 06:30:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=75
05/29/2022 06:30:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=75
05/29/2022 06:30:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.25 on epoch=75
05/29/2022 06:30:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=76
05/29/2022 06:30:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=76
05/29/2022 06:30:34 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.8317013668642215 on epoch=76
05/29/2022 06:30:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=76
05/29/2022 06:30:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=77
05/29/2022 06:30:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=77
05/29/2022 06:30:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=77
05/29/2022 06:30:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=78
05/29/2022 06:30:54 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.8599494815413742 on epoch=78
05/29/2022 06:30:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=78
05/29/2022 06:30:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=78
05/29/2022 06:31:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.25 on epoch=79
05/29/2022 06:31:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=79
05/29/2022 06:31:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=79
05/29/2022 06:31:14 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.8315379815379815 on epoch=79
05/29/2022 06:31:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=79
05/29/2022 06:31:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=80
05/29/2022 06:31:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=80
05/29/2022 06:31:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=80
05/29/2022 06:31:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/29/2022 06:31:34 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.8518209049258085 on epoch=81
05/29/2022 06:31:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
05/29/2022 06:31:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=81
05/29/2022 06:31:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=82
05/29/2022 06:31:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=82
05/29/2022 06:31:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=82
05/29/2022 06:31:54 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.827807861075148 on epoch=82
05/29/2022 06:31:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=83
05/29/2022 06:31:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/29/2022 06:32:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=83
05/29/2022 06:32:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=84
05/29/2022 06:32:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=84
05/29/2022 06:32:14 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.8440681310548226 on epoch=84
05/29/2022 06:32:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=84
05/29/2022 06:32:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=84
05/29/2022 06:32:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=85
05/29/2022 06:32:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=85
05/29/2022 06:32:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=85
05/29/2022 06:32:34 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.8464222923834833 on epoch=85
05/29/2022 06:32:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=86
05/29/2022 06:32:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=86
05/29/2022 06:32:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=86
05/29/2022 06:32:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=87
05/29/2022 06:32:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=87
05/29/2022 06:32:54 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.8304489250875506 on epoch=87
05/29/2022 06:32:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=87
05/29/2022 06:32:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=88
05/29/2022 06:33:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
05/29/2022 06:33:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=88
05/29/2022 06:33:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=89
05/29/2022 06:33:14 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.8415170108938224 on epoch=89
05/29/2022 06:33:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=89
05/29/2022 06:33:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=89
05/29/2022 06:33:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=89
05/29/2022 06:33:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=90
05/29/2022 06:33:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=90
05/29/2022 06:33:34 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.8446038503788611 on epoch=90
05/29/2022 06:33:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.28 on epoch=90
05/29/2022 06:33:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
05/29/2022 06:33:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=91
05/29/2022 06:33:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=91
05/29/2022 06:33:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=92
05/29/2022 06:33:54 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.8571034540136888 on epoch=92
05/29/2022 06:33:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=92
05/29/2022 06:33:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=92
05/29/2022 06:34:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=93
05/29/2022 06:34:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
05/29/2022 06:34:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=93
05/29/2022 06:34:08 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:34:08 - INFO - __main__ - Printing 3 examples
05/29/2022 06:34:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:34:08 - INFO - __main__ - ['happy']
05/29/2022 06:34:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:34:08 - INFO - __main__ - ['happy']
05/29/2022 06:34:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:34:08 - INFO - __main__ - ['happy']
05/29/2022 06:34:08 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:34:08 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:34:09 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:34:09 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:34:09 - INFO - __main__ - Printing 3 examples
05/29/2022 06:34:09 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:34:09 - INFO - __main__ - ['happy']
05/29/2022 06:34:09 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:34:09 - INFO - __main__ - ['happy']
05/29/2022 06:34:09 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:34:09 - INFO - __main__ - ['happy']
05/29/2022 06:34:09 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:34:09 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:34:10 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:34:14 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.824812030075188 on epoch=93
05/29/2022 06:34:14 - INFO - __main__ - save last model!
05/29/2022 06:34:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 06:34:14 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 06:34:14 - INFO - __main__ - Printing 3 examples
05/29/2022 06:34:14 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 06:34:14 - INFO - __main__ - ['others']
05/29/2022 06:34:14 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 06:34:14 - INFO - __main__ - ['others']
05/29/2022 06:34:14 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 06:34:14 - INFO - __main__ - ['others']
05/29/2022 06:34:14 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:34:16 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:34:21 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 06:34:27 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:34:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:34:28 - INFO - __main__ - Starting training!
05/29/2022 06:35:36 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_42_0.4_8_predictions.txt
05/29/2022 06:35:36 - INFO - __main__ - Classification-F1 on test data: 0.3655
05/29/2022 06:35:36 - INFO - __main__ - prefix=emo_128_42, lr=0.4, bsz=8, dev_performance=0.8609242589984294, test_performance=0.36549119475603287
05/29/2022 06:35:36 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.3, bsz=8 ...
05/29/2022 06:35:37 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:35:37 - INFO - __main__ - Printing 3 examples
05/29/2022 06:35:37 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:35:37 - INFO - __main__ - ['happy']
05/29/2022 06:35:37 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:35:37 - INFO - __main__ - ['happy']
05/29/2022 06:35:37 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:35:37 - INFO - __main__ - ['happy']
05/29/2022 06:35:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:35:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:35:38 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:35:38 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:35:38 - INFO - __main__ - Printing 3 examples
05/29/2022 06:35:38 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:35:38 - INFO - __main__ - ['happy']
05/29/2022 06:35:38 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:35:38 - INFO - __main__ - ['happy']
05/29/2022 06:35:38 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:35:38 - INFO - __main__ - ['happy']
05/29/2022 06:35:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:35:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:35:39 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:35:54 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:35:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:35:55 - INFO - __main__ - Starting training!
05/29/2022 06:35:58 - INFO - __main__ - Step 10 Global step 10 Train loss 4.46 on epoch=0
05/29/2022 06:36:01 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=0
05/29/2022 06:36:03 - INFO - __main__ - Step 30 Global step 30 Train loss 2.75 on epoch=0
05/29/2022 06:36:05 - INFO - __main__ - Step 40 Global step 40 Train loss 2.47 on epoch=1
05/29/2022 06:36:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.89 on epoch=1
05/29/2022 06:36:15 - INFO - __main__ - Global step 50 Train loss 2.93 Classification-F1 0.035493262885086516 on epoch=1
05/29/2022 06:36:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.035493262885086516 on epoch=1, global_step=50
05/29/2022 06:36:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.87 on epoch=1
05/29/2022 06:36:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.40 on epoch=2
05/29/2022 06:36:22 - INFO - __main__ - Step 80 Global step 80 Train loss 1.31 on epoch=2
05/29/2022 06:36:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.09 on epoch=2
05/29/2022 06:36:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=3
05/29/2022 06:36:34 - INFO - __main__ - Global step 100 Train loss 1.35 Classification-F1 0.41915350580206534 on epoch=3
05/29/2022 06:36:34 - INFO - __main__ - Saving model with best Classification-F1: 0.035493262885086516 -> 0.41915350580206534 on epoch=3, global_step=100
05/29/2022 06:36:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=3
05/29/2022 06:36:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.82 on epoch=3
05/29/2022 06:36:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=4
05/29/2022 06:36:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=4
05/29/2022 06:36:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.74 on epoch=4
05/29/2022 06:36:53 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.5082267219904378 on epoch=4
05/29/2022 06:36:53 - INFO - __main__ - Saving model with best Classification-F1: 0.41915350580206534 -> 0.5082267219904378 on epoch=4, global_step=150
05/29/2022 06:36:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=4
05/29/2022 06:36:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=5
05/29/2022 06:37:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=5
05/29/2022 06:37:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.77 on epoch=5
05/29/2022 06:37:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=6
05/29/2022 06:37:13 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.5443612720668881 on epoch=6
05/29/2022 06:37:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5082267219904378 -> 0.5443612720668881 on epoch=6, global_step=200
05/29/2022 06:37:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=6
05/29/2022 06:37:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=6
05/29/2022 06:37:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=7
05/29/2022 06:37:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=7
05/29/2022 06:37:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=7
05/29/2022 06:37:32 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.5421800840254287 on epoch=7
05/29/2022 06:37:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.71 on epoch=8
05/29/2022 06:37:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=8
05/29/2022 06:37:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=8
05/29/2022 06:37:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=9
05/29/2022 06:37:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=9
05/29/2022 06:37:51 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.5981720537923217 on epoch=9
05/29/2022 06:37:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5443612720668881 -> 0.5981720537923217 on epoch=9, global_step=300
05/29/2022 06:37:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.59 on epoch=9
05/29/2022 06:37:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=9
05/29/2022 06:37:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=10
05/29/2022 06:38:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=10
05/29/2022 06:38:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=10
05/29/2022 06:38:10 - INFO - __main__ - Global step 350 Train loss 0.59 Classification-F1 0.6401558784538147 on epoch=10
05/29/2022 06:38:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5981720537923217 -> 0.6401558784538147 on epoch=10, global_step=350
05/29/2022 06:38:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=11
05/29/2022 06:38:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=11
05/29/2022 06:38:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=11
05/29/2022 06:38:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=12
05/29/2022 06:38:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=12
05/29/2022 06:38:29 - INFO - __main__ - Global step 400 Train loss 0.60 Classification-F1 0.6475850936250733 on epoch=12
05/29/2022 06:38:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6401558784538147 -> 0.6475850936250733 on epoch=12, global_step=400
05/29/2022 06:38:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=12
05/29/2022 06:38:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=13
05/29/2022 06:38:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=13
05/29/2022 06:38:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=13
05/29/2022 06:38:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=14
05/29/2022 06:38:49 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.7243083615525885 on epoch=14
05/29/2022 06:38:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6475850936250733 -> 0.7243083615525885 on epoch=14, global_step=450
05/29/2022 06:38:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=14
05/29/2022 06:38:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=14
05/29/2022 06:38:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.57 on epoch=14
05/29/2022 06:38:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=15
05/29/2022 06:39:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=15
05/29/2022 06:39:08 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.7388242749356223 on epoch=15
05/29/2022 06:39:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7243083615525885 -> 0.7388242749356223 on epoch=15, global_step=500
05/29/2022 06:39:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=15
05/29/2022 06:39:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=16
05/29/2022 06:39:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=16
05/29/2022 06:39:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=16
05/29/2022 06:39:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=17
05/29/2022 06:39:27 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.7713516415153095 on epoch=17
05/29/2022 06:39:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7388242749356223 -> 0.7713516415153095 on epoch=17, global_step=550
05/29/2022 06:39:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=17
05/29/2022 06:39:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=17
05/29/2022 06:39:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=18
05/29/2022 06:39:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=18
05/29/2022 06:39:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=18
05/29/2022 06:39:46 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.7644869358227693 on epoch=18
05/29/2022 06:39:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=19
05/29/2022 06:39:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=19
05/29/2022 06:39:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=19
05/29/2022 06:39:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=19
05/29/2022 06:39:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=20
05/29/2022 06:40:06 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.7893362402612554 on epoch=20
05/29/2022 06:40:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7713516415153095 -> 0.7893362402612554 on epoch=20, global_step=650
05/29/2022 06:40:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=20
05/29/2022 06:40:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=20
05/29/2022 06:40:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=21
05/29/2022 06:40:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=21
05/29/2022 06:40:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=21
05/29/2022 06:40:25 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.7710471984923506 on epoch=21
05/29/2022 06:40:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=22
05/29/2022 06:40:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=22
05/29/2022 06:40:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=22
05/29/2022 06:40:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=23
05/29/2022 06:40:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=23
05/29/2022 06:40:44 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.7962375809899339 on epoch=23
05/29/2022 06:40:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7893362402612554 -> 0.7962375809899339 on epoch=23, global_step=750
05/29/2022 06:40:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=23
05/29/2022 06:40:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=24
05/29/2022 06:40:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=24
05/29/2022 06:40:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=24
05/29/2022 06:40:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=24
05/29/2022 06:41:04 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.7991789308048973 on epoch=24
05/29/2022 06:41:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7962375809899339 -> 0.7991789308048973 on epoch=24, global_step=800
05/29/2022 06:41:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=25
05/29/2022 06:41:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=25
05/29/2022 06:41:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=25
05/29/2022 06:41:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=26
05/29/2022 06:41:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=26
05/29/2022 06:41:23 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.7964766769560385 on epoch=26
05/29/2022 06:41:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=26
05/29/2022 06:41:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=27
05/29/2022 06:41:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=27
05/29/2022 06:41:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=27
05/29/2022 06:41:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=28
05/29/2022 06:41:42 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.8324928855742821 on epoch=28
05/29/2022 06:41:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7991789308048973 -> 0.8324928855742821 on epoch=28, global_step=900
05/29/2022 06:41:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=28
05/29/2022 06:41:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=28
05/29/2022 06:41:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=29
05/29/2022 06:41:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=29
05/29/2022 06:41:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=29
05/29/2022 06:42:01 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.7890145132977816 on epoch=29
05/29/2022 06:42:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=29
05/29/2022 06:42:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=30
05/29/2022 06:42:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=30
05/29/2022 06:42:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=30
05/29/2022 06:42:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=31
05/29/2022 06:42:21 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.8319967978695302 on epoch=31
05/29/2022 06:42:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=31
05/29/2022 06:42:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=31
05/29/2022 06:42:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=32
05/29/2022 06:42:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=32
05/29/2022 06:42:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=32
05/29/2022 06:42:40 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.7935865399111004 on epoch=32
05/29/2022 06:42:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=33
05/29/2022 06:42:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=33
05/29/2022 06:42:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=33
05/29/2022 06:42:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=34
05/29/2022 06:42:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=34
05/29/2022 06:42:59 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.8347521556577708 on epoch=34
05/29/2022 06:42:59 - INFO - __main__ - Saving model with best Classification-F1: 0.8324928855742821 -> 0.8347521556577708 on epoch=34, global_step=1100
05/29/2022 06:43:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/29/2022 06:43:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=34
05/29/2022 06:43:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=35
05/29/2022 06:43:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=35
05/29/2022 06:43:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=35
05/29/2022 06:43:18 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.8281998452296997 on epoch=35
05/29/2022 06:43:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=36
05/29/2022 06:43:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=36
05/29/2022 06:43:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=36
05/29/2022 06:43:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=37
05/29/2022 06:43:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=37
05/29/2022 06:43:38 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.8372288455594009 on epoch=37
05/29/2022 06:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.8347521556577708 -> 0.8372288455594009 on epoch=37, global_step=1200
05/29/2022 06:43:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=37
05/29/2022 06:43:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=38
05/29/2022 06:43:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=38
05/29/2022 06:43:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=38
05/29/2022 06:43:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=39
05/29/2022 06:43:57 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.8445825719310412 on epoch=39
05/29/2022 06:43:57 - INFO - __main__ - Saving model with best Classification-F1: 0.8372288455594009 -> 0.8445825719310412 on epoch=39, global_step=1250
05/29/2022 06:44:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=39
05/29/2022 06:44:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=39
05/29/2022 06:44:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=39
05/29/2022 06:44:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=40
05/29/2022 06:44:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=40
05/29/2022 06:44:16 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.843235888432808 on epoch=40
05/29/2022 06:44:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=40
05/29/2022 06:44:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=41
05/29/2022 06:44:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=41
05/29/2022 06:44:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=41
05/29/2022 06:44:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=42
05/29/2022 06:44:36 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.8460285530491147 on epoch=42
05/29/2022 06:44:36 - INFO - __main__ - Saving model with best Classification-F1: 0.8445825719310412 -> 0.8460285530491147 on epoch=42, global_step=1350
05/29/2022 06:44:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=42
05/29/2022 06:44:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=42
05/29/2022 06:44:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=43
05/29/2022 06:44:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=43
05/29/2022 06:44:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=43
05/29/2022 06:44:55 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.8341786660935597 on epoch=43
05/29/2022 06:44:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=44
05/29/2022 06:45:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=44
05/29/2022 06:45:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=44
05/29/2022 06:45:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=44
05/29/2022 06:45:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=45
05/29/2022 06:45:14 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.85425960455393 on epoch=45
05/29/2022 06:45:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8460285530491147 -> 0.85425960455393 on epoch=45, global_step=1450
05/29/2022 06:45:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=45
05/29/2022 06:45:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=45
05/29/2022 06:45:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=46
05/29/2022 06:45:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=46
05/29/2022 06:45:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=46
05/29/2022 06:45:34 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.836446856638613 on epoch=46
05/29/2022 06:45:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=47
05/29/2022 06:45:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/29/2022 06:45:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=47
05/29/2022 06:45:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=48
05/29/2022 06:45:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=48
05/29/2022 06:45:53 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.8425132456684585 on epoch=48
05/29/2022 06:45:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=48
05/29/2022 06:45:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=49
05/29/2022 06:46:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=49
05/29/2022 06:46:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=49
05/29/2022 06:46:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=49
05/29/2022 06:46:12 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.847522417216523 on epoch=49
05/29/2022 06:46:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=50
05/29/2022 06:46:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=50
05/29/2022 06:46:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=50
05/29/2022 06:46:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=51
05/29/2022 06:46:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=51
05/29/2022 06:46:32 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.844027472853578 on epoch=51
05/29/2022 06:46:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=51
05/29/2022 06:46:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=52
05/29/2022 06:46:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=52
05/29/2022 06:46:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=52
05/29/2022 06:46:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=53
05/29/2022 06:46:51 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.8509752125504942 on epoch=53
05/29/2022 06:46:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=53
05/29/2022 06:46:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=53
05/29/2022 06:46:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=54
05/29/2022 06:47:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=54
05/29/2022 06:47:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=54
05/29/2022 06:47:11 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.8566573451472164 on epoch=54
05/29/2022 06:47:11 - INFO - __main__ - Saving model with best Classification-F1: 0.85425960455393 -> 0.8566573451472164 on epoch=54, global_step=1750
05/29/2022 06:47:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/29/2022 06:47:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=55
05/29/2022 06:47:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=55
05/29/2022 06:47:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=55
05/29/2022 06:47:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=56
05/29/2022 06:47:30 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.8534679461380247 on epoch=56
05/29/2022 06:47:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=56
05/29/2022 06:47:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=56
05/29/2022 06:47:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=57
05/29/2022 06:47:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=57
05/29/2022 06:47:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=57
05/29/2022 06:47:50 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.8323805689321677 on epoch=57
05/29/2022 06:47:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=58
05/29/2022 06:47:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=58
05/29/2022 06:47:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=58
05/29/2022 06:48:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=59
05/29/2022 06:48:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=59
05/29/2022 06:48:09 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.8513859970616077 on epoch=59
05/29/2022 06:48:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=59
05/29/2022 06:48:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
05/29/2022 06:48:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
05/29/2022 06:48:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=60
05/29/2022 06:48:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=60
05/29/2022 06:48:28 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.8514195098479351 on epoch=60
05/29/2022 06:48:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=61
05/29/2022 06:48:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
05/29/2022 06:48:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=61
05/29/2022 06:48:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=62
05/29/2022 06:48:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
05/29/2022 06:48:48 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.8523740329362554 on epoch=62
05/29/2022 06:48:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=62
05/29/2022 06:48:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=63
05/29/2022 06:48:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=63
05/29/2022 06:48:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/29/2022 06:49:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=64
05/29/2022 06:49:08 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.8582535371358425 on epoch=64
05/29/2022 06:49:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8566573451472164 -> 0.8582535371358425 on epoch=64, global_step=2050
05/29/2022 06:49:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=64
05/29/2022 06:49:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=64
05/29/2022 06:49:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=64
05/29/2022 06:49:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=65
05/29/2022 06:49:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=65
05/29/2022 06:49:27 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.8558998086752172 on epoch=65
05/29/2022 06:49:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=65
05/29/2022 06:49:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=66
05/29/2022 06:49:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=66
05/29/2022 06:49:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=66
05/29/2022 06:49:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=67
05/29/2022 06:49:47 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.8595326880507633 on epoch=67
05/29/2022 06:49:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8582535371358425 -> 0.8595326880507633 on epoch=67, global_step=2150
05/29/2022 06:49:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=67
05/29/2022 06:49:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=67
05/29/2022 06:49:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=68
05/29/2022 06:49:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=68
05/29/2022 06:49:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=68
05/29/2022 06:50:06 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.8582612121605893 on epoch=68
05/29/2022 06:50:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.33 on epoch=69
05/29/2022 06:50:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=69
05/29/2022 06:50:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=69
05/29/2022 06:50:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=69
05/29/2022 06:50:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=70
05/29/2022 06:50:26 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.8618252640162752 on epoch=70
05/29/2022 06:50:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8595326880507633 -> 0.8618252640162752 on epoch=70, global_step=2250
05/29/2022 06:50:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=70
05/29/2022 06:50:31 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 06:50:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=71
05/29/2022 06:50:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=71
05/29/2022 06:50:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=71
05/29/2022 06:50:45 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.8400901329929931 on epoch=71
05/29/2022 06:50:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=72
05/29/2022 06:50:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=72
05/29/2022 06:50:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.24 on epoch=72
05/29/2022 06:50:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=73
05/29/2022 06:50:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=73
05/29/2022 06:51:05 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.8581383042452977 on epoch=73
05/29/2022 06:51:07 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=73
05/29/2022 06:51:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=74
05/29/2022 06:51:12 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=74
05/29/2022 06:51:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=74
05/29/2022 06:51:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=74
05/29/2022 06:51:24 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.8576922800565596 on epoch=74
05/29/2022 06:51:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=75
05/29/2022 06:51:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=75
05/29/2022 06:51:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=75
05/29/2022 06:51:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=76
05/29/2022 06:51:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=76
05/29/2022 06:51:44 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.852820341782606 on epoch=76
05/29/2022 06:51:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.29 on epoch=76
05/29/2022 06:51:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=77
05/29/2022 06:51:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=77
05/29/2022 06:51:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=77
05/29/2022 06:51:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=78
05/29/2022 06:52:04 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.8588007081542066 on epoch=78
05/29/2022 06:52:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=78
05/29/2022 06:52:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=78
05/29/2022 06:52:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=79
05/29/2022 06:52:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=79
05/29/2022 06:52:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=79
05/29/2022 06:52:23 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.8520250118206322 on epoch=79
05/29/2022 06:52:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.24 on epoch=79
05/29/2022 06:52:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=80
05/29/2022 06:52:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=80
05/29/2022 06:52:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=80
05/29/2022 06:52:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=81
05/29/2022 06:52:42 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.8464178436690841 on epoch=81
05/29/2022 06:52:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=81
05/29/2022 06:52:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
05/29/2022 06:52:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=82
05/29/2022 06:52:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=82
05/29/2022 06:52:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=82
05/29/2022 06:53:02 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.8590797757683624 on epoch=82
05/29/2022 06:53:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=83
05/29/2022 06:53:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=83
05/29/2022 06:53:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=83
05/29/2022 06:53:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.24 on epoch=84
05/29/2022 06:53:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=84
05/29/2022 06:53:21 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.8548141058943173 on epoch=84
05/29/2022 06:53:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=84
05/29/2022 06:53:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=84
05/29/2022 06:53:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=85
05/29/2022 06:53:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=85
05/29/2022 06:53:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=85
05/29/2022 06:53:41 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.8542683061568672 on epoch=85
05/29/2022 06:53:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=86
05/29/2022 06:53:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=86
05/29/2022 06:53:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.22 on epoch=86
05/29/2022 06:53:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=87
05/29/2022 06:53:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=87
05/29/2022 06:54:01 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.8561543170352394 on epoch=87
05/29/2022 06:54:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.21 on epoch=87
05/29/2022 06:54:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=88
05/29/2022 06:54:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.22 on epoch=88
05/29/2022 06:54:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=88
05/29/2022 06:54:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=89
05/29/2022 06:54:20 - INFO - __main__ - Global step 2850 Train loss 0.21 Classification-F1 0.8525187115626304 on epoch=89
05/29/2022 06:54:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=89
05/29/2022 06:54:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
05/29/2022 06:54:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=89
05/29/2022 06:54:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=90
05/29/2022 06:54:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.17 on epoch=90
05/29/2022 06:54:40 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.8512338194002029 on epoch=90
05/29/2022 06:54:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=90
05/29/2022 06:54:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=91
05/29/2022 06:54:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=91
05/29/2022 06:54:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=91
05/29/2022 06:54:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=92
05/29/2022 06:54:59 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.844642759520165 on epoch=92
05/29/2022 06:55:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=92
05/29/2022 06:55:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=92
05/29/2022 06:55:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=93
05/29/2022 06:55:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=93
05/29/2022 06:55:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=93
05/29/2022 06:55:13 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:55:13 - INFO - __main__ - Printing 3 examples
05/29/2022 06:55:13 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:55:13 - INFO - __main__ - ['happy']
05/29/2022 06:55:13 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:55:13 - INFO - __main__ - ['happy']
05/29/2022 06:55:13 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:55:13 - INFO - __main__ - ['happy']
05/29/2022 06:55:13 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:55:13 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:55:14 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:55:14 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:55:14 - INFO - __main__ - Printing 3 examples
05/29/2022 06:55:14 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:55:14 - INFO - __main__ - ['happy']
05/29/2022 06:55:14 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:55:14 - INFO - __main__ - ['happy']
05/29/2022 06:55:14 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:55:14 - INFO - __main__ - ['happy']
05/29/2022 06:55:14 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:55:14 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:55:14 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:55:19 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.8417002295599418 on epoch=93
05/29/2022 06:55:19 - INFO - __main__ - save last model!
05/29/2022 06:55:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 06:55:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 06:55:19 - INFO - __main__ - Printing 3 examples
05/29/2022 06:55:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 06:55:19 - INFO - __main__ - ['others']
05/29/2022 06:55:19 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 06:55:19 - INFO - __main__ - ['others']
05/29/2022 06:55:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 06:55:19 - INFO - __main__ - ['others']
05/29/2022 06:55:19 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:55:21 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:55:26 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 06:55:32 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:55:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:55:33 - INFO - __main__ - Starting training!
05/29/2022 06:56:42 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_42_0.3_8_predictions.txt
05/29/2022 06:56:42 - INFO - __main__ - Classification-F1 on test data: 0.4887
05/29/2022 06:56:42 - INFO - __main__ - prefix=emo_128_42, lr=0.3, bsz=8, dev_performance=0.8618252640162752, test_performance=0.48866285758541406
05/29/2022 06:56:42 - INFO - __main__ - Running ... prefix=emo_128_42, lr=0.2, bsz=8 ...
05/29/2022 06:56:43 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:56:43 - INFO - __main__ - Printing 3 examples
05/29/2022 06:56:43 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/29/2022 06:56:43 - INFO - __main__ - ['happy']
05/29/2022 06:56:43 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/29/2022 06:56:43 - INFO - __main__ - ['happy']
05/29/2022 06:56:43 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/29/2022 06:56:43 - INFO - __main__ - ['happy']
05/29/2022 06:56:43 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:56:43 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:56:44 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 06:56:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 06:56:44 - INFO - __main__ - Printing 3 examples
05/29/2022 06:56:44 - INFO - __main__ -  [emo] ok then r u boy or a girl maybe a horse oooo nice joke
05/29/2022 06:56:44 - INFO - __main__ - ['happy']
05/29/2022 06:56:44 - INFO - __main__ -  [emo] do you know manoj mehta ohhh wow  he is the coolest faculty we had  haha
05/29/2022 06:56:44 - INFO - __main__ - ['happy']
05/29/2022 06:56:44 - INFO - __main__ -  [emo] you are funny and you are sarcastic thanks for the complement
05/29/2022 06:56:44 - INFO - __main__ - ['happy']
05/29/2022 06:56:44 - INFO - __main__ - Tokenizing Input ...
05/29/2022 06:56:44 - INFO - __main__ - Tokenizing Output ...
05/29/2022 06:56:45 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 06:57:03 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 06:57:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 06:57:04 - INFO - __main__ - Starting training!
05/29/2022 06:57:07 - INFO - __main__ - Step 10 Global step 10 Train loss 4.61 on epoch=0
05/29/2022 06:57:10 - INFO - __main__ - Step 20 Global step 20 Train loss 3.53 on epoch=0
05/29/2022 06:57:12 - INFO - __main__ - Step 30 Global step 30 Train loss 3.14 on epoch=0
05/29/2022 06:57:15 - INFO - __main__ - Step 40 Global step 40 Train loss 2.69 on epoch=1
05/29/2022 06:57:17 - INFO - __main__ - Step 50 Global step 50 Train loss 2.38 on epoch=1
05/29/2022 06:57:25 - INFO - __main__ - Global step 50 Train loss 3.27 Classification-F1 0.002640845070422535 on epoch=1
05/29/2022 06:57:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.002640845070422535 on epoch=1, global_step=50
05/29/2022 06:57:27 - INFO - __main__ - Step 60 Global step 60 Train loss 2.19 on epoch=1
05/29/2022 06:57:30 - INFO - __main__ - Step 70 Global step 70 Train loss 2.06 on epoch=2
05/29/2022 06:57:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.81 on epoch=2
05/29/2022 06:57:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.58 on epoch=2
05/29/2022 06:57:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.59 on epoch=3
05/29/2022 06:57:44 - INFO - __main__ - Global step 100 Train loss 1.85 Classification-F1 0.16549568943428006 on epoch=3
05/29/2022 06:57:44 - INFO - __main__ - Saving model with best Classification-F1: 0.002640845070422535 -> 0.16549568943428006 on epoch=3, global_step=100
05/29/2022 06:57:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.44 on epoch=3
05/29/2022 06:57:49 - INFO - __main__ - Step 120 Global step 120 Train loss 1.27 on epoch=3
05/29/2022 06:57:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.15 on epoch=4
05/29/2022 06:57:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.36 on epoch=4
05/29/2022 06:57:57 - INFO - __main__ - Step 150 Global step 150 Train loss 1.03 on epoch=4
05/29/2022 06:58:04 - INFO - __main__ - Global step 150 Train loss 1.25 Classification-F1 0.5057065428851572 on epoch=4
05/29/2022 06:58:04 - INFO - __main__ - Saving model with best Classification-F1: 0.16549568943428006 -> 0.5057065428851572 on epoch=4, global_step=150
05/29/2022 06:58:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=4
05/29/2022 06:58:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=5
05/29/2022 06:58:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=5
05/29/2022 06:58:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=5
05/29/2022 06:58:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=6
05/29/2022 06:58:23 - INFO - __main__ - Global step 200 Train loss 0.94 Classification-F1 0.5069220091835839 on epoch=6
05/29/2022 06:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5057065428851572 -> 0.5069220091835839 on epoch=6, global_step=200
05/29/2022 06:58:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=6
05/29/2022 06:58:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=6
05/29/2022 06:58:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=7
05/29/2022 06:58:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=7
05/29/2022 06:58:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=7
05/29/2022 06:58:43 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.5135703987120572 on epoch=7
05/29/2022 06:58:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5069220091835839 -> 0.5135703987120572 on epoch=7, global_step=250
05/29/2022 06:58:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=8
05/29/2022 06:58:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=8
05/29/2022 06:58:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=8
05/29/2022 06:58:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=9
05/29/2022 06:58:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=9
05/29/2022 06:59:03 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.5356597144227864 on epoch=9
05/29/2022 06:59:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5135703987120572 -> 0.5356597144227864 on epoch=9, global_step=300
05/29/2022 06:59:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=9
05/29/2022 06:59:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.76 on epoch=9
05/29/2022 06:59:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.73 on epoch=10
05/29/2022 06:59:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.71 on epoch=10
05/29/2022 06:59:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.72 on epoch=10
05/29/2022 06:59:22 - INFO - __main__ - Global step 350 Train loss 0.72 Classification-F1 0.5590719148398866 on epoch=10
05/29/2022 06:59:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5356597144227864 -> 0.5590719148398866 on epoch=10, global_step=350
05/29/2022 06:59:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=11
05/29/2022 06:59:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.61 on epoch=11
05/29/2022 06:59:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=11
05/29/2022 06:59:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.61 on epoch=12
05/29/2022 06:59:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.70 on epoch=12
05/29/2022 06:59:42 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.5883547714056189 on epoch=12
05/29/2022 06:59:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5590719148398866 -> 0.5883547714056189 on epoch=12, global_step=400
05/29/2022 06:59:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=12
05/29/2022 06:59:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=13
05/29/2022 06:59:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=13
05/29/2022 06:59:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=13
05/29/2022 06:59:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=14
05/29/2022 07:00:02 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.6628120882403539 on epoch=14
05/29/2022 07:00:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5883547714056189 -> 0.6628120882403539 on epoch=14, global_step=450
05/29/2022 07:00:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=14
05/29/2022 07:00:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.63 on epoch=14
05/29/2022 07:00:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=14
05/29/2022 07:00:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=15
05/29/2022 07:00:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=15
05/29/2022 07:00:21 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.6686012770048267 on epoch=15
05/29/2022 07:00:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6628120882403539 -> 0.6686012770048267 on epoch=15, global_step=500
05/29/2022 07:00:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=15
05/29/2022 07:00:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=16
05/29/2022 07:00:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=16
05/29/2022 07:00:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=16
05/29/2022 07:00:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.58 on epoch=17
05/29/2022 07:00:41 - INFO - __main__ - Global step 550 Train loss 0.57 Classification-F1 0.710426494359093 on epoch=17
05/29/2022 07:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6686012770048267 -> 0.710426494359093 on epoch=17, global_step=550
05/29/2022 07:00:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=17
05/29/2022 07:00:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=17
05/29/2022 07:00:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.59 on epoch=18
05/29/2022 07:00:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=18
05/29/2022 07:00:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=18
05/29/2022 07:01:00 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.688670236724012 on epoch=18
05/29/2022 07:01:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=19
05/29/2022 07:01:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=19
05/29/2022 07:01:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
05/29/2022 07:01:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=19
05/29/2022 07:01:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=20
05/29/2022 07:01:20 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.721901324049224 on epoch=20
05/29/2022 07:01:20 - INFO - __main__ - Saving model with best Classification-F1: 0.710426494359093 -> 0.721901324049224 on epoch=20, global_step=650
05/29/2022 07:01:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=20
05/29/2022 07:01:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=20
05/29/2022 07:01:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=21
05/29/2022 07:01:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=21
05/29/2022 07:01:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=21
05/29/2022 07:01:40 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.6826186288753875 on epoch=21
05/29/2022 07:01:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=22
05/29/2022 07:01:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=22
05/29/2022 07:01:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=22
05/29/2022 07:01:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=23
05/29/2022 07:01:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=23
05/29/2022 07:01:59 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.7541691266986438 on epoch=23
05/29/2022 07:01:59 - INFO - __main__ - Saving model with best Classification-F1: 0.721901324049224 -> 0.7541691266986438 on epoch=23, global_step=750
05/29/2022 07:02:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=23
05/29/2022 07:02:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=24
05/29/2022 07:02:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=24
05/29/2022 07:02:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=24
05/29/2022 07:02:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=24
05/29/2022 07:02:19 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.7825631386157702 on epoch=24
05/29/2022 07:02:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7541691266986438 -> 0.7825631386157702 on epoch=24, global_step=800
05/29/2022 07:02:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=25
05/29/2022 07:02:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=25
05/29/2022 07:02:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=25
05/29/2022 07:02:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=26
05/29/2022 07:02:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=26
05/29/2022 07:02:38 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.7605179278854622 on epoch=26
05/29/2022 07:02:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=26
05/29/2022 07:02:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=27
05/29/2022 07:02:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=27
05/29/2022 07:02:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=27
05/29/2022 07:02:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=28
05/29/2022 07:02:58 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.800526789386873 on epoch=28
05/29/2022 07:02:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7825631386157702 -> 0.800526789386873 on epoch=28, global_step=900
05/29/2022 07:03:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=28
05/29/2022 07:03:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=28
05/29/2022 07:03:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=29
05/29/2022 07:03:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=29
05/29/2022 07:03:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=29
05/29/2022 07:03:17 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.7813150658625471 on epoch=29
05/29/2022 07:03:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
05/29/2022 07:03:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=30
05/29/2022 07:03:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=30
05/29/2022 07:03:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=30
05/29/2022 07:03:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=31
05/29/2022 07:03:37 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.812915990822442 on epoch=31
05/29/2022 07:03:37 - INFO - __main__ - Saving model with best Classification-F1: 0.800526789386873 -> 0.812915990822442 on epoch=31, global_step=1000
05/29/2022 07:03:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=31
05/29/2022 07:03:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=31
05/29/2022 07:03:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=32
05/29/2022 07:03:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=32
05/29/2022 07:03:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=32
05/29/2022 07:03:57 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.767285098999275 on epoch=32
05/29/2022 07:03:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=33
05/29/2022 07:04:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/29/2022 07:04:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=33
05/29/2022 07:04:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=34
05/29/2022 07:04:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=34
05/29/2022 07:04:16 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.8026364057418759 on epoch=34
05/29/2022 07:04:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=34
05/29/2022 07:04:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=34
05/29/2022 07:04:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=35
05/29/2022 07:04:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=35
05/29/2022 07:04:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=35
05/29/2022 07:04:35 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.7865310251197382 on epoch=35
05/29/2022 07:04:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=36
05/29/2022 07:04:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=36
05/29/2022 07:04:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=36
05/29/2022 07:04:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=37
05/29/2022 07:04:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=37
05/29/2022 07:04:55 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.8366346560094041 on epoch=37
05/29/2022 07:04:55 - INFO - __main__ - Saving model with best Classification-F1: 0.812915990822442 -> 0.8366346560094041 on epoch=37, global_step=1200
05/29/2022 07:04:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=37
05/29/2022 07:04:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=38
05/29/2022 07:05:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=38
05/29/2022 07:05:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=38
05/29/2022 07:05:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=39
05/29/2022 07:05:14 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.8324841217494859 on epoch=39
05/29/2022 07:05:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=39
05/29/2022 07:05:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
05/29/2022 07:05:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=39
05/29/2022 07:05:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=40
05/29/2022 07:05:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=40
05/29/2022 07:05:33 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.8109953336626199 on epoch=40
05/29/2022 07:05:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=40
05/29/2022 07:05:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=41
05/29/2022 07:05:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=41
05/29/2022 07:05:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=41
05/29/2022 07:05:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=42
05/29/2022 07:05:52 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.8314132425999101 on epoch=42
05/29/2022 07:05:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=42
05/29/2022 07:05:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=42
05/29/2022 07:06:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=43
05/29/2022 07:06:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=43
05/29/2022 07:06:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=43
05/29/2022 07:06:12 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.8023134265362399 on epoch=43
05/29/2022 07:06:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=44
05/29/2022 07:06:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=44
05/29/2022 07:06:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=44
05/29/2022 07:06:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=44
05/29/2022 07:06:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=45
05/29/2022 07:06:31 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.8245204671057258 on epoch=45
05/29/2022 07:06:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=45
05/29/2022 07:06:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=45
05/29/2022 07:06:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=46
05/29/2022 07:06:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=46
05/29/2022 07:06:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=46
05/29/2022 07:06:50 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.8185516172002434 on epoch=46
05/29/2022 07:06:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=47
05/29/2022 07:06:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=47
05/29/2022 07:06:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=47
05/29/2022 07:07:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=48
05/29/2022 07:07:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=48
05/29/2022 07:07:09 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.8359184211752886 on epoch=48
05/29/2022 07:07:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=48
05/29/2022 07:07:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=49
05/29/2022 07:07:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=49
05/29/2022 07:07:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=49
05/29/2022 07:07:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=49
05/29/2022 07:07:29 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.807986837660616 on epoch=49
05/29/2022 07:07:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=50
05/29/2022 07:07:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=50
05/29/2022 07:07:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=50
05/29/2022 07:07:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=51
05/29/2022 07:07:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=51
05/29/2022 07:07:48 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.8250383346324306 on epoch=51
05/29/2022 07:07:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=51
05/29/2022 07:07:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=52
05/29/2022 07:07:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=52
05/29/2022 07:07:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=52
05/29/2022 07:08:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=53
05/29/2022 07:08:08 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.8340315454434577 on epoch=53
05/29/2022 07:08:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=53
05/29/2022 07:08:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=53
05/29/2022 07:08:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=54
05/29/2022 07:08:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=54
05/29/2022 07:08:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=54
05/29/2022 07:08:28 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.8102920193790193 on epoch=54
05/29/2022 07:08:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=54
05/29/2022 07:08:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=55
05/29/2022 07:08:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=55
05/29/2022 07:08:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=55
05/29/2022 07:08:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=56
05/29/2022 07:08:48 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.8398326139039669 on epoch=56
05/29/2022 07:08:48 - INFO - __main__ - Saving model with best Classification-F1: 0.8366346560094041 -> 0.8398326139039669 on epoch=56, global_step=1800
05/29/2022 07:08:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=56
05/29/2022 07:08:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=56
05/29/2022 07:08:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=57
05/29/2022 07:08:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=57
05/29/2022 07:09:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=57
05/29/2022 07:09:07 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.8075377878836175 on epoch=57
05/29/2022 07:09:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=58
05/29/2022 07:09:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=58
05/29/2022 07:09:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=58
05/29/2022 07:09:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=59
05/29/2022 07:09:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=59
05/29/2022 07:09:27 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.8364820138216079 on epoch=59
05/29/2022 07:09:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=59
05/29/2022 07:09:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=59
05/29/2022 07:09:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=60
05/29/2022 07:09:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=60
05/29/2022 07:09:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=60
05/29/2022 07:09:47 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.8074730637308025 on epoch=60
05/29/2022 07:09:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=61
05/29/2022 07:09:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=61
05/29/2022 07:09:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=61
05/29/2022 07:09:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=62
05/29/2022 07:10:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=62
05/29/2022 07:10:07 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.8174058749676121 on epoch=62
05/29/2022 07:10:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=62
05/29/2022 07:10:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=63
05/29/2022 07:10:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=63
05/29/2022 07:10:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=63
05/29/2022 07:10:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=64
05/29/2022 07:10:27 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.8382793854524997 on epoch=64
05/29/2022 07:10:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=64
05/29/2022 07:10:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=64
05/29/2022 07:10:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=64
05/29/2022 07:10:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=65
05/29/2022 07:10:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=65
05/29/2022 07:10:46 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.8349552674769778 on epoch=65
05/29/2022 07:10:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=65
05/29/2022 07:10:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=66
05/29/2022 07:10:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=66
05/29/2022 07:10:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=66
05/29/2022 07:10:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=67
05/29/2022 07:11:06 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.841971153182618 on epoch=67
05/29/2022 07:11:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8398326139039669 -> 0.841971153182618 on epoch=67, global_step=2150
05/29/2022 07:11:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=67
05/29/2022 07:11:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.33 on epoch=67
05/29/2022 07:11:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=68
05/29/2022 07:11:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=68
05/29/2022 07:11:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=68
05/29/2022 07:11:26 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.8191531582819425 on epoch=68
05/29/2022 07:11:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=69
05/29/2022 07:11:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=69
05/29/2022 07:11:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.22 on epoch=69
05/29/2022 07:11:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.36 on epoch=69
05/29/2022 07:11:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=70
05/29/2022 07:11:46 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.8446963016522153 on epoch=70
05/29/2022 07:11:46 - INFO - __main__ - Saving model with best Classification-F1: 0.841971153182618 -> 0.8446963016522153 on epoch=70, global_step=2250
05/29/2022 07:11:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=70
05/29/2022 07:11:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.24 on epoch=70
05/29/2022 07:11:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=71
05/29/2022 07:11:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=71
05/29/2022 07:11:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=71
05/29/2022 07:12:06 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.8281979031741389 on epoch=71
05/29/2022 07:12:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=72
05/29/2022 07:12:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=72
05/29/2022 07:12:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=72
05/29/2022 07:12:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=73
05/29/2022 07:12:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=73
05/29/2022 07:12:25 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.8480677826194165 on epoch=73
05/29/2022 07:12:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8446963016522153 -> 0.8480677826194165 on epoch=73, global_step=2350
05/29/2022 07:12:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=73
05/29/2022 07:12:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.26 on epoch=74
05/29/2022 07:12:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.29 on epoch=74
05/29/2022 07:12:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=74
05/29/2022 07:12:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.32 on epoch=74
05/29/2022 07:12:45 - INFO - __main__ - Global step 2400 Train loss 0.26 Classification-F1 0.8339561907397486 on epoch=74
05/29/2022 07:12:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.31 on epoch=75
05/29/2022 07:12:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=75
05/29/2022 07:12:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=75
05/29/2022 07:12:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.29 on epoch=76
05/29/2022 07:12:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=76
05/29/2022 07:13:05 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.8415142700356826 on epoch=76
05/29/2022 07:13:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=76
05/29/2022 07:13:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=77
05/29/2022 07:13:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=77
05/29/2022 07:13:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=77
05/29/2022 07:13:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.26 on epoch=78
05/29/2022 07:13:25 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.8503668039539971 on epoch=78
05/29/2022 07:13:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8480677826194165 -> 0.8503668039539971 on epoch=78, global_step=2500
05/29/2022 07:13:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=78
05/29/2022 07:13:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=78
05/29/2022 07:13:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=79
05/29/2022 07:13:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=79
05/29/2022 07:13:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.25 on epoch=79
05/29/2022 07:13:45 - INFO - __main__ - Global step 2550 Train loss 0.27 Classification-F1 0.8308975138728876 on epoch=79
05/29/2022 07:13:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.26 on epoch=79
05/29/2022 07:13:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=80
05/29/2022 07:13:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=80
05/29/2022 07:13:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=80
05/29/2022 07:13:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.25 on epoch=81
05/29/2022 07:14:05 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.8516842028938804 on epoch=81
05/29/2022 07:14:05 - INFO - __main__ - Saving model with best Classification-F1: 0.8503668039539971 -> 0.8516842028938804 on epoch=81, global_step=2600
05/29/2022 07:14:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=81
05/29/2022 07:14:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=81
05/29/2022 07:14:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=82
05/29/2022 07:14:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=82
05/29/2022 07:14:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=82
05/29/2022 07:14:25 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.828460666977423 on epoch=82
05/29/2022 07:14:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.27 on epoch=83
05/29/2022 07:14:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.33 on epoch=83
05/29/2022 07:14:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.26 on epoch=83
05/29/2022 07:14:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.35 on epoch=84
05/29/2022 07:14:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=84
05/29/2022 07:14:44 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.8473280338499994 on epoch=84
05/29/2022 07:14:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=84
05/29/2022 07:14:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=84
05/29/2022 07:14:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.26 on epoch=85
05/29/2022 07:14:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=85
05/29/2022 07:14:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=85
05/29/2022 07:15:04 - INFO - __main__ - Global step 2750 Train loss 0.25 Classification-F1 0.832150042629864 on epoch=85
05/29/2022 07:15:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=86
05/29/2022 07:15:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=86
05/29/2022 07:15:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=86
05/29/2022 07:15:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.25 on epoch=87
05/29/2022 07:15:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.24 on epoch=87
05/29/2022 07:15:23 - INFO - __main__ - Global step 2800 Train loss 0.25 Classification-F1 0.8412072073572266 on epoch=87
05/29/2022 07:15:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=87
05/29/2022 07:15:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=88
05/29/2022 07:15:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=88
05/29/2022 07:15:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
05/29/2022 07:15:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=89
05/29/2022 07:15:43 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.8513249153568196 on epoch=89
05/29/2022 07:15:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
05/29/2022 07:15:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=89
05/29/2022 07:15:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=89
05/29/2022 07:15:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=90
05/29/2022 07:15:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=90
05/29/2022 07:16:02 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.8436148040023003 on epoch=90
05/29/2022 07:16:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=90
05/29/2022 07:16:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.21 on epoch=91
05/29/2022 07:16:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=91
05/29/2022 07:16:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.22 on epoch=91
05/29/2022 07:16:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=92
05/29/2022 07:16:22 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.8592819201878004 on epoch=92
05/29/2022 07:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8516842028938804 -> 0.8592819201878004 on epoch=92, global_step=2950
05/29/2022 07:16:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=92
05/29/2022 07:16:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=92
05/29/2022 07:16:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=93
05/29/2022 07:16:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=93
05/29/2022 07:16:35 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=93
05/29/2022 07:16:36 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:16:36 - INFO - __main__ - Printing 3 examples
05/29/2022 07:16:36 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 07:16:36 - INFO - __main__ - ['others']
05/29/2022 07:16:36 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 07:16:36 - INFO - __main__ - ['others']
05/29/2022 07:16:36 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 07:16:36 - INFO - __main__ - ['others']
05/29/2022 07:16:36 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:16:36 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:16:37 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 07:16:37 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:16:37 - INFO - __main__ - Printing 3 examples
05/29/2022 07:16:37 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 07:16:37 - INFO - __main__ - ['others']
05/29/2022 07:16:37 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 07:16:37 - INFO - __main__ - ['others']
05/29/2022 07:16:37 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 07:16:37 - INFO - __main__ - ['others']
05/29/2022 07:16:37 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:16:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:16:37 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 07:16:42 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.8361857110366158 on epoch=93
05/29/2022 07:16:42 - INFO - __main__ - save last model!
05/29/2022 07:16:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 07:16:42 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 07:16:42 - INFO - __main__ - Printing 3 examples
05/29/2022 07:16:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 07:16:42 - INFO - __main__ - ['others']
05/29/2022 07:16:42 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 07:16:42 - INFO - __main__ - ['others']
05/29/2022 07:16:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 07:16:42 - INFO - __main__ - ['others']
05/29/2022 07:16:42 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:16:44 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:16:49 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 07:16:55 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 07:16:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 07:16:56 - INFO - __main__ - Starting training!
05/29/2022 07:18:02 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_42_0.2_8_predictions.txt
05/29/2022 07:18:02 - INFO - __main__ - Classification-F1 on test data: 0.3766
05/29/2022 07:18:02 - INFO - __main__ - prefix=emo_128_42, lr=0.2, bsz=8, dev_performance=0.8592819201878004, test_performance=0.3766421023276127
05/29/2022 07:18:02 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.5, bsz=8 ...
05/29/2022 07:18:03 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:18:03 - INFO - __main__ - Printing 3 examples
05/29/2022 07:18:03 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 07:18:03 - INFO - __main__ - ['others']
05/29/2022 07:18:03 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 07:18:03 - INFO - __main__ - ['others']
05/29/2022 07:18:03 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 07:18:03 - INFO - __main__ - ['others']
05/29/2022 07:18:03 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:18:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:18:04 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 07:18:04 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:18:04 - INFO - __main__ - Printing 3 examples
05/29/2022 07:18:04 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 07:18:04 - INFO - __main__ - ['others']
05/29/2022 07:18:04 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 07:18:04 - INFO - __main__ - ['others']
05/29/2022 07:18:04 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 07:18:04 - INFO - __main__ - ['others']
05/29/2022 07:18:04 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:18:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:18:05 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 07:18:20 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 07:18:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 07:18:21 - INFO - __main__ - Starting training!
05/29/2022 07:18:24 - INFO - __main__ - Step 10 Global step 10 Train loss 3.63 on epoch=0
05/29/2022 07:18:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.69 on epoch=0
05/29/2022 07:18:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.32 on epoch=0
05/29/2022 07:18:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.56 on epoch=1
05/29/2022 07:18:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.35 on epoch=1
05/29/2022 07:18:41 - INFO - __main__ - Global step 50 Train loss 2.31 Classification-F1 0.28607118929185227 on epoch=1
05/29/2022 07:18:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28607118929185227 on epoch=1, global_step=50
05/29/2022 07:18:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=1
05/29/2022 07:18:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=2
05/29/2022 07:18:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.76 on epoch=2
05/29/2022 07:18:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.76 on epoch=2
05/29/2022 07:18:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=3
05/29/2022 07:19:01 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.484062674005342 on epoch=3
05/29/2022 07:19:01 - INFO - __main__ - Saving model with best Classification-F1: 0.28607118929185227 -> 0.484062674005342 on epoch=3, global_step=100
05/29/2022 07:19:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.67 on epoch=3
05/29/2022 07:19:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.73 on epoch=3
05/29/2022 07:19:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=4
05/29/2022 07:19:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=4
05/29/2022 07:19:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=4
05/29/2022 07:19:20 - INFO - __main__ - Global step 150 Train loss 0.69 Classification-F1 0.5646703510830615 on epoch=4
05/29/2022 07:19:20 - INFO - __main__ - Saving model with best Classification-F1: 0.484062674005342 -> 0.5646703510830615 on epoch=4, global_step=150
05/29/2022 07:19:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=4
05/29/2022 07:19:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.71 on epoch=5
05/29/2022 07:19:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=5
05/29/2022 07:19:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=5
05/29/2022 07:19:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=6
05/29/2022 07:19:39 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.5938808852638956 on epoch=6
05/29/2022 07:19:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5646703510830615 -> 0.5938808852638956 on epoch=6, global_step=200
05/29/2022 07:19:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=6
05/29/2022 07:19:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=6
05/29/2022 07:19:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=7
05/29/2022 07:19:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=7
05/29/2022 07:19:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=7
05/29/2022 07:19:59 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.6309476547950268 on epoch=7
05/29/2022 07:19:59 - INFO - __main__ - Saving model with best Classification-F1: 0.5938808852638956 -> 0.6309476547950268 on epoch=7, global_step=250
05/29/2022 07:20:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.59 on epoch=8
05/29/2022 07:20:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=8
05/29/2022 07:20:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=8
05/29/2022 07:20:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=9
05/29/2022 07:20:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.64 on epoch=9
05/29/2022 07:20:18 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.701601087084958 on epoch=9
05/29/2022 07:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6309476547950268 -> 0.701601087084958 on epoch=9, global_step=300
05/29/2022 07:20:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=9
05/29/2022 07:20:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=9
05/29/2022 07:20:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=10
05/29/2022 07:20:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=10
05/29/2022 07:20:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=10
05/29/2022 07:20:37 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.698069740120852 on epoch=10
05/29/2022 07:20:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=11
05/29/2022 07:20:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=11
05/29/2022 07:20:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=11
05/29/2022 07:20:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=12
05/29/2022 07:20:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=12
05/29/2022 07:20:57 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.7476533520748248 on epoch=12
05/29/2022 07:20:57 - INFO - __main__ - Saving model with best Classification-F1: 0.701601087084958 -> 0.7476533520748248 on epoch=12, global_step=400
05/29/2022 07:20:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=12
05/29/2022 07:21:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=13
05/29/2022 07:21:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=13
05/29/2022 07:21:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=13
05/29/2022 07:21:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=14
05/29/2022 07:21:16 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.760520130279897 on epoch=14
05/29/2022 07:21:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7476533520748248 -> 0.760520130279897 on epoch=14, global_step=450
05/29/2022 07:21:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=14
05/29/2022 07:21:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=14
05/29/2022 07:21:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=14
05/29/2022 07:21:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=15
05/29/2022 07:21:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=15
05/29/2022 07:21:35 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.7364752195292885 on epoch=15
05/29/2022 07:21:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=15
05/29/2022 07:21:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=16
05/29/2022 07:21:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=16
05/29/2022 07:21:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=16
05/29/2022 07:21:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=17
05/29/2022 07:21:55 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.7745813301033382 on epoch=17
05/29/2022 07:21:55 - INFO - __main__ - Saving model with best Classification-F1: 0.760520130279897 -> 0.7745813301033382 on epoch=17, global_step=550
05/29/2022 07:21:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=17
05/29/2022 07:22:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=17
05/29/2022 07:22:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=18
05/29/2022 07:22:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=18
05/29/2022 07:22:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=18
05/29/2022 07:22:14 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.7757891568817288 on epoch=18
05/29/2022 07:22:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7745813301033382 -> 0.7757891568817288 on epoch=18, global_step=600
05/29/2022 07:22:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=19
05/29/2022 07:22:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=19
05/29/2022 07:22:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=19
05/29/2022 07:22:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=19
05/29/2022 07:22:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=20
05/29/2022 07:22:33 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.7946270565084579 on epoch=20
05/29/2022 07:22:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7757891568817288 -> 0.7946270565084579 on epoch=20, global_step=650
05/29/2022 07:22:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=20
05/29/2022 07:22:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=20
05/29/2022 07:22:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=21
05/29/2022 07:22:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=21
05/29/2022 07:22:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=21
05/29/2022 07:22:53 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.7928510925418029 on epoch=21
05/29/2022 07:22:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=22
05/29/2022 07:22:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=22
05/29/2022 07:23:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=22
05/29/2022 07:23:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=23
05/29/2022 07:23:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=23
05/29/2022 07:23:12 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.8001444796822789 on epoch=23
05/29/2022 07:23:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7946270565084579 -> 0.8001444796822789 on epoch=23, global_step=750
05/29/2022 07:23:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=23
05/29/2022 07:23:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=24
05/29/2022 07:23:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=24
05/29/2022 07:23:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=24
05/29/2022 07:23:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=24
05/29/2022 07:23:32 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.8146630664755258 on epoch=24
05/29/2022 07:23:32 - INFO - __main__ - Saving model with best Classification-F1: 0.8001444796822789 -> 0.8146630664755258 on epoch=24, global_step=800
05/29/2022 07:23:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=25
05/29/2022 07:23:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=25
05/29/2022 07:23:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=25
05/29/2022 07:23:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=26
05/29/2022 07:23:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=26
05/29/2022 07:23:51 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.7927155622200353 on epoch=26
05/29/2022 07:23:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=26
05/29/2022 07:23:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=27
05/29/2022 07:23:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=27
05/29/2022 07:24:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=27
05/29/2022 07:24:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=28
05/29/2022 07:24:11 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.7969920094466625 on epoch=28
05/29/2022 07:24:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=28
05/29/2022 07:24:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=28
05/29/2022 07:24:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=29
05/29/2022 07:24:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=29
05/29/2022 07:24:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=29
05/29/2022 07:24:30 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.7979924596340928 on epoch=29
05/29/2022 07:24:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=29
05/29/2022 07:24:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=30
05/29/2022 07:24:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=30
05/29/2022 07:24:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=30
05/29/2022 07:24:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=31
05/29/2022 07:24:50 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.8277275193904488 on epoch=31
05/29/2022 07:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8146630664755258 -> 0.8277275193904488 on epoch=31, global_step=1000
05/29/2022 07:24:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=31
05/29/2022 07:24:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=31
05/29/2022 07:24:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=32
05/29/2022 07:25:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=32
05/29/2022 07:25:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=32
05/29/2022 07:25:09 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.8035500584694487 on epoch=32
05/29/2022 07:25:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=33
05/29/2022 07:25:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=33
05/29/2022 07:25:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=33
05/29/2022 07:25:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=34
05/29/2022 07:25:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=34
05/29/2022 07:25:28 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.809354750155797 on epoch=34
05/29/2022 07:25:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=34
05/29/2022 07:25:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=34
05/29/2022 07:25:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=35
05/29/2022 07:25:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=35
05/29/2022 07:25:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=35
05/29/2022 07:25:48 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.8004892809470371 on epoch=35
05/29/2022 07:25:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=36
05/29/2022 07:25:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=36
05/29/2022 07:25:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=36
05/29/2022 07:25:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=37
05/29/2022 07:26:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=37
05/29/2022 07:26:07 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.8192128622744155 on epoch=37
05/29/2022 07:26:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=37
05/29/2022 07:26:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=38
05/29/2022 07:26:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=38
05/29/2022 07:26:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=38
05/29/2022 07:26:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=39
05/29/2022 07:26:27 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.8036277966031373 on epoch=39
05/29/2022 07:26:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=39
05/29/2022 07:26:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=39
05/29/2022 07:26:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=39
05/29/2022 07:26:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=40
05/29/2022 07:26:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=40
05/29/2022 07:26:46 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.80047379530819 on epoch=40
05/29/2022 07:26:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=40
05/29/2022 07:26:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=41
05/29/2022 07:26:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=41
05/29/2022 07:26:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=41
05/29/2022 07:26:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=42
05/29/2022 07:27:05 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.7973352429951109 on epoch=42
05/29/2022 07:27:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=42
05/29/2022 07:27:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=42
05/29/2022 07:27:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=43
05/29/2022 07:27:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=43
05/29/2022 07:27:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=43
05/29/2022 07:27:25 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7996407547732901 on epoch=43
05/29/2022 07:27:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=44
05/29/2022 07:27:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=44
05/29/2022 07:27:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=44
05/29/2022 07:27:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=44
05/29/2022 07:27:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=45
05/29/2022 07:27:44 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.8158208498480473 on epoch=45
05/29/2022 07:27:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=45
05/29/2022 07:27:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=45
05/29/2022 07:27:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=46
05/29/2022 07:27:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=46
05/29/2022 07:27:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=46
05/29/2022 07:28:03 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.8130850484931704 on epoch=46
05/29/2022 07:28:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=47
05/29/2022 07:28:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=47
05/29/2022 07:28:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=47
05/29/2022 07:28:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=48
05/29/2022 07:28:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=48
05/29/2022 07:28:23 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.8243226802826495 on epoch=48
05/29/2022 07:28:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=48
05/29/2022 07:28:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=49
05/29/2022 07:28:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=49
05/29/2022 07:28:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=49
05/29/2022 07:28:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=49
05/29/2022 07:28:42 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.8106290804846761 on epoch=49
05/29/2022 07:28:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=50
05/29/2022 07:28:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=50
05/29/2022 07:28:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=50
05/29/2022 07:28:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=51
05/29/2022 07:28:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=51
05/29/2022 07:29:02 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.8181959010566294 on epoch=51
05/29/2022 07:29:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=51
05/29/2022 07:29:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=52
05/29/2022 07:29:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=52
05/29/2022 07:29:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=52
05/29/2022 07:29:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=53
05/29/2022 07:29:21 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.82052987849746 on epoch=53
05/29/2022 07:29:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=53
05/29/2022 07:29:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=53
05/29/2022 07:29:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=54
05/29/2022 07:29:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=54
05/29/2022 07:29:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=54
05/29/2022 07:29:40 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.8242769304162951 on epoch=54
05/29/2022 07:29:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=54
05/29/2022 07:29:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=55
05/29/2022 07:29:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=55
05/29/2022 07:29:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=55
05/29/2022 07:29:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=56
05/29/2022 07:30:00 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.8334924241633521 on epoch=56
05/29/2022 07:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.8277275193904488 -> 0.8334924241633521 on epoch=56, global_step=1800
05/29/2022 07:30:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/29/2022 07:30:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=56
05/29/2022 07:30:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=57
05/29/2022 07:30:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=57
05/29/2022 07:30:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=57
05/29/2022 07:30:19 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.7974605527446467 on epoch=57
05/29/2022 07:30:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=58
05/29/2022 07:30:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=58
05/29/2022 07:30:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=58
05/29/2022 07:30:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=59
05/29/2022 07:30:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=59
05/29/2022 07:30:39 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.8239505880537206 on epoch=59
05/29/2022 07:30:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=59
05/29/2022 07:30:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=59
05/29/2022 07:30:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=60
05/29/2022 07:30:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=60
05/29/2022 07:30:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=60
05/29/2022 07:30:58 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.8316469241383493 on epoch=60
05/29/2022 07:31:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=61
05/29/2022 07:31:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=61
05/29/2022 07:31:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=61
05/29/2022 07:31:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=62
05/29/2022 07:31:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=62
05/29/2022 07:31:18 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.8278254493477841 on epoch=62
05/29/2022 07:31:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=62
05/29/2022 07:31:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=63
05/29/2022 07:31:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=63
05/29/2022 07:31:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=63
05/29/2022 07:31:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=64
05/29/2022 07:31:37 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.8168630646797813 on epoch=64
05/29/2022 07:31:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=64
05/29/2022 07:31:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=64
05/29/2022 07:31:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=64
05/29/2022 07:31:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=65
05/29/2022 07:31:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=65
05/29/2022 07:31:56 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.8369668865815848 on epoch=65
05/29/2022 07:31:57 - INFO - __main__ - Saving model with best Classification-F1: 0.8334924241633521 -> 0.8369668865815848 on epoch=65, global_step=2100
05/29/2022 07:31:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=65
05/29/2022 07:32:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=66
05/29/2022 07:32:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=66
05/29/2022 07:32:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=66
05/29/2022 07:32:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=67
05/29/2022 07:32:16 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.8182517426197211 on epoch=67
05/29/2022 07:32:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=67
05/29/2022 07:32:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=67
05/29/2022 07:32:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=68
05/29/2022 07:32:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=68
05/29/2022 07:32:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=68
05/29/2022 07:32:36 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.7989985835925797 on epoch=68
05/29/2022 07:32:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=69
05/29/2022 07:32:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=69
05/29/2022 07:32:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=69
05/29/2022 07:32:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=69
05/29/2022 07:32:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=70
05/29/2022 07:32:55 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.831577677588607 on epoch=70
05/29/2022 07:32:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=70
05/29/2022 07:33:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=70
05/29/2022 07:33:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=71
05/29/2022 07:33:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=71
05/29/2022 07:33:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=71
05/29/2022 07:33:15 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.8133398715090676 on epoch=71
05/29/2022 07:33:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=72
05/29/2022 07:33:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=72
05/29/2022 07:33:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=72
05/29/2022 07:33:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=73
05/29/2022 07:33:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=73
05/29/2022 07:33:34 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.8371780845587529 on epoch=73
05/29/2022 07:33:35 - INFO - __main__ - Saving model with best Classification-F1: 0.8369668865815848 -> 0.8371780845587529 on epoch=73, global_step=2350
05/29/2022 07:33:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=73
05/29/2022 07:33:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=74
05/29/2022 07:33:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=74
05/29/2022 07:33:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=74
05/29/2022 07:33:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=74
05/29/2022 07:33:54 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.8158108640764274 on epoch=74
05/29/2022 07:33:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=75
05/29/2022 07:33:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=75
05/29/2022 07:34:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=75
05/29/2022 07:34:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=76
05/29/2022 07:34:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=76
05/29/2022 07:34:14 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.8149034032571598 on epoch=76
05/29/2022 07:34:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=76
05/29/2022 07:34:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=77
05/29/2022 07:34:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=77
05/29/2022 07:34:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=77
05/29/2022 07:34:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=78
05/29/2022 07:34:33 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.808625388139752 on epoch=78
05/29/2022 07:34:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=78
05/29/2022 07:34:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=78
05/29/2022 07:34:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=79
05/29/2022 07:34:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/29/2022 07:34:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=79
05/29/2022 07:34:53 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.8190439502890405 on epoch=79
05/29/2022 07:34:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=79
05/29/2022 07:34:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=80
05/29/2022 07:35:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=80
05/29/2022 07:35:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=80
05/29/2022 07:35:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=81
05/29/2022 07:35:12 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.809121356710233 on epoch=81
05/29/2022 07:35:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=81
05/29/2022 07:35:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
05/29/2022 07:35:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=82
05/29/2022 07:35:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=82
05/29/2022 07:35:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=82
05/29/2022 07:35:32 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.8135915168750822 on epoch=82
05/29/2022 07:35:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=83
05/29/2022 07:35:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=83
05/29/2022 07:35:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=83
05/29/2022 07:35:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=84
05/29/2022 07:35:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=84
05/29/2022 07:35:51 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.8332754177738289 on epoch=84
05/29/2022 07:35:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=84
05/29/2022 07:35:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=84
05/29/2022 07:35:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=85
05/29/2022 07:36:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=85
05/29/2022 07:36:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=85
05/29/2022 07:36:11 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.8252726919406609 on epoch=85
05/29/2022 07:36:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=86
05/29/2022 07:36:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=86
05/29/2022 07:36:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=86
05/29/2022 07:36:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=87
05/29/2022 07:36:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=87
05/29/2022 07:36:30 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.8333154692601887 on epoch=87
05/29/2022 07:36:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=87
05/29/2022 07:36:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=88
05/29/2022 07:36:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=88
05/29/2022 07:36:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=88
05/29/2022 07:36:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=89
05/29/2022 07:36:50 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.8322397516501195 on epoch=89
05/29/2022 07:36:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=89
05/29/2022 07:36:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=89
05/29/2022 07:36:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=89
05/29/2022 07:37:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=90
05/29/2022 07:37:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=90
05/29/2022 07:37:09 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.8191224875399339 on epoch=90
05/29/2022 07:37:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=90
05/29/2022 07:37:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=91
05/29/2022 07:37:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=91
05/29/2022 07:37:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=91
05/29/2022 07:37:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=92
05/29/2022 07:37:29 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.8173530234404439 on epoch=92
05/29/2022 07:37:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=92
05/29/2022 07:37:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=92
05/29/2022 07:37:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/29/2022 07:37:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=93
05/29/2022 07:37:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=93
05/29/2022 07:37:43 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:37:43 - INFO - __main__ - Printing 3 examples
05/29/2022 07:37:43 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 07:37:43 - INFO - __main__ - ['others']
05/29/2022 07:37:43 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 07:37:43 - INFO - __main__ - ['others']
05/29/2022 07:37:43 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 07:37:43 - INFO - __main__ - ['others']
05/29/2022 07:37:43 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:37:43 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:37:44 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 07:37:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:37:44 - INFO - __main__ - Printing 3 examples
05/29/2022 07:37:44 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 07:37:44 - INFO - __main__ - ['others']
05/29/2022 07:37:44 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 07:37:44 - INFO - __main__ - ['others']
05/29/2022 07:37:44 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 07:37:44 - INFO - __main__ - ['others']
05/29/2022 07:37:44 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:37:44 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:37:44 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 07:37:48 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.8288933567316245 on epoch=93
05/29/2022 07:37:48 - INFO - __main__ - save last model!
05/29/2022 07:37:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 07:37:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 07:37:48 - INFO - __main__ - Printing 3 examples
05/29/2022 07:37:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 07:37:48 - INFO - __main__ - ['others']
05/29/2022 07:37:48 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 07:37:48 - INFO - __main__ - ['others']
05/29/2022 07:37:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 07:37:48 - INFO - __main__ - ['others']
05/29/2022 07:37:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:37:51 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:37:56 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 07:38:01 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 07:38:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 07:38:01 - INFO - __main__ - Starting training!
05/29/2022 07:39:09 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_87_0.5_8_predictions.txt
05/29/2022 07:39:09 - INFO - __main__ - Classification-F1 on test data: 0.5311
05/29/2022 07:39:10 - INFO - __main__ - prefix=emo_128_87, lr=0.5, bsz=8, dev_performance=0.8371780845587529, test_performance=0.5311267495616843
05/29/2022 07:39:10 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.4, bsz=8 ...
05/29/2022 07:39:11 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:39:11 - INFO - __main__ - Printing 3 examples
05/29/2022 07:39:11 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 07:39:11 - INFO - __main__ - ['others']
05/29/2022 07:39:11 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 07:39:11 - INFO - __main__ - ['others']
05/29/2022 07:39:11 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 07:39:11 - INFO - __main__ - ['others']
05/29/2022 07:39:11 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:39:11 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:39:12 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 07:39:12 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:39:12 - INFO - __main__ - Printing 3 examples
05/29/2022 07:39:12 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 07:39:12 - INFO - __main__ - ['others']
05/29/2022 07:39:12 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 07:39:12 - INFO - __main__ - ['others']
05/29/2022 07:39:12 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 07:39:12 - INFO - __main__ - ['others']
05/29/2022 07:39:12 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:39:12 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:39:12 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 07:39:28 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 07:39:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 07:39:28 - INFO - __main__ - Starting training!
05/29/2022 07:39:31 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=0
05/29/2022 07:39:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.02 on epoch=0
05/29/2022 07:39:36 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=0
05/29/2022 07:39:39 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=1
05/29/2022 07:39:41 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=1
05/29/2022 07:39:49 - INFO - __main__ - Global step 50 Train loss 2.70 Classification-F1 0.17970072673977242 on epoch=1
05/29/2022 07:39:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17970072673977242 on epoch=1, global_step=50
05/29/2022 07:39:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=1
05/29/2022 07:39:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.27 on epoch=2
05/29/2022 07:39:56 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=2
05/29/2022 07:39:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=2
05/29/2022 07:40:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=3
05/29/2022 07:40:08 - INFO - __main__ - Global step 100 Train loss 1.16 Classification-F1 0.4687916782380027 on epoch=3
05/29/2022 07:40:08 - INFO - __main__ - Saving model with best Classification-F1: 0.17970072673977242 -> 0.4687916782380027 on epoch=3, global_step=100
05/29/2022 07:40:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.69 on epoch=3
05/29/2022 07:40:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.78 on epoch=3
05/29/2022 07:40:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=4
05/29/2022 07:40:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=4
05/29/2022 07:40:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=4
05/29/2022 07:40:27 - INFO - __main__ - Global step 150 Train loss 0.76 Classification-F1 0.5745649667669792 on epoch=4
05/29/2022 07:40:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4687916782380027 -> 0.5745649667669792 on epoch=4, global_step=150
05/29/2022 07:40:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=4
05/29/2022 07:40:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=5
05/29/2022 07:40:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.74 on epoch=5
05/29/2022 07:40:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=5
05/29/2022 07:40:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.71 on epoch=6
05/29/2022 07:40:46 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5869628281288082 on epoch=6
05/29/2022 07:40:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5745649667669792 -> 0.5869628281288082 on epoch=6, global_step=200
05/29/2022 07:40:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=6
05/29/2022 07:40:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=6
05/29/2022 07:40:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.66 on epoch=7
05/29/2022 07:40:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=7
05/29/2022 07:40:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=7
05/29/2022 07:41:06 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.6118062454558979 on epoch=7
05/29/2022 07:41:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5869628281288082 -> 0.6118062454558979 on epoch=7, global_step=250
05/29/2022 07:41:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=8
05/29/2022 07:41:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=8
05/29/2022 07:41:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=8
05/29/2022 07:41:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=9
05/29/2022 07:41:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=9
05/29/2022 07:41:25 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.6644169079657071 on epoch=9
05/29/2022 07:41:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6118062454558979 -> 0.6644169079657071 on epoch=9, global_step=300
05/29/2022 07:41:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=9
05/29/2022 07:41:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.56 on epoch=9
05/29/2022 07:41:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=10
05/29/2022 07:41:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=10
05/29/2022 07:41:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=10
05/29/2022 07:41:45 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.7214248241565933 on epoch=10
05/29/2022 07:41:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6644169079657071 -> 0.7214248241565933 on epoch=10, global_step=350
05/29/2022 07:41:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=11
05/29/2022 07:41:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=11
05/29/2022 07:41:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=11
05/29/2022 07:41:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=12
05/29/2022 07:41:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=12
05/29/2022 07:42:04 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.7229259862459702 on epoch=12
05/29/2022 07:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7214248241565933 -> 0.7229259862459702 on epoch=12, global_step=400
05/29/2022 07:42:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=12
05/29/2022 07:42:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=13
05/29/2022 07:42:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=13
05/29/2022 07:42:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=13
05/29/2022 07:42:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=14
05/29/2022 07:42:23 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.7204374375912128 on epoch=14
05/29/2022 07:42:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=14
05/29/2022 07:42:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=14
05/29/2022 07:42:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=14
05/29/2022 07:42:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=15
05/29/2022 07:42:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=15
05/29/2022 07:42:43 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.7396919223285038 on epoch=15
05/29/2022 07:42:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7229259862459702 -> 0.7396919223285038 on epoch=15, global_step=500
05/29/2022 07:42:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=15
05/29/2022 07:42:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=16
05/29/2022 07:42:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=16
05/29/2022 07:42:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=16
05/29/2022 07:42:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=17
05/29/2022 07:43:02 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.728040683027717 on epoch=17
05/29/2022 07:43:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=17
05/29/2022 07:43:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=17
05/29/2022 07:43:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=18
05/29/2022 07:43:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=18
05/29/2022 07:43:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=18
05/29/2022 07:43:21 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.7858291581757 on epoch=18
05/29/2022 07:43:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7396919223285038 -> 0.7858291581757 on epoch=18, global_step=600
05/29/2022 07:43:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=19
05/29/2022 07:43:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=19
05/29/2022 07:43:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=19
05/29/2022 07:43:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=19
05/29/2022 07:43:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=20
05/29/2022 07:43:40 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.7748156260828583 on epoch=20
05/29/2022 07:43:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=20
05/29/2022 07:43:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=20
05/29/2022 07:43:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=21
05/29/2022 07:43:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=21
05/29/2022 07:43:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=21
05/29/2022 07:44:00 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.7765819027334465 on epoch=21
05/29/2022 07:44:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=22
05/29/2022 07:44:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=22
05/29/2022 07:44:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=22
05/29/2022 07:44:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=23
05/29/2022 07:44:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=23
05/29/2022 07:44:19 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.7903619321313711 on epoch=23
05/29/2022 07:44:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7858291581757 -> 0.7903619321313711 on epoch=23, global_step=750
05/29/2022 07:44:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=23
05/29/2022 07:44:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=24
05/29/2022 07:44:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=24
05/29/2022 07:44:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=24
05/29/2022 07:44:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=24
05/29/2022 07:44:38 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.8012813970311965 on epoch=24
05/29/2022 07:44:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7903619321313711 -> 0.8012813970311965 on epoch=24, global_step=800
05/29/2022 07:44:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=25
05/29/2022 07:44:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=25
05/29/2022 07:44:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=25
05/29/2022 07:44:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=26
05/29/2022 07:44:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=26
05/29/2022 07:44:58 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.7734083919225427 on epoch=26
05/29/2022 07:45:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=26
05/29/2022 07:45:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=27
05/29/2022 07:45:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=27
05/29/2022 07:45:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=27
05/29/2022 07:45:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=28
05/29/2022 07:45:17 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.782026017071837 on epoch=28
05/29/2022 07:45:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=28
05/29/2022 07:45:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=28
05/29/2022 07:45:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=29
05/29/2022 07:45:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=29
05/29/2022 07:45:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=29
05/29/2022 07:45:36 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.7868572722490486 on epoch=29
05/29/2022 07:45:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=29
05/29/2022 07:45:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=30
05/29/2022 07:45:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=30
05/29/2022 07:45:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=30
05/29/2022 07:45:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=31
05/29/2022 07:45:55 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.7929810256538667 on epoch=31
05/29/2022 07:45:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=31
05/29/2022 07:46:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=31
05/29/2022 07:46:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=32
05/29/2022 07:46:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=32
05/29/2022 07:46:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=32
05/29/2022 07:46:15 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.767306509444756 on epoch=32
05/29/2022 07:46:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=33
05/29/2022 07:46:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=33
05/29/2022 07:46:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=33
05/29/2022 07:46:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=34
05/29/2022 07:46:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=34
05/29/2022 07:46:34 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.7974919750947066 on epoch=34
05/29/2022 07:46:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=34
05/29/2022 07:46:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=34
05/29/2022 07:46:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=35
05/29/2022 07:46:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=35
05/29/2022 07:46:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=35
05/29/2022 07:46:53 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.7988248836034814 on epoch=35
05/29/2022 07:46:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=36
05/29/2022 07:46:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=36
05/29/2022 07:47:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=36
05/29/2022 07:47:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=37
05/29/2022 07:47:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=37
05/29/2022 07:47:13 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.8056432327486813 on epoch=37
05/29/2022 07:47:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8012813970311965 -> 0.8056432327486813 on epoch=37, global_step=1200
05/29/2022 07:47:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=37
05/29/2022 07:47:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=38
05/29/2022 07:47:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=38
05/29/2022 07:47:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=38
05/29/2022 07:47:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=39
05/29/2022 07:47:32 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.8202010069376482 on epoch=39
05/29/2022 07:47:32 - INFO - __main__ - Saving model with best Classification-F1: 0.8056432327486813 -> 0.8202010069376482 on epoch=39, global_step=1250
05/29/2022 07:47:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=39
05/29/2022 07:47:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=39
05/29/2022 07:47:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=39
05/29/2022 07:47:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=40
05/29/2022 07:47:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=40
05/29/2022 07:47:51 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.8073556068792193 on epoch=40
05/29/2022 07:47:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=40
05/29/2022 07:47:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=41
05/29/2022 07:47:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=41
05/29/2022 07:48:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=41
05/29/2022 07:48:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=42
05/29/2022 07:48:11 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.8059822575492718 on epoch=42
05/29/2022 07:48:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=42
05/29/2022 07:48:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=42
05/29/2022 07:48:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=43
05/29/2022 07:48:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=43
05/29/2022 07:48:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=43
05/29/2022 07:48:30 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.8081694875314515 on epoch=43
05/29/2022 07:48:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=44
05/29/2022 07:48:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=44
05/29/2022 07:48:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=44
05/29/2022 07:48:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=44
05/29/2022 07:48:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=45
05/29/2022 07:48:50 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.8244269337535806 on epoch=45
05/29/2022 07:48:50 - INFO - __main__ - Saving model with best Classification-F1: 0.8202010069376482 -> 0.8244269337535806 on epoch=45, global_step=1450
05/29/2022 07:48:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=45
05/29/2022 07:48:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=45
05/29/2022 07:48:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=46
05/29/2022 07:49:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=46
05/29/2022 07:49:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=46
05/29/2022 07:49:09 - INFO - __main__ - Global step 1500 Train loss 0.25 Classification-F1 0.809288479886821 on epoch=46
05/29/2022 07:49:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=47
05/29/2022 07:49:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=47
05/29/2022 07:49:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=47
05/29/2022 07:49:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=48
05/29/2022 07:49:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=48
05/29/2022 07:49:28 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.8107174777641872 on epoch=48
05/29/2022 07:49:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=48
05/29/2022 07:49:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=49
05/29/2022 07:49:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=49
05/29/2022 07:49:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=49
05/29/2022 07:49:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=49
05/29/2022 07:49:48 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.8192404214827116 on epoch=49
05/29/2022 07:49:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=50
05/29/2022 07:49:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=50
05/29/2022 07:49:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=50
05/29/2022 07:49:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=51
05/29/2022 07:50:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=51
05/29/2022 07:50:07 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.8051961344837923 on epoch=51
05/29/2022 07:50:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=51
05/29/2022 07:50:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=52
05/29/2022 07:50:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=52
05/29/2022 07:50:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=52
05/29/2022 07:50:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=53
05/29/2022 07:50:26 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.8283409567371831 on epoch=53
05/29/2022 07:50:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8244269337535806 -> 0.8283409567371831 on epoch=53, global_step=1700
05/29/2022 07:50:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=53
05/29/2022 07:50:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=53
05/29/2022 07:50:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=54
05/29/2022 07:50:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=54
05/29/2022 07:50:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=54
05/29/2022 07:50:46 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.8187606148931281 on epoch=54
05/29/2022 07:50:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=54
05/29/2022 07:50:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=55
05/29/2022 07:50:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=55
05/29/2022 07:50:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=55
05/29/2022 07:50:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=56
05/29/2022 07:51:05 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.8134194319734401 on epoch=56
05/29/2022 07:51:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=56
05/29/2022 07:51:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=56
05/29/2022 07:51:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=57
05/29/2022 07:51:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=57
05/29/2022 07:51:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=57
05/29/2022 07:51:24 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.8259640387926935 on epoch=57
05/29/2022 07:51:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=58
05/29/2022 07:51:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=58
05/29/2022 07:51:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=58
05/29/2022 07:51:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=59
05/29/2022 07:51:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=59
05/29/2022 07:51:44 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.817598387345469 on epoch=59
05/29/2022 07:51:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=59
05/29/2022 07:51:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=59
05/29/2022 07:51:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=60
05/29/2022 07:51:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=60
05/29/2022 07:51:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=60
05/29/2022 07:52:03 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.828787772410773 on epoch=60
05/29/2022 07:52:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8283409567371831 -> 0.828787772410773 on epoch=60, global_step=1950
05/29/2022 07:52:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=61
05/29/2022 07:52:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=61
05/29/2022 07:52:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=61
05/29/2022 07:52:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=62
05/29/2022 07:52:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=62
05/29/2022 07:52:22 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.8328630459390486 on epoch=62
05/29/2022 07:52:22 - INFO - __main__ - Saving model with best Classification-F1: 0.828787772410773 -> 0.8328630459390486 on epoch=62, global_step=2000
05/29/2022 07:52:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=62
05/29/2022 07:52:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=63
05/29/2022 07:52:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=63
05/29/2022 07:52:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=63
05/29/2022 07:52:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=64
05/29/2022 07:52:42 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.8198008258347743 on epoch=64
05/29/2022 07:52:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=64
05/29/2022 07:52:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=64
05/29/2022 07:52:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=64
05/29/2022 07:52:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=65
05/29/2022 07:52:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=65
05/29/2022 07:53:02 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.822024688973493 on epoch=65
05/29/2022 07:53:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=65
05/29/2022 07:53:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=66
05/29/2022 07:53:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=66
05/29/2022 07:53:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=66
05/29/2022 07:53:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=67
05/29/2022 07:53:21 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.8155715197526445 on epoch=67
05/29/2022 07:53:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=67
05/29/2022 07:53:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=67
05/29/2022 07:53:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=68
05/29/2022 07:53:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=68
05/29/2022 07:53:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=68
05/29/2022 07:53:40 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.8279612114751433 on epoch=68
05/29/2022 07:53:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=69
05/29/2022 07:53:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=69
05/29/2022 07:53:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=69
05/29/2022 07:53:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=69
05/29/2022 07:53:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=70
05/29/2022 07:54:00 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.8265085824318396 on epoch=70
05/29/2022 07:54:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=70
05/29/2022 07:54:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 07:54:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=71
05/29/2022 07:54:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=71
05/29/2022 07:54:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=71
05/29/2022 07:54:19 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.8154785305769395 on epoch=71
05/29/2022 07:54:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=72
05/29/2022 07:54:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=72
05/29/2022 07:54:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=72
05/29/2022 07:54:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=73
05/29/2022 07:54:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=73
05/29/2022 07:54:39 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.815567175332491 on epoch=73
05/29/2022 07:54:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=73
05/29/2022 07:54:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=74
05/29/2022 07:54:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=74
05/29/2022 07:54:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=74
05/29/2022 07:54:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=74
05/29/2022 07:54:58 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.831282520843484 on epoch=74
05/29/2022 07:55:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=75
05/29/2022 07:55:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=75
05/29/2022 07:55:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=75
05/29/2022 07:55:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=76
05/29/2022 07:55:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=76
05/29/2022 07:55:17 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.827206025004231 on epoch=76
05/29/2022 07:55:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=76
05/29/2022 07:55:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=77
05/29/2022 07:55:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=77
05/29/2022 07:55:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=77
05/29/2022 07:55:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=78
05/29/2022 07:55:37 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.8252246487933454 on epoch=78
05/29/2022 07:55:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=78
05/29/2022 07:55:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=78
05/29/2022 07:55:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=79
05/29/2022 07:55:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=79
05/29/2022 07:55:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=79
05/29/2022 07:55:56 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.8311401627836639 on epoch=79
05/29/2022 07:55:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=79
05/29/2022 07:56:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=80
05/29/2022 07:56:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=80
05/29/2022 07:56:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=80
05/29/2022 07:56:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=81
05/29/2022 07:56:15 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.8218016800470774 on epoch=81
05/29/2022 07:56:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=81
05/29/2022 07:56:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=81
05/29/2022 07:56:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=82
05/29/2022 07:56:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=82
05/29/2022 07:56:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=82
05/29/2022 07:56:35 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.814514332031655 on epoch=82
05/29/2022 07:56:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=83
05/29/2022 07:56:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=83
05/29/2022 07:56:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=83
05/29/2022 07:56:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=84
05/29/2022 07:56:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=84
05/29/2022 07:56:54 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.82738030613622 on epoch=84
05/29/2022 07:56:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=84
05/29/2022 07:56:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=84
05/29/2022 07:57:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=85
05/29/2022 07:57:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=85
05/29/2022 07:57:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=85
05/29/2022 07:57:13 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.8302902902902902 on epoch=85
05/29/2022 07:57:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=86
05/29/2022 07:57:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=86
05/29/2022 07:57:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=86
05/29/2022 07:57:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=87
05/29/2022 07:57:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=87
05/29/2022 07:57:32 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.8262842615408206 on epoch=87
05/29/2022 07:57:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=87
05/29/2022 07:57:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=88
05/29/2022 07:57:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=88
05/29/2022 07:57:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=88
05/29/2022 07:57:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=89
05/29/2022 07:57:51 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.8355655548119709 on epoch=89
05/29/2022 07:57:51 - INFO - __main__ - Saving model with best Classification-F1: 0.8328630459390486 -> 0.8355655548119709 on epoch=89, global_step=2850
05/29/2022 07:57:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=89
05/29/2022 07:57:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=89
05/29/2022 07:57:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=89
05/29/2022 07:58:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=90
05/29/2022 07:58:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=90
05/29/2022 07:58:11 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.8213941342776973 on epoch=90
05/29/2022 07:58:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=90
05/29/2022 07:58:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=91
05/29/2022 07:58:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=91
05/29/2022 07:58:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=91
05/29/2022 07:58:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=92
05/29/2022 07:58:30 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.823601676755294 on epoch=92
05/29/2022 07:58:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=92
05/29/2022 07:58:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=92
05/29/2022 07:58:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=93
05/29/2022 07:58:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=93
05/29/2022 07:58:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=93
05/29/2022 07:58:44 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:58:44 - INFO - __main__ - Printing 3 examples
05/29/2022 07:58:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 07:58:44 - INFO - __main__ - ['others']
05/29/2022 07:58:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 07:58:44 - INFO - __main__ - ['others']
05/29/2022 07:58:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 07:58:44 - INFO - __main__ - ['others']
05/29/2022 07:58:44 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:58:44 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:58:45 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 07:58:45 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 07:58:45 - INFO - __main__ - Printing 3 examples
05/29/2022 07:58:45 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 07:58:45 - INFO - __main__ - ['others']
05/29/2022 07:58:45 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 07:58:45 - INFO - __main__ - ['others']
05/29/2022 07:58:45 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 07:58:45 - INFO - __main__ - ['others']
05/29/2022 07:58:45 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:58:45 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:58:45 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 07:58:49 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.8197690990271885 on epoch=93
05/29/2022 07:58:49 - INFO - __main__ - save last model!
05/29/2022 07:58:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 07:58:50 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 07:58:50 - INFO - __main__ - Printing 3 examples
05/29/2022 07:58:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 07:58:50 - INFO - __main__ - ['others']
05/29/2022 07:58:50 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 07:58:50 - INFO - __main__ - ['others']
05/29/2022 07:58:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 07:58:50 - INFO - __main__ - ['others']
05/29/2022 07:58:50 - INFO - __main__ - Tokenizing Input ...
05/29/2022 07:58:52 - INFO - __main__ - Tokenizing Output ...
05/29/2022 07:58:57 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 07:59:03 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 07:59:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 07:59:04 - INFO - __main__ - Starting training!
05/29/2022 08:00:10 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_87_0.4_8_predictions.txt
05/29/2022 08:00:10 - INFO - __main__ - Classification-F1 on test data: 0.5019
05/29/2022 08:00:10 - INFO - __main__ - prefix=emo_128_87, lr=0.4, bsz=8, dev_performance=0.8355655548119709, test_performance=0.5018816493968474
05/29/2022 08:00:10 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.3, bsz=8 ...
05/29/2022 08:00:11 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:00:11 - INFO - __main__ - Printing 3 examples
05/29/2022 08:00:11 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 08:00:11 - INFO - __main__ - ['others']
05/29/2022 08:00:11 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 08:00:11 - INFO - __main__ - ['others']
05/29/2022 08:00:11 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 08:00:11 - INFO - __main__ - ['others']
05/29/2022 08:00:11 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:00:11 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:00:12 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 08:00:12 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:00:12 - INFO - __main__ - Printing 3 examples
05/29/2022 08:00:12 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 08:00:12 - INFO - __main__ - ['others']
05/29/2022 08:00:12 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 08:00:12 - INFO - __main__ - ['others']
05/29/2022 08:00:12 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 08:00:12 - INFO - __main__ - ['others']
05/29/2022 08:00:12 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:00:12 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:00:13 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 08:00:28 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 08:00:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 08:00:28 - INFO - __main__ - Starting training!
05/29/2022 08:00:32 - INFO - __main__ - Step 10 Global step 10 Train loss 4.35 on epoch=0
05/29/2022 08:00:34 - INFO - __main__ - Step 20 Global step 20 Train loss 3.36 on epoch=0
05/29/2022 08:00:37 - INFO - __main__ - Step 30 Global step 30 Train loss 2.90 on epoch=0
05/29/2022 08:00:39 - INFO - __main__ - Step 40 Global step 40 Train loss 2.41 on epoch=1
05/29/2022 08:00:42 - INFO - __main__ - Step 50 Global step 50 Train loss 2.23 on epoch=1
05/29/2022 08:00:49 - INFO - __main__ - Global step 50 Train loss 3.05 Classification-F1 0.0655700348486142 on epoch=1
05/29/2022 08:00:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0655700348486142 on epoch=1, global_step=50
05/29/2022 08:00:52 - INFO - __main__ - Step 60 Global step 60 Train loss 2.08 on epoch=1
05/29/2022 08:00:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.64 on epoch=2
05/29/2022 08:00:57 - INFO - __main__ - Step 80 Global step 80 Train loss 1.53 on epoch=2
05/29/2022 08:00:59 - INFO - __main__ - Step 90 Global step 90 Train loss 1.13 on epoch=2
05/29/2022 08:01:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=3
05/29/2022 08:01:08 - INFO - __main__ - Global step 100 Train loss 1.48 Classification-F1 0.3817461236432825 on epoch=3
05/29/2022 08:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.0655700348486142 -> 0.3817461236432825 on epoch=3, global_step=100
05/29/2022 08:01:11 - INFO - __main__ - Step 110 Global step 110 Train loss 1.08 on epoch=3
05/29/2022 08:01:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=3
05/29/2022 08:01:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=4
05/29/2022 08:01:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=4
05/29/2022 08:01:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.70 on epoch=4
05/29/2022 08:01:28 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.5209780589561677 on epoch=4
05/29/2022 08:01:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3817461236432825 -> 0.5209780589561677 on epoch=4, global_step=150
05/29/2022 08:01:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=4
05/29/2022 08:01:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.76 on epoch=5
05/29/2022 08:01:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=5
05/29/2022 08:01:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.79 on epoch=5
05/29/2022 08:01:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=6
05/29/2022 08:01:47 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.49942247910514165 on epoch=6
05/29/2022 08:01:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.85 on epoch=6
05/29/2022 08:01:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=6
05/29/2022 08:01:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.69 on epoch=7
05/29/2022 08:01:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.72 on epoch=7
05/29/2022 08:02:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=7
05/29/2022 08:02:07 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.5466356251310233 on epoch=7
05/29/2022 08:02:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5209780589561677 -> 0.5466356251310233 on epoch=7, global_step=250
05/29/2022 08:02:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=8
05/29/2022 08:02:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.70 on epoch=8
05/29/2022 08:02:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=8
05/29/2022 08:02:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=9
05/29/2022 08:02:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=9
05/29/2022 08:02:26 - INFO - __main__ - Global step 300 Train loss 0.63 Classification-F1 0.613210574934379 on epoch=9
05/29/2022 08:02:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5466356251310233 -> 0.613210574934379 on epoch=9, global_step=300
05/29/2022 08:02:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=9
05/29/2022 08:02:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=9
05/29/2022 08:02:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=10
05/29/2022 08:02:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=10
05/29/2022 08:02:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=10
05/29/2022 08:02:46 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.6339256888569181 on epoch=10
05/29/2022 08:02:46 - INFO - __main__ - Saving model with best Classification-F1: 0.613210574934379 -> 0.6339256888569181 on epoch=10, global_step=350
05/29/2022 08:02:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=11
05/29/2022 08:02:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.57 on epoch=11
05/29/2022 08:02:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=11
05/29/2022 08:02:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=12
05/29/2022 08:02:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.57 on epoch=12
05/29/2022 08:03:05 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.646667655444919 on epoch=12
05/29/2022 08:03:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6339256888569181 -> 0.646667655444919 on epoch=12, global_step=400
05/29/2022 08:03:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=12
05/29/2022 08:03:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=13
05/29/2022 08:03:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=13
05/29/2022 08:03:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=13
05/29/2022 08:03:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=14
05/29/2022 08:03:24 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.6918172781740999 on epoch=14
05/29/2022 08:03:24 - INFO - __main__ - Saving model with best Classification-F1: 0.646667655444919 -> 0.6918172781740999 on epoch=14, global_step=450
05/29/2022 08:03:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=14
05/29/2022 08:03:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=14
05/29/2022 08:03:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=14
05/29/2022 08:03:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=15
05/29/2022 08:03:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=15
05/29/2022 08:03:44 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.6900546448087432 on epoch=15
05/29/2022 08:03:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=15
05/29/2022 08:03:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=16
05/29/2022 08:03:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=16
05/29/2022 08:03:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=16
05/29/2022 08:03:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=17
05/29/2022 08:04:03 - INFO - __main__ - Global step 550 Train loss 0.53 Classification-F1 0.7192880360545715 on epoch=17
05/29/2022 08:04:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6918172781740999 -> 0.7192880360545715 on epoch=17, global_step=550
05/29/2022 08:04:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=17
05/29/2022 08:04:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=17
05/29/2022 08:04:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=18
05/29/2022 08:04:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=18
05/29/2022 08:04:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=18
05/29/2022 08:04:22 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.7621425839261693 on epoch=18
05/29/2022 08:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7192880360545715 -> 0.7621425839261693 on epoch=18, global_step=600
05/29/2022 08:04:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=19
05/29/2022 08:04:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=19
05/29/2022 08:04:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=19
05/29/2022 08:04:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=19
05/29/2022 08:04:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=20
05/29/2022 08:04:42 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.7125666058872451 on epoch=20
05/29/2022 08:04:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=20
05/29/2022 08:04:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=20
05/29/2022 08:04:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=21
05/29/2022 08:04:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=21
05/29/2022 08:04:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=21
05/29/2022 08:05:01 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.759013204039075 on epoch=21
05/29/2022 08:05:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=22
05/29/2022 08:05:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=22
05/29/2022 08:05:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=22
05/29/2022 08:05:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=23
05/29/2022 08:05:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=23
05/29/2022 08:05:20 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.7431287875774091 on epoch=23
05/29/2022 08:05:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=23
05/29/2022 08:05:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=24
05/29/2022 08:05:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=24
05/29/2022 08:05:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=24
05/29/2022 08:05:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=24
05/29/2022 08:05:40 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.7639825964676628 on epoch=24
05/29/2022 08:05:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7621425839261693 -> 0.7639825964676628 on epoch=24, global_step=800
05/29/2022 08:05:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=25
05/29/2022 08:05:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=25
05/29/2022 08:05:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=25
05/29/2022 08:05:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=26
05/29/2022 08:05:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=26
05/29/2022 08:05:59 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.7479754026510003 on epoch=26
05/29/2022 08:06:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=26
05/29/2022 08:06:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=27
05/29/2022 08:06:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=27
05/29/2022 08:06:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=27
05/29/2022 08:06:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=28
05/29/2022 08:06:19 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.7735248055537775 on epoch=28
05/29/2022 08:06:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7639825964676628 -> 0.7735248055537775 on epoch=28, global_step=900
05/29/2022 08:06:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=28
05/29/2022 08:06:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=28
05/29/2022 08:06:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=29
05/29/2022 08:06:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=29
05/29/2022 08:06:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=29
05/29/2022 08:06:38 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7771157551430317 on epoch=29
05/29/2022 08:06:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7735248055537775 -> 0.7771157551430317 on epoch=29, global_step=950
05/29/2022 08:06:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.28 on epoch=29
05/29/2022 08:06:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=30
05/29/2022 08:06:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=30
05/29/2022 08:06:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=30
05/29/2022 08:06:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=31
05/29/2022 08:06:58 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.7799275111053916 on epoch=31
05/29/2022 08:06:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7771157551430317 -> 0.7799275111053916 on epoch=31, global_step=1000
05/29/2022 08:07:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=31
05/29/2022 08:07:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=31
05/29/2022 08:07:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=32
05/29/2022 08:07:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=32
05/29/2022 08:07:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=32
05/29/2022 08:07:17 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.7881231533360618 on epoch=32
05/29/2022 08:07:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7799275111053916 -> 0.7881231533360618 on epoch=32, global_step=1050
05/29/2022 08:07:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=33
05/29/2022 08:07:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=33
05/29/2022 08:07:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=33
05/29/2022 08:07:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=34
05/29/2022 08:07:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=34
05/29/2022 08:07:37 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.8053548260183597 on epoch=34
05/29/2022 08:07:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7881231533360618 -> 0.8053548260183597 on epoch=34, global_step=1100
05/29/2022 08:07:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=34
05/29/2022 08:07:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=34
05/29/2022 08:07:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=35
05/29/2022 08:07:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=35
05/29/2022 08:07:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=35
05/29/2022 08:07:56 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.789758655647783 on epoch=35
05/29/2022 08:07:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=36
05/29/2022 08:08:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=36
05/29/2022 08:08:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=36
05/29/2022 08:08:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=37
05/29/2022 08:08:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=37
05/29/2022 08:08:15 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.8008531410896482 on epoch=37
05/29/2022 08:08:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=37
05/29/2022 08:08:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=38
05/29/2022 08:08:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=38
05/29/2022 08:08:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=38
05/29/2022 08:08:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=39
05/29/2022 08:08:35 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.8003947402563204 on epoch=39
05/29/2022 08:08:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=39
05/29/2022 08:08:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=39
05/29/2022 08:08:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=39
05/29/2022 08:08:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=40
05/29/2022 08:08:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=40
05/29/2022 08:08:54 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.8066880189460802 on epoch=40
05/29/2022 08:08:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8053548260183597 -> 0.8066880189460802 on epoch=40, global_step=1300
05/29/2022 08:08:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=40
05/29/2022 08:08:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=41
05/29/2022 08:09:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=41
05/29/2022 08:09:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=41
05/29/2022 08:09:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=42
05/29/2022 08:09:14 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.8035273524635228 on epoch=42
05/29/2022 08:09:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=42
05/29/2022 08:09:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=42
05/29/2022 08:09:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=43
05/29/2022 08:09:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=43
05/29/2022 08:09:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=43
05/29/2022 08:09:33 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7890472918813695 on epoch=43
05/29/2022 08:09:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=44
05/29/2022 08:09:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=44
05/29/2022 08:09:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=44
05/29/2022 08:09:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=44
05/29/2022 08:09:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=45
05/29/2022 08:09:52 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.8265305357410621 on epoch=45
05/29/2022 08:09:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8066880189460802 -> 0.8265305357410621 on epoch=45, global_step=1450
05/29/2022 08:09:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=45
05/29/2022 08:09:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=45
05/29/2022 08:10:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=46
05/29/2022 08:10:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=46
05/29/2022 08:10:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=46
05/29/2022 08:10:12 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.6514521046887334 on epoch=46
05/29/2022 08:10:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=47
05/29/2022 08:10:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=47
05/29/2022 08:10:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=47
05/29/2022 08:10:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=48
05/29/2022 08:10:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=48
05/29/2022 08:10:31 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.8225720667606146 on epoch=48
05/29/2022 08:10:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=48
05/29/2022 08:10:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=49
05/29/2022 08:10:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=49
05/29/2022 08:10:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=49
05/29/2022 08:10:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=49
05/29/2022 08:10:51 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.805736753848477 on epoch=49
05/29/2022 08:10:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=50
05/29/2022 08:10:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=50
05/29/2022 08:10:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=50
05/29/2022 08:11:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=51
05/29/2022 08:11:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=51
05/29/2022 08:11:10 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.8278077200939655 on epoch=51
05/29/2022 08:11:10 - INFO - __main__ - Saving model with best Classification-F1: 0.8265305357410621 -> 0.8278077200939655 on epoch=51, global_step=1650
05/29/2022 08:11:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=51
05/29/2022 08:11:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=52
05/29/2022 08:11:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=52
05/29/2022 08:11:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=52
05/29/2022 08:11:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=53
05/29/2022 08:11:29 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.8057014795816683 on epoch=53
05/29/2022 08:11:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=53
05/29/2022 08:11:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=53
05/29/2022 08:11:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=54
05/29/2022 08:11:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=54
05/29/2022 08:11:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=54
05/29/2022 08:11:49 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.8288209599473568 on epoch=54
05/29/2022 08:11:49 - INFO - __main__ - Saving model with best Classification-F1: 0.8278077200939655 -> 0.8288209599473568 on epoch=54, global_step=1750
05/29/2022 08:11:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/29/2022 08:11:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=55
05/29/2022 08:11:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.31 on epoch=55
05/29/2022 08:11:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=55
05/29/2022 08:12:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=56
05/29/2022 08:12:08 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.8254610416319677 on epoch=56
05/29/2022 08:12:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=56
05/29/2022 08:12:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=56
05/29/2022 08:12:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=57
05/29/2022 08:12:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=57
05/29/2022 08:12:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=57
05/29/2022 08:12:28 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7991306321806122 on epoch=57
05/29/2022 08:12:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=58
05/29/2022 08:12:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=58
05/29/2022 08:12:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=58
05/29/2022 08:12:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=59
05/29/2022 08:12:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=59
05/29/2022 08:12:47 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.8150797886128298 on epoch=59
05/29/2022 08:12:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=59
05/29/2022 08:12:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=59
05/29/2022 08:12:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=60
05/29/2022 08:12:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=60
05/29/2022 08:12:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=60
05/29/2022 08:13:06 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.8089976607242377 on epoch=60
05/29/2022 08:13:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=61
05/29/2022 08:13:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=61
05/29/2022 08:13:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=61
05/29/2022 08:13:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=62
05/29/2022 08:13:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=62
05/29/2022 08:13:26 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.8288417587827396 on epoch=62
05/29/2022 08:13:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8288209599473568 -> 0.8288417587827396 on epoch=62, global_step=2000
05/29/2022 08:13:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=62
05/29/2022 08:13:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=63
05/29/2022 08:13:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=63
05/29/2022 08:13:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=63
05/29/2022 08:13:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=64
05/29/2022 08:13:45 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.8164334368330497 on epoch=64
05/29/2022 08:13:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.26 on epoch=64
05/29/2022 08:13:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=64
05/29/2022 08:13:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=64
05/29/2022 08:13:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=65
05/29/2022 08:13:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=65
05/29/2022 08:14:05 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.8110649372506251 on epoch=65
05/29/2022 08:14:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=65
05/29/2022 08:14:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=66
05/29/2022 08:14:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=66
05/29/2022 08:14:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=66
05/29/2022 08:14:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.25 on epoch=67
05/29/2022 08:14:24 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.8014559774781811 on epoch=67
05/29/2022 08:14:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=67
05/29/2022 08:14:29 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.20 on epoch=67
05/29/2022 08:14:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=68
05/29/2022 08:14:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.32 on epoch=68
05/29/2022 08:14:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=68
05/29/2022 08:14:44 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.8346407851530124 on epoch=68
05/29/2022 08:14:44 - INFO - __main__ - Saving model with best Classification-F1: 0.8288417587827396 -> 0.8346407851530124 on epoch=68, global_step=2200
05/29/2022 08:14:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=69
05/29/2022 08:14:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=69
05/29/2022 08:14:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=69
05/29/2022 08:14:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=69
05/29/2022 08:14:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=70
05/29/2022 08:15:03 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.8150512979242536 on epoch=70
05/29/2022 08:15:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=70
05/29/2022 08:15:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=70
05/29/2022 08:15:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=71
05/29/2022 08:15:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=71
05/29/2022 08:15:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=71
05/29/2022 08:15:23 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.8119849430579122 on epoch=71
05/29/2022 08:15:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=72
05/29/2022 08:15:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=72
05/29/2022 08:15:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=72
05/29/2022 08:15:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=73
05/29/2022 08:15:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=73
05/29/2022 08:15:42 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.8359471837542892 on epoch=73
05/29/2022 08:15:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8346407851530124 -> 0.8359471837542892 on epoch=73, global_step=2350
05/29/2022 08:15:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=73
05/29/2022 08:15:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=74
05/29/2022 08:15:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=74
05/29/2022 08:15:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=74
05/29/2022 08:15:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=74
05/29/2022 08:16:01 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.8300767797919222 on epoch=74
05/29/2022 08:16:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=75
05/29/2022 08:16:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=75
05/29/2022 08:16:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=75
05/29/2022 08:16:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=76
05/29/2022 08:16:14 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=76
05/29/2022 08:16:21 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.8359979419252058 on epoch=76
05/29/2022 08:16:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8359471837542892 -> 0.8359979419252058 on epoch=76, global_step=2450
05/29/2022 08:16:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=76
05/29/2022 08:16:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=77
05/29/2022 08:16:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=77
05/29/2022 08:16:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=77
05/29/2022 08:16:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=78
05/29/2022 08:16:40 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.827276044874699 on epoch=78
05/29/2022 08:16:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=78
05/29/2022 08:16:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=78
05/29/2022 08:16:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.25 on epoch=79
05/29/2022 08:16:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=79
05/29/2022 08:16:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=79
05/29/2022 08:16:59 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.8195974237177405 on epoch=79
05/29/2022 08:17:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=79
05/29/2022 08:17:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=80
05/29/2022 08:17:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=80
05/29/2022 08:17:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=80
05/29/2022 08:17:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=81
05/29/2022 08:17:19 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.827382797308295 on epoch=81
05/29/2022 08:17:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=81
05/29/2022 08:17:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=81
05/29/2022 08:17:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=82
05/29/2022 08:17:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=82
05/29/2022 08:17:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=82
05/29/2022 08:17:39 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.8129203763881184 on epoch=82
05/29/2022 08:17:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=83
05/29/2022 08:17:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=83
05/29/2022 08:17:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=83
05/29/2022 08:17:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.18 on epoch=84
05/29/2022 08:17:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=84
05/29/2022 08:17:58 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.8332093704363981 on epoch=84
05/29/2022 08:18:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=84
05/29/2022 08:18:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=84
05/29/2022 08:18:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=85
05/29/2022 08:18:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=85
05/29/2022 08:18:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=85
05/29/2022 08:18:17 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.8171537537573917 on epoch=85
05/29/2022 08:18:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=86
05/29/2022 08:18:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=86
05/29/2022 08:18:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=86
05/29/2022 08:18:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=87
05/29/2022 08:18:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=87
05/29/2022 08:18:37 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.8263429128078115 on epoch=87
05/29/2022 08:18:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=87
05/29/2022 08:18:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=88
05/29/2022 08:18:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.25 on epoch=88
05/29/2022 08:18:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=88
05/29/2022 08:18:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=89
05/29/2022 08:18:56 - INFO - __main__ - Global step 2850 Train loss 0.18 Classification-F1 0.830699639943117 on epoch=89
05/29/2022 08:18:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.21 on epoch=89
05/29/2022 08:19:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=89
05/29/2022 08:19:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=89
05/29/2022 08:19:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=90
05/29/2022 08:19:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=90
05/29/2022 08:19:15 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.8295397874103048 on epoch=90
05/29/2022 08:19:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=90
05/29/2022 08:19:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=91
05/29/2022 08:19:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=91
05/29/2022 08:19:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=91
05/29/2022 08:19:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=92
05/29/2022 08:19:35 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.8235526532912081 on epoch=92
05/29/2022 08:19:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=92
05/29/2022 08:19:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=92
05/29/2022 08:19:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=93
05/29/2022 08:19:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=93
05/29/2022 08:19:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=93
05/29/2022 08:19:48 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:19:48 - INFO - __main__ - Printing 3 examples
05/29/2022 08:19:48 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 08:19:48 - INFO - __main__ - ['others']
05/29/2022 08:19:48 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 08:19:48 - INFO - __main__ - ['others']
05/29/2022 08:19:48 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 08:19:48 - INFO - __main__ - ['others']
05/29/2022 08:19:48 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:19:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:19:49 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 08:19:49 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:19:49 - INFO - __main__ - Printing 3 examples
05/29/2022 08:19:49 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 08:19:49 - INFO - __main__ - ['others']
05/29/2022 08:19:49 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 08:19:49 - INFO - __main__ - ['others']
05/29/2022 08:19:49 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 08:19:49 - INFO - __main__ - ['others']
05/29/2022 08:19:49 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:19:49 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:19:50 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 08:19:54 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.8156353576511204 on epoch=93
05/29/2022 08:19:54 - INFO - __main__ - save last model!
05/29/2022 08:19:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 08:19:54 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 08:19:54 - INFO - __main__ - Printing 3 examples
05/29/2022 08:19:54 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 08:19:54 - INFO - __main__ - ['others']
05/29/2022 08:19:54 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 08:19:54 - INFO - __main__ - ['others']
05/29/2022 08:19:54 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 08:19:54 - INFO - __main__ - ['others']
05/29/2022 08:19:54 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:19:56 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:20:02 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 08:20:08 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 08:20:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 08:20:08 - INFO - __main__ - Starting training!
05/29/2022 08:21:15 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_87_0.3_8_predictions.txt
05/29/2022 08:21:15 - INFO - __main__ - Classification-F1 on test data: 0.4948
05/29/2022 08:21:16 - INFO - __main__ - prefix=emo_128_87, lr=0.3, bsz=8, dev_performance=0.8359979419252058, test_performance=0.49475650739209714
05/29/2022 08:21:16 - INFO - __main__ - Running ... prefix=emo_128_87, lr=0.2, bsz=8 ...
05/29/2022 08:21:17 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:21:17 - INFO - __main__ - Printing 3 examples
05/29/2022 08:21:17 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:21:17 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:21:17 - INFO - __main__ - Loaded 512 examples from train data
05/29/2022 08:21:17 - INFO - __main__ - Start tokenizing ... 512 instances
05/29/2022 08:21:17 - INFO - __main__ - Printing 3 examples
05/29/2022 08:21:17 - INFO - __main__ -  [emo] means we're you going i want to go when are you going but where
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ -  [emo] okay thanks  you're welcome smilingfacewithhearteyes not welcome if you a send piuture
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ -  [emo] than dont ever message me have followed you sorry about that wearyfacewearyface cool
05/29/2022 08:21:17 - INFO - __main__ - ['others']
05/29/2022 08:21:17 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:21:17 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:21:18 - INFO - __main__ - Loaded 512 examples from dev data
05/29/2022 08:21:37 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 08:21:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 08:21:38 - INFO - __main__ - Starting training!
05/29/2022 08:21:41 - INFO - __main__ - Step 10 Global step 10 Train loss 4.30 on epoch=0
05/29/2022 08:21:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.55 on epoch=0
05/29/2022 08:21:46 - INFO - __main__ - Step 30 Global step 30 Train loss 3.11 on epoch=0
05/29/2022 08:21:48 - INFO - __main__ - Step 40 Global step 40 Train loss 2.66 on epoch=1
05/29/2022 08:21:51 - INFO - __main__ - Step 50 Global step 50 Train loss 2.33 on epoch=1
05/29/2022 08:21:58 - INFO - __main__ - Global step 50 Train loss 3.19 Classification-F1 0.0032314452498556673 on epoch=1
05/29/2022 08:21:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0032314452498556673 on epoch=1, global_step=50
05/29/2022 08:22:01 - INFO - __main__ - Step 60 Global step 60 Train loss 2.36 on epoch=1
05/29/2022 08:22:03 - INFO - __main__ - Step 70 Global step 70 Train loss 1.96 on epoch=2
05/29/2022 08:22:06 - INFO - __main__ - Step 80 Global step 80 Train loss 2.01 on epoch=2
05/29/2022 08:22:08 - INFO - __main__ - Step 90 Global step 90 Train loss 1.59 on epoch=2
05/29/2022 08:22:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.65 on epoch=3
05/29/2022 08:22:18 - INFO - __main__ - Global step 100 Train loss 1.92 Classification-F1 0.18317925384957207 on epoch=3
05/29/2022 08:22:18 - INFO - __main__ - Saving model with best Classification-F1: 0.0032314452498556673 -> 0.18317925384957207 on epoch=3, global_step=100
05/29/2022 08:22:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.62 on epoch=3
05/29/2022 08:22:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.31 on epoch=3
05/29/2022 08:22:25 - INFO - __main__ - Step 130 Global step 130 Train loss 1.08 on epoch=4
05/29/2022 08:22:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.25 on epoch=4
05/29/2022 08:22:30 - INFO - __main__ - Step 150 Global step 150 Train loss 1.17 on epoch=4
05/29/2022 08:22:37 - INFO - __main__ - Global step 150 Train loss 1.29 Classification-F1 0.4163645194712725 on epoch=4
05/29/2022 08:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.18317925384957207 -> 0.4163645194712725 on epoch=4, global_step=150
05/29/2022 08:22:40 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=4
05/29/2022 08:22:42 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=5
05/29/2022 08:22:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=5
05/29/2022 08:22:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=5
05/29/2022 08:22:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=6
05/29/2022 08:22:57 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.5083171329761752 on epoch=6
05/29/2022 08:22:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4163645194712725 -> 0.5083171329761752 on epoch=6, global_step=200
05/29/2022 08:22:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=6
05/29/2022 08:23:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=6
05/29/2022 08:23:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.73 on epoch=7
05/29/2022 08:23:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=7
05/29/2022 08:23:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=7
05/29/2022 08:23:16 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5419921721326886 on epoch=7
05/29/2022 08:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5083171329761752 -> 0.5419921721326886 on epoch=7, global_step=250
05/29/2022 08:23:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.72 on epoch=8
05/29/2022 08:23:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.71 on epoch=8
05/29/2022 08:23:23 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=8
05/29/2022 08:23:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=9
05/29/2022 08:23:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=9
05/29/2022 08:23:35 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.530959564078422 on epoch=9
05/29/2022 08:23:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.78 on epoch=9
05/29/2022 08:23:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=9
05/29/2022 08:23:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=10
05/29/2022 08:23:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=10
05/29/2022 08:23:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=10
05/29/2022 08:23:55 - INFO - __main__ - Global step 350 Train loss 0.64 Classification-F1 0.606027287025142 on epoch=10
05/29/2022 08:23:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5419921721326886 -> 0.606027287025142 on epoch=10, global_step=350
05/29/2022 08:23:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=11
05/29/2022 08:24:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=11
05/29/2022 08:24:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.61 on epoch=11
05/29/2022 08:24:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.72 on epoch=12
05/29/2022 08:24:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=12
05/29/2022 08:24:14 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.6067324971273818 on epoch=12
05/29/2022 08:24:14 - INFO - __main__ - Saving model with best Classification-F1: 0.606027287025142 -> 0.6067324971273818 on epoch=12, global_step=400
05/29/2022 08:24:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.62 on epoch=12
05/29/2022 08:24:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=13
05/29/2022 08:24:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=13
05/29/2022 08:24:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=13
05/29/2022 08:24:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=14
05/29/2022 08:24:33 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.6076031248445041 on epoch=14
05/29/2022 08:24:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6067324971273818 -> 0.6076031248445041 on epoch=14, global_step=450
05/29/2022 08:24:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=14
05/29/2022 08:24:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.60 on epoch=14
05/29/2022 08:24:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=14
05/29/2022 08:24:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=15
05/29/2022 08:24:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=15
05/29/2022 08:24:53 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.6041535023443558 on epoch=15
05/29/2022 08:24:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=15
05/29/2022 08:24:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=16
05/29/2022 08:25:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=16
05/29/2022 08:25:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.67 on epoch=16
05/29/2022 08:25:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.65 on epoch=17
05/29/2022 08:25:12 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6169571597569943 on epoch=17
05/29/2022 08:25:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6076031248445041 -> 0.6169571597569943 on epoch=17, global_step=550
05/29/2022 08:25:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.60 on epoch=17
05/29/2022 08:25:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=17
05/29/2022 08:25:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.62 on epoch=18
05/29/2022 08:25:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=18
05/29/2022 08:25:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.60 on epoch=18
05/29/2022 08:25:32 - INFO - __main__ - Global step 600 Train loss 0.57 Classification-F1 0.6780097636641261 on epoch=18
05/29/2022 08:25:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6169571597569943 -> 0.6780097636641261 on epoch=18, global_step=600
05/29/2022 08:25:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=19
05/29/2022 08:25:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=19
05/29/2022 08:25:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=19
05/29/2022 08:25:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=19
05/29/2022 08:25:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=20
05/29/2022 08:25:51 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.6539005195487976 on epoch=20
05/29/2022 08:25:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=20
05/29/2022 08:25:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=20
05/29/2022 08:25:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=21
05/29/2022 08:26:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=21
05/29/2022 08:26:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=21
05/29/2022 08:26:11 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.6920842840835824 on epoch=21
05/29/2022 08:26:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6780097636641261 -> 0.6920842840835824 on epoch=21, global_step=700
05/29/2022 08:26:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=22
05/29/2022 08:26:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=22
05/29/2022 08:26:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=22
05/29/2022 08:26:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.55 on epoch=23
05/29/2022 08:26:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=23
05/29/2022 08:26:30 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.7219089339833882 on epoch=23
05/29/2022 08:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6920842840835824 -> 0.7219089339833882 on epoch=23, global_step=750
05/29/2022 08:26:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=23
05/29/2022 08:26:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=24
05/29/2022 08:26:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.63 on epoch=24
05/29/2022 08:26:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=24
05/29/2022 08:26:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=24
05/29/2022 08:26:50 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.7465383327205221 on epoch=24
05/29/2022 08:26:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7219089339833882 -> 0.7465383327205221 on epoch=24, global_step=800
05/29/2022 08:26:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.65 on epoch=25
05/29/2022 08:26:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=25
05/29/2022 08:26:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=25
05/29/2022 08:27:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=26
05/29/2022 08:27:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=26
05/29/2022 08:27:09 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.7189941409619978 on epoch=26
05/29/2022 08:27:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=26
05/29/2022 08:27:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=27
05/29/2022 08:27:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=27
05/29/2022 08:27:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=27
05/29/2022 08:27:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.59 on epoch=28
05/29/2022 08:27:29 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.7408856494382811 on epoch=28
05/29/2022 08:27:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.56 on epoch=28
05/29/2022 08:27:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.55 on epoch=28
05/29/2022 08:27:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=29
05/29/2022 08:27:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=29
05/29/2022 08:27:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=29
05/29/2022 08:27:48 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.7532820839726343 on epoch=29
05/29/2022 08:27:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7465383327205221 -> 0.7532820839726343 on epoch=29, global_step=950
05/29/2022 08:27:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=29
05/29/2022 08:27:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=30
05/29/2022 08:27:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=30
05/29/2022 08:27:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=30
05/29/2022 08:28:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=31
05/29/2022 08:28:08 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.7486380965991242 on epoch=31
05/29/2022 08:28:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=31
05/29/2022 08:28:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=31
05/29/2022 08:28:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=32
05/29/2022 08:28:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=32
05/29/2022 08:28:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=32
05/29/2022 08:28:27 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.7427885322224944 on epoch=32
05/29/2022 08:28:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=33
05/29/2022 08:28:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=33
05/29/2022 08:28:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=33
05/29/2022 08:28:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=34
05/29/2022 08:28:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=34
05/29/2022 08:28:47 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.7577320450759559 on epoch=34
05/29/2022 08:28:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7532820839726343 -> 0.7577320450759559 on epoch=34, global_step=1100
05/29/2022 08:28:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=34
05/29/2022 08:28:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=34
05/29/2022 08:28:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=35
05/29/2022 08:28:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=35
05/29/2022 08:28:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=35
05/29/2022 08:29:06 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.7707998411141086 on epoch=35
05/29/2022 08:29:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7577320450759559 -> 0.7707998411141086 on epoch=35, global_step=1150
05/29/2022 08:29:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=36
05/29/2022 08:29:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=36
05/29/2022 08:29:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=36
05/29/2022 08:29:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=37
05/29/2022 08:29:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=37
05/29/2022 08:29:25 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.7992613562049838 on epoch=37
05/29/2022 08:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7707998411141086 -> 0.7992613562049838 on epoch=37, global_step=1200
05/29/2022 08:29:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=37
05/29/2022 08:29:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=38
05/29/2022 08:29:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=38
05/29/2022 08:29:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=38
05/29/2022 08:29:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=39
05/29/2022 08:29:45 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.7811135009593402 on epoch=39
05/29/2022 08:29:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.52 on epoch=39
05/29/2022 08:29:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=39
05/29/2022 08:29:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=39
05/29/2022 08:29:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=40
05/29/2022 08:29:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=40
05/29/2022 08:30:04 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.7482967668314129 on epoch=40
05/29/2022 08:30:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=40
05/29/2022 08:30:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=41
05/29/2022 08:30:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=41
05/29/2022 08:30:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=41
05/29/2022 08:30:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=42
05/29/2022 08:30:24 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.7722354951442508 on epoch=42
05/29/2022 08:30:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=42
05/29/2022 08:30:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=42
05/29/2022 08:30:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=43
05/29/2022 08:30:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=43
05/29/2022 08:30:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=43
05/29/2022 08:30:43 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.7852054009479897 on epoch=43
05/29/2022 08:30:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=44
05/29/2022 08:30:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=44
05/29/2022 08:30:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=44
05/29/2022 08:30:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=44
05/29/2022 08:30:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=45
05/29/2022 08:31:03 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.7877934693264688 on epoch=45
05/29/2022 08:31:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=45
05/29/2022 08:31:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=45
05/29/2022 08:31:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=46
05/29/2022 08:31:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=46
05/29/2022 08:31:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=46
05/29/2022 08:31:22 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.8099985184127687 on epoch=46
05/29/2022 08:31:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7992613562049838 -> 0.8099985184127687 on epoch=46, global_step=1500
05/29/2022 08:31:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=47
05/29/2022 08:31:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=47
05/29/2022 08:31:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=47
05/29/2022 08:31:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=48
05/29/2022 08:31:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=48
05/29/2022 08:31:41 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.8016470691993166 on epoch=48
05/29/2022 08:31:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=48
05/29/2022 08:31:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=49
05/29/2022 08:31:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=49
05/29/2022 08:31:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=49
05/29/2022 08:31:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=49
05/29/2022 08:32:01 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.8116722968311819 on epoch=49
05/29/2022 08:32:01 - INFO - __main__ - Saving model with best Classification-F1: 0.8099985184127687 -> 0.8116722968311819 on epoch=49, global_step=1600
05/29/2022 08:32:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=50
05/29/2022 08:32:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=50
05/29/2022 08:32:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=50
05/29/2022 08:32:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=51
05/29/2022 08:32:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=51
05/29/2022 08:32:20 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.8112692065417779 on epoch=51
05/29/2022 08:32:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=51
05/29/2022 08:32:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=52
05/29/2022 08:32:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=52
05/29/2022 08:32:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=52
05/29/2022 08:32:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=53
05/29/2022 08:32:39 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.7965672077785485 on epoch=53
05/29/2022 08:32:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=53
05/29/2022 08:32:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=53
05/29/2022 08:32:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=54
05/29/2022 08:32:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=54
05/29/2022 08:32:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=54
05/29/2022 08:32:59 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.8154201001081791 on epoch=54
05/29/2022 08:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.8116722968311819 -> 0.8154201001081791 on epoch=54, global_step=1750
05/29/2022 08:33:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=54
05/29/2022 08:33:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=55
05/29/2022 08:33:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=55
05/29/2022 08:33:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=55
05/29/2022 08:33:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=56
05/29/2022 08:33:18 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.7999243055522676 on epoch=56
05/29/2022 08:33:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=56
05/29/2022 08:33:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=56
05/29/2022 08:33:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=57
05/29/2022 08:33:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=57
05/29/2022 08:33:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=57
05/29/2022 08:33:38 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.791495987568804 on epoch=57
05/29/2022 08:33:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=58
05/29/2022 08:33:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=58
05/29/2022 08:33:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=58
05/29/2022 08:33:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=59
05/29/2022 08:33:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=59
05/29/2022 08:33:57 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.8147502662627772 on epoch=59
05/29/2022 08:33:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=59
05/29/2022 08:34:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=59
05/29/2022 08:34:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=60
05/29/2022 08:34:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=60
05/29/2022 08:34:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=60
05/29/2022 08:34:16 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.8090719003219002 on epoch=60
05/29/2022 08:34:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=61
05/29/2022 08:34:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=61
05/29/2022 08:34:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=61
05/29/2022 08:34:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=62
05/29/2022 08:34:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=62
05/29/2022 08:34:36 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.8249820663917581 on epoch=62
05/29/2022 08:34:36 - INFO - __main__ - Saving model with best Classification-F1: 0.8154201001081791 -> 0.8249820663917581 on epoch=62, global_step=2000
05/29/2022 08:34:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=62
05/29/2022 08:34:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=63
05/29/2022 08:34:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=63
05/29/2022 08:34:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=63
05/29/2022 08:34:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.31 on epoch=64
05/29/2022 08:34:55 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.8068057688262292 on epoch=64
05/29/2022 08:34:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=64
05/29/2022 08:35:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=64
05/29/2022 08:35:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=64
05/29/2022 08:35:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=65
05/29/2022 08:35:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=65
05/29/2022 08:35:14 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.8060362687421613 on epoch=65
05/29/2022 08:35:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=65
05/29/2022 08:35:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.33 on epoch=66
05/29/2022 08:35:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=66
05/29/2022 08:35:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=66
05/29/2022 08:35:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=67
05/29/2022 08:35:34 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.819884017636775 on epoch=67
05/29/2022 08:35:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=67
05/29/2022 08:35:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=67
05/29/2022 08:35:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=68
05/29/2022 08:35:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.33 on epoch=68
05/29/2022 08:35:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=68
05/29/2022 08:35:53 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.8075739968346807 on epoch=68
05/29/2022 08:35:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=69
05/29/2022 08:35:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=69
05/29/2022 08:36:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=69
05/29/2022 08:36:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=69
05/29/2022 08:36:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.32 on epoch=70
05/29/2022 08:36:12 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.8264982360634534 on epoch=70
05/29/2022 08:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.8249820663917581 -> 0.8264982360634534 on epoch=70, global_step=2250
05/29/2022 08:36:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=70
05/29/2022 08:36:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=70
05/29/2022 08:36:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=71
05/29/2022 08:36:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.30 on epoch=71
05/29/2022 08:36:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=71
05/29/2022 08:36:31 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.8185257563506633 on epoch=71
05/29/2022 08:36:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=72
05/29/2022 08:36:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=72
05/29/2022 08:36:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=72
05/29/2022 08:36:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.26 on epoch=73
05/29/2022 08:36:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=73
05/29/2022 08:36:51 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.816228109613117 on epoch=73
05/29/2022 08:36:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=73
05/29/2022 08:36:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=74
05/29/2022 08:36:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.33 on epoch=74
05/29/2022 08:37:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=74
05/29/2022 08:37:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=74
05/29/2022 08:37:10 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.8078096097347721 on epoch=74
05/29/2022 08:37:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.33 on epoch=75
05/29/2022 08:37:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=75
05/29/2022 08:37:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=75
05/29/2022 08:37:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=76
05/29/2022 08:37:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.31 on epoch=76
05/29/2022 08:37:29 - INFO - __main__ - Global step 2450 Train loss 0.25 Classification-F1 0.819795019279669 on epoch=76
05/29/2022 08:37:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=76
05/29/2022 08:37:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=77
05/29/2022 08:37:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=77
05/29/2022 08:37:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=77
05/29/2022 08:37:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=78
05/29/2022 08:37:49 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.8183946110573103 on epoch=78
05/29/2022 08:37:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=78
05/29/2022 08:37:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=78
05/29/2022 08:37:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=79
05/29/2022 08:37:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=79
05/29/2022 08:38:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=79
05/29/2022 08:38:08 - INFO - __main__ - Global step 2550 Train loss 0.23 Classification-F1 0.8202778116473136 on epoch=79
05/29/2022 08:38:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=79
05/29/2022 08:38:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=80
05/29/2022 08:38:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=80
05/29/2022 08:38:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=80
05/29/2022 08:38:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=81
05/29/2022 08:38:28 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.8013834965451971 on epoch=81
05/29/2022 08:38:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.27 on epoch=81
05/29/2022 08:38:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=81
05/29/2022 08:38:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=82
05/29/2022 08:38:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.31 on epoch=82
05/29/2022 08:38:40 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=82
05/29/2022 08:38:47 - INFO - __main__ - Global step 2650 Train loss 0.28 Classification-F1 0.816564681911657 on epoch=82
05/29/2022 08:38:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.26 on epoch=83
05/29/2022 08:38:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.25 on epoch=83
05/29/2022 08:38:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=83
05/29/2022 08:38:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.24 on epoch=84
05/29/2022 08:38:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=84
05/29/2022 08:39:06 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.8267753444952542 on epoch=84
05/29/2022 08:39:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8264982360634534 -> 0.8267753444952542 on epoch=84, global_step=2700
05/29/2022 08:39:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.30 on epoch=84
05/29/2022 08:39:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=84
05/29/2022 08:39:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=85
05/29/2022 08:39:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=85
05/29/2022 08:39:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=85
05/29/2022 08:39:26 - INFO - __main__ - Global step 2750 Train loss 0.24 Classification-F1 0.8228262699048703 on epoch=85
05/29/2022 08:39:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.32 on epoch=86
05/29/2022 08:39:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.30 on epoch=86
05/29/2022 08:39:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=86
05/29/2022 08:39:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.20 on epoch=87
05/29/2022 08:39:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=87
05/29/2022 08:39:45 - INFO - __main__ - Global step 2800 Train loss 0.25 Classification-F1 0.8301742310950955 on epoch=87
05/29/2022 08:39:45 - INFO - __main__ - Saving model with best Classification-F1: 0.8267753444952542 -> 0.8301742310950955 on epoch=87, global_step=2800
05/29/2022 08:39:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=87
05/29/2022 08:39:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.28 on epoch=88
05/29/2022 08:39:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=88
05/29/2022 08:39:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=88
05/29/2022 08:39:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=89
05/29/2022 08:40:04 - INFO - __main__ - Global step 2850 Train loss 0.23 Classification-F1 0.8156373497106368 on epoch=89
05/29/2022 08:40:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=89
05/29/2022 08:40:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=89
05/29/2022 08:40:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=89
05/29/2022 08:40:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=90
05/29/2022 08:40:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=90
05/29/2022 08:40:24 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.8224031479398279 on epoch=90
05/29/2022 08:40:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=90
05/29/2022 08:40:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=91
05/29/2022 08:40:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=91
05/29/2022 08:40:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.23 on epoch=91
05/29/2022 08:40:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=92
05/29/2022 08:40:43 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.7975591017387178 on epoch=92
05/29/2022 08:40:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=92
05/29/2022 08:40:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=92
05/29/2022 08:40:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.24 on epoch=93
05/29/2022 08:40:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=93
05/29/2022 08:40:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=93
05/29/2022 08:41:02 - INFO - __main__ - Global step 3000 Train loss 0.22 Classification-F1 0.813864565293342 on epoch=93
05/29/2022 08:41:02 - INFO - __main__ - save last model!
05/29/2022 08:41:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 08:41:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/29/2022 08:41:02 - INFO - __main__ - Printing 3 examples
05/29/2022 08:41:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/29/2022 08:41:02 - INFO - __main__ - ['others']
05/29/2022 08:41:02 - INFO - __main__ -  [emo] what you like very little things ok
05/29/2022 08:41:02 - INFO - __main__ - ['others']
05/29/2022 08:41:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/29/2022 08:41:02 - INFO - __main__ - ['others']
05/29/2022 08:41:02 - INFO - __main__ - Tokenizing Input ...
05/29/2022 08:41:04 - INFO - __main__ - Tokenizing Output ...
05/29/2022 08:41:10 - INFO - __main__ - Loaded 5509 examples from test data
05/29/2022 08:42:25 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-emo/emo_128_87_0.2_8_predictions.txt
05/29/2022 08:42:25 - INFO - __main__ - Classification-F1 on test data: 0.4955
05/29/2022 08:42:25 - INFO - __main__ - prefix=emo_128_87, lr=0.2, bsz=8, dev_performance=0.8301742310950955, test_performance=0.495476826102514
