05/30/2022 23:24:28 - INFO - __main__ - Namespace(task_dir='data_128/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/30/2022 23:24:28 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli
05/30/2022 23:24:28 - INFO - __main__ - Namespace(task_dir='data_128/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/30/2022 23:24:28 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli
05/30/2022 23:24:29 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/30/2022 23:24:29 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/30/2022 23:24:29 - INFO - __main__ - args.device: cuda:1
05/30/2022 23:24:29 - INFO - __main__ - args.device: cuda:0
05/30/2022 23:24:29 - INFO - __main__ - Using 2 gpus
05/30/2022 23:24:29 - INFO - __main__ - Using 2 gpus
05/30/2022 23:24:29 - INFO - __main__ - Fine-tuning the following samples: ['anli_128_100', 'anli_128_13', 'anli_128_21', 'anli_128_42', 'anli_128_87']
05/30/2022 23:24:29 - INFO - __main__ - Fine-tuning the following samples: ['anli_128_100', 'anli_128_13', 'anli_128_21', 'anli_128_42', 'anli_128_87']
05/30/2022 23:24:34 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.5, bsz=8 ...
05/30/2022 23:24:35 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:24:35 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:35 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:24:35 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/30/2022 23:24:35 - INFO - __main__ - ['neutral']
05/30/2022 23:24:35 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:35 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:36 - INFO - __main__ - Loaded 384 examples from train data
05/30/2022 23:24:36 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:24:36 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:36 - INFO - __main__ - Loaded 384 examples from train data
05/30/2022 23:24:36 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:24:36 - INFO - __main__ - Printing 3 examples
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/30/2022 23:24:36 - INFO - __main__ - ['neutral']
05/30/2022 23:24:36 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:24:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:36 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:24:36 - INFO - __main__ - Loaded 384 examples from dev data
05/30/2022 23:24:36 - INFO - __main__ - Loaded 384 examples from dev data
05/30/2022 23:24:54 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 23:24:55 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 23:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 23:24:56 - INFO - __main__ - Starting training!
05/30/2022 23:25:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 23:25:00 - INFO - __main__ - Starting training!
05/30/2022 23:25:04 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=0
05/30/2022 23:25:06 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=0
05/30/2022 23:25:09 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=1
05/30/2022 23:25:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=1
05/30/2022 23:25:14 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=2
05/30/2022 23:25:25 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.18336361800928727 on epoch=2
05/30/2022 23:25:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18336361800928727 on epoch=2, global_step=50
05/30/2022 23:25:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=2
05/30/2022 23:25:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=2
05/30/2022 23:25:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=3
05/30/2022 23:25:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=3
05/30/2022 23:25:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=4
05/30/2022 23:25:48 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.1818181818181818 on epoch=4
05/30/2022 23:25:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=4
05/30/2022 23:25:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=4
05/30/2022 23:25:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=5
05/30/2022 23:25:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=5
05/30/2022 23:26:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=6
05/30/2022 23:26:11 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=6
05/30/2022 23:26:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=6
05/30/2022 23:26:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=7
05/30/2022 23:26:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=7
05/30/2022 23:26:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=7
05/30/2022 23:26:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=8
05/30/2022 23:26:34 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/30/2022 23:26:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=8
05/30/2022 23:26:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=9
05/30/2022 23:26:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=9
05/30/2022 23:26:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=9
05/30/2022 23:26:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=10
05/30/2022 23:26:58 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=10
05/30/2022 23:27:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=10
05/30/2022 23:27:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=11
05/30/2022 23:27:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=11
05/30/2022 23:27:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=12
05/30/2022 23:27:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/30/2022 23:27:22 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
05/30/2022 23:27:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=12
05/30/2022 23:27:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/30/2022 23:27:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=13
05/30/2022 23:27:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/30/2022 23:27:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=14
05/30/2022 23:27:46 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.3640249893118936 on epoch=14
05/30/2022 23:27:46 - INFO - __main__ - Saving model with best Classification-F1: 0.18336361800928727 -> 0.3640249893118936 on epoch=14, global_step=350
05/30/2022 23:27:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=14
05/30/2022 23:27:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/30/2022 23:27:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=15
05/30/2022 23:27:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/30/2022 23:27:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=16
05/30/2022 23:28:09 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.27553968170597193 on epoch=16
05/30/2022 23:28:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=17
05/30/2022 23:28:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=17
05/30/2022 23:28:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=17
05/30/2022 23:28:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/30/2022 23:28:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=18
05/30/2022 23:28:32 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.32455166298560517 on epoch=18
05/30/2022 23:28:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=19
05/30/2022 23:28:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=19
05/30/2022 23:28:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=19
05/30/2022 23:28:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=20
05/30/2022 23:28:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=20
05/30/2022 23:28:54 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.3197492163009404 on epoch=20
05/30/2022 23:28:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=21
05/30/2022 23:29:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=21
05/30/2022 23:29:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=22
05/30/2022 23:29:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=22
05/30/2022 23:29:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=22
05/30/2022 23:29:17 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.36616117086933525 on epoch=22
05/30/2022 23:29:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3640249893118936 -> 0.36616117086933525 on epoch=22, global_step=550
05/30/2022 23:29:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=23
05/30/2022 23:29:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=23
05/30/2022 23:29:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=24
05/30/2022 23:29:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=24
05/30/2022 23:29:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/30/2022 23:29:40 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.35779833660405574 on epoch=24
05/30/2022 23:29:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=25
05/30/2022 23:29:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=25
05/30/2022 23:29:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/30/2022 23:29:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=26
05/30/2022 23:29:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=27
05/30/2022 23:30:03 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.37770278715160605 on epoch=27
05/30/2022 23:30:03 - INFO - __main__ - Saving model with best Classification-F1: 0.36616117086933525 -> 0.37770278715160605 on epoch=27, global_step=650
05/30/2022 23:30:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=27
05/30/2022 23:30:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=27
05/30/2022 23:30:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=28
05/30/2022 23:30:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=28
05/30/2022 23:30:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=29
05/30/2022 23:30:26 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.3830753385831563 on epoch=29
05/30/2022 23:30:26 - INFO - __main__ - Saving model with best Classification-F1: 0.37770278715160605 -> 0.3830753385831563 on epoch=29, global_step=700
05/30/2022 23:30:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=29
05/30/2022 23:30:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/30/2022 23:30:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=30
05/30/2022 23:30:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=30
05/30/2022 23:30:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=31
05/30/2022 23:30:51 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.4195435416125668 on epoch=31
05/30/2022 23:30:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3830753385831563 -> 0.4195435416125668 on epoch=31, global_step=750
05/30/2022 23:30:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=31
05/30/2022 23:30:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=32
05/30/2022 23:30:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=32
05/30/2022 23:31:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=32
05/30/2022 23:31:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=33
05/30/2022 23:31:15 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.42191866545510653 on epoch=33
05/30/2022 23:31:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4195435416125668 -> 0.42191866545510653 on epoch=33, global_step=800
05/30/2022 23:31:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=33
05/30/2022 23:31:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=34
05/30/2022 23:31:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=34
05/30/2022 23:31:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=34
05/30/2022 23:31:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=35
05/30/2022 23:31:38 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.3472834067547724 on epoch=35
05/30/2022 23:31:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=35
05/30/2022 23:31:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=36
05/30/2022 23:31:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/30/2022 23:31:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=37
05/30/2022 23:31:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=37
05/30/2022 23:32:02 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.37748705746150657 on epoch=37
05/30/2022 23:32:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/30/2022 23:32:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=38
05/30/2022 23:32:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=38
05/30/2022 23:32:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=39
05/30/2022 23:32:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=39
05/30/2022 23:32:27 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.42832724452255794 on epoch=39
05/30/2022 23:32:27 - INFO - __main__ - Saving model with best Classification-F1: 0.42191866545510653 -> 0.42832724452255794 on epoch=39, global_step=950
05/30/2022 23:32:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=39
05/30/2022 23:32:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=40
05/30/2022 23:32:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=40
05/30/2022 23:32:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=41
05/30/2022 23:32:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=41
05/30/2022 23:32:51 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.4117874396135266 on epoch=41
05/30/2022 23:32:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=42
05/30/2022 23:32:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=42
05/30/2022 23:32:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=42
05/30/2022 23:33:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=43
05/30/2022 23:33:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=43
05/30/2022 23:33:15 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.4279631118171456 on epoch=43
05/30/2022 23:33:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=44
05/30/2022 23:33:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=44
05/30/2022 23:33:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=44
05/30/2022 23:33:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=45
05/30/2022 23:33:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=45
05/30/2022 23:33:38 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.4257970876558485 on epoch=45
05/30/2022 23:33:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=46
05/30/2022 23:33:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=46
05/30/2022 23:33:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/30/2022 23:33:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=47
05/30/2022 23:33:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=47
05/30/2022 23:34:01 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3918103808289121 on epoch=47
05/30/2022 23:34:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=48
05/30/2022 23:34:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=48
05/30/2022 23:34:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=49
05/30/2022 23:34:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=49
05/30/2022 23:34:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=49
05/30/2022 23:34:24 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3914042517749295 on epoch=49
05/30/2022 23:34:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=50
05/30/2022 23:34:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=50
05/30/2022 23:34:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=51
05/30/2022 23:34:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=51
05/30/2022 23:34:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=52
05/30/2022 23:34:46 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.40579344027619885 on epoch=52
05/30/2022 23:34:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=52
05/30/2022 23:34:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=52
05/30/2022 23:34:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=53
05/30/2022 23:34:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=53
05/30/2022 23:34:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=54
05/30/2022 23:35:08 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.4212396299352821 on epoch=54
05/30/2022 23:35:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=54
05/30/2022 23:35:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=54
05/30/2022 23:35:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=55
05/30/2022 23:35:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=55
05/30/2022 23:35:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=56
05/30/2022 23:35:32 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.4369830080273827 on epoch=56
05/30/2022 23:35:32 - INFO - __main__ - Saving model with best Classification-F1: 0.42832724452255794 -> 0.4369830080273827 on epoch=56, global_step=1350
05/30/2022 23:35:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=56
05/30/2022 23:35:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=57
05/30/2022 23:35:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=57
05/30/2022 23:35:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=57
05/30/2022 23:35:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=58
05/30/2022 23:35:55 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.42659058162187674 on epoch=58
05/30/2022 23:35:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=58
05/30/2022 23:36:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=59
05/30/2022 23:36:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=59
05/30/2022 23:36:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=59
05/30/2022 23:36:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=60
05/30/2022 23:36:18 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.3682120738554147 on epoch=60
05/30/2022 23:36:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=60
05/30/2022 23:36:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=61
05/30/2022 23:36:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=61
05/30/2022 23:36:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=62
05/30/2022 23:36:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=62
05/30/2022 23:36:41 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.40729484869429494 on epoch=62
05/30/2022 23:36:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=62
05/30/2022 23:36:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=63
05/30/2022 23:36:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=63
05/30/2022 23:36:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=64
05/30/2022 23:36:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=64
05/30/2022 23:37:05 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.41892165805209275 on epoch=64
05/30/2022 23:37:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=64
05/30/2022 23:37:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=65
05/30/2022 23:37:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=65
05/30/2022 23:37:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=66
05/30/2022 23:37:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=66
05/30/2022 23:37:29 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.4042533847153309 on epoch=66
05/30/2022 23:37:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=67
05/30/2022 23:37:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=67
05/30/2022 23:37:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=67
05/30/2022 23:37:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=68
05/30/2022 23:37:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=68
05/30/2022 23:37:53 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.4487547121818221 on epoch=68
05/30/2022 23:37:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4369830080273827 -> 0.4487547121818221 on epoch=68, global_step=1650
05/30/2022 23:37:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=69
05/30/2022 23:37:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=69
05/30/2022 23:38:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=69
05/30/2022 23:38:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=70
05/30/2022 23:38:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=70
05/30/2022 23:38:17 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.48566201561909716 on epoch=70
05/30/2022 23:38:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4487547121818221 -> 0.48566201561909716 on epoch=70, global_step=1700
05/30/2022 23:38:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=71
05/30/2022 23:38:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=71
05/30/2022 23:38:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=72
05/30/2022 23:38:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=72
05/30/2022 23:38:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=72
05/30/2022 23:38:41 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.39769619431697034 on epoch=72
05/30/2022 23:38:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=73
05/30/2022 23:38:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=73
05/30/2022 23:38:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=74
05/30/2022 23:38:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=74
05/30/2022 23:38:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=74
05/30/2022 23:39:04 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.4005636070853462 on epoch=74
05/30/2022 23:39:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.34 on epoch=75
05/30/2022 23:39:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=75
05/30/2022 23:39:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=76
05/30/2022 23:39:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=76
05/30/2022 23:39:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=77
05/30/2022 23:39:28 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.46350701761020047 on epoch=77
05/30/2022 23:39:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=77
05/30/2022 23:39:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=77
05/30/2022 23:39:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=78
05/30/2022 23:39:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=78
05/30/2022 23:39:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=79
05/30/2022 23:39:52 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.4758519961294379 on epoch=79
05/30/2022 23:39:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=79
05/30/2022 23:39:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=79
05/30/2022 23:40:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=80
05/30/2022 23:40:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=80
05/30/2022 23:40:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=81
05/30/2022 23:40:16 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.5016104259836472 on epoch=81
05/30/2022 23:40:16 - INFO - __main__ - Saving model with best Classification-F1: 0.48566201561909716 -> 0.5016104259836472 on epoch=81, global_step=1950
05/30/2022 23:40:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=81
05/30/2022 23:40:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=82
05/30/2022 23:40:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=82
05/30/2022 23:40:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=82
05/30/2022 23:40:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=83
05/30/2022 23:40:40 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.5234646561183675 on epoch=83
05/30/2022 23:40:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5016104259836472 -> 0.5234646561183675 on epoch=83, global_step=2000
05/30/2022 23:40:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=83
05/30/2022 23:40:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=84
05/30/2022 23:40:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=84
05/30/2022 23:40:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=84
05/30/2022 23:40:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=85
05/30/2022 23:41:03 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.44625396936811174 on epoch=85
05/30/2022 23:41:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=85
05/30/2022 23:41:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=86
05/30/2022 23:41:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=86
05/30/2022 23:41:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=87
05/30/2022 23:41:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=87
05/30/2022 23:41:27 - INFO - __main__ - Global step 2100 Train loss 0.27 Classification-F1 0.5106935549326738 on epoch=87
05/30/2022 23:41:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=87
05/30/2022 23:41:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=88
05/30/2022 23:41:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=88
05/30/2022 23:41:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=89
05/30/2022 23:41:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.33 on epoch=89
05/30/2022 23:41:51 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.5323330951813924 on epoch=89
05/30/2022 23:41:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5234646561183675 -> 0.5323330951813924 on epoch=89, global_step=2150
05/30/2022 23:41:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=89
05/30/2022 23:41:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=90
05/30/2022 23:41:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=90
05/30/2022 23:42:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=91
05/30/2022 23:42:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=91
05/30/2022 23:42:15 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.5039210930552187 on epoch=91
05/30/2022 23:42:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=92
05/30/2022 23:42:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=92
05/30/2022 23:42:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=92
05/30/2022 23:42:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.30 on epoch=93
05/30/2022 23:42:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=93
05/30/2022 23:42:38 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.5483261274622748 on epoch=93
05/30/2022 23:42:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5323330951813924 -> 0.5483261274622748 on epoch=93, global_step=2250
05/30/2022 23:42:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=94
05/30/2022 23:42:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=94
05/30/2022 23:42:46 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=94
05/30/2022 23:42:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=95
05/30/2022 23:42:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=95
05/30/2022 23:43:01 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.47929339800498455 on epoch=95
05/30/2022 23:43:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=96
05/30/2022 23:43:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=96
05/30/2022 23:43:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=97
05/30/2022 23:43:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=97
05/30/2022 23:43:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=97
05/30/2022 23:43:25 - INFO - __main__ - Global step 2350 Train loss 0.28 Classification-F1 0.5137824874318846 on epoch=97
05/30/2022 23:43:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=98
05/30/2022 23:43:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=98
05/30/2022 23:43:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=99
05/30/2022 23:43:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=99
05/30/2022 23:43:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=99
05/30/2022 23:43:49 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.4869082896436441 on epoch=99
05/30/2022 23:43:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=100
05/30/2022 23:43:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=100
05/30/2022 23:43:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=101
05/30/2022 23:43:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.28 on epoch=101
05/30/2022 23:44:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.26 on epoch=102
05/30/2022 23:44:12 - INFO - __main__ - Global step 2450 Train loss 0.25 Classification-F1 0.5040218674389131 on epoch=102
05/30/2022 23:44:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.31 on epoch=102
05/30/2022 23:44:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=102
05/30/2022 23:44:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=103
05/30/2022 23:44:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.26 on epoch=103
05/30/2022 23:44:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=104
05/30/2022 23:44:37 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.5426647426647426 on epoch=104
05/30/2022 23:44:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.29 on epoch=104
05/30/2022 23:44:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=104
05/30/2022 23:44:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=105
05/30/2022 23:44:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=105
05/30/2022 23:44:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.24 on epoch=106
05/30/2022 23:45:01 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.5274732301868211 on epoch=106
05/30/2022 23:45:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.24 on epoch=106
05/30/2022 23:45:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=107
05/30/2022 23:45:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=107
05/30/2022 23:45:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.23 on epoch=107
05/30/2022 23:45:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.26 on epoch=108
05/30/2022 23:45:26 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.41067079888782076 on epoch=108
05/30/2022 23:45:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=108
05/30/2022 23:45:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=109
05/30/2022 23:45:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=109
05/30/2022 23:45:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.22 on epoch=109
05/30/2022 23:45:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=110
05/30/2022 23:45:49 - INFO - __main__ - Global step 2650 Train loss 0.23 Classification-F1 0.5014790549930513 on epoch=110
05/30/2022 23:45:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.29 on epoch=110
05/30/2022 23:45:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=111
05/30/2022 23:45:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=111
05/30/2022 23:46:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=112
05/30/2022 23:46:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=112
05/30/2022 23:46:14 - INFO - __main__ - Global step 2700 Train loss 0.23 Classification-F1 0.4926785940870448 on epoch=112
05/30/2022 23:46:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=112
05/30/2022 23:46:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.24 on epoch=113
05/30/2022 23:46:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=113
05/30/2022 23:46:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=114
05/30/2022 23:46:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=114
05/30/2022 23:46:37 - INFO - __main__ - Global step 2750 Train loss 0.25 Classification-F1 0.5281201513568284 on epoch=114
05/30/2022 23:46:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=114
05/30/2022 23:46:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.26 on epoch=115
05/30/2022 23:46:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.25 on epoch=115
05/30/2022 23:46:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.24 on epoch=116
05/30/2022 23:46:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=116
05/30/2022 23:47:02 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.5307481550059842 on epoch=116
05/30/2022 23:47:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=117
05/30/2022 23:47:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=117
05/30/2022 23:47:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=117
05/30/2022 23:47:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=118
05/30/2022 23:47:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.23 on epoch=118
05/30/2022 23:47:26 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.5213903743315509 on epoch=118
05/30/2022 23:47:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=119
05/30/2022 23:47:31 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=119
05/30/2022 23:47:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=119
05/30/2022 23:47:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=120
05/30/2022 23:47:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=120
05/30/2022 23:47:50 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.579179204730408 on epoch=120
05/30/2022 23:47:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5483261274622748 -> 0.579179204730408 on epoch=120, global_step=2900
05/30/2022 23:47:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=121
05/30/2022 23:47:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=121
05/30/2022 23:47:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=122
05/30/2022 23:48:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.26 on epoch=122
05/30/2022 23:48:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.26 on epoch=122
05/30/2022 23:48:14 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.49127825680506065 on epoch=122
05/30/2022 23:48:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=123
05/30/2022 23:48:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=123
05/30/2022 23:48:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=124
05/30/2022 23:48:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=124
05/30/2022 23:48:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=124
05/30/2022 23:48:29 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:48:29 - INFO - __main__ - Printing 3 examples
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:48:29 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:48:29 - INFO - __main__ - Loaded 384 examples from train data
05/30/2022 23:48:29 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:48:29 - INFO - __main__ - Printing 3 examples
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/30/2022 23:48:29 - INFO - __main__ - ['neutral']
05/30/2022 23:48:29 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:48:30 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:48:30 - INFO - __main__ - Loaded 384 examples from dev data
05/30/2022 23:48:39 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.5307115300519637 on epoch=124
05/30/2022 23:48:39 - INFO - __main__ - save last model!
05/30/2022 23:48:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/30/2022 23:48:39 - INFO - __main__ - Start tokenizing ... 1000 instances
05/30/2022 23:48:39 - INFO - __main__ - Printing 3 examples
05/30/2022 23:48:39 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/30/2022 23:48:39 - INFO - __main__ - ['contradiction']
05/30/2022 23:48:39 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/30/2022 23:48:39 - INFO - __main__ - ['entailment']
05/30/2022 23:48:39 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/30/2022 23:48:39 - INFO - __main__ - ['contradiction']
05/30/2022 23:48:39 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:48:39 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:48:40 - INFO - __main__ - Loaded 1000 examples from test data
05/30/2022 23:48:49 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 23:48:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 23:48:50 - INFO - __main__ - Starting training!
05/30/2022 23:49:10 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_100_0.5_8_predictions.txt
05/30/2022 23:49:10 - INFO - __main__ - Classification-F1 on test data: 0.3596
05/30/2022 23:49:10 - INFO - __main__ - prefix=anli_128_100, lr=0.5, bsz=8, dev_performance=0.579179204730408, test_performance=0.3596457521082291
05/30/2022 23:49:10 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.4, bsz=8 ...
05/30/2022 23:49:11 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:49:11 - INFO - __main__ - Printing 3 examples
05/30/2022 23:49:11 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/30/2022 23:49:11 - INFO - __main__ - ['neutral']
05/30/2022 23:49:11 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/30/2022 23:49:11 - INFO - __main__ - ['neutral']
05/30/2022 23:49:11 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/30/2022 23:49:11 - INFO - __main__ - ['neutral']
05/30/2022 23:49:11 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:49:11 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:49:12 - INFO - __main__ - Loaded 384 examples from train data
05/30/2022 23:49:12 - INFO - __main__ - Start tokenizing ... 384 instances
05/30/2022 23:49:12 - INFO - __main__ - Printing 3 examples
05/30/2022 23:49:12 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/30/2022 23:49:12 - INFO - __main__ - ['neutral']
05/30/2022 23:49:12 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/30/2022 23:49:12 - INFO - __main__ - ['neutral']
05/30/2022 23:49:12 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/30/2022 23:49:12 - INFO - __main__ - ['neutral']
05/30/2022 23:49:12 - INFO - __main__ - Tokenizing Input ...
05/30/2022 23:49:12 - INFO - __main__ - Tokenizing Output ...
05/30/2022 23:49:12 - INFO - __main__ - Loaded 384 examples from dev data
05/30/2022 23:49:31 - INFO - __main__ - load prompt embedding from ckpt
05/30/2022 23:49:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/30/2022 23:49:32 - INFO - __main__ - Starting training!
05/30/2022 23:49:36 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=0
05/30/2022 23:49:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=0
05/30/2022 23:49:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
05/30/2022 23:49:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=1
05/30/2022 23:49:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=2
05/30/2022 23:49:57 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=2
05/30/2022 23:49:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/30/2022 23:50:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=2
05/30/2022 23:50:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=2
05/30/2022 23:50:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=3
05/30/2022 23:50:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/30/2022 23:50:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=4
05/30/2022 23:50:22 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.18704552307791175 on epoch=4
05/30/2022 23:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18704552307791175 on epoch=4, global_step=100
05/30/2022 23:50:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=4
05/30/2022 23:50:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=4
05/30/2022 23:50:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=5
05/30/2022 23:50:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=5
05/30/2022 23:50:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/30/2022 23:50:45 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/30/2022 23:50:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=6
05/30/2022 23:50:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=7
05/30/2022 23:50:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=7
05/30/2022 23:50:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/30/2022 23:50:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=8
05/30/2022 23:51:10 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/30/2022 23:51:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.55 on epoch=8
05/30/2022 23:51:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=9
05/30/2022 23:51:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=9
05/30/2022 23:51:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=9
05/30/2022 23:51:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=10
05/30/2022 23:51:34 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=10
05/30/2022 23:51:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=10
05/30/2022 23:51:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=11
05/30/2022 23:51:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=11
05/30/2022 23:51:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=12
05/30/2022 23:51:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=12
05/30/2022 23:51:57 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
05/30/2022 23:51:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=12
05/30/2022 23:52:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/30/2022 23:52:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=13
05/30/2022 23:52:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/30/2022 23:52:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=14
05/30/2022 23:52:21 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.1775766716943188 on epoch=14
05/30/2022 23:52:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=14
05/30/2022 23:52:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=15
05/30/2022 23:52:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/30/2022 23:52:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/30/2022 23:52:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=16
05/30/2022 23:52:44 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/30/2022 23:52:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=17
05/30/2022 23:52:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/30/2022 23:52:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=17
05/30/2022 23:52:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=18
05/30/2022 23:52:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=18
05/30/2022 23:53:08 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.23362313723759512 on epoch=18
05/30/2022 23:53:08 - INFO - __main__ - Saving model with best Classification-F1: 0.18704552307791175 -> 0.23362313723759512 on epoch=18, global_step=450
05/30/2022 23:53:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/30/2022 23:53:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=19
05/30/2022 23:53:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=19
05/30/2022 23:53:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=20
05/30/2022 23:53:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=20
05/30/2022 23:53:33 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.30248684839789575 on epoch=20
05/30/2022 23:53:33 - INFO - __main__ - Saving model with best Classification-F1: 0.23362313723759512 -> 0.30248684839789575 on epoch=20, global_step=500
05/30/2022 23:53:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=21
05/30/2022 23:53:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=21
05/30/2022 23:53:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=22
05/30/2022 23:53:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=22
05/30/2022 23:53:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=22
05/30/2022 23:53:57 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.2531678019482897 on epoch=22
05/30/2022 23:53:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=23
05/30/2022 23:54:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=23
05/30/2022 23:54:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=24
05/30/2022 23:54:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=24
05/30/2022 23:54:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=24
05/30/2022 23:54:20 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/30/2022 23:54:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/30/2022 23:54:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=25
05/30/2022 23:54:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=26
05/30/2022 23:54:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=26
05/30/2022 23:54:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=27
05/30/2022 23:54:43 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.3137215927913602 on epoch=27
05/30/2022 23:54:43 - INFO - __main__ - Saving model with best Classification-F1: 0.30248684839789575 -> 0.3137215927913602 on epoch=27, global_step=650
05/30/2022 23:54:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/30/2022 23:54:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=27
05/30/2022 23:54:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=28
05/30/2022 23:54:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=28
05/30/2022 23:54:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=29
05/30/2022 23:55:07 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.38611111111111107 on epoch=29
05/30/2022 23:55:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3137215927913602 -> 0.38611111111111107 on epoch=29, global_step=700
05/30/2022 23:55:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=29
05/30/2022 23:55:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=29
05/30/2022 23:55:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=30
05/30/2022 23:55:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=30
05/30/2022 23:55:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=31
05/30/2022 23:55:30 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.3263069731364467 on epoch=31
05/30/2022 23:55:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=31
05/30/2022 23:55:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=32
05/30/2022 23:55:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=32
05/30/2022 23:55:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/30/2022 23:55:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=33
05/30/2022 23:55:53 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.3530762001156738 on epoch=33
05/30/2022 23:55:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/30/2022 23:55:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=34
05/30/2022 23:56:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=34
05/30/2022 23:56:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=34
05/30/2022 23:56:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=35
05/30/2022 23:56:16 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.3164110101885702 on epoch=35
05/30/2022 23:56:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/30/2022 23:56:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=36
05/30/2022 23:56:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=36
05/30/2022 23:56:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=37
05/30/2022 23:56:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=37
05/30/2022 23:56:40 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.3595475781989412 on epoch=37
05/30/2022 23:56:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=37
05/30/2022 23:56:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=38
05/30/2022 23:56:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=38
05/30/2022 23:56:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=39
05/30/2022 23:56:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=39
05/30/2022 23:57:04 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.4000590694292985 on epoch=39
05/30/2022 23:57:05 - INFO - __main__ - Saving model with best Classification-F1: 0.38611111111111107 -> 0.4000590694292985 on epoch=39, global_step=950
05/30/2022 23:57:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=39
05/30/2022 23:57:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=40
05/30/2022 23:57:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=40
05/30/2022 23:57:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=41
05/30/2022 23:57:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=41
05/30/2022 23:57:29 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.40172086427764553 on epoch=41
05/30/2022 23:57:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4000590694292985 -> 0.40172086427764553 on epoch=41, global_step=1000
05/30/2022 23:57:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=42
05/30/2022 23:57:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/30/2022 23:57:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=42
05/30/2022 23:57:40 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=43
05/30/2022 23:57:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=43
05/30/2022 23:57:53 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.4150833188461774 on epoch=43
05/30/2022 23:57:54 - INFO - __main__ - Saving model with best Classification-F1: 0.40172086427764553 -> 0.4150833188461774 on epoch=43, global_step=1050
05/30/2022 23:57:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/30/2022 23:57:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=44
05/30/2022 23:58:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=44
05/30/2022 23:58:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=45
05/30/2022 23:58:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=45
05/30/2022 23:58:18 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.40125384558537486 on epoch=45
05/30/2022 23:58:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=46
05/30/2022 23:58:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=46
05/30/2022 23:58:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=47
05/30/2022 23:58:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=47
05/30/2022 23:58:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=47
05/30/2022 23:58:42 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.3998701185241904 on epoch=47
05/30/2022 23:58:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=48
05/30/2022 23:58:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=48
05/30/2022 23:58:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/30/2022 23:58:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=49
05/30/2022 23:58:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=49
05/30/2022 23:59:06 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.3961758103914967 on epoch=49
05/30/2022 23:59:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=50
05/30/2022 23:59:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=50
05/30/2022 23:59:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=51
05/30/2022 23:59:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/30/2022 23:59:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=52
05/30/2022 23:59:31 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.4270653252207956 on epoch=52
05/30/2022 23:59:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4150833188461774 -> 0.4270653252207956 on epoch=52, global_step=1250
05/30/2022 23:59:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=52
05/30/2022 23:59:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=52
05/30/2022 23:59:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=53
05/30/2022 23:59:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=53
05/30/2022 23:59:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=54
05/30/2022 23:59:55 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.4109572953487884 on epoch=54
05/30/2022 23:59:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=54
05/31/2022 00:00:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=54
05/31/2022 00:00:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=55
05/31/2022 00:00:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=55
05/31/2022 00:00:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=56
05/31/2022 00:00:20 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.3948707717969116 on epoch=56
05/31/2022 00:00:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=56
05/31/2022 00:00:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=57
05/31/2022 00:00:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=57
05/31/2022 00:00:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=57
05/31/2022 00:00:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=58
05/31/2022 00:00:44 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.3893593943444393 on epoch=58
05/31/2022 00:00:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=58
05/31/2022 00:00:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=59
05/31/2022 00:00:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=59
05/31/2022 00:00:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=59
05/31/2022 00:00:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=60
05/31/2022 00:01:07 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.3808065572771455 on epoch=60
05/31/2022 00:01:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=60
05/31/2022 00:01:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=61
05/31/2022 00:01:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=61
05/31/2022 00:01:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=62
05/31/2022 00:01:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=62
05/31/2022 00:01:31 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.39829790882879407 on epoch=62
05/31/2022 00:01:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=62
05/31/2022 00:01:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=63
05/31/2022 00:01:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=63
05/31/2022 00:01:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=64
05/31/2022 00:01:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=64
05/31/2022 00:01:55 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.4240067668101548 on epoch=64
05/31/2022 00:01:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 00:02:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=65
05/31/2022 00:02:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=65
05/31/2022 00:02:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=66
05/31/2022 00:02:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=66
05/31/2022 00:02:20 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.47187693402125275 on epoch=66
05/31/2022 00:02:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4270653252207956 -> 0.47187693402125275 on epoch=66, global_step=1600
05/31/2022 00:02:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=67
05/31/2022 00:02:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.47 on epoch=67
05/31/2022 00:02:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=67
05/31/2022 00:02:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=68
05/31/2022 00:02:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=68
05/31/2022 00:02:45 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.4955903440875038 on epoch=68
05/31/2022 00:02:45 - INFO - __main__ - Saving model with best Classification-F1: 0.47187693402125275 -> 0.4955903440875038 on epoch=68, global_step=1650
05/31/2022 00:02:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=69
05/31/2022 00:02:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=69
05/31/2022 00:02:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=69
05/31/2022 00:02:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=70
05/31/2022 00:02:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=70
05/31/2022 00:03:10 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.48489066569182054 on epoch=70
05/31/2022 00:03:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=71
05/31/2022 00:03:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=71
05/31/2022 00:03:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=72
05/31/2022 00:03:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=72
05/31/2022 00:03:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=72
05/31/2022 00:03:35 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.47934710344373777 on epoch=72
05/31/2022 00:03:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=73
05/31/2022 00:03:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=73
05/31/2022 00:03:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=74
05/31/2022 00:03:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=74
05/31/2022 00:03:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=74
05/31/2022 00:03:59 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.44114543556545965 on epoch=74
05/31/2022 00:04:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=75
05/31/2022 00:04:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=75
05/31/2022 00:04:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=76
05/31/2022 00:04:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=76
05/31/2022 00:04:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=77
05/31/2022 00:04:24 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.46676270967422084 on epoch=77
05/31/2022 00:04:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=77
05/31/2022 00:04:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=77
05/31/2022 00:04:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=78
05/31/2022 00:04:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=78
05/31/2022 00:04:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=79
05/31/2022 00:04:49 - INFO - __main__ - Global step 1900 Train loss 0.32 Classification-F1 0.4564485538169749 on epoch=79
05/31/2022 00:04:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=79
05/31/2022 00:04:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=79
05/31/2022 00:04:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=80
05/31/2022 00:04:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=80
05/31/2022 00:05:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=81
05/31/2022 00:05:13 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.4662364035732181 on epoch=81
05/31/2022 00:05:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=81
05/31/2022 00:05:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=82
05/31/2022 00:05:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=82
05/31/2022 00:05:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=82
05/31/2022 00:05:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=83
05/31/2022 00:05:38 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.4997232743816749 on epoch=83
05/31/2022 00:05:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4955903440875038 -> 0.4997232743816749 on epoch=83, global_step=2000
05/31/2022 00:05:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=83
05/31/2022 00:05:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=84
05/31/2022 00:05:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=84
05/31/2022 00:05:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.33 on epoch=84
05/31/2022 00:05:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=85
05/31/2022 00:06:03 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.43827038575962546 on epoch=85
05/31/2022 00:06:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.34 on epoch=85
05/31/2022 00:06:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=86
05/31/2022 00:06:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=86
05/31/2022 00:06:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=87
05/31/2022 00:06:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.33 on epoch=87
05/31/2022 00:06:27 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.5159422237151495 on epoch=87
05/31/2022 00:06:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4997232743816749 -> 0.5159422237151495 on epoch=87, global_step=2100
05/31/2022 00:06:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=87
05/31/2022 00:06:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.33 on epoch=88
05/31/2022 00:06:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.32 on epoch=88
05/31/2022 00:06:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=89
05/31/2022 00:06:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=89
05/31/2022 00:06:52 - INFO - __main__ - Global step 2150 Train loss 0.33 Classification-F1 0.5236241400875546 on epoch=89
05/31/2022 00:06:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5159422237151495 -> 0.5236241400875546 on epoch=89, global_step=2150
05/31/2022 00:06:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=89
05/31/2022 00:06:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.31 on epoch=90
05/31/2022 00:07:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=90
05/31/2022 00:07:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=91
05/31/2022 00:07:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.32 on epoch=91
05/31/2022 00:07:17 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.5195404129437113 on epoch=91
05/31/2022 00:07:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.30 on epoch=92
05/31/2022 00:07:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.32 on epoch=92
05/31/2022 00:07:25 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=92
05/31/2022 00:07:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=93
05/31/2022 00:07:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.34 on epoch=93
05/31/2022 00:07:42 - INFO - __main__ - Global step 2250 Train loss 0.31 Classification-F1 0.5128028293545536 on epoch=93
05/31/2022 00:07:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=94
05/31/2022 00:07:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=94
05/31/2022 00:07:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=94
05/31/2022 00:07:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=95
05/31/2022 00:07:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=95
05/31/2022 00:08:07 - INFO - __main__ - Global step 2300 Train loss 0.30 Classification-F1 0.4834824060313252 on epoch=95
05/31/2022 00:08:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=96
05/31/2022 00:08:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=96
05/31/2022 00:08:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=97
05/31/2022 00:08:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.33 on epoch=97
05/31/2022 00:08:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=97
05/31/2022 00:08:32 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.4724894424041259 on epoch=97
05/31/2022 00:08:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=98
05/31/2022 00:08:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=98
05/31/2022 00:08:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=99
05/31/2022 00:08:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=99
05/31/2022 00:08:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=99
05/31/2022 00:08:56 - INFO - __main__ - Global step 2400 Train loss 0.29 Classification-F1 0.4498143241604328 on epoch=99
05/31/2022 00:08:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=100
05/31/2022 00:09:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=100
05/31/2022 00:09:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=101
05/31/2022 00:09:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.28 on epoch=101
05/31/2022 00:09:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.29 on epoch=102
05/31/2022 00:09:21 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.4558575987053884 on epoch=102
05/31/2022 00:09:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.31 on epoch=102
05/31/2022 00:09:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=102
05/31/2022 00:09:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.31 on epoch=103
05/31/2022 00:09:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=103
05/31/2022 00:09:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.29 on epoch=104
05/31/2022 00:09:46 - INFO - __main__ - Global step 2500 Train loss 0.29 Classification-F1 0.5137120542292957 on epoch=104
05/31/2022 00:09:48 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=104
05/31/2022 00:09:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=104
05/31/2022 00:09:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.29 on epoch=105
05/31/2022 00:09:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=105
05/31/2022 00:09:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.30 on epoch=106
05/31/2022 00:10:10 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.5353749113379233 on epoch=106
05/31/2022 00:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5236241400875546 -> 0.5353749113379233 on epoch=106, global_step=2550
05/31/2022 00:10:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=106
05/31/2022 00:10:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.27 on epoch=107
05/31/2022 00:10:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.30 on epoch=107
05/31/2022 00:10:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.34 on epoch=107
05/31/2022 00:10:24 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=108
05/31/2022 00:10:35 - INFO - __main__ - Global step 2600 Train loss 0.30 Classification-F1 0.5361049516108967 on epoch=108
05/31/2022 00:10:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5353749113379233 -> 0.5361049516108967 on epoch=108, global_step=2600
05/31/2022 00:10:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=108
05/31/2022 00:10:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=109
05/31/2022 00:10:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=109
05/31/2022 00:10:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.31 on epoch=109
05/31/2022 00:10:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.29 on epoch=110
05/31/2022 00:10:59 - INFO - __main__ - Global step 2650 Train loss 0.30 Classification-F1 0.4678611167462761 on epoch=110
05/31/2022 00:11:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.28 on epoch=110
05/31/2022 00:11:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.28 on epoch=111
05/31/2022 00:11:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.25 on epoch=111
05/31/2022 00:11:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.33 on epoch=112
05/31/2022 00:11:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=112
05/31/2022 00:11:23 - INFO - __main__ - Global step 2700 Train loss 0.27 Classification-F1 0.5092850469742297 on epoch=112
05/31/2022 00:11:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=112
05/31/2022 00:11:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=113
05/31/2022 00:11:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.27 on epoch=113
05/31/2022 00:11:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=114
05/31/2022 00:11:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.24 on epoch=114
05/31/2022 00:11:48 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.4991730104810847 on epoch=114
05/31/2022 00:11:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=114
05/31/2022 00:11:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.32 on epoch=115
05/31/2022 00:11:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=115
05/31/2022 00:11:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.29 on epoch=116
05/31/2022 00:12:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.28 on epoch=116
05/31/2022 00:12:12 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.49138693913349457 on epoch=116
05/31/2022 00:12:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.28 on epoch=117
05/31/2022 00:12:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.27 on epoch=117
05/31/2022 00:12:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=117
05/31/2022 00:12:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=118
05/31/2022 00:12:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=118
05/31/2022 00:12:36 - INFO - __main__ - Global step 2850 Train loss 0.28 Classification-F1 0.5208521709532967 on epoch=118
05/31/2022 00:12:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=119
05/31/2022 00:12:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.29 on epoch=119
05/31/2022 00:12:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=119
05/31/2022 00:12:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.27 on epoch=120
05/31/2022 00:12:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=120
05/31/2022 00:13:00 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.4838511699271193 on epoch=120
05/31/2022 00:13:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=121
05/31/2022 00:13:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=121
05/31/2022 00:13:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.22 on epoch=122
05/31/2022 00:13:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=122
05/31/2022 00:13:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=122
05/31/2022 00:13:25 - INFO - __main__ - Global step 2950 Train loss 0.27 Classification-F1 0.49767030039334603 on epoch=122
05/31/2022 00:13:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=123
05/31/2022 00:13:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=123
05/31/2022 00:13:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=124
05/31/2022 00:13:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.25 on epoch=124
05/31/2022 00:13:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.25 on epoch=124
05/31/2022 00:13:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:13:40 - INFO - __main__ - Printing 3 examples
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:13:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:13:40 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 00:13:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:13:40 - INFO - __main__ - Printing 3 examples
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/31/2022 00:13:40 - INFO - __main__ - ['neutral']
05/31/2022 00:13:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:13:41 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:13:41 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 00:13:49 - INFO - __main__ - Global step 3000 Train loss 0.24 Classification-F1 0.4625472658225731 on epoch=124
05/31/2022 00:13:49 - INFO - __main__ - save last model!
05/31/2022 00:13:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 00:13:50 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 00:13:50 - INFO - __main__ - Printing 3 examples
05/31/2022 00:13:50 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 00:13:50 - INFO - __main__ - ['contradiction']
05/31/2022 00:13:50 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 00:13:50 - INFO - __main__ - ['entailment']
05/31/2022 00:13:50 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 00:13:50 - INFO - __main__ - ['contradiction']
05/31/2022 00:13:50 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:13:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:13:51 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 00:13:58 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 00:13:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 00:13:59 - INFO - __main__ - Starting training!
05/31/2022 00:14:20 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_100_0.4_8_predictions.txt
05/31/2022 00:14:20 - INFO - __main__ - Classification-F1 on test data: 0.3185
05/31/2022 00:14:21 - INFO - __main__ - prefix=anli_128_100, lr=0.4, bsz=8, dev_performance=0.5361049516108967, test_performance=0.31852103662672565
05/31/2022 00:14:21 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.3, bsz=8 ...
05/31/2022 00:14:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:14:22 - INFO - __main__ - Printing 3 examples
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:14:22 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:14:22 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 00:14:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:14:22 - INFO - __main__ - Printing 3 examples
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/31/2022 00:14:22 - INFO - __main__ - ['neutral']
05/31/2022 00:14:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:14:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:14:23 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 00:14:41 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 00:14:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 00:14:42 - INFO - __main__ - Starting training!
05/31/2022 00:14:45 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=0
05/31/2022 00:14:48 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=0
05/31/2022 00:14:51 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=1
05/31/2022 00:14:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=1
05/31/2022 00:14:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=2
05/31/2022 00:15:07 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 00:15:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 00:15:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.63 on epoch=2
05/31/2022 00:15:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=2
05/31/2022 00:15:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.58 on epoch=3
05/31/2022 00:15:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=3
05/31/2022 00:15:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=4
05/31/2022 00:15:32 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.1721607831834019 on epoch=4
05/31/2022 00:15:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1721607831834019 on epoch=4, global_step=100
05/31/2022 00:15:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=4
05/31/2022 00:15:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=4
05/31/2022 00:15:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=5
05/31/2022 00:15:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.59 on epoch=5
05/31/2022 00:15:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=6
05/31/2022 00:15:57 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 00:15:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=6
05/31/2022 00:16:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=7
05/31/2022 00:16:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=7
05/31/2022 00:16:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=7
05/31/2022 00:16:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=8
05/31/2022 00:16:21 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 00:16:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=8
05/31/2022 00:16:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=9
05/31/2022 00:16:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=9
05/31/2022 00:16:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/31/2022 00:16:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=10
05/31/2022 00:16:46 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 00:16:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=10
05/31/2022 00:16:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/31/2022 00:16:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=11
05/31/2022 00:16:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=12
05/31/2022 00:16:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=12
05/31/2022 00:17:10 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 00:17:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=12
05/31/2022 00:17:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=13
05/31/2022 00:17:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=13
05/31/2022 00:17:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.54 on epoch=14
05/31/2022 00:17:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=14
05/31/2022 00:17:35 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=14
05/31/2022 00:17:35 - INFO - __main__ - Saving model with best Classification-F1: 0.1721607831834019 -> 0.1775766716943188 on epoch=14, global_step=350
05/31/2022 00:17:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/31/2022 00:17:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=15
05/31/2022 00:17:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=15
05/31/2022 00:17:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=16
05/31/2022 00:17:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=16
05/31/2022 00:18:00 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 00:18:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=17
05/31/2022 00:18:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=17
05/31/2022 00:18:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=17
05/31/2022 00:18:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=18
05/31/2022 00:18:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 00:18:25 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.18998672691691101 on epoch=18
05/31/2022 00:18:25 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.18998672691691101 on epoch=18, global_step=450
05/31/2022 00:18:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=19
05/31/2022 00:18:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=19
05/31/2022 00:18:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=19
05/31/2022 00:18:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=20
05/31/2022 00:18:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=20
05/31/2022 00:18:49 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.2535182840681822 on epoch=20
05/31/2022 00:18:49 - INFO - __main__ - Saving model with best Classification-F1: 0.18998672691691101 -> 0.2535182840681822 on epoch=20, global_step=500
05/31/2022 00:18:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=21
05/31/2022 00:18:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/31/2022 00:18:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=22
05/31/2022 00:19:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/31/2022 00:19:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=22
05/31/2022 00:19:14 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.1820127423713435 on epoch=22
05/31/2022 00:19:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=23
05/31/2022 00:19:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=23
05/31/2022 00:19:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=24
05/31/2022 00:19:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=24
05/31/2022 00:19:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=24
05/31/2022 00:19:38 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.1775766716943188 on epoch=24
05/31/2022 00:19:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=25
05/31/2022 00:19:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=25
05/31/2022 00:19:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=26
05/31/2022 00:19:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=26
05/31/2022 00:19:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/31/2022 00:20:03 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.25387019672733957 on epoch=27
05/31/2022 00:20:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2535182840681822 -> 0.25387019672733957 on epoch=27, global_step=650
05/31/2022 00:20:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=27
05/31/2022 00:20:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=27
05/31/2022 00:20:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=28
05/31/2022 00:20:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=28
05/31/2022 00:20:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/31/2022 00:20:27 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.3683572814943956 on epoch=29
05/31/2022 00:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.25387019672733957 -> 0.3683572814943956 on epoch=29, global_step=700
05/31/2022 00:20:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=29
05/31/2022 00:20:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/31/2022 00:20:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=30
05/31/2022 00:20:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=30
05/31/2022 00:20:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=31
05/31/2022 00:20:51 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.30613503455608715 on epoch=31
05/31/2022 00:20:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=31
05/31/2022 00:20:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/31/2022 00:20:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=32
05/31/2022 00:21:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=32
05/31/2022 00:21:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=33
05/31/2022 00:21:16 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.32125577711148456 on epoch=33
05/31/2022 00:21:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/31/2022 00:21:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=34
05/31/2022 00:21:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/31/2022 00:21:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=34
05/31/2022 00:21:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=35
05/31/2022 00:21:39 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.28209732845582525 on epoch=35
05/31/2022 00:21:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=35
05/31/2022 00:21:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=36
05/31/2022 00:21:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=36
05/31/2022 00:21:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=37
05/31/2022 00:21:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=37
05/31/2022 00:22:02 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3198998748435544 on epoch=37
05/31/2022 00:22:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=37
05/31/2022 00:22:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=38
05/31/2022 00:22:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=38
05/31/2022 00:22:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=39
05/31/2022 00:22:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=39
05/31/2022 00:22:25 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.3617360325737855 on epoch=39
05/31/2022 00:22:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=39
05/31/2022 00:22:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/31/2022 00:22:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=40
05/31/2022 00:22:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=41
05/31/2022 00:22:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=41
05/31/2022 00:22:48 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.3411848403253864 on epoch=41
05/31/2022 00:22:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=42
05/31/2022 00:22:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=42
05/31/2022 00:22:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=42
05/31/2022 00:22:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=43
05/31/2022 00:23:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=43
05/31/2022 00:23:11 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.3821243356127077 on epoch=43
05/31/2022 00:23:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3683572814943956 -> 0.3821243356127077 on epoch=43, global_step=1050
05/31/2022 00:23:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=44
05/31/2022 00:23:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=44
05/31/2022 00:23:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/31/2022 00:23:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=45
05/31/2022 00:23:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=45
05/31/2022 00:23:34 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.37694011616175666 on epoch=45
05/31/2022 00:23:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=46
05/31/2022 00:23:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=46
05/31/2022 00:23:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=47
05/31/2022 00:23:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=47
05/31/2022 00:23:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=47
05/31/2022 00:23:57 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.38784926084227694 on epoch=47
05/31/2022 00:23:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3821243356127077 -> 0.38784926084227694 on epoch=47, global_step=1150
05/31/2022 00:24:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=48
05/31/2022 00:24:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=48
05/31/2022 00:24:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=49
05/31/2022 00:24:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=49
05/31/2022 00:24:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=49
05/31/2022 00:24:21 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.3832592314856113 on epoch=49
05/31/2022 00:24:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=50
05/31/2022 00:24:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=50
05/31/2022 00:24:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=51
05/31/2022 00:24:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=51
05/31/2022 00:24:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=52
05/31/2022 00:24:44 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.38260203009285015 on epoch=52
05/31/2022 00:24:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=52
05/31/2022 00:24:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=52
05/31/2022 00:24:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=53
05/31/2022 00:24:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=53
05/31/2022 00:24:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=54
05/31/2022 00:25:07 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.3958322569534143 on epoch=54
05/31/2022 00:25:07 - INFO - __main__ - Saving model with best Classification-F1: 0.38784926084227694 -> 0.3958322569534143 on epoch=54, global_step=1300
05/31/2022 00:25:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=54
05/31/2022 00:25:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=54
05/31/2022 00:25:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=55
05/31/2022 00:25:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=55
05/31/2022 00:25:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=56
05/31/2022 00:25:30 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.3869540947118786 on epoch=56
05/31/2022 00:25:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=56
05/31/2022 00:25:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=57
05/31/2022 00:25:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=57
05/31/2022 00:25:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=57
05/31/2022 00:25:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=58
05/31/2022 00:25:53 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.3882769726247986 on epoch=58
05/31/2022 00:25:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=58
05/31/2022 00:25:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=59
05/31/2022 00:26:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=59
05/31/2022 00:26:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=59
05/31/2022 00:26:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=60
05/31/2022 00:26:16 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.37354573320115314 on epoch=60
05/31/2022 00:26:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=60
05/31/2022 00:26:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=61
05/31/2022 00:26:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=61
05/31/2022 00:26:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=62
05/31/2022 00:26:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=62
05/31/2022 00:26:39 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.3876237141046268 on epoch=62
05/31/2022 00:26:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=62
05/31/2022 00:26:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=63
05/31/2022 00:26:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=63
05/31/2022 00:26:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=64
05/31/2022 00:26:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=64
05/31/2022 00:27:02 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.39078982597054884 on epoch=64
05/31/2022 00:27:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 00:27:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=65
05/31/2022 00:27:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=65
05/31/2022 00:27:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=66
05/31/2022 00:27:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=66
05/31/2022 00:27:24 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.4037482766841849 on epoch=66
05/31/2022 00:27:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3958322569534143 -> 0.4037482766841849 on epoch=66, global_step=1600
05/31/2022 00:27:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=67
05/31/2022 00:27:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=67
05/31/2022 00:27:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=67
05/31/2022 00:27:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=68
05/31/2022 00:27:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=68
05/31/2022 00:27:47 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.39389550139597374 on epoch=68
05/31/2022 00:27:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=69
05/31/2022 00:27:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=69
05/31/2022 00:27:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=69
05/31/2022 00:27:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=70
05/31/2022 00:28:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=70
05/31/2022 00:28:10 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.396235019166916 on epoch=70
05/31/2022 00:28:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.33 on epoch=71
05/31/2022 00:28:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=71
05/31/2022 00:28:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=72
05/31/2022 00:28:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=72
05/31/2022 00:28:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=72
05/31/2022 00:28:32 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.3890362511052166 on epoch=72
05/31/2022 00:28:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=73
05/31/2022 00:28:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=73
05/31/2022 00:28:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=74
05/31/2022 00:28:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=74
05/31/2022 00:28:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=74
05/31/2022 00:28:55 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.38615187514558585 on epoch=74
05/31/2022 00:28:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=75
05/31/2022 00:29:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=75
05/31/2022 00:29:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=76
05/31/2022 00:29:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=76
05/31/2022 00:29:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=77
05/31/2022 00:29:17 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.395353712641997 on epoch=77
05/31/2022 00:29:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=77
05/31/2022 00:29:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=77
05/31/2022 00:29:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=78
05/31/2022 00:29:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=78
05/31/2022 00:29:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=79
05/31/2022 00:29:40 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.4076166775456919 on epoch=79
05/31/2022 00:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4037482766841849 -> 0.4076166775456919 on epoch=79, global_step=1900
05/31/2022 00:29:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=79
05/31/2022 00:29:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=79
05/31/2022 00:29:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=80
05/31/2022 00:29:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=80
05/31/2022 00:29:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=81
05/31/2022 00:30:04 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.4067901822065713 on epoch=81
05/31/2022 00:30:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=81
05/31/2022 00:30:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=82
05/31/2022 00:30:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=82
05/31/2022 00:30:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=82
05/31/2022 00:30:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=83
05/31/2022 00:30:26 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.400390843104411 on epoch=83
05/31/2022 00:30:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=83
05/31/2022 00:30:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.40 on epoch=84
05/31/2022 00:30:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=84
05/31/2022 00:30:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=84
05/31/2022 00:30:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=85
05/31/2022 00:30:49 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.38774410335984605 on epoch=85
05/31/2022 00:30:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=85
05/31/2022 00:30:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=86
05/31/2022 00:30:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=86
05/31/2022 00:30:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=87
05/31/2022 00:31:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=87
05/31/2022 00:31:11 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.39496584909247806 on epoch=87
05/31/2022 00:31:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.34 on epoch=87
05/31/2022 00:31:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=88
05/31/2022 00:31:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=88
05/31/2022 00:31:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=89
05/31/2022 00:31:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=89
05/31/2022 00:31:34 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.40295327906960604 on epoch=89
05/31/2022 00:31:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=89
05/31/2022 00:31:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=90
05/31/2022 00:31:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.33 on epoch=90
05/31/2022 00:31:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=91
05/31/2022 00:31:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=91
05/31/2022 00:31:57 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.4109166196122718 on epoch=91
05/31/2022 00:31:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4076166775456919 -> 0.4109166196122718 on epoch=91, global_step=2200
05/31/2022 00:32:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.30 on epoch=92
05/31/2022 00:32:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=92
05/31/2022 00:32:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=92
05/31/2022 00:32:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.42 on epoch=93
05/31/2022 00:32:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.36 on epoch=93
05/31/2022 00:32:20 - INFO - __main__ - Global step 2250 Train loss 0.35 Classification-F1 0.40143797643797646 on epoch=93
05/31/2022 00:32:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=94
05/31/2022 00:32:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.39 on epoch=94
05/31/2022 00:32:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.29 on epoch=94
05/31/2022 00:32:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=95
05/31/2022 00:32:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=95
05/31/2022 00:32:43 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.40473665606221276 on epoch=95
05/31/2022 00:32:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=96
05/31/2022 00:32:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=96
05/31/2022 00:32:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=97
05/31/2022 00:32:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=97
05/31/2022 00:32:56 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.32 on epoch=97
05/31/2022 00:33:05 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.40265186816910953 on epoch=97
05/31/2022 00:33:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=98
05/31/2022 00:33:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.30 on epoch=98
05/31/2022 00:33:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=99
05/31/2022 00:33:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=99
05/31/2022 00:33:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=99
05/31/2022 00:33:27 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.40191387559808617 on epoch=99
05/31/2022 00:33:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.31 on epoch=100
05/31/2022 00:33:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=100
05/31/2022 00:33:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.32 on epoch=101
05/31/2022 00:33:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.27 on epoch=101
05/31/2022 00:33:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=102
05/31/2022 00:33:50 - INFO - __main__ - Global step 2450 Train loss 0.32 Classification-F1 0.39761163032191077 on epoch=102
05/31/2022 00:33:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.31 on epoch=102
05/31/2022 00:33:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.29 on epoch=102
05/31/2022 00:33:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=103
05/31/2022 00:34:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.29 on epoch=103
05/31/2022 00:34:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=104
05/31/2022 00:34:13 - INFO - __main__ - Global step 2500 Train loss 0.31 Classification-F1 0.4309540041017142 on epoch=104
05/31/2022 00:34:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4109166196122718 -> 0.4309540041017142 on epoch=104, global_step=2500
05/31/2022 00:34:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=104
05/31/2022 00:34:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=104
05/31/2022 00:34:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=105
05/31/2022 00:34:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=105
05/31/2022 00:34:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=106
05/31/2022 00:34:37 - INFO - __main__ - Global step 2550 Train loss 0.31 Classification-F1 0.41803197951209903 on epoch=106
05/31/2022 00:34:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.31 on epoch=106
05/31/2022 00:34:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=107
05/31/2022 00:34:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.33 on epoch=107
05/31/2022 00:34:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.30 on epoch=107
05/31/2022 00:34:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=108
05/31/2022 00:35:00 - INFO - __main__ - Global step 2600 Train loss 0.33 Classification-F1 0.43386974504871095 on epoch=108
05/31/2022 00:35:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4309540041017142 -> 0.43386974504871095 on epoch=108, global_step=2600
05/31/2022 00:35:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=108
05/31/2022 00:35:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.31 on epoch=109
05/31/2022 00:35:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.33 on epoch=109
05/31/2022 00:35:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.30 on epoch=109
05/31/2022 00:35:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.37 on epoch=110
05/31/2022 00:35:24 - INFO - __main__ - Global step 2650 Train loss 0.32 Classification-F1 0.40030210089670176 on epoch=110
05/31/2022 00:35:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.31 on epoch=110
05/31/2022 00:35:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.30 on epoch=111
05/31/2022 00:35:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.36 on epoch=111
05/31/2022 00:35:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.29 on epoch=112
05/31/2022 00:35:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.33 on epoch=112
05/31/2022 00:35:47 - INFO - __main__ - Global step 2700 Train loss 0.32 Classification-F1 0.440368934843109 on epoch=112
05/31/2022 00:35:47 - INFO - __main__ - Saving model with best Classification-F1: 0.43386974504871095 -> 0.440368934843109 on epoch=112, global_step=2700
05/31/2022 00:35:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=112
05/31/2022 00:35:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=113
05/31/2022 00:35:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.32 on epoch=113
05/31/2022 00:35:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.32 on epoch=114
05/31/2022 00:36:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.32 on epoch=114
05/31/2022 00:36:11 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.43461897579544634 on epoch=114
05/31/2022 00:36:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.25 on epoch=114
05/31/2022 00:36:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.32 on epoch=115
05/31/2022 00:36:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=115
05/31/2022 00:36:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.27 on epoch=116
05/31/2022 00:36:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.28 on epoch=116
05/31/2022 00:36:35 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.447748568555239 on epoch=116
05/31/2022 00:36:35 - INFO - __main__ - Saving model with best Classification-F1: 0.440368934843109 -> 0.447748568555239 on epoch=116, global_step=2800
05/31/2022 00:36:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=117
05/31/2022 00:36:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.29 on epoch=117
05/31/2022 00:36:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=117
05/31/2022 00:36:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.29 on epoch=118
05/31/2022 00:36:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.24 on epoch=118
05/31/2022 00:36:59 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.42564380795507617 on epoch=118
05/31/2022 00:37:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.29 on epoch=119
05/31/2022 00:37:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.29 on epoch=119
05/31/2022 00:37:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.30 on epoch=119
05/31/2022 00:37:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=120
05/31/2022 00:37:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.31 on epoch=120
05/31/2022 00:37:22 - INFO - __main__ - Global step 2900 Train loss 0.30 Classification-F1 0.41824822449187904 on epoch=120
05/31/2022 00:37:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.33 on epoch=121
05/31/2022 00:37:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=121
05/31/2022 00:37:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=122
05/31/2022 00:37:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=122
05/31/2022 00:37:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.29 on epoch=122
05/31/2022 00:37:46 - INFO - __main__ - Global step 2950 Train loss 0.29 Classification-F1 0.43190080781321666 on epoch=122
05/31/2022 00:37:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=123
05/31/2022 00:37:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.31 on epoch=123
05/31/2022 00:37:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=124
05/31/2022 00:37:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=124
05/31/2022 00:37:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.27 on epoch=124
05/31/2022 00:38:00 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:38:00 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:00 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/31/2022 00:38:00 - INFO - __main__ - ['neutral']
05/31/2022 00:38:00 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/31/2022 00:38:00 - INFO - __main__ - ['neutral']
05/31/2022 00:38:00 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/31/2022 00:38:00 - INFO - __main__ - ['neutral']
05/31/2022 00:38:00 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:00 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:01 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 00:38:01 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:38:01 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:01 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/31/2022 00:38:01 - INFO - __main__ - ['neutral']
05/31/2022 00:38:01 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/31/2022 00:38:01 - INFO - __main__ - ['neutral']
05/31/2022 00:38:01 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/31/2022 00:38:01 - INFO - __main__ - ['neutral']
05/31/2022 00:38:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:02 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 00:38:09 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.43182317495867323 on epoch=124
05/31/2022 00:38:09 - INFO - __main__ - save last model!
05/31/2022 00:38:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 00:38:09 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 00:38:09 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:09 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 00:38:09 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:09 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 00:38:09 - INFO - __main__ - ['entailment']
05/31/2022 00:38:09 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 00:38:09 - INFO - __main__ - ['contradiction']
05/31/2022 00:38:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:10 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:11 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 00:38:18 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 00:38:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 00:38:19 - INFO - __main__ - Starting training!
05/31/2022 00:38:38 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_100_0.3_8_predictions.txt
05/31/2022 00:38:38 - INFO - __main__ - Classification-F1 on test data: 0.3107
05/31/2022 00:38:38 - INFO - __main__ - prefix=anli_128_100, lr=0.3, bsz=8, dev_performance=0.447748568555239, test_performance=0.31071885886199574
05/31/2022 00:38:38 - INFO - __main__ - Running ... prefix=anli_128_100, lr=0.2, bsz=8 ...
05/31/2022 00:38:39 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:38:39 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:39 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/31/2022 00:38:39 - INFO - __main__ - ['neutral']
05/31/2022 00:38:39 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/31/2022 00:38:39 - INFO - __main__ - ['neutral']
05/31/2022 00:38:39 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/31/2022 00:38:39 - INFO - __main__ - ['neutral']
05/31/2022 00:38:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:39 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:40 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 00:38:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 00:38:40 - INFO - __main__ - Printing 3 examples
05/31/2022 00:38:40 - INFO - __main__ -  [anli] premise: Nauset Regional High School an NEASC accredited high school located in North Eastham, Massachusetts. Nauset is inside the Cape Cod National Seashore, making it the only high school on the East Coast located within a National Park. The open campus is situated about a half-mile from Nauset Light. Nauset's colors are Black and Gold and the school's mascot is the Warrior. [SEP] hypothesis: Nauset Regional High School is a short walk from the Atlantic Ocean.
05/31/2022 00:38:40 - INFO - __main__ - ['neutral']
05/31/2022 00:38:40 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club is the oldest football club in England. 
05/31/2022 00:38:40 - INFO - __main__ - ['neutral']
05/31/2022 00:38:40 - INFO - __main__ -  [anli] premise: Brontë was a 2005 play by British playwright Polly Teale about the lives of the Brontë sisters, their brother Branwell and their father Patrick. It also featured characters from the sisters' novels such as Cathy and Heathcliff from "Wuthering Heights". [SEP] hypothesis: Bronte features characters from more than 3 other novels.
05/31/2022 00:38:40 - INFO - __main__ - ['neutral']
05/31/2022 00:38:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 00:38:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 00:38:40 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 00:38:56 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 00:38:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 00:38:57 - INFO - __main__ - Starting training!
05/31/2022 00:39:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=0
05/31/2022 00:39:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=0
05/31/2022 00:39:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
05/31/2022 00:39:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=1
05/31/2022 00:39:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=2
05/31/2022 00:39:22 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 00:39:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 00:39:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=2
05/31/2022 00:39:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=2
05/31/2022 00:39:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=3
05/31/2022 00:39:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=3
05/31/2022 00:39:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=4
05/31/2022 00:39:46 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 00:39:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=4
05/31/2022 00:39:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=4
05/31/2022 00:39:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=5
05/31/2022 00:39:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=5
05/31/2022 00:39:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=6
05/31/2022 00:40:09 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 00:40:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=6
05/31/2022 00:40:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/31/2022 00:40:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=7
05/31/2022 00:40:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=7
05/31/2022 00:40:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=8
05/31/2022 00:40:33 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 00:40:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=8
05/31/2022 00:40:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=9
05/31/2022 00:40:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=9
05/31/2022 00:40:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=9
05/31/2022 00:40:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=10
05/31/2022 00:40:56 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 00:40:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=10
05/31/2022 00:41:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/31/2022 00:41:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=11
05/31/2022 00:41:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=12
05/31/2022 00:41:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=12
05/31/2022 00:41:19 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 00:41:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=12
05/31/2022 00:41:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=13
05/31/2022 00:41:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=13
05/31/2022 00:41:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/31/2022 00:41:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=14
05/31/2022 00:41:43 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 00:41:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/31/2022 00:41:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=15
05/31/2022 00:41:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=15
05/31/2022 00:41:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=16
05/31/2022 00:41:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=16
05/31/2022 00:42:07 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 00:42:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=17
05/31/2022 00:42:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/31/2022 00:42:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=17
05/31/2022 00:42:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/31/2022 00:42:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=18
05/31/2022 00:42:31 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 00:42:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=19
05/31/2022 00:42:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=19
05/31/2022 00:42:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=19
05/31/2022 00:42:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=20
05/31/2022 00:42:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=20
05/31/2022 00:42:55 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.2041097575981297 on epoch=20
05/31/2022 00:42:55 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2041097575981297 on epoch=20, global_step=500
05/31/2022 00:42:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=21
05/31/2022 00:43:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=21
05/31/2022 00:43:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=22
05/31/2022 00:43:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=22
05/31/2022 00:43:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=22
05/31/2022 00:43:19 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=22
05/31/2022 00:43:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/31/2022 00:43:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=23
05/31/2022 00:43:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/31/2022 00:43:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=24
05/31/2022 00:43:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=24
05/31/2022 00:43:44 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/31/2022 00:43:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=25
05/31/2022 00:43:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=25
05/31/2022 00:43:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=26
05/31/2022 00:43:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=26
05/31/2022 00:43:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=27
05/31/2022 00:44:08 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.17244846656611368 on epoch=27
05/31/2022 00:44:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=27
05/31/2022 00:44:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/31/2022 00:44:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=28
05/31/2022 00:44:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/31/2022 00:44:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=29
05/31/2022 00:44:33 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2311617160006791 on epoch=29
05/31/2022 00:44:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2041097575981297 -> 0.2311617160006791 on epoch=29, global_step=700
05/31/2022 00:44:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=29
05/31/2022 00:44:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/31/2022 00:44:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=30
05/31/2022 00:44:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=30
05/31/2022 00:44:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=31
05/31/2022 00:44:56 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.17248822009423928 on epoch=31
05/31/2022 00:44:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=31
05/31/2022 00:45:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/31/2022 00:45:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=32
05/31/2022 00:45:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=32
05/31/2022 00:45:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.50 on epoch=33
05/31/2022 00:45:20 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.24918535829309285 on epoch=33
05/31/2022 00:45:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2311617160006791 -> 0.24918535829309285 on epoch=33, global_step=800
05/31/2022 00:45:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=33
05/31/2022 00:45:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=34
05/31/2022 00:45:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=34
05/31/2022 00:45:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=34
05/31/2022 00:45:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=35
05/31/2022 00:45:44 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.17882344123629845 on epoch=35
05/31/2022 00:45:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=35
05/31/2022 00:45:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=36
05/31/2022 00:45:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=36
05/31/2022 00:45:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/31/2022 00:45:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=37
05/31/2022 00:46:07 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.184479473410705 on epoch=37
05/31/2022 00:46:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=37
05/31/2022 00:46:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=38
05/31/2022 00:46:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=38
05/31/2022 00:46:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=39
05/31/2022 00:46:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=39
05/31/2022 00:46:31 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.26982734939072117 on epoch=39
05/31/2022 00:46:31 - INFO - __main__ - Saving model with best Classification-F1: 0.24918535829309285 -> 0.26982734939072117 on epoch=39, global_step=950
05/31/2022 00:46:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=39
05/31/2022 00:46:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=40
05/31/2022 00:46:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=40
05/31/2022 00:46:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=41
05/31/2022 00:46:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=41
05/31/2022 00:46:55 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.27034601999911895 on epoch=41
05/31/2022 00:46:55 - INFO - __main__ - Saving model with best Classification-F1: 0.26982734939072117 -> 0.27034601999911895 on epoch=41, global_step=1000
05/31/2022 00:46:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=42
05/31/2022 00:47:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/31/2022 00:47:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=42
05/31/2022 00:47:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=43
05/31/2022 00:47:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=43
05/31/2022 00:47:19 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.2682111671950285 on epoch=43
05/31/2022 00:47:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=44
05/31/2022 00:47:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=44
05/31/2022 00:47:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=44
05/31/2022 00:47:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=45
05/31/2022 00:47:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=45
05/31/2022 00:47:43 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.2801214067036852 on epoch=45
05/31/2022 00:47:43 - INFO - __main__ - Saving model with best Classification-F1: 0.27034601999911895 -> 0.2801214067036852 on epoch=45, global_step=1100
05/31/2022 00:47:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=46
05/31/2022 00:47:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=46
05/31/2022 00:47:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=47
05/31/2022 00:47:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=47
05/31/2022 00:47:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=47
05/31/2022 00:48:06 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.24479702131206757 on epoch=47
05/31/2022 00:48:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=48
05/31/2022 00:48:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=48
05/31/2022 00:48:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=49
05/31/2022 00:48:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=49
05/31/2022 00:48:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=49
05/31/2022 00:48:30 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.2614426883715471 on epoch=49
05/31/2022 00:48:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=50
05/31/2022 00:48:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=50
05/31/2022 00:48:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=51
05/31/2022 00:48:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=51
05/31/2022 00:48:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=52
05/31/2022 00:48:53 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.3245743627198734 on epoch=52
05/31/2022 00:48:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2801214067036852 -> 0.3245743627198734 on epoch=52, global_step=1250
05/31/2022 00:48:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.51 on epoch=52
05/31/2022 00:48:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=52
05/31/2022 00:49:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=53
05/31/2022 00:49:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=53
05/31/2022 00:49:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=54
05/31/2022 00:49:17 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.32687825651913954 on epoch=54
05/31/2022 00:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3245743627198734 -> 0.32687825651913954 on epoch=54, global_step=1300
05/31/2022 00:49:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=54
05/31/2022 00:49:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=54
05/31/2022 00:49:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=55
05/31/2022 00:49:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=55
05/31/2022 00:49:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.50 on epoch=56
05/31/2022 00:49:40 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.3205893476723727 on epoch=56
05/31/2022 00:49:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=56
05/31/2022 00:49:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=57
05/31/2022 00:49:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=57
05/31/2022 00:49:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=57
05/31/2022 00:49:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/31/2022 00:50:02 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3311145616421318 on epoch=58
05/31/2022 00:50:03 - INFO - __main__ - Saving model with best Classification-F1: 0.32687825651913954 -> 0.3311145616421318 on epoch=58, global_step=1400
05/31/2022 00:50:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=58
05/31/2022 00:50:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=59
05/31/2022 00:50:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=59
05/31/2022 00:50:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=59
05/31/2022 00:50:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=60
05/31/2022 00:50:25 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.2991699747383616 on epoch=60
05/31/2022 00:50:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=60
05/31/2022 00:50:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/31/2022 00:50:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=61
05/31/2022 00:50:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=62
05/31/2022 00:50:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=62
05/31/2022 00:50:48 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.3500450551482885 on epoch=62
05/31/2022 00:50:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3311145616421318 -> 0.3500450551482885 on epoch=62, global_step=1500
05/31/2022 00:50:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=62
05/31/2022 00:50:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=63
05/31/2022 00:50:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=63
05/31/2022 00:50:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=64
05/31/2022 00:51:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=64
05/31/2022 00:51:11 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.3796234772978959 on epoch=64
05/31/2022 00:51:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3500450551482885 -> 0.3796234772978959 on epoch=64, global_step=1550
05/31/2022 00:51:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 00:51:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=65
05/31/2022 00:51:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=65
05/31/2022 00:51:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=66
05/31/2022 00:51:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=66
05/31/2022 00:51:34 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.3486159844054581 on epoch=66
05/31/2022 00:51:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=67
05/31/2022 00:51:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=67
05/31/2022 00:51:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/31/2022 00:51:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=68
05/31/2022 00:51:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=68
05/31/2022 00:51:58 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.3971630574684009 on epoch=68
05/31/2022 00:51:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3796234772978959 -> 0.3971630574684009 on epoch=68, global_step=1650
05/31/2022 00:52:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=69
05/31/2022 00:52:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=69
05/31/2022 00:52:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=69
05/31/2022 00:52:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=70
05/31/2022 00:52:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=70
05/31/2022 00:52:21 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.3877433035102173 on epoch=70
05/31/2022 00:52:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=71
05/31/2022 00:52:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=71
05/31/2022 00:52:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=72
05/31/2022 00:52:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=72
05/31/2022 00:52:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=72
05/31/2022 00:52:43 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.3832857824798233 on epoch=72
05/31/2022 00:52:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=73
05/31/2022 00:52:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/31/2022 00:52:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=74
05/31/2022 00:52:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=74
05/31/2022 00:52:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=74
05/31/2022 00:53:06 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.3339332328096373 on epoch=74
05/31/2022 00:53:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=75
05/31/2022 00:53:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=75
05/31/2022 00:53:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=76
05/31/2022 00:53:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=76
05/31/2022 00:53:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=77
05/31/2022 00:53:28 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.38724203171698024 on epoch=77
05/31/2022 00:53:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=77
05/31/2022 00:53:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=77
05/31/2022 00:53:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=78
05/31/2022 00:53:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=78
05/31/2022 00:53:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/31/2022 00:53:51 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.394024296046626 on epoch=79
05/31/2022 00:53:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=79
05/31/2022 00:53:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=79
05/31/2022 00:53:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.45 on epoch=80
05/31/2022 00:54:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=80
05/31/2022 00:54:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=81
05/31/2022 00:54:13 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.37659365805818895 on epoch=81
05/31/2022 00:54:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=81
05/31/2022 00:54:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=82
05/31/2022 00:54:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=82
05/31/2022 00:54:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=82
05/31/2022 00:54:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.49 on epoch=83
05/31/2022 00:54:35 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.39150515727832236 on epoch=83
05/31/2022 00:54:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=83
05/31/2022 00:54:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=84
05/31/2022 00:54:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=84
05/31/2022 00:54:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=84
05/31/2022 00:54:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=85
05/31/2022 00:54:57 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.3596632214518393 on epoch=85
05/31/2022 00:54:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.36 on epoch=85
05/31/2022 00:55:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=86
05/31/2022 00:55:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=86
05/31/2022 00:55:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=87
05/31/2022 00:55:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=87
05/31/2022 00:55:18 - INFO - __main__ - Global step 2100 Train loss 0.39 Classification-F1 0.38553231918160824 on epoch=87
05/31/2022 00:55:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=87
05/31/2022 00:55:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=88
05/31/2022 00:55:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=88
05/31/2022 00:55:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=89
05/31/2022 00:55:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=89
05/31/2022 00:55:41 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.38873818420832834 on epoch=89
05/31/2022 00:55:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=89
05/31/2022 00:55:46 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=90
05/31/2022 00:55:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=90
05/31/2022 00:55:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=91
05/31/2022 00:55:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=91
05/31/2022 00:56:04 - INFO - __main__ - Global step 2200 Train loss 0.39 Classification-F1 0.3875968992248062 on epoch=91
05/31/2022 00:56:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.37 on epoch=92
05/31/2022 00:56:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.39 on epoch=92
05/31/2022 00:56:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=92
05/31/2022 00:56:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=93
05/31/2022 00:56:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=93
05/31/2022 00:56:27 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.40041638564700116 on epoch=93
05/31/2022 00:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3971630574684009 -> 0.40041638564700116 on epoch=93, global_step=2250
05/31/2022 00:56:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.35 on epoch=94
05/31/2022 00:56:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=94
05/31/2022 00:56:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.43 on epoch=94
05/31/2022 00:56:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.48 on epoch=95
05/31/2022 00:56:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=95
05/31/2022 00:56:50 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.39430231427985185 on epoch=95
05/31/2022 00:56:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=96
05/31/2022 00:56:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=96
05/31/2022 00:56:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=97
05/31/2022 00:57:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=97
05/31/2022 00:57:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=97
05/31/2022 00:57:12 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.40332249470324344 on epoch=97
05/31/2022 00:57:12 - INFO - __main__ - Saving model with best Classification-F1: 0.40041638564700116 -> 0.40332249470324344 on epoch=97, global_step=2350
05/31/2022 00:57:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.36 on epoch=98
05/31/2022 00:57:18 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=98
05/31/2022 00:57:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=99
05/31/2022 00:57:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.49 on epoch=99
05/31/2022 00:57:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=99
05/31/2022 00:57:35 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.3972069597069597 on epoch=99
05/31/2022 00:57:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=100
05/31/2022 00:57:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=100
05/31/2022 00:57:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=101
05/31/2022 00:57:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=101
05/31/2022 00:57:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=102
05/31/2022 00:57:58 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.40843453458477724 on epoch=102
05/31/2022 00:57:58 - INFO - __main__ - Saving model with best Classification-F1: 0.40332249470324344 -> 0.40843453458477724 on epoch=102, global_step=2450
05/31/2022 00:58:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=102
05/31/2022 00:58:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=102
05/31/2022 00:58:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.42 on epoch=103
05/31/2022 00:58:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.35 on epoch=103
05/31/2022 00:58:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=104
05/31/2022 00:58:21 - INFO - __main__ - Global step 2500 Train loss 0.39 Classification-F1 0.3980183290528118 on epoch=104
05/31/2022 00:58:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=104
05/31/2022 00:58:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=104
05/31/2022 00:58:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=105
05/31/2022 00:58:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=105
05/31/2022 00:58:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=106
05/31/2022 00:58:43 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.40873976688424607 on epoch=106
05/31/2022 00:58:43 - INFO - __main__ - Saving model with best Classification-F1: 0.40843453458477724 -> 0.40873976688424607 on epoch=106, global_step=2550
05/31/2022 00:58:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=106
05/31/2022 00:58:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=107
05/31/2022 00:58:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=107
05/31/2022 00:58:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.36 on epoch=107
05/31/2022 00:58:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=108
05/31/2022 00:59:06 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.4034562005798614 on epoch=108
05/31/2022 00:59:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.42 on epoch=108
05/31/2022 00:59:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.40 on epoch=109
05/31/2022 00:59:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=109
05/31/2022 00:59:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.35 on epoch=109
05/31/2022 00:59:19 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=110
05/31/2022 00:59:28 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.39839546887398897 on epoch=110
05/31/2022 00:59:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.34 on epoch=110
05/31/2022 00:59:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=111
05/31/2022 00:59:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=111
05/31/2022 00:59:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.33 on epoch=112
05/31/2022 00:59:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=112
05/31/2022 00:59:51 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.40235567375618003 on epoch=112
05/31/2022 00:59:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=112
05/31/2022 00:59:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.38 on epoch=113
05/31/2022 00:59:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.35 on epoch=113
05/31/2022 01:00:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.32 on epoch=114
05/31/2022 01:00:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=114
05/31/2022 01:00:14 - INFO - __main__ - Global step 2750 Train loss 0.35 Classification-F1 0.40283841533841525 on epoch=114
05/31/2022 01:00:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=114
05/31/2022 01:00:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=115
05/31/2022 01:00:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.31 on epoch=115
05/31/2022 01:00:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=116
05/31/2022 01:00:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.41 on epoch=116
05/31/2022 01:00:36 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.4078739695829083 on epoch=116
05/31/2022 01:00:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.34 on epoch=117
05/31/2022 01:00:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=117
05/31/2022 01:00:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.33 on epoch=117
05/31/2022 01:00:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.37 on epoch=118
05/31/2022 01:00:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=118
05/31/2022 01:00:58 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.43416936889241836 on epoch=118
05/31/2022 01:00:58 - INFO - __main__ - Saving model with best Classification-F1: 0.40873976688424607 -> 0.43416936889241836 on epoch=118, global_step=2850
05/31/2022 01:01:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.28 on epoch=119
05/31/2022 01:01:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=119
05/31/2022 01:01:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=119
05/31/2022 01:01:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=120
05/31/2022 01:01:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=120
05/31/2022 01:01:20 - INFO - __main__ - Global step 2900 Train loss 0.35 Classification-F1 0.40384786122369515 on epoch=120
05/31/2022 01:01:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=121
05/31/2022 01:01:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=121
05/31/2022 01:01:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=122
05/31/2022 01:01:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=122
05/31/2022 01:01:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.34 on epoch=122
05/31/2022 01:01:43 - INFO - __main__ - Global step 2950 Train loss 0.40 Classification-F1 0.41390088186316304 on epoch=122
05/31/2022 01:01:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=123
05/31/2022 01:01:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.34 on epoch=123
05/31/2022 01:01:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.36 on epoch=124
05/31/2022 01:01:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=124
05/31/2022 01:01:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=124
05/31/2022 01:01:57 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:01:57 - INFO - __main__ - Printing 3 examples
05/31/2022 01:01:57 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:01:57 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:57 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:01:57 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:57 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:01:57 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:57 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:01:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:01:58 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:01:58 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:01:58 - INFO - __main__ - Printing 3 examples
05/31/2022 01:01:58 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:01:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:58 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:01:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:58 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:01:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:01:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:01:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:01:59 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:02:05 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.4033872978580719 on epoch=124
05/31/2022 01:02:05 - INFO - __main__ - save last model!
05/31/2022 01:02:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 01:02:05 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 01:02:05 - INFO - __main__ - Printing 3 examples
05/31/2022 01:02:05 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 01:02:05 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:05 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 01:02:05 - INFO - __main__ - ['entailment']
05/31/2022 01:02:05 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 01:02:05 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:02:06 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:02:07 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 01:02:17 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:02:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:02:17 - INFO - __main__ - Starting training!
05/31/2022 01:02:30 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_100_0.2_8_predictions.txt
05/31/2022 01:02:30 - INFO - __main__ - Classification-F1 on test data: 0.2869
05/31/2022 01:02:30 - INFO - __main__ - prefix=anli_128_100, lr=0.2, bsz=8, dev_performance=0.43416936889241836, test_performance=0.2869327770500611
05/31/2022 01:02:30 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.5, bsz=8 ...
05/31/2022 01:02:31 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:02:31 - INFO - __main__ - Printing 3 examples
05/31/2022 01:02:31 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:02:31 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:31 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:02:31 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:31 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:02:31 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:02:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:02:32 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:02:32 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:02:32 - INFO - __main__ - Printing 3 examples
05/31/2022 01:02:32 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:02:32 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:32 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:02:32 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:32 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:02:32 - INFO - __main__ - ['contradiction']
05/31/2022 01:02:32 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:02:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:02:32 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:02:50 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:02:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:02:51 - INFO - __main__ - Starting training!
05/31/2022 01:02:55 - INFO - __main__ - Step 10 Global step 10 Train loss 0.63 on epoch=0
05/31/2022 01:02:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=0
05/31/2022 01:03:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=1
05/31/2022 01:03:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=1
05/31/2022 01:03:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=2
05/31/2022 01:03:17 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 01:03:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 01:03:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=2
05/31/2022 01:03:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=2
05/31/2022 01:03:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=3
05/31/2022 01:03:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/31/2022 01:03:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=4
05/31/2022 01:03:40 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 01:03:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=4
05/31/2022 01:03:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=4
05/31/2022 01:03:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=5
05/31/2022 01:03:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=5
05/31/2022 01:03:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/31/2022 01:04:03 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.17273804346195953 on epoch=6
05/31/2022 01:04:03 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17273804346195953 on epoch=6, global_step=150
05/31/2022 01:04:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=6
05/31/2022 01:04:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=7
05/31/2022 01:04:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=7
05/31/2022 01:04:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=7
05/31/2022 01:04:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=8
05/31/2022 01:04:26 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 01:04:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=8
05/31/2022 01:04:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=9
05/31/2022 01:04:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=9
05/31/2022 01:04:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/31/2022 01:04:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=10
05/31/2022 01:04:49 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.18836044746975922 on epoch=10
05/31/2022 01:04:49 - INFO - __main__ - Saving model with best Classification-F1: 0.17273804346195953 -> 0.18836044746975922 on epoch=10, global_step=250
05/31/2022 01:04:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=10
05/31/2022 01:04:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/31/2022 01:04:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=11
05/31/2022 01:05:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=12
05/31/2022 01:05:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/31/2022 01:05:12 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 01:05:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=12
05/31/2022 01:05:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=13
05/31/2022 01:05:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=13
05/31/2022 01:05:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/31/2022 01:05:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=14
05/31/2022 01:05:35 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 01:05:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=14
05/31/2022 01:05:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=15
05/31/2022 01:05:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=15
05/31/2022 01:05:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/31/2022 01:05:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=16
05/31/2022 01:05:58 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.1721607831834019 on epoch=16
05/31/2022 01:06:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=17
05/31/2022 01:06:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=17
05/31/2022 01:06:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=17
05/31/2022 01:06:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=18
05/31/2022 01:06:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=18
05/31/2022 01:06:21 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.21346498065708183 on epoch=18
05/31/2022 01:06:21 - INFO - __main__ - Saving model with best Classification-F1: 0.18836044746975922 -> 0.21346498065708183 on epoch=18, global_step=450
05/31/2022 01:06:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=19
05/31/2022 01:06:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=19
05/31/2022 01:06:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=19
05/31/2022 01:06:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=20
05/31/2022 01:06:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=20
05/31/2022 01:06:44 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.2037120343631641 on epoch=20
05/31/2022 01:06:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=21
05/31/2022 01:06:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=21
05/31/2022 01:06:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=22
05/31/2022 01:06:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=22
05/31/2022 01:06:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/31/2022 01:07:07 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.20362589480236537 on epoch=22
05/31/2022 01:07:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=23
05/31/2022 01:07:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=23
05/31/2022 01:07:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=24
05/31/2022 01:07:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=24
05/31/2022 01:07:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=24
05/31/2022 01:07:30 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.3196798493408663 on epoch=24
05/31/2022 01:07:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21346498065708183 -> 0.3196798493408663 on epoch=24, global_step=600
05/31/2022 01:07:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=25
05/31/2022 01:07:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=25
05/31/2022 01:07:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=26
05/31/2022 01:07:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=26
05/31/2022 01:07:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=27
05/31/2022 01:07:53 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.24135608963195168 on epoch=27
05/31/2022 01:07:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=27
05/31/2022 01:07:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=27
05/31/2022 01:08:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=28
05/31/2022 01:08:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=28
05/31/2022 01:08:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=29
05/31/2022 01:08:15 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.27108301682525626 on epoch=29
05/31/2022 01:08:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=29
05/31/2022 01:08:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=29
05/31/2022 01:08:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=30
05/31/2022 01:08:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=30
05/31/2022 01:08:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=31
05/31/2022 01:08:38 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.320129297430509 on epoch=31
05/31/2022 01:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3196798493408663 -> 0.320129297430509 on epoch=31, global_step=750
05/31/2022 01:08:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=31
05/31/2022 01:08:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=32
05/31/2022 01:08:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=32
05/31/2022 01:08:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=32
05/31/2022 01:08:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=33
05/31/2022 01:09:01 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.3500303026605329 on epoch=33
05/31/2022 01:09:01 - INFO - __main__ - Saving model with best Classification-F1: 0.320129297430509 -> 0.3500303026605329 on epoch=33, global_step=800
05/31/2022 01:09:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=33
05/31/2022 01:09:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=34
05/31/2022 01:09:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=34
05/31/2022 01:09:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=34
05/31/2022 01:09:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=35
05/31/2022 01:09:24 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.38272727272727275 on epoch=35
05/31/2022 01:09:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3500303026605329 -> 0.38272727272727275 on epoch=35, global_step=850
05/31/2022 01:09:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=35
05/31/2022 01:09:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=36
05/31/2022 01:09:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=36
05/31/2022 01:09:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=37
05/31/2022 01:09:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=37
05/31/2022 01:09:47 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.38045614516202747 on epoch=37
05/31/2022 01:09:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/31/2022 01:09:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=38
05/31/2022 01:09:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=38
05/31/2022 01:09:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=39
05/31/2022 01:10:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=39
05/31/2022 01:10:09 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.34403828128854025 on epoch=39
05/31/2022 01:10:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 01:10:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=40
05/31/2022 01:10:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=40
05/31/2022 01:10:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=41
05/31/2022 01:10:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=41
05/31/2022 01:10:32 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.3743388123095384 on epoch=41
05/31/2022 01:10:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=42
05/31/2022 01:10:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=42
05/31/2022 01:10:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=42
05/31/2022 01:10:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=43
05/31/2022 01:10:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/31/2022 01:10:55 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.38534675615212527 on epoch=43
05/31/2022 01:10:55 - INFO - __main__ - Saving model with best Classification-F1: 0.38272727272727275 -> 0.38534675615212527 on epoch=43, global_step=1050
05/31/2022 01:10:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=44
05/31/2022 01:11:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=44
05/31/2022 01:11:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=44
05/31/2022 01:11:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=45
05/31/2022 01:11:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=45
05/31/2022 01:11:18 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.3980957395463121 on epoch=45
05/31/2022 01:11:18 - INFO - __main__ - Saving model with best Classification-F1: 0.38534675615212527 -> 0.3980957395463121 on epoch=45, global_step=1100
05/31/2022 01:11:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=46
05/31/2022 01:11:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=46
05/31/2022 01:11:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=47
05/31/2022 01:11:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=47
05/31/2022 01:11:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=47
05/31/2022 01:11:41 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.40432476540187673 on epoch=47
05/31/2022 01:11:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3980957395463121 -> 0.40432476540187673 on epoch=47, global_step=1150
05/31/2022 01:11:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=48
05/31/2022 01:11:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=48
05/31/2022 01:11:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=49
05/31/2022 01:11:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=49
05/31/2022 01:11:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=49
05/31/2022 01:12:04 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.42478200616137896 on epoch=49
05/31/2022 01:12:04 - INFO - __main__ - Saving model with best Classification-F1: 0.40432476540187673 -> 0.42478200616137896 on epoch=49, global_step=1200
05/31/2022 01:12:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=50
05/31/2022 01:12:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=50
05/31/2022 01:12:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=51
05/31/2022 01:12:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=51
05/31/2022 01:12:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=52
05/31/2022 01:12:26 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.40121100434381995 on epoch=52
05/31/2022 01:12:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=52
05/31/2022 01:12:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=52
05/31/2022 01:12:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=53
05/31/2022 01:12:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=53
05/31/2022 01:12:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=54
05/31/2022 01:12:49 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.41611111111111115 on epoch=54
05/31/2022 01:12:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=54
05/31/2022 01:12:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=54
05/31/2022 01:12:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=55
05/31/2022 01:12:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=55
05/31/2022 01:13:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=56
05/31/2022 01:13:11 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.40706490969038284 on epoch=56
05/31/2022 01:13:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=56
05/31/2022 01:13:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=57
05/31/2022 01:13:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=57
05/31/2022 01:13:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=57
05/31/2022 01:13:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=58
05/31/2022 01:13:34 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.40334047236599435 on epoch=58
05/31/2022 01:13:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=58
05/31/2022 01:13:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=59
05/31/2022 01:13:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=59
05/31/2022 01:13:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=59
05/31/2022 01:13:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=60
05/31/2022 01:13:56 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.410260669739884 on epoch=60
05/31/2022 01:13:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=60
05/31/2022 01:14:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=61
05/31/2022 01:14:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=61
05/31/2022 01:14:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=62
05/31/2022 01:14:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=62
05/31/2022 01:14:19 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.39854085050814736 on epoch=62
05/31/2022 01:14:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=62
05/31/2022 01:14:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=63
05/31/2022 01:14:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=63
05/31/2022 01:14:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=64
05/31/2022 01:14:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=64
05/31/2022 01:14:43 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.4226675665853279 on epoch=64
05/31/2022 01:14:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=64
05/31/2022 01:14:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=65
05/31/2022 01:14:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=65
05/31/2022 01:14:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=66
05/31/2022 01:14:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=66
05/31/2022 01:15:07 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.41082418546873517 on epoch=66
05/31/2022 01:15:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=67
05/31/2022 01:15:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=67
05/31/2022 01:15:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=67
05/31/2022 01:15:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=68
05/31/2022 01:15:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=68
05/31/2022 01:15:30 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.4097344097344098 on epoch=68
05/31/2022 01:15:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=69
05/31/2022 01:15:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=69
05/31/2022 01:15:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=69
05/31/2022 01:15:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=70
05/31/2022 01:15:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=70
05/31/2022 01:15:53 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.4036832651180538 on epoch=70
05/31/2022 01:15:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=71
05/31/2022 01:15:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=71
05/31/2022 01:16:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=72
05/31/2022 01:16:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=72
05/31/2022 01:16:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=72
05/31/2022 01:16:16 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.43383815758569916 on epoch=72
05/31/2022 01:16:16 - INFO - __main__ - Saving model with best Classification-F1: 0.42478200616137896 -> 0.43383815758569916 on epoch=72, global_step=1750
05/31/2022 01:16:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=73
05/31/2022 01:16:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=73
05/31/2022 01:16:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=74
05/31/2022 01:16:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=74
05/31/2022 01:16:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=74
05/31/2022 01:16:38 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.42324570453826366 on epoch=74
05/31/2022 01:16:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=75
05/31/2022 01:16:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=75
05/31/2022 01:16:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=76
05/31/2022 01:16:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=76
05/31/2022 01:16:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=77
05/31/2022 01:17:01 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.4148983013089653 on epoch=77
05/31/2022 01:17:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=77
05/31/2022 01:17:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=77
05/31/2022 01:17:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=78
05/31/2022 01:17:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=78
05/31/2022 01:17:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=79
05/31/2022 01:17:24 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.4068324851163598 on epoch=79
05/31/2022 01:17:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=79
05/31/2022 01:17:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=79
05/31/2022 01:17:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=80
05/31/2022 01:17:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=80
05/31/2022 01:17:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=81
05/31/2022 01:17:48 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.4425479920835958 on epoch=81
05/31/2022 01:17:48 - INFO - __main__ - Saving model with best Classification-F1: 0.43383815758569916 -> 0.4425479920835958 on epoch=81, global_step=1950
05/31/2022 01:17:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=81
05/31/2022 01:17:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=82
05/31/2022 01:17:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=82
05/31/2022 01:17:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=82
05/31/2022 01:18:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=83
05/31/2022 01:18:11 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.4218012245332454 on epoch=83
05/31/2022 01:18:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.26 on epoch=83
05/31/2022 01:18:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=84
05/31/2022 01:18:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=84
05/31/2022 01:18:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=84
05/31/2022 01:18:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.32 on epoch=85
05/31/2022 01:18:34 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.4452043329464996 on epoch=85
05/31/2022 01:18:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4425479920835958 -> 0.4452043329464996 on epoch=85, global_step=2050
05/31/2022 01:18:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=85
05/31/2022 01:18:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=86
05/31/2022 01:18:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=86
05/31/2022 01:18:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=87
05/31/2022 01:18:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=87
05/31/2022 01:18:57 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.44516796251993623 on epoch=87
05/31/2022 01:19:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=87
05/31/2022 01:19:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=88
05/31/2022 01:19:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=88
05/31/2022 01:19:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.29 on epoch=89
05/31/2022 01:19:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=89
05/31/2022 01:19:21 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.4353632336342736 on epoch=89
05/31/2022 01:19:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=89
05/31/2022 01:19:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=90
05/31/2022 01:19:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=90
05/31/2022 01:19:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=91
05/31/2022 01:19:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=91
05/31/2022 01:19:45 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.4323520968786309 on epoch=91
05/31/2022 01:19:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=92
05/31/2022 01:19:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.25 on epoch=92
05/31/2022 01:19:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=92
05/31/2022 01:19:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=93
05/31/2022 01:19:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=93
05/31/2022 01:20:09 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.4542053608898528 on epoch=93
05/31/2022 01:20:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4452043329464996 -> 0.4542053608898528 on epoch=93, global_step=2250
05/31/2022 01:20:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.31 on epoch=94
05/31/2022 01:20:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=94
05/31/2022 01:20:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=94
05/31/2022 01:20:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=95
05/31/2022 01:20:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=95
05/31/2022 01:20:33 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.4351348746540393 on epoch=95
05/31/2022 01:20:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=96
05/31/2022 01:20:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=96
05/31/2022 01:20:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=97
05/31/2022 01:20:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.25 on epoch=97
05/31/2022 01:20:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=97
05/31/2022 01:20:57 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.44848073396150473 on epoch=97
05/31/2022 01:21:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=98
05/31/2022 01:21:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=98
05/31/2022 01:21:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.32 on epoch=99
05/31/2022 01:21:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=99
05/31/2022 01:21:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=99
05/31/2022 01:21:21 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.4565728801658285 on epoch=99
05/31/2022 01:21:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4542053608898528 -> 0.4565728801658285 on epoch=99, global_step=2400
05/31/2022 01:21:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=100
05/31/2022 01:21:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=100
05/31/2022 01:21:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=101
05/31/2022 01:21:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=101
05/31/2022 01:21:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.24 on epoch=102
05/31/2022 01:21:46 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.4763017424887928 on epoch=102
05/31/2022 01:21:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4565728801658285 -> 0.4763017424887928 on epoch=102, global_step=2450
05/31/2022 01:21:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.27 on epoch=102
05/31/2022 01:21:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=102
05/31/2022 01:21:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=103
05/31/2022 01:21:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=103
05/31/2022 01:21:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.26 on epoch=104
05/31/2022 01:22:09 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.45763015927933465 on epoch=104
05/31/2022 01:22:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=104
05/31/2022 01:22:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=104
05/31/2022 01:22:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=105
05/31/2022 01:22:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=105
05/31/2022 01:22:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.27 on epoch=106
05/31/2022 01:22:33 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.4414764654615679 on epoch=106
05/31/2022 01:22:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=106
05/31/2022 01:22:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.28 on epoch=107
05/31/2022 01:22:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=107
05/31/2022 01:22:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=107
05/31/2022 01:22:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=108
05/31/2022 01:22:57 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.4508063481591438 on epoch=108
05/31/2022 01:23:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=108
05/31/2022 01:23:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=109
05/31/2022 01:23:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=109
05/31/2022 01:23:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=109
05/31/2022 01:23:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=110
05/31/2022 01:23:21 - INFO - __main__ - Global step 2650 Train loss 0.23 Classification-F1 0.4580380623429372 on epoch=110
05/31/2022 01:23:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=110
05/31/2022 01:23:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.27 on epoch=111
05/31/2022 01:23:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=111
05/31/2022 01:23:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.25 on epoch=112
05/31/2022 01:23:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=112
05/31/2022 01:23:44 - INFO - __main__ - Global step 2700 Train loss 0.21 Classification-F1 0.45668841124673176 on epoch=112
05/31/2022 01:23:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=112
05/31/2022 01:23:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.22 on epoch=113
05/31/2022 01:23:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=113
05/31/2022 01:23:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=114
05/31/2022 01:23:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=114
05/31/2022 01:24:07 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.44009228056704913 on epoch=114
05/31/2022 01:24:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=114
05/31/2022 01:24:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=115
05/31/2022 01:24:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.24 on epoch=115
05/31/2022 01:24:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=116
05/31/2022 01:24:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=116
05/31/2022 01:24:31 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.4742212078618415 on epoch=116
05/31/2022 01:24:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=117
05/31/2022 01:24:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=117
05/31/2022 01:24:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=117
05/31/2022 01:24:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=118
05/31/2022 01:24:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=118
05/31/2022 01:24:55 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.4501587874911426 on epoch=118
05/31/2022 01:24:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=119
05/31/2022 01:25:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.19 on epoch=119
05/31/2022 01:25:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=119
05/31/2022 01:25:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.29 on epoch=120
05/31/2022 01:25:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=120
05/31/2022 01:25:19 - INFO - __main__ - Global step 2900 Train loss 0.22 Classification-F1 0.48995915564136033 on epoch=120
05/31/2022 01:25:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4763017424887928 -> 0.48995915564136033 on epoch=120, global_step=2900
05/31/2022 01:25:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=121
05/31/2022 01:25:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=121
05/31/2022 01:25:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=122
05/31/2022 01:25:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=122
05/31/2022 01:25:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=122
05/31/2022 01:25:43 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.4843562357192199 on epoch=122
05/31/2022 01:25:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=123
05/31/2022 01:25:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=123
05/31/2022 01:25:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=124
05/31/2022 01:25:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=124
05/31/2022 01:25:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=124
05/31/2022 01:25:58 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:25:58 - INFO - __main__ - Printing 3 examples
05/31/2022 01:25:58 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:25:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:58 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:25:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:58 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:25:58 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:25:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:25:59 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:25:59 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:25:59 - INFO - __main__ - Printing 3 examples
05/31/2022 01:25:59 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:25:59 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:59 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:25:59 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:59 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:25:59 - INFO - __main__ - ['contradiction']
05/31/2022 01:25:59 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:25:59 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:25:59 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:26:07 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.4945943090915254 on epoch=124
05/31/2022 01:26:07 - INFO - __main__ - Saving model with best Classification-F1: 0.48995915564136033 -> 0.4945943090915254 on epoch=124, global_step=3000
05/31/2022 01:26:07 - INFO - __main__ - save last model!
05/31/2022 01:26:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 01:26:07 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 01:26:07 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:07 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 01:26:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:07 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 01:26:07 - INFO - __main__ - ['entailment']
05/31/2022 01:26:07 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 01:26:07 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:07 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:08 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:09 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 01:26:17 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:26:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:26:18 - INFO - __main__ - Starting training!
05/31/2022 01:26:36 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_13_0.5_8_predictions.txt
05/31/2022 01:26:36 - INFO - __main__ - Classification-F1 on test data: 0.3135
05/31/2022 01:26:37 - INFO - __main__ - prefix=anli_128_13, lr=0.5, bsz=8, dev_performance=0.4945943090915254, test_performance=0.3134647162148268
05/31/2022 01:26:37 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.4, bsz=8 ...
05/31/2022 01:26:38 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:26:38 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:38 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:26:38 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:26:38 - INFO - __main__ - Printing 3 examples
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:26:38 - INFO - __main__ - ['contradiction']
05/31/2022 01:26:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:26:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:26:39 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:26:57 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:26:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:26:58 - INFO - __main__ - Starting training!
05/31/2022 01:27:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=0
05/31/2022 01:27:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=0
05/31/2022 01:27:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
05/31/2022 01:27:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=1
05/31/2022 01:27:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=2
05/31/2022 01:27:22 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 01:27:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 01:27:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=2
05/31/2022 01:27:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=2
05/31/2022 01:27:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=3
05/31/2022 01:27:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=3
05/31/2022 01:27:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=4
05/31/2022 01:27:45 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 01:27:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=4
05/31/2022 01:27:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=4
05/31/2022 01:27:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=5
05/31/2022 01:27:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=5
05/31/2022 01:27:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=6
05/31/2022 01:28:09 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 01:28:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=6
05/31/2022 01:28:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=7
05/31/2022 01:28:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=7
05/31/2022 01:28:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=7
05/31/2022 01:28:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=8
05/31/2022 01:28:33 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 01:28:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=8
05/31/2022 01:28:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=9
05/31/2022 01:28:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=9
05/31/2022 01:28:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=9
05/31/2022 01:28:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=10
05/31/2022 01:28:56 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 01:28:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=10
05/31/2022 01:29:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=11
05/31/2022 01:29:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=11
05/31/2022 01:29:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=12
05/31/2022 01:29:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=12
05/31/2022 01:29:19 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.1721607831834019 on epoch=12
05/31/2022 01:29:19 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1721607831834019 on epoch=12, global_step=300
05/31/2022 01:29:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=12
05/31/2022 01:29:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=13
05/31/2022 01:29:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=13
05/31/2022 01:29:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=14
05/31/2022 01:29:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=14
05/31/2022 01:29:42 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 01:29:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=14
05/31/2022 01:29:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=15
05/31/2022 01:29:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=15
05/31/2022 01:29:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/31/2022 01:29:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=16
05/31/2022 01:30:05 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 01:30:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=17
05/31/2022 01:30:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=17
05/31/2022 01:30:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=17
05/31/2022 01:30:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=18
05/31/2022 01:30:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=18
05/31/2022 01:30:29 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.1721607831834019 on epoch=18
05/31/2022 01:30:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=19
05/31/2022 01:30:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=19
05/31/2022 01:30:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=19
05/31/2022 01:30:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=20
05/31/2022 01:30:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=20
05/31/2022 01:30:52 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.17765617875057005 on epoch=20
05/31/2022 01:30:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1721607831834019 -> 0.17765617875057005 on epoch=20, global_step=500
05/31/2022 01:30:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=21
05/31/2022 01:30:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=21
05/31/2022 01:31:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/31/2022 01:31:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=22
05/31/2022 01:31:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=22
05/31/2022 01:31:14 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=22
05/31/2022 01:31:14 - INFO - __main__ - Saving model with best Classification-F1: 0.17765617875057005 -> 0.1881810228266921 on epoch=22, global_step=550
05/31/2022 01:31:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/31/2022 01:31:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/31/2022 01:31:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=24
05/31/2022 01:31:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=24
05/31/2022 01:31:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=24
05/31/2022 01:31:37 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.23222819593787336 on epoch=24
05/31/2022 01:31:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.23222819593787336 on epoch=24, global_step=600
05/31/2022 01:31:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=25
05/31/2022 01:31:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=25
05/31/2022 01:31:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=26
05/31/2022 01:31:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=26
05/31/2022 01:31:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=27
05/31/2022 01:32:00 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.18291616051030557 on epoch=27
05/31/2022 01:32:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=27
05/31/2022 01:32:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=27
05/31/2022 01:32:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=28
05/31/2022 01:32:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/31/2022 01:32:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=29
05/31/2022 01:32:22 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.19351857314219414 on epoch=29
05/31/2022 01:32:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=29
05/31/2022 01:32:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=29
05/31/2022 01:32:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=30
05/31/2022 01:32:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=30
05/31/2022 01:32:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=31
05/31/2022 01:32:43 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.2791708236337435 on epoch=31
05/31/2022 01:32:43 - INFO - __main__ - Saving model with best Classification-F1: 0.23222819593787336 -> 0.2791708236337435 on epoch=31, global_step=750
05/31/2022 01:32:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=31
05/31/2022 01:32:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=32
05/31/2022 01:32:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=32
05/31/2022 01:32:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=32
05/31/2022 01:32:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=33
05/31/2022 01:33:05 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.19860652731939862 on epoch=33
05/31/2022 01:33:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=33
05/31/2022 01:33:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=34
05/31/2022 01:33:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=34
05/31/2022 01:33:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=34
05/31/2022 01:33:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=35
05/31/2022 01:33:26 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.3672486453943408 on epoch=35
05/31/2022 01:33:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2791708236337435 -> 0.3672486453943408 on epoch=35, global_step=850
05/31/2022 01:33:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/31/2022 01:33:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=36
05/31/2022 01:33:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/31/2022 01:33:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=37
05/31/2022 01:33:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=37
05/31/2022 01:33:48 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3161372814624451 on epoch=37
05/31/2022 01:33:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=37
05/31/2022 01:33:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=38
05/31/2022 01:33:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=38
05/31/2022 01:33:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=39
05/31/2022 01:34:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=39
05/31/2022 01:34:09 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.3621414975126765 on epoch=39
05/31/2022 01:34:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=39
05/31/2022 01:34:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=40
05/31/2022 01:34:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=40
05/31/2022 01:34:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=41
05/31/2022 01:34:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=41
05/31/2022 01:34:32 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.33388941453778864 on epoch=41
05/31/2022 01:34:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=42
05/31/2022 01:34:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=42
05/31/2022 01:34:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=42
05/31/2022 01:34:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=43
05/31/2022 01:34:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=43
05/31/2022 01:34:54 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.3590237404558714 on epoch=43
05/31/2022 01:34:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=44
05/31/2022 01:34:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=44
05/31/2022 01:35:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/31/2022 01:35:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=45
05/31/2022 01:35:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=45
05/31/2022 01:35:15 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.3330156411811547 on epoch=45
05/31/2022 01:35:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=46
05/31/2022 01:35:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=46
05/31/2022 01:35:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=47
05/31/2022 01:35:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=47
05/31/2022 01:35:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=47
05/31/2022 01:35:37 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.3701919860860259 on epoch=47
05/31/2022 01:35:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3672486453943408 -> 0.3701919860860259 on epoch=47, global_step=1150
05/31/2022 01:35:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=48
05/31/2022 01:35:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=48
05/31/2022 01:35:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=49
05/31/2022 01:35:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=49
05/31/2022 01:35:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=49
05/31/2022 01:35:58 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.38551358620715703 on epoch=49
05/31/2022 01:35:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3701919860860259 -> 0.38551358620715703 on epoch=49, global_step=1200
05/31/2022 01:36:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=50
05/31/2022 01:36:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=50
05/31/2022 01:36:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=51
05/31/2022 01:36:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=51
05/31/2022 01:36:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=52
05/31/2022 01:36:19 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.37704886173268504 on epoch=52
05/31/2022 01:36:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=52
05/31/2022 01:36:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=52
05/31/2022 01:36:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=53
05/31/2022 01:36:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=53
05/31/2022 01:36:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=54
05/31/2022 01:36:41 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.3626571626571627 on epoch=54
05/31/2022 01:36:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=54
05/31/2022 01:36:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=54
05/31/2022 01:36:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=55
05/31/2022 01:36:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=55
05/31/2022 01:36:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=56
05/31/2022 01:37:03 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.37704886173268504 on epoch=56
05/31/2022 01:37:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=56
05/31/2022 01:37:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=57
05/31/2022 01:37:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=57
05/31/2022 01:37:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=57
05/31/2022 01:37:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=58
05/31/2022 01:37:25 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.39059402810783467 on epoch=58
05/31/2022 01:37:25 - INFO - __main__ - Saving model with best Classification-F1: 0.38551358620715703 -> 0.39059402810783467 on epoch=58, global_step=1400
05/31/2022 01:37:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=58
05/31/2022 01:37:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=59
05/31/2022 01:37:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=59
05/31/2022 01:37:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=59
05/31/2022 01:37:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=60
05/31/2022 01:37:47 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.40894660894660895 on epoch=60
05/31/2022 01:37:47 - INFO - __main__ - Saving model with best Classification-F1: 0.39059402810783467 -> 0.40894660894660895 on epoch=60, global_step=1450
05/31/2022 01:37:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=60
05/31/2022 01:37:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=61
05/31/2022 01:37:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=61
05/31/2022 01:37:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=62
05/31/2022 01:38:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=62
05/31/2022 01:38:09 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.40484509666899604 on epoch=62
05/31/2022 01:38:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=62
05/31/2022 01:38:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=63
05/31/2022 01:38:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=63
05/31/2022 01:38:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=64
05/31/2022 01:38:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=64
05/31/2022 01:38:30 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.4022321147733286 on epoch=64
05/31/2022 01:38:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=64
05/31/2022 01:38:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=65
05/31/2022 01:38:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=65
05/31/2022 01:38:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=66
05/31/2022 01:38:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=66
05/31/2022 01:38:53 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.411515768808429 on epoch=66
05/31/2022 01:38:53 - INFO - __main__ - Saving model with best Classification-F1: 0.40894660894660895 -> 0.411515768808429 on epoch=66, global_step=1600
05/31/2022 01:38:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=67
05/31/2022 01:38:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=67
05/31/2022 01:39:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=67
05/31/2022 01:39:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=68
05/31/2022 01:39:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=68
05/31/2022 01:39:15 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.4041537598733693 on epoch=68
05/31/2022 01:39:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=69
05/31/2022 01:39:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=69
05/31/2022 01:39:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=69
05/31/2022 01:39:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=70
05/31/2022 01:39:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=70
05/31/2022 01:39:37 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.3914812156018186 on epoch=70
05/31/2022 01:39:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=71
05/31/2022 01:39:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=71
05/31/2022 01:39:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=72
05/31/2022 01:39:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=72
05/31/2022 01:39:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=72
05/31/2022 01:40:00 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.40438655604532386 on epoch=72
05/31/2022 01:40:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=73
05/31/2022 01:40:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=73
05/31/2022 01:40:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=74
05/31/2022 01:40:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=74
05/31/2022 01:40:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=74
05/31/2022 01:40:22 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.4242952105545963 on epoch=74
05/31/2022 01:40:22 - INFO - __main__ - Saving model with best Classification-F1: 0.411515768808429 -> 0.4242952105545963 on epoch=74, global_step=1800
05/31/2022 01:40:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=75
05/31/2022 01:40:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=75
05/31/2022 01:40:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=76
05/31/2022 01:40:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=76
05/31/2022 01:40:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=77
05/31/2022 01:40:45 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.40320215620093225 on epoch=77
05/31/2022 01:40:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=77
05/31/2022 01:40:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=77
05/31/2022 01:40:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=78
05/31/2022 01:40:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=78
05/31/2022 01:40:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=79
05/31/2022 01:41:08 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.4026593561477283 on epoch=79
05/31/2022 01:41:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=79
05/31/2022 01:41:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=79
05/31/2022 01:41:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=80
05/31/2022 01:41:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=80
05/31/2022 01:41:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=81
05/31/2022 01:41:30 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.42263270016390636 on epoch=81
05/31/2022 01:41:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=81
05/31/2022 01:41:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=82
05/31/2022 01:41:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=82
05/31/2022 01:41:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=82
05/31/2022 01:41:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=83
05/31/2022 01:41:54 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.4097326847157509 on epoch=83
05/31/2022 01:41:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=83
05/31/2022 01:42:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=84
05/31/2022 01:42:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=84
05/31/2022 01:42:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=84
05/31/2022 01:42:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=85
05/31/2022 01:42:18 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.4309750674161323 on epoch=85
05/31/2022 01:42:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4242952105545963 -> 0.4309750674161323 on epoch=85, global_step=2050
05/31/2022 01:42:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=85
05/31/2022 01:42:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=86
05/31/2022 01:42:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=86
05/31/2022 01:42:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=87
05/31/2022 01:42:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=87
05/31/2022 01:42:42 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.4085747863724669 on epoch=87
05/31/2022 01:42:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=87
05/31/2022 01:42:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=88
05/31/2022 01:42:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=88
05/31/2022 01:42:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=89
05/31/2022 01:42:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=89
05/31/2022 01:43:06 - INFO - __main__ - Global step 2150 Train loss 0.31 Classification-F1 0.41898244398244405 on epoch=89
05/31/2022 01:43:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.31 on epoch=89
05/31/2022 01:43:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=90
05/31/2022 01:43:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=90
05/31/2022 01:43:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=91
05/31/2022 01:43:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.27 on epoch=91
05/31/2022 01:43:30 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.44155091051642775 on epoch=91
05/31/2022 01:43:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4309750674161323 -> 0.44155091051642775 on epoch=91, global_step=2200
05/31/2022 01:43:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=92
05/31/2022 01:43:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.34 on epoch=92
05/31/2022 01:43:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=92
05/31/2022 01:43:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=93
05/31/2022 01:43:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.27 on epoch=93
05/31/2022 01:43:54 - INFO - __main__ - Global step 2250 Train loss 0.31 Classification-F1 0.4402958645741634 on epoch=93
05/31/2022 01:43:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=94
05/31/2022 01:43:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=94
05/31/2022 01:44:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=94
05/31/2022 01:44:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=95
05/31/2022 01:44:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=95
05/31/2022 01:44:18 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.4647830713372694 on epoch=95
05/31/2022 01:44:18 - INFO - __main__ - Saving model with best Classification-F1: 0.44155091051642775 -> 0.4647830713372694 on epoch=95, global_step=2300
05/31/2022 01:44:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=96
05/31/2022 01:44:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.27 on epoch=96
05/31/2022 01:44:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=97
05/31/2022 01:44:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.31 on epoch=97
05/31/2022 01:44:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=97
05/31/2022 01:44:41 - INFO - __main__ - Global step 2350 Train loss 0.28 Classification-F1 0.4498980507299568 on epoch=97
05/31/2022 01:44:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=98
05/31/2022 01:44:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=98
05/31/2022 01:44:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.29 on epoch=99
05/31/2022 01:44:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=99
05/31/2022 01:44:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=99
05/31/2022 01:45:04 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.45374835687972376 on epoch=99
05/31/2022 01:45:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=100
05/31/2022 01:45:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.28 on epoch=100
05/31/2022 01:45:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=101
05/31/2022 01:45:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.27 on epoch=101
05/31/2022 01:45:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.27 on epoch=102
05/31/2022 01:45:28 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.45850934925438763 on epoch=102
05/31/2022 01:45:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=102
05/31/2022 01:45:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=102
05/31/2022 01:45:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=103
05/31/2022 01:45:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.24 on epoch=103
05/31/2022 01:45:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.33 on epoch=104
05/31/2022 01:45:52 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.45406911928651056 on epoch=104
05/31/2022 01:45:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=104
05/31/2022 01:45:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.25 on epoch=104
05/31/2022 01:46:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.29 on epoch=105
05/31/2022 01:46:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=105
05/31/2022 01:46:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.27 on epoch=106
05/31/2022 01:46:16 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.4510288140163415 on epoch=106
05/31/2022 01:46:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=106
05/31/2022 01:46:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.25 on epoch=107
05/31/2022 01:46:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=107
05/31/2022 01:46:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.28 on epoch=107
05/31/2022 01:46:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=108
05/31/2022 01:46:40 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.45354101160379307 on epoch=108
05/31/2022 01:46:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.24 on epoch=108
05/31/2022 01:46:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.28 on epoch=109
05/31/2022 01:46:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=109
05/31/2022 01:46:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.28 on epoch=109
05/31/2022 01:46:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=110
05/31/2022 01:47:04 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.4809953981717106 on epoch=110
05/31/2022 01:47:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4647830713372694 -> 0.4809953981717106 on epoch=110, global_step=2650
05/31/2022 01:47:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=110
05/31/2022 01:47:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=111
05/31/2022 01:47:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.28 on epoch=111
05/31/2022 01:47:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=112
05/31/2022 01:47:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=112
05/31/2022 01:47:28 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.4670742411461963 on epoch=112
05/31/2022 01:47:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.28 on epoch=112
05/31/2022 01:47:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=113
05/31/2022 01:47:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=113
05/31/2022 01:47:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=114
05/31/2022 01:47:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=114
05/31/2022 01:47:53 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.4543062771700266 on epoch=114
05/31/2022 01:47:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=114
05/31/2022 01:47:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=115
05/31/2022 01:48:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=115
05/31/2022 01:48:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.23 on epoch=116
05/31/2022 01:48:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.23 on epoch=116
05/31/2022 01:48:17 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.4761408730158731 on epoch=116
05/31/2022 01:48:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=117
05/31/2022 01:48:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=117
05/31/2022 01:48:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.28 on epoch=117
05/31/2022 01:48:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=118
05/31/2022 01:48:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.33 on epoch=118
05/31/2022 01:48:41 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.4811977625246302 on epoch=118
05/31/2022 01:48:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4809953981717106 -> 0.4811977625246302 on epoch=118, global_step=2850
05/31/2022 01:48:43 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.28 on epoch=119
05/31/2022 01:48:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=119
05/31/2022 01:48:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=119
05/31/2022 01:48:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=120
05/31/2022 01:48:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=120
05/31/2022 01:49:05 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.4470845945110062 on epoch=120
05/31/2022 01:49:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.27 on epoch=121
05/31/2022 01:49:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=121
05/31/2022 01:49:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=122
05/31/2022 01:49:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=122
05/31/2022 01:49:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=122
05/31/2022 01:49:28 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.4289325811064941 on epoch=122
05/31/2022 01:49:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.26 on epoch=123
05/31/2022 01:49:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=123
05/31/2022 01:49:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=124
05/31/2022 01:49:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=124
05/31/2022 01:49:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.20 on epoch=124
05/31/2022 01:49:43 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:49:43 - INFO - __main__ - Printing 3 examples
05/31/2022 01:49:43 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:49:43 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:43 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:49:43 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:43 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:49:43 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:43 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:49:43 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:49:44 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:49:44 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:49:44 - INFO - __main__ - Printing 3 examples
05/31/2022 01:49:44 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:49:44 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:44 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:49:44 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:44 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:49:44 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:44 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:49:44 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:49:44 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:49:52 - INFO - __main__ - Global step 3000 Train loss 0.23 Classification-F1 0.47070138400187655 on epoch=124
05/31/2022 01:49:52 - INFO - __main__ - save last model!
05/31/2022 01:49:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 01:49:52 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 01:49:52 - INFO - __main__ - Printing 3 examples
05/31/2022 01:49:52 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 01:49:52 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:52 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 01:49:52 - INFO - __main__ - ['entailment']
05/31/2022 01:49:52 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 01:49:52 - INFO - __main__ - ['contradiction']
05/31/2022 01:49:52 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:49:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:49:54 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 01:50:02 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:50:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:50:03 - INFO - __main__ - Starting training!
05/31/2022 01:50:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_13_0.4_8_predictions.txt
05/31/2022 01:50:21 - INFO - __main__ - Classification-F1 on test data: 0.3127
05/31/2022 01:50:21 - INFO - __main__ - prefix=anli_128_13, lr=0.4, bsz=8, dev_performance=0.4811977625246302, test_performance=0.3127268986097805
05/31/2022 01:50:21 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.3, bsz=8 ...
05/31/2022 01:50:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:50:22 - INFO - __main__ - Printing 3 examples
05/31/2022 01:50:22 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 01:50:22 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:22 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 01:50:22 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:22 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 01:50:22 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:50:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:50:23 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 01:50:23 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 01:50:23 - INFO - __main__ - Printing 3 examples
05/31/2022 01:50:23 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 01:50:23 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:23 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 01:50:23 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:23 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 01:50:23 - INFO - __main__ - ['contradiction']
05/31/2022 01:50:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 01:50:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 01:50:24 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 01:50:40 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 01:50:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 01:50:41 - INFO - __main__ - Starting training!
05/31/2022 01:50:44 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=0
05/31/2022 01:50:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=0
05/31/2022 01:50:50 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
05/31/2022 01:50:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=1
05/31/2022 01:50:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=2
05/31/2022 01:51:06 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 01:51:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 01:51:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=2
05/31/2022 01:51:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=2
05/31/2022 01:51:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=3
05/31/2022 01:51:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=3
05/31/2022 01:51:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=4
05/31/2022 01:51:28 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 01:51:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=4
05/31/2022 01:51:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=4
05/31/2022 01:51:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=5
05/31/2022 01:51:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=5
05/31/2022 01:51:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=6
05/31/2022 01:51:51 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 01:51:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=6
05/31/2022 01:51:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=7
05/31/2022 01:51:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=7
05/31/2022 01:52:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=7
05/31/2022 01:52:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=8
05/31/2022 01:52:14 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 01:52:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=8
05/31/2022 01:52:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=9
05/31/2022 01:52:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=9
05/31/2022 01:52:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=9
05/31/2022 01:52:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=10
05/31/2022 01:52:36 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.17702046042127015 on epoch=10
05/31/2022 01:52:36 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17702046042127015 on epoch=10, global_step=250
05/31/2022 01:52:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=10
05/31/2022 01:52:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=11
05/31/2022 01:52:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=11
05/31/2022 01:52:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=12
05/31/2022 01:52:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=12
05/31/2022 01:53:00 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 01:53:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=12
05/31/2022 01:53:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=13
05/31/2022 01:53:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=13
05/31/2022 01:53:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=14
05/31/2022 01:53:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=14
05/31/2022 01:53:24 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 01:53:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=14
05/31/2022 01:53:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/31/2022 01:53:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=15
05/31/2022 01:53:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=16
05/31/2022 01:53:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=16
05/31/2022 01:53:48 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 01:53:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=17
05/31/2022 01:53:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=17
05/31/2022 01:53:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=17
05/31/2022 01:53:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/31/2022 01:54:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=18
05/31/2022 01:54:12 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 01:54:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=19
05/31/2022 01:54:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=19
05/31/2022 01:54:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=19
05/31/2022 01:54:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=20
05/31/2022 01:54:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=20
05/31/2022 01:54:36 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/31/2022 01:54:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=21
05/31/2022 01:54:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=21
05/31/2022 01:54:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=22
05/31/2022 01:54:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=22
05/31/2022 01:54:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=22
05/31/2022 01:55:00 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.17248822009423928 on epoch=22
05/31/2022 01:55:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/31/2022 01:55:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/31/2022 01:55:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=24
05/31/2022 01:55:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/31/2022 01:55:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/31/2022 01:55:24 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.2042284973847446 on epoch=24
05/31/2022 01:55:24 - INFO - __main__ - Saving model with best Classification-F1: 0.17702046042127015 -> 0.2042284973847446 on epoch=24, global_step=600
05/31/2022 01:55:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=25
05/31/2022 01:55:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=25
05/31/2022 01:55:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 01:55:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=26
05/31/2022 01:55:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=27
05/31/2022 01:55:47 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.178080012725682 on epoch=27
05/31/2022 01:55:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=27
05/31/2022 01:55:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=27
05/31/2022 01:55:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=28
05/31/2022 01:55:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=28
05/31/2022 01:56:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/31/2022 01:56:10 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=29
05/31/2022 01:56:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=29
05/31/2022 01:56:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=29
05/31/2022 01:56:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=30
05/31/2022 01:56:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=30
05/31/2022 01:56:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=31
05/31/2022 01:56:33 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.21828571428571428 on epoch=31
05/31/2022 01:56:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2042284973847446 -> 0.21828571428571428 on epoch=31, global_step=750
05/31/2022 01:56:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=31
05/31/2022 01:56:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=32
05/31/2022 01:56:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 01:56:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/31/2022 01:56:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=33
05/31/2022 01:56:56 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.2085782144118259 on epoch=33
05/31/2022 01:56:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/31/2022 01:57:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=34
05/31/2022 01:57:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=34
05/31/2022 01:57:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=34
05/31/2022 01:57:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=35
05/31/2022 01:57:19 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.30713571585921745 on epoch=35
05/31/2022 01:57:19 - INFO - __main__ - Saving model with best Classification-F1: 0.21828571428571428 -> 0.30713571585921745 on epoch=35, global_step=850
05/31/2022 01:57:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=35
05/31/2022 01:57:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=36
05/31/2022 01:57:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/31/2022 01:57:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=37
05/31/2022 01:57:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=37
05/31/2022 01:57:42 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.2545228154984253 on epoch=37
05/31/2022 01:57:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=37
05/31/2022 01:57:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=38
05/31/2022 01:57:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=38
05/31/2022 01:57:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=39
05/31/2022 01:57:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=39
05/31/2022 01:58:04 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.25880616192129474 on epoch=39
05/31/2022 01:58:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=39
05/31/2022 01:58:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/31/2022 01:58:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=40
05/31/2022 01:58:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=41
05/31/2022 01:58:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=41
05/31/2022 01:58:27 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.2791708236337435 on epoch=41
05/31/2022 01:58:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=42
05/31/2022 01:58:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=42
05/31/2022 01:58:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=42
05/31/2022 01:58:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=43
05/31/2022 01:58:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=43
05/31/2022 01:58:49 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.2997422127685649 on epoch=43
05/31/2022 01:58:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/31/2022 01:58:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=44
05/31/2022 01:58:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=44
05/31/2022 01:59:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=45
05/31/2022 01:59:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/31/2022 01:59:11 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.29518357056568095 on epoch=45
05/31/2022 01:59:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/31/2022 01:59:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=46
05/31/2022 01:59:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=47
05/31/2022 01:59:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=47
05/31/2022 01:59:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=47
05/31/2022 01:59:34 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3377742946708464 on epoch=47
05/31/2022 01:59:34 - INFO - __main__ - Saving model with best Classification-F1: 0.30713571585921745 -> 0.3377742946708464 on epoch=47, global_step=1150
05/31/2022 01:59:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=48
05/31/2022 01:59:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=48
05/31/2022 01:59:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=49
05/31/2022 01:59:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=49
05/31/2022 01:59:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=49
05/31/2022 01:59:56 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3538880663785875 on epoch=49
05/31/2022 01:59:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3377742946708464 -> 0.3538880663785875 on epoch=49, global_step=1200
05/31/2022 01:59:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=50
05/31/2022 02:00:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/31/2022 02:00:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=51
05/31/2022 02:00:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=51
05/31/2022 02:00:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=52
05/31/2022 02:00:19 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.3687203157401833 on epoch=52
05/31/2022 02:00:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3538880663785875 -> 0.3687203157401833 on epoch=52, global_step=1250
05/31/2022 02:00:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=52
05/31/2022 02:00:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=52
05/31/2022 02:00:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=53
05/31/2022 02:00:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=53
05/31/2022 02:00:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=54
05/31/2022 02:00:43 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.34678683385579934 on epoch=54
05/31/2022 02:00:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=54
05/31/2022 02:00:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=54
05/31/2022 02:00:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=55
05/31/2022 02:00:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=55
05/31/2022 02:00:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=56
05/31/2022 02:01:06 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.38026496041999924 on epoch=56
05/31/2022 02:01:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3687203157401833 -> 0.38026496041999924 on epoch=56, global_step=1350
05/31/2022 02:01:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=56
05/31/2022 02:01:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=57
05/31/2022 02:01:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=57
05/31/2022 02:01:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=57
05/31/2022 02:01:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=58
05/31/2022 02:01:28 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.35072463768115947 on epoch=58
05/31/2022 02:01:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=58
05/31/2022 02:01:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=59
05/31/2022 02:01:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=59
05/31/2022 02:01:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=59
05/31/2022 02:01:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=60
05/31/2022 02:01:50 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.40174013113288115 on epoch=60
05/31/2022 02:01:50 - INFO - __main__ - Saving model with best Classification-F1: 0.38026496041999924 -> 0.40174013113288115 on epoch=60, global_step=1450
05/31/2022 02:01:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=60
05/31/2022 02:01:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=61
05/31/2022 02:01:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=61
05/31/2022 02:02:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=62
05/31/2022 02:02:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=62
05/31/2022 02:02:11 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.38053926312884556 on epoch=62
05/31/2022 02:02:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=62
05/31/2022 02:02:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=63
05/31/2022 02:02:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=63
05/31/2022 02:02:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=64
05/31/2022 02:02:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=64
05/31/2022 02:02:33 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3799603174603174 on epoch=64
05/31/2022 02:02:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=64
05/31/2022 02:02:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=65
05/31/2022 02:02:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=65
05/31/2022 02:02:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=66
05/31/2022 02:02:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=66
05/31/2022 02:02:53 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3813564233635868 on epoch=66
05/31/2022 02:02:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=67
05/31/2022 02:02:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=67
05/31/2022 02:03:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=67
05/31/2022 02:03:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=68
05/31/2022 02:03:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=68
05/31/2022 02:03:15 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.3863020791073091 on epoch=68
05/31/2022 02:03:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=69
05/31/2022 02:03:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=69
05/31/2022 02:03:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=69
05/31/2022 02:03:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=70
05/31/2022 02:03:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=70
05/31/2022 02:03:36 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.3942424242424242 on epoch=70
05/31/2022 02:03:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=71
05/31/2022 02:03:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=71
05/31/2022 02:03:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=72
05/31/2022 02:03:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=72
05/31/2022 02:03:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=72
05/31/2022 02:03:57 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.3915788839157888 on epoch=72
05/31/2022 02:03:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=73
05/31/2022 02:04:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=73
05/31/2022 02:04:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=74
05/31/2022 02:04:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=74
05/31/2022 02:04:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=74
05/31/2022 02:04:19 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3981145573458545 on epoch=74
05/31/2022 02:04:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=75
05/31/2022 02:04:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=75
05/31/2022 02:04:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=76
05/31/2022 02:04:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=76
05/31/2022 02:04:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=77
05/31/2022 02:04:41 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.39859726836471027 on epoch=77
05/31/2022 02:04:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=77
05/31/2022 02:04:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=77
05/31/2022 02:04:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=78
05/31/2022 02:04:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=78
05/31/2022 02:04:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=79
05/31/2022 02:05:03 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.4010313545197266 on epoch=79
05/31/2022 02:05:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=79
05/31/2022 02:05:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=79
05/31/2022 02:05:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=80
05/31/2022 02:05:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=80
05/31/2022 02:05:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=81
05/31/2022 02:05:26 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.401612786576987 on epoch=81
05/31/2022 02:05:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=81
05/31/2022 02:05:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=82
05/31/2022 02:05:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=82
05/31/2022 02:05:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=82
05/31/2022 02:05:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=83
05/31/2022 02:05:48 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.39797482330551254 on epoch=83
05/31/2022 02:05:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=83
05/31/2022 02:05:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=84
05/31/2022 02:05:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=84
05/31/2022 02:05:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=84
05/31/2022 02:06:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=85
05/31/2022 02:06:11 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.4053754719467643 on epoch=85
05/31/2022 02:06:11 - INFO - __main__ - Saving model with best Classification-F1: 0.40174013113288115 -> 0.4053754719467643 on epoch=85, global_step=2050
05/31/2022 02:06:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.34 on epoch=85
05/31/2022 02:06:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=86
05/31/2022 02:06:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=86
05/31/2022 02:06:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=87
05/31/2022 02:06:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=87
05/31/2022 02:06:34 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.3912363067292645 on epoch=87
05/31/2022 02:06:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=87
05/31/2022 02:06:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=88
05/31/2022 02:06:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.34 on epoch=88
05/31/2022 02:06:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=89
05/31/2022 02:06:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=89
05/31/2022 02:06:56 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.40586575790485463 on epoch=89
05/31/2022 02:06:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4053754719467643 -> 0.40586575790485463 on epoch=89, global_step=2150
05/31/2022 02:06:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=89
05/31/2022 02:07:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=90
05/31/2022 02:07:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=90
05/31/2022 02:07:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.34 on epoch=91
05/31/2022 02:07:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=91
05/31/2022 02:07:19 - INFO - __main__ - Global step 2200 Train loss 0.33 Classification-F1 0.40770457612562877 on epoch=91
05/31/2022 02:07:20 - INFO - __main__ - Saving model with best Classification-F1: 0.40586575790485463 -> 0.40770457612562877 on epoch=91, global_step=2200
05/31/2022 02:07:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=92
05/31/2022 02:07:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=92
05/31/2022 02:07:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.30 on epoch=92
05/31/2022 02:07:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=93
05/31/2022 02:07:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.32 on epoch=93
05/31/2022 02:07:43 - INFO - __main__ - Global step 2250 Train loss 0.34 Classification-F1 0.40600187099870116 on epoch=93
05/31/2022 02:07:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.38 on epoch=94
05/31/2022 02:07:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=94
05/31/2022 02:07:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.34 on epoch=94
05/31/2022 02:07:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.31 on epoch=95
05/31/2022 02:07:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=95
05/31/2022 02:08:06 - INFO - __main__ - Global step 2300 Train loss 0.35 Classification-F1 0.4066331017780893 on epoch=95
05/31/2022 02:08:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=96
05/31/2022 02:08:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=96
05/31/2022 02:08:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=97
05/31/2022 02:08:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=97
05/31/2022 02:08:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.29 on epoch=97
05/31/2022 02:08:29 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.39537600688679825 on epoch=97
05/31/2022 02:08:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=98
05/31/2022 02:08:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=98
05/31/2022 02:08:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.31 on epoch=99
05/31/2022 02:08:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.31 on epoch=99
05/31/2022 02:08:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.30 on epoch=99
05/31/2022 02:08:53 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.410069856585536 on epoch=99
05/31/2022 02:08:53 - INFO - __main__ - Saving model with best Classification-F1: 0.40770457612562877 -> 0.410069856585536 on epoch=99, global_step=2400
05/31/2022 02:08:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=100
05/31/2022 02:08:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=100
05/31/2022 02:09:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=101
05/31/2022 02:09:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.28 on epoch=101
05/31/2022 02:09:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=102
05/31/2022 02:09:16 - INFO - __main__ - Global step 2450 Train loss 0.30 Classification-F1 0.4136979918229918 on epoch=102
05/31/2022 02:09:16 - INFO - __main__ - Saving model with best Classification-F1: 0.410069856585536 -> 0.4136979918229918 on epoch=102, global_step=2450
05/31/2022 02:09:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=102
05/31/2022 02:09:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=102
05/31/2022 02:09:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=103
05/31/2022 02:09:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=103
05/31/2022 02:09:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.29 on epoch=104
05/31/2022 02:09:39 - INFO - __main__ - Global step 2500 Train loss 0.29 Classification-F1 0.41481060359309324 on epoch=104
05/31/2022 02:09:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4136979918229918 -> 0.41481060359309324 on epoch=104, global_step=2500
05/31/2022 02:09:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=104
05/31/2022 02:09:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=104
05/31/2022 02:09:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=105
05/31/2022 02:09:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=105
05/31/2022 02:09:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.32 on epoch=106
05/31/2022 02:10:02 - INFO - __main__ - Global step 2550 Train loss 0.32 Classification-F1 0.4206608137222296 on epoch=106
05/31/2022 02:10:02 - INFO - __main__ - Saving model with best Classification-F1: 0.41481060359309324 -> 0.4206608137222296 on epoch=106, global_step=2550
05/31/2022 02:10:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=106
05/31/2022 02:10:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=107
05/31/2022 02:10:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.25 on epoch=107
05/31/2022 02:10:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=107
05/31/2022 02:10:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.26 on epoch=108
05/31/2022 02:10:25 - INFO - __main__ - Global step 2600 Train loss 0.26 Classification-F1 0.41528721837701005 on epoch=108
05/31/2022 02:10:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.32 on epoch=108
05/31/2022 02:10:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.32 on epoch=109
05/31/2022 02:10:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=109
05/31/2022 02:10:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.32 on epoch=109
05/31/2022 02:10:38 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=110
05/31/2022 02:10:47 - INFO - __main__ - Global step 2650 Train loss 0.30 Classification-F1 0.41817641115065 on epoch=110
05/31/2022 02:10:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.31 on epoch=110
05/31/2022 02:10:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=111
05/31/2022 02:10:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=111
05/31/2022 02:10:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=112
05/31/2022 02:11:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.32 on epoch=112
05/31/2022 02:11:10 - INFO - __main__ - Global step 2700 Train loss 0.31 Classification-F1 0.411972473843129 on epoch=112
05/31/2022 02:11:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=112
05/31/2022 02:11:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.32 on epoch=113
05/31/2022 02:11:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=113
05/31/2022 02:11:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=114
05/31/2022 02:11:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.30 on epoch=114
05/31/2022 02:11:33 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.43920158207910037 on epoch=114
05/31/2022 02:11:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4206608137222296 -> 0.43920158207910037 on epoch=114, global_step=2750
05/31/2022 02:11:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.33 on epoch=114
05/31/2022 02:11:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=115
05/31/2022 02:11:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=115
05/31/2022 02:11:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=116
05/31/2022 02:11:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=116
05/31/2022 02:11:56 - INFO - __main__ - Global step 2800 Train loss 0.31 Classification-F1 0.42302014357212014 on epoch=116
05/31/2022 02:11:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.30 on epoch=117
05/31/2022 02:12:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=117
05/31/2022 02:12:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=117
05/31/2022 02:12:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.27 on epoch=118
05/31/2022 02:12:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=118
05/31/2022 02:12:20 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.4154040188986603 on epoch=118
05/31/2022 02:12:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.29 on epoch=119
05/31/2022 02:12:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.26 on epoch=119
05/31/2022 02:12:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=119
05/31/2022 02:12:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=120
05/31/2022 02:12:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=120
05/31/2022 02:12:43 - INFO - __main__ - Global step 2900 Train loss 0.26 Classification-F1 0.43997000780512047 on epoch=120
05/31/2022 02:12:43 - INFO - __main__ - Saving model with best Classification-F1: 0.43920158207910037 -> 0.43997000780512047 on epoch=120, global_step=2900
05/31/2022 02:12:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.32 on epoch=121
05/31/2022 02:12:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.27 on epoch=121
05/31/2022 02:12:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=122
05/31/2022 02:12:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=122
05/31/2022 02:12:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.26 on epoch=122
05/31/2022 02:13:07 - INFO - __main__ - Global step 2950 Train loss 0.29 Classification-F1 0.41755425560170084 on epoch=122
05/31/2022 02:13:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=123
05/31/2022 02:13:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.28 on epoch=123
05/31/2022 02:13:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=124
05/31/2022 02:13:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=124
05/31/2022 02:13:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=124
05/31/2022 02:13:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:13:22 - INFO - __main__ - Printing 3 examples
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:13:22 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:13:22 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 02:13:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:13:22 - INFO - __main__ - Printing 3 examples
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 02:13:22 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:13:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:13:23 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 02:13:31 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.4428621300691103 on epoch=124
05/31/2022 02:13:31 - INFO - __main__ - Saving model with best Classification-F1: 0.43997000780512047 -> 0.4428621300691103 on epoch=124, global_step=3000
05/31/2022 02:13:31 - INFO - __main__ - save last model!
05/31/2022 02:13:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 02:13:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 02:13:31 - INFO - __main__ - Printing 3 examples
05/31/2022 02:13:31 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 02:13:31 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:31 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 02:13:31 - INFO - __main__ - ['entailment']
05/31/2022 02:13:31 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 02:13:31 - INFO - __main__ - ['contradiction']
05/31/2022 02:13:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:13:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:13:32 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 02:13:39 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 02:13:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 02:13:40 - INFO - __main__ - Starting training!
05/31/2022 02:14:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_13_0.3_8_predictions.txt
05/31/2022 02:14:00 - INFO - __main__ - Classification-F1 on test data: 0.3215
05/31/2022 02:14:01 - INFO - __main__ - prefix=anli_128_13, lr=0.3, bsz=8, dev_performance=0.4428621300691103, test_performance=0.3214834777864703
05/31/2022 02:14:01 - INFO - __main__ - Running ... prefix=anli_128_13, lr=0.2, bsz=8 ...
05/31/2022 02:14:01 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:14:01 - INFO - __main__ - Printing 3 examples
05/31/2022 02:14:01 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/31/2022 02:14:01 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:01 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/31/2022 02:14:01 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:01 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/31/2022 02:14:01 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:14:02 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:14:02 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 02:14:02 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:14:02 - INFO - __main__ - Printing 3 examples
05/31/2022 02:14:02 - INFO - __main__ -  [anli] premise: Dexter Alexander Nottage (born November 14, 1970) is a former American football defensive end in the National Football League (NFL) for the Washington Redskins and the Kansas City Chiefs. He played college football at Florida A&M University and was selected in the sixth round of the 1994 NFL Draft. He played high school football at Hollywood Hills High School. [SEP] hypothesis: Dexter Alexander Nottage (born November 14, 1970) is a former power forward basketball in the NBA.
05/31/2022 02:14:02 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:02 - INFO - __main__ -  [anli] premise: A grasshopper is a sweet, mint-flavored, after-dinner drink. The name of the drink derives from its green color, which comes from crème de menthe. The drink reputedly originated at Tujague's, a landmark bar in the French Quarter of New Orleans, Louisiana, and was invented by its owner, Philip Guichet. The drink gained popularity during the 1950s and 1960s throughout the American South. [SEP] hypothesis: The drink is white
05/31/2022 02:14:02 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:02 - INFO - __main__ -  [anli] premise: Denis Hale Johnson (July 1, 1949 – May 24, 2017) was an American writer best known for his short story collection "Jesus' Son" (1992) and his novel "Tree of Smoke" (2007), which won the National Book Award for Fiction. He also wrote plays, poetry, journalism, and non-fiction. [SEP] hypothesis: Denis Hale Johnson (July 1, 1949 – May 24, 2010) was an American writer best known for his short story collection "Jesus' Son"
05/31/2022 02:14:02 - INFO - __main__ - ['contradiction']
05/31/2022 02:14:02 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:14:02 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:14:03 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 02:14:21 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 02:14:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 02:14:22 - INFO - __main__ - Starting training!
05/31/2022 02:14:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=0
05/31/2022 02:14:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=0
05/31/2022 02:14:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=1
05/31/2022 02:14:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=1
05/31/2022 02:14:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=2
05/31/2022 02:14:47 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 02:14:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 02:14:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=2
05/31/2022 02:14:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=2
05/31/2022 02:14:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=3
05/31/2022 02:14:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=3
05/31/2022 02:15:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=4
05/31/2022 02:15:10 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 02:15:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=4
05/31/2022 02:15:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=4
05/31/2022 02:15:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=5
05/31/2022 02:15:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=5
05/31/2022 02:15:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/31/2022 02:15:34 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 02:15:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=6
05/31/2022 02:15:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=7
05/31/2022 02:15:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=7
05/31/2022 02:15:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=7
05/31/2022 02:15:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=8
05/31/2022 02:15:57 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 02:16:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=8
05/31/2022 02:16:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=9
05/31/2022 02:16:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=9
05/31/2022 02:16:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=9
05/31/2022 02:16:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=10
05/31/2022 02:16:20 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 02:16:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=10
05/31/2022 02:16:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=11
05/31/2022 02:16:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=11
05/31/2022 02:16:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=12
05/31/2022 02:16:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=12
05/31/2022 02:16:44 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 02:16:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=12
05/31/2022 02:16:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=13
05/31/2022 02:16:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=13
05/31/2022 02:16:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=14
05/31/2022 02:16:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=14
05/31/2022 02:17:08 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 02:17:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=14
05/31/2022 02:17:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/31/2022 02:17:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=15
05/31/2022 02:17:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/31/2022 02:17:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=16
05/31/2022 02:17:32 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 02:17:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=17
05/31/2022 02:17:38 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=17
05/31/2022 02:17:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=17
05/31/2022 02:17:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=18
05/31/2022 02:17:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 02:17:57 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 02:18:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=19
05/31/2022 02:18:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=19
05/31/2022 02:18:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=19
05/31/2022 02:18:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/31/2022 02:18:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=20
05/31/2022 02:18:21 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/31/2022 02:18:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/31/2022 02:18:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=21
05/31/2022 02:18:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=22
05/31/2022 02:18:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/31/2022 02:18:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=22
05/31/2022 02:18:46 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=22
05/31/2022 02:18:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=23
05/31/2022 02:18:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=23
05/31/2022 02:18:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=24
05/31/2022 02:18:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/31/2022 02:18:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=24
05/31/2022 02:19:10 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=24
05/31/2022 02:19:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=25
05/31/2022 02:19:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=25
05/31/2022 02:19:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=26
05/31/2022 02:19:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=26
05/31/2022 02:19:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=27
05/31/2022 02:19:35 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=27
05/31/2022 02:19:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.54 on epoch=27
05/31/2022 02:19:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=27
05/31/2022 02:19:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=28
05/31/2022 02:19:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=28
05/31/2022 02:19:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=29
05/31/2022 02:19:59 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=29
05/31/2022 02:20:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=29
05/31/2022 02:20:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=29
05/31/2022 02:20:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=30
05/31/2022 02:20:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=30
05/31/2022 02:20:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=31
05/31/2022 02:20:24 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.17244846656611368 on epoch=31
05/31/2022 02:20:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17244846656611368 on epoch=31, global_step=750
05/31/2022 02:20:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=31
05/31/2022 02:20:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=32
05/31/2022 02:20:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=32
05/31/2022 02:20:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=32
05/31/2022 02:20:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=33
05/31/2022 02:20:49 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.1721607831834019 on epoch=33
05/31/2022 02:20:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=33
05/31/2022 02:20:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=34
05/31/2022 02:20:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=34
05/31/2022 02:20:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=34
05/31/2022 02:21:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.54 on epoch=35
05/31/2022 02:21:13 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.18870570285720023 on epoch=35
05/31/2022 02:21:13 - INFO - __main__ - Saving model with best Classification-F1: 0.17244846656611368 -> 0.18870570285720023 on epoch=35, global_step=850
05/31/2022 02:21:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=35
05/31/2022 02:21:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=36
05/31/2022 02:21:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/31/2022 02:21:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/31/2022 02:21:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.57 on epoch=37
05/31/2022 02:21:38 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.17765617875057005 on epoch=37
05/31/2022 02:21:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=37
05/31/2022 02:21:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=38
05/31/2022 02:21:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=38
05/31/2022 02:21:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=39
05/31/2022 02:21:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=39
05/31/2022 02:22:02 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=39
05/31/2022 02:22:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/31/2022 02:22:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/31/2022 02:22:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=40
05/31/2022 02:22:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=41
05/31/2022 02:22:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=41
05/31/2022 02:22:27 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.17765617875057005 on epoch=41
05/31/2022 02:22:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/31/2022 02:22:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/31/2022 02:22:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=42
05/31/2022 02:22:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=43
05/31/2022 02:22:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=43
05/31/2022 02:22:52 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.17765617875057005 on epoch=43
05/31/2022 02:22:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/31/2022 02:22:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=44
05/31/2022 02:23:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=44
05/31/2022 02:23:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=45
05/31/2022 02:23:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/31/2022 02:23:16 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.1939731765818722 on epoch=45
05/31/2022 02:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.18870570285720023 -> 0.1939731765818722 on epoch=45, global_step=1100
05/31/2022 02:23:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=46
05/31/2022 02:23:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=46
05/31/2022 02:23:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=47
05/31/2022 02:23:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=47
05/31/2022 02:23:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=47
05/31/2022 02:23:41 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.1721607831834019 on epoch=47
05/31/2022 02:23:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=48
05/31/2022 02:23:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=48
05/31/2022 02:23:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=49
05/31/2022 02:23:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=49
05/31/2022 02:23:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=49
05/31/2022 02:24:05 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.20429112070903113 on epoch=49
05/31/2022 02:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1939731765818722 -> 0.20429112070903113 on epoch=49, global_step=1200
05/31/2022 02:24:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=50
05/31/2022 02:24:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=50
05/31/2022 02:24:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=51
05/31/2022 02:24:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/31/2022 02:24:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=52
05/31/2022 02:24:30 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.20488797110667836 on epoch=52
05/31/2022 02:24:30 - INFO - __main__ - Saving model with best Classification-F1: 0.20429112070903113 -> 0.20488797110667836 on epoch=52, global_step=1250
05/31/2022 02:24:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=52
05/31/2022 02:24:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=52
05/31/2022 02:24:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=53
05/31/2022 02:24:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=53
05/31/2022 02:24:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=54
05/31/2022 02:24:54 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.1881810228266921 on epoch=54
05/31/2022 02:24:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=54
05/31/2022 02:25:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=54
05/31/2022 02:25:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=55
05/31/2022 02:25:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=55
05/31/2022 02:25:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=56
05/31/2022 02:25:19 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.21862773165378377 on epoch=56
05/31/2022 02:25:19 - INFO - __main__ - Saving model with best Classification-F1: 0.20488797110667836 -> 0.21862773165378377 on epoch=56, global_step=1350
05/31/2022 02:25:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=56
05/31/2022 02:25:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=57
05/31/2022 02:25:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=57
05/31/2022 02:25:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/31/2022 02:25:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=58
05/31/2022 02:25:43 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.21862773165378377 on epoch=58
05/31/2022 02:25:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=58
05/31/2022 02:25:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=59
05/31/2022 02:25:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=59
05/31/2022 02:25:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=59
05/31/2022 02:25:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=60
05/31/2022 02:26:06 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.25018743129435683 on epoch=60
05/31/2022 02:26:07 - INFO - __main__ - Saving model with best Classification-F1: 0.21862773165378377 -> 0.25018743129435683 on epoch=60, global_step=1450
05/31/2022 02:26:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=60
05/31/2022 02:26:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/31/2022 02:26:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=61
05/31/2022 02:26:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/31/2022 02:26:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=62
05/31/2022 02:26:31 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.2950360692296176 on epoch=62
05/31/2022 02:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.25018743129435683 -> 0.2950360692296176 on epoch=62, global_step=1500
05/31/2022 02:26:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=62
05/31/2022 02:26:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=63
05/31/2022 02:26:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=63
05/31/2022 02:26:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=64
05/31/2022 02:26:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/31/2022 02:26:54 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.2916897921787316 on epoch=64
05/31/2022 02:26:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=64
05/31/2022 02:27:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=65
05/31/2022 02:27:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=65
05/31/2022 02:27:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=66
05/31/2022 02:27:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=66
05/31/2022 02:27:18 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.28781157443577826 on epoch=66
05/31/2022 02:27:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=67
05/31/2022 02:27:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=67
05/31/2022 02:27:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=67
05/31/2022 02:27:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=68
05/31/2022 02:27:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=68
05/31/2022 02:27:41 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.26518043824745463 on epoch=68
05/31/2022 02:27:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=69
05/31/2022 02:27:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=69
05/31/2022 02:27:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=69
05/31/2022 02:27:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=70
05/31/2022 02:27:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=70
05/31/2022 02:28:05 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.25507438747856864 on epoch=70
05/31/2022 02:28:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=71
05/31/2022 02:28:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=71
05/31/2022 02:28:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=72
05/31/2022 02:28:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=72
05/31/2022 02:28:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=72
05/31/2022 02:28:28 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.32819850261710726 on epoch=72
05/31/2022 02:28:28 - INFO - __main__ - Saving model with best Classification-F1: 0.2950360692296176 -> 0.32819850261710726 on epoch=72, global_step=1750
05/31/2022 02:28:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=73
05/31/2022 02:28:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=73
05/31/2022 02:28:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=74
05/31/2022 02:28:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=74
05/31/2022 02:28:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=74
05/31/2022 02:28:51 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3419562798612475 on epoch=74
05/31/2022 02:28:51 - INFO - __main__ - Saving model with best Classification-F1: 0.32819850261710726 -> 0.3419562798612475 on epoch=74, global_step=1800
05/31/2022 02:28:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=75
05/31/2022 02:28:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=75
05/31/2022 02:28:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=76
05/31/2022 02:29:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=76
05/31/2022 02:29:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=77
05/31/2022 02:29:14 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.33096701367114006 on epoch=77
05/31/2022 02:29:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=77
05/31/2022 02:29:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=77
05/31/2022 02:29:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=78
05/31/2022 02:29:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=78
05/31/2022 02:29:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=79
05/31/2022 02:29:38 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3444320850327524 on epoch=79
05/31/2022 02:29:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3419562798612475 -> 0.3444320850327524 on epoch=79, global_step=1900
05/31/2022 02:29:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=79
05/31/2022 02:29:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=79
05/31/2022 02:29:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=80
05/31/2022 02:29:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=80
05/31/2022 02:29:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=81
05/31/2022 02:30:01 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.37008588896478195 on epoch=81
05/31/2022 02:30:01 - INFO - __main__ - Saving model with best Classification-F1: 0.3444320850327524 -> 0.37008588896478195 on epoch=81, global_step=1950
05/31/2022 02:30:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=81
05/31/2022 02:30:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=82
05/31/2022 02:30:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=82
05/31/2022 02:30:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=82
05/31/2022 02:30:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=83
05/31/2022 02:30:25 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.33516552468438626 on epoch=83
05/31/2022 02:30:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=83
05/31/2022 02:30:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=84
05/31/2022 02:30:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.39 on epoch=84
05/31/2022 02:30:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=84
05/31/2022 02:30:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=85
05/31/2022 02:30:49 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.3925714605861615 on epoch=85
05/31/2022 02:30:49 - INFO - __main__ - Saving model with best Classification-F1: 0.37008588896478195 -> 0.3925714605861615 on epoch=85, global_step=2050
05/31/2022 02:30:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=85
05/31/2022 02:30:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=86
05/31/2022 02:30:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=86
05/31/2022 02:30:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=87
05/31/2022 02:31:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=87
05/31/2022 02:31:12 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.37212202555714 on epoch=87
05/31/2022 02:31:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=87
05/31/2022 02:31:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=88
05/31/2022 02:31:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=88
05/31/2022 02:31:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=89
05/31/2022 02:31:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=89
05/31/2022 02:31:36 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.35723356174665194 on epoch=89
05/31/2022 02:31:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=89
05/31/2022 02:31:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=90
05/31/2022 02:31:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=90
05/31/2022 02:31:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=91
05/31/2022 02:31:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=91
05/31/2022 02:31:59 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.36719358918712053 on epoch=91
05/31/2022 02:32:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.33 on epoch=92
05/31/2022 02:32:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=92
05/31/2022 02:32:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=92
05/31/2022 02:32:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.38 on epoch=93
05/31/2022 02:32:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=93
05/31/2022 02:32:22 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.3577743972104059 on epoch=93
05/31/2022 02:32:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=94
05/31/2022 02:32:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=94
05/31/2022 02:32:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=94
05/31/2022 02:32:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=95
05/31/2022 02:32:36 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=95
05/31/2022 02:32:46 - INFO - __main__ - Global step 2300 Train loss 0.38 Classification-F1 0.3825594825594825 on epoch=95
05/31/2022 02:32:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=96
05/31/2022 02:32:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=96
05/31/2022 02:32:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=97
05/31/2022 02:32:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=97
05/31/2022 02:32:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=97
05/31/2022 02:33:09 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.3782071586949636 on epoch=97
05/31/2022 02:33:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=98
05/31/2022 02:33:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=98
05/31/2022 02:33:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.39 on epoch=99
05/31/2022 02:33:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=99
05/31/2022 02:33:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=99
05/31/2022 02:33:32 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.3821836455002284 on epoch=99
05/31/2022 02:33:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.36 on epoch=100
05/31/2022 02:33:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.34 on epoch=100
05/31/2022 02:33:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=101
05/31/2022 02:33:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=101
05/31/2022 02:33:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=102
05/31/2022 02:33:55 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.37922032992090077 on epoch=102
05/31/2022 02:33:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.43 on epoch=102
05/31/2022 02:34:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.34 on epoch=102
05/31/2022 02:34:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=103
05/31/2022 02:34:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=103
05/31/2022 02:34:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=104
05/31/2022 02:34:18 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.37292700829818726 on epoch=104
05/31/2022 02:34:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.38 on epoch=104
05/31/2022 02:34:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=104
05/31/2022 02:34:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=105
05/31/2022 02:34:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=105
05/31/2022 02:34:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=106
05/31/2022 02:34:41 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.38067019400352736 on epoch=106
05/31/2022 02:34:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.35 on epoch=106
05/31/2022 02:34:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=107
05/31/2022 02:34:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=107
05/31/2022 02:34:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.38 on epoch=107
05/31/2022 02:34:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.38 on epoch=108
05/31/2022 02:35:04 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.3765560895557378 on epoch=108
05/31/2022 02:35:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=108
05/31/2022 02:35:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.46 on epoch=109
05/31/2022 02:35:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=109
05/31/2022 02:35:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.31 on epoch=109
05/31/2022 02:35:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=110
05/31/2022 02:35:27 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.39321492595772883 on epoch=110
05/31/2022 02:35:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3925714605861615 -> 0.39321492595772883 on epoch=110, global_step=2650
05/31/2022 02:35:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=110
05/31/2022 02:35:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.41 on epoch=111
05/31/2022 02:35:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=111
05/31/2022 02:35:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.34 on epoch=112
05/31/2022 02:35:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.38 on epoch=112
05/31/2022 02:35:50 - INFO - __main__ - Global step 2700 Train loss 0.35 Classification-F1 0.3853902734908862 on epoch=112
05/31/2022 02:35:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=112
05/31/2022 02:35:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=113
05/31/2022 02:35:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.36 on epoch=113
05/31/2022 02:36:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=114
05/31/2022 02:36:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.34 on epoch=114
05/31/2022 02:36:13 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.38198891294666854 on epoch=114
05/31/2022 02:36:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.33 on epoch=114
05/31/2022 02:36:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=115
05/31/2022 02:36:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=115
05/31/2022 02:36:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=116
05/31/2022 02:36:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=116
05/31/2022 02:36:36 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.39437515618255753 on epoch=116
05/31/2022 02:36:36 - INFO - __main__ - Saving model with best Classification-F1: 0.39321492595772883 -> 0.39437515618255753 on epoch=116, global_step=2800
05/31/2022 02:36:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.43 on epoch=117
05/31/2022 02:36:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=117
05/31/2022 02:36:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=117
05/31/2022 02:36:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.31 on epoch=118
05/31/2022 02:36:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=118
05/31/2022 02:36:59 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.39425763213210585 on epoch=118
05/31/2022 02:37:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.42 on epoch=119
05/31/2022 02:37:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.36 on epoch=119
05/31/2022 02:37:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=119
05/31/2022 02:37:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=120
05/31/2022 02:37:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=120
05/31/2022 02:37:22 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.3883067577828398 on epoch=120
05/31/2022 02:37:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.33 on epoch=121
05/31/2022 02:37:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.34 on epoch=121
05/31/2022 02:37:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=122
05/31/2022 02:37:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=122
05/31/2022 02:37:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.33 on epoch=122
05/31/2022 02:37:45 - INFO - __main__ - Global step 2950 Train loss 0.34 Classification-F1 0.3827389541675255 on epoch=122
05/31/2022 02:37:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=123
05/31/2022 02:37:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.30 on epoch=123
05/31/2022 02:37:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=124
05/31/2022 02:37:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=124
05/31/2022 02:37:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=124
05/31/2022 02:38:00 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:38:00 - INFO - __main__ - Printing 3 examples
05/31/2022 02:38:00 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 02:38:00 - INFO - __main__ - ['entailment']
05/31/2022 02:38:00 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 02:38:00 - INFO - __main__ - ['entailment']
05/31/2022 02:38:00 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 02:38:00 - INFO - __main__ - ['entailment']
05/31/2022 02:38:00 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:38:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:38:01 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 02:38:01 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:38:01 - INFO - __main__ - Printing 3 examples
05/31/2022 02:38:01 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 02:38:01 - INFO - __main__ - ['entailment']
05/31/2022 02:38:01 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 02:38:01 - INFO - __main__ - ['entailment']
05/31/2022 02:38:01 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 02:38:01 - INFO - __main__ - ['entailment']
05/31/2022 02:38:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:38:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:38:02 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 02:38:09 - INFO - __main__ - Global step 3000 Train loss 0.34 Classification-F1 0.39651576493681756 on epoch=124
05/31/2022 02:38:09 - INFO - __main__ - Saving model with best Classification-F1: 0.39437515618255753 -> 0.39651576493681756 on epoch=124, global_step=3000
05/31/2022 02:38:09 - INFO - __main__ - save last model!
05/31/2022 02:38:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 02:38:09 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 02:38:09 - INFO - __main__ - Printing 3 examples
05/31/2022 02:38:09 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 02:38:09 - INFO - __main__ - ['contradiction']
05/31/2022 02:38:09 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 02:38:09 - INFO - __main__ - ['entailment']
05/31/2022 02:38:09 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 02:38:09 - INFO - __main__ - ['contradiction']
05/31/2022 02:38:09 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:38:09 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:38:11 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 02:38:18 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 02:38:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 02:38:19 - INFO - __main__ - Starting training!
05/31/2022 02:38:37 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_13_0.2_8_predictions.txt
05/31/2022 02:38:37 - INFO - __main__ - Classification-F1 on test data: 0.2850
05/31/2022 02:38:37 - INFO - __main__ - prefix=anli_128_13, lr=0.2, bsz=8, dev_performance=0.39651576493681756, test_performance=0.28496502833420395
05/31/2022 02:38:37 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.5, bsz=8 ...
05/31/2022 02:38:38 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:38:38 - INFO - __main__ - Printing 3 examples
05/31/2022 02:38:38 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 02:38:38 - INFO - __main__ - ['entailment']
05/31/2022 02:38:38 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 02:38:38 - INFO - __main__ - ['entailment']
05/31/2022 02:38:38 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 02:38:38 - INFO - __main__ - ['entailment']
05/31/2022 02:38:38 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:38:38 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:38:39 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 02:38:39 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 02:38:39 - INFO - __main__ - Printing 3 examples
05/31/2022 02:38:39 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 02:38:39 - INFO - __main__ - ['entailment']
05/31/2022 02:38:39 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 02:38:39 - INFO - __main__ - ['entailment']
05/31/2022 02:38:39 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 02:38:39 - INFO - __main__ - ['entailment']
05/31/2022 02:38:39 - INFO - __main__ - Tokenizing Input ...
05/31/2022 02:38:39 - INFO - __main__ - Tokenizing Output ...
05/31/2022 02:38:39 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 02:38:57 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 02:38:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 02:38:58 - INFO - __main__ - Starting training!
05/31/2022 02:39:02 - INFO - __main__ - Step 10 Global step 10 Train loss 0.59 on epoch=0
05/31/2022 02:39:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.45 on epoch=0
05/31/2022 02:39:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=1
05/31/2022 02:39:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=1
05/31/2022 02:39:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=2
05/31/2022 02:39:23 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.17200792909294935 on epoch=2
05/31/2022 02:39:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17200792909294935 on epoch=2, global_step=50
05/31/2022 02:39:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=2
05/31/2022 02:39:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=2
05/31/2022 02:39:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=3
05/31/2022 02:39:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=3
05/31/2022 02:39:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=4
05/31/2022 02:39:47 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.17277719006599165 on epoch=4
05/31/2022 02:39:47 - INFO - __main__ - Saving model with best Classification-F1: 0.17200792909294935 -> 0.17277719006599165 on epoch=4, global_step=100
05/31/2022 02:39:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=4
05/31/2022 02:39:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=4
05/31/2022 02:39:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=5
05/31/2022 02:39:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=5
05/31/2022 02:40:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/31/2022 02:40:10 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 02:40:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=6
05/31/2022 02:40:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=7
05/31/2022 02:40:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=7
05/31/2022 02:40:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/31/2022 02:40:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=8
05/31/2022 02:40:34 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.1784113322574861 on epoch=8
05/31/2022 02:40:34 - INFO - __main__ - Saving model with best Classification-F1: 0.17277719006599165 -> 0.1784113322574861 on epoch=8, global_step=200
05/31/2022 02:40:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=8
05/31/2022 02:40:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=9
05/31/2022 02:40:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=9
05/31/2022 02:40:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=9
05/31/2022 02:40:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=10
05/31/2022 02:40:59 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 02:41:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=10
05/31/2022 02:41:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=11
05/31/2022 02:41:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=11
05/31/2022 02:41:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=12
05/31/2022 02:41:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=12
05/31/2022 02:41:23 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.17782710198613258 on epoch=12
05/31/2022 02:41:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=12
05/31/2022 02:41:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/31/2022 02:41:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=13
05/31/2022 02:41:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=14
05/31/2022 02:41:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=14
05/31/2022 02:41:48 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.20813256729595833 on epoch=14
05/31/2022 02:41:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1784113322574861 -> 0.20813256729595833 on epoch=14, global_step=350
05/31/2022 02:41:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=14
05/31/2022 02:41:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=15
05/31/2022 02:41:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=15
05/31/2022 02:41:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=16
05/31/2022 02:42:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=16
05/31/2022 02:42:13 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.29192126658108686 on epoch=16
05/31/2022 02:42:13 - INFO - __main__ - Saving model with best Classification-F1: 0.20813256729595833 -> 0.29192126658108686 on epoch=16, global_step=400
05/31/2022 02:42:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=17
05/31/2022 02:42:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=17
05/31/2022 02:42:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=17
05/31/2022 02:42:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=18
05/31/2022 02:42:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=18
05/31/2022 02:42:37 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.34269331499663697 on epoch=18
05/31/2022 02:42:37 - INFO - __main__ - Saving model with best Classification-F1: 0.29192126658108686 -> 0.34269331499663697 on epoch=18, global_step=450
05/31/2022 02:42:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=19
05/31/2022 02:42:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=19
05/31/2022 02:42:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=19
05/31/2022 02:42:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=20
05/31/2022 02:42:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=20
05/31/2022 02:43:01 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.3059890872551706 on epoch=20
05/31/2022 02:43:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=21
05/31/2022 02:43:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=21
05/31/2022 02:43:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=22
05/31/2022 02:43:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=22
05/31/2022 02:43:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/31/2022 02:43:26 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.30741438409041777 on epoch=22
05/31/2022 02:43:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/31/2022 02:43:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=23
05/31/2022 02:43:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=24
05/31/2022 02:43:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=24
05/31/2022 02:43:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=24
05/31/2022 02:43:50 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.33231565458639695 on epoch=24
05/31/2022 02:43:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=25
05/31/2022 02:43:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=25
05/31/2022 02:43:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=26
05/31/2022 02:44:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=26
05/31/2022 02:44:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/31/2022 02:44:14 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.4092596326606186 on epoch=27
05/31/2022 02:44:14 - INFO - __main__ - Saving model with best Classification-F1: 0.34269331499663697 -> 0.4092596326606186 on epoch=27, global_step=650
05/31/2022 02:44:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=27
05/31/2022 02:44:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=27
05/31/2022 02:44:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=28
05/31/2022 02:44:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=28
05/31/2022 02:44:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/31/2022 02:44:37 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.358809112070369 on epoch=29
05/31/2022 02:44:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=29
05/31/2022 02:44:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=29
05/31/2022 02:44:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=30
05/31/2022 02:44:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=30
05/31/2022 02:44:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=31
05/31/2022 02:45:01 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.3590816991675478 on epoch=31
05/31/2022 02:45:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=31
05/31/2022 02:45:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=32
05/31/2022 02:45:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=32
05/31/2022 02:45:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=32
05/31/2022 02:45:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=33
05/31/2022 02:45:24 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.36897546897546896 on epoch=33
05/31/2022 02:45:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=33
05/31/2022 02:45:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=34
05/31/2022 02:45:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=34
05/31/2022 02:45:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=34
05/31/2022 02:45:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=35
05/31/2022 02:45:47 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.35257179794994925 on epoch=35
05/31/2022 02:45:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/31/2022 02:45:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=36
05/31/2022 02:45:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=36
05/31/2022 02:45:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=37
05/31/2022 02:46:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=37
05/31/2022 02:46:10 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3347136119129872 on epoch=37
05/31/2022 02:46:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=37
05/31/2022 02:46:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=38
05/31/2022 02:46:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=38
05/31/2022 02:46:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=39
05/31/2022 02:46:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=39
05/31/2022 02:46:32 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.35865520829134967 on epoch=39
05/31/2022 02:46:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 02:46:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=40
05/31/2022 02:46:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=40
05/31/2022 02:46:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=41
05/31/2022 02:46:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=41
05/31/2022 02:46:55 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3965599685916308 on epoch=41
05/31/2022 02:46:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/31/2022 02:47:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=42
05/31/2022 02:47:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=42
05/31/2022 02:47:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=43
05/31/2022 02:47:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=43
05/31/2022 02:47:19 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.40291461562922204 on epoch=43
05/31/2022 02:47:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=44
05/31/2022 02:47:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=44
05/31/2022 02:47:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=44
05/31/2022 02:47:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=45
05/31/2022 02:47:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=45
05/31/2022 02:47:42 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3655531317663492 on epoch=45
05/31/2022 02:47:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=46
05/31/2022 02:47:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=46
05/31/2022 02:47:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=47
05/31/2022 02:47:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=47
05/31/2022 02:47:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=47
05/31/2022 02:48:04 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.4051894233712416 on epoch=47
05/31/2022 02:48:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=48
05/31/2022 02:48:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=48
05/31/2022 02:48:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=49
05/31/2022 02:48:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=49
05/31/2022 02:48:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=49
05/31/2022 02:48:26 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3852409626994271 on epoch=49
05/31/2022 02:48:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=50
05/31/2022 02:48:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=50
05/31/2022 02:48:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=51
05/31/2022 02:48:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=51
05/31/2022 02:48:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=52
05/31/2022 02:48:50 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.412689760457915 on epoch=52
05/31/2022 02:48:50 - INFO - __main__ - Saving model with best Classification-F1: 0.4092596326606186 -> 0.412689760457915 on epoch=52, global_step=1250
05/31/2022 02:48:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=52
05/31/2022 02:48:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=52
05/31/2022 02:48:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=53
05/31/2022 02:49:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=53
05/31/2022 02:49:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=54
05/31/2022 02:49:13 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.4329272937372328 on epoch=54
05/31/2022 02:49:13 - INFO - __main__ - Saving model with best Classification-F1: 0.412689760457915 -> 0.4329272937372328 on epoch=54, global_step=1300
05/31/2022 02:49:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=54
05/31/2022 02:49:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=54
05/31/2022 02:49:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=55
05/31/2022 02:49:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=55
05/31/2022 02:49:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=56
05/31/2022 02:49:36 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.3971726377203562 on epoch=56
05/31/2022 02:49:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=56
05/31/2022 02:49:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=57
05/31/2022 02:49:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=57
05/31/2022 02:49:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=57
05/31/2022 02:49:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=58
05/31/2022 02:49:59 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.4331675913522329 on epoch=58
05/31/2022 02:49:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4329272937372328 -> 0.4331675913522329 on epoch=58, global_step=1400
05/31/2022 02:50:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=58
05/31/2022 02:50:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=59
05/31/2022 02:50:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=59
05/31/2022 02:50:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=59
05/31/2022 02:50:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=60
05/31/2022 02:50:23 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.39371378180901995 on epoch=60
05/31/2022 02:50:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=60
05/31/2022 02:50:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=61
05/31/2022 02:50:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=61
05/31/2022 02:50:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=62
05/31/2022 02:50:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=62
05/31/2022 02:50:47 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.41682688028482423 on epoch=62
05/31/2022 02:50:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=62
05/31/2022 02:50:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=63
05/31/2022 02:50:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=63
05/31/2022 02:50:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=64
05/31/2022 02:51:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=64
05/31/2022 02:51:10 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.42026050891809347 on epoch=64
05/31/2022 02:51:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=64
05/31/2022 02:51:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=65
05/31/2022 02:51:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=65
05/31/2022 02:51:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=66
05/31/2022 02:51:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=66
05/31/2022 02:51:34 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.4722658617731114 on epoch=66
05/31/2022 02:51:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4331675913522329 -> 0.4722658617731114 on epoch=66, global_step=1600
05/31/2022 02:51:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=67
05/31/2022 02:51:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=67
05/31/2022 02:51:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=67
05/31/2022 02:51:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=68
05/31/2022 02:51:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=68
05/31/2022 02:51:57 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.4507783796543938 on epoch=68
05/31/2022 02:52:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=69
05/31/2022 02:52:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=69
05/31/2022 02:52:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=69
05/31/2022 02:52:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=70
05/31/2022 02:52:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=70
05/31/2022 02:52:20 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.4365458937198068 on epoch=70
05/31/2022 02:52:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=71
05/31/2022 02:52:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=71
05/31/2022 02:52:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=72
05/31/2022 02:52:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=72
05/31/2022 02:52:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=72
05/31/2022 02:52:44 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.4534557396626362 on epoch=72
05/31/2022 02:52:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=73
05/31/2022 02:52:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=73
05/31/2022 02:52:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=74
05/31/2022 02:52:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=74
05/31/2022 02:52:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=74
05/31/2022 02:53:06 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.4236876123853252 on epoch=74
05/31/2022 02:53:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=75
05/31/2022 02:53:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=75
05/31/2022 02:53:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=76
05/31/2022 02:53:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=76
05/31/2022 02:53:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=77
05/31/2022 02:53:29 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.4560066579823419 on epoch=77
05/31/2022 02:53:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=77
05/31/2022 02:53:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=77
05/31/2022 02:53:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=78
05/31/2022 02:53:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=78
05/31/2022 02:53:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=79
05/31/2022 02:53:53 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.46907884818332574 on epoch=79
05/31/2022 02:53:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=79
05/31/2022 02:53:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=79
05/31/2022 02:54:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=80
05/31/2022 02:54:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=80
05/31/2022 02:54:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=81
05/31/2022 02:54:17 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.49502787407001964 on epoch=81
05/31/2022 02:54:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4722658617731114 -> 0.49502787407001964 on epoch=81, global_step=1950
05/31/2022 02:54:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=81
05/31/2022 02:54:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=82
05/31/2022 02:54:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=82
05/31/2022 02:54:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=82
05/31/2022 02:54:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=83
05/31/2022 02:54:42 - INFO - __main__ - Global step 2000 Train loss 0.27 Classification-F1 0.4993453552164482 on epoch=83
05/31/2022 02:54:42 - INFO - __main__ - Saving model with best Classification-F1: 0.49502787407001964 -> 0.4993453552164482 on epoch=83, global_step=2000
05/31/2022 02:54:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=83
05/31/2022 02:54:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=84
05/31/2022 02:54:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=84
05/31/2022 02:54:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=84
05/31/2022 02:54:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=85
05/31/2022 02:55:06 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.432962531499207 on epoch=85
05/31/2022 02:55:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=85
05/31/2022 02:55:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=86
05/31/2022 02:55:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=86
05/31/2022 02:55:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=87
05/31/2022 02:55:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=87
05/31/2022 02:55:29 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.4342214694820347 on epoch=87
05/31/2022 02:55:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=87
05/31/2022 02:55:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=88
05/31/2022 02:55:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=88
05/31/2022 02:55:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=89
05/31/2022 02:55:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=89
05/31/2022 02:55:54 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.4355646301049616 on epoch=89
05/31/2022 02:55:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=89
05/31/2022 02:55:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=90
05/31/2022 02:56:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=90
05/31/2022 02:56:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=91
05/31/2022 02:56:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=91
05/31/2022 02:56:18 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.43794145487309705 on epoch=91
05/31/2022 02:56:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=92
05/31/2022 02:56:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=92
05/31/2022 02:56:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=92
05/31/2022 02:56:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=93
05/31/2022 02:56:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=93
05/31/2022 02:56:43 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.4528800260896255 on epoch=93
05/31/2022 02:56:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.28 on epoch=94
05/31/2022 02:56:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=94
05/31/2022 02:56:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=94
05/31/2022 02:56:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=95
05/31/2022 02:56:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.27 on epoch=95
05/31/2022 02:57:07 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.43787325145617756 on epoch=95
05/31/2022 02:57:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=96
05/31/2022 02:57:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.23 on epoch=96
05/31/2022 02:57:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=97
05/31/2022 02:57:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=97
05/31/2022 02:57:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=97
05/31/2022 02:57:31 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.44349566023984627 on epoch=97
05/31/2022 02:57:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=98
05/31/2022 02:57:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=98
05/31/2022 02:57:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=99
05/31/2022 02:57:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=99
05/31/2022 02:57:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=99
05/31/2022 02:57:56 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.44379685949306197 on epoch=99
05/31/2022 02:57:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=100
05/31/2022 02:58:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=100
05/31/2022 02:58:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=101
05/31/2022 02:58:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=101
05/31/2022 02:58:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=102
05/31/2022 02:58:20 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.472923821938762 on epoch=102
05/31/2022 02:58:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.23 on epoch=102
05/31/2022 02:58:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=102
05/31/2022 02:58:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=103
05/31/2022 02:58:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=103
05/31/2022 02:58:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=104
05/31/2022 02:58:44 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.27356160747465097 on epoch=104
05/31/2022 02:58:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=104
05/31/2022 02:58:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.21 on epoch=104
05/31/2022 02:58:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=105
05/31/2022 02:58:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=105
05/31/2022 02:58:58 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=106
05/31/2022 02:59:09 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.256473124461052 on epoch=106
05/31/2022 02:59:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=106
05/31/2022 02:59:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=107
05/31/2022 02:59:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=107
05/31/2022 02:59:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=107
05/31/2022 02:59:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=108
05/31/2022 02:59:33 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.34494697779926986 on epoch=108
05/31/2022 02:59:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=108
05/31/2022 02:59:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=109
05/31/2022 02:59:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=109
05/31/2022 02:59:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=109
05/31/2022 02:59:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=110
05/31/2022 02:59:58 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.3138963073550237 on epoch=110
05/31/2022 03:00:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=110
05/31/2022 03:00:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=111
05/31/2022 03:00:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=111
05/31/2022 03:00:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=112
05/31/2022 03:00:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=112
05/31/2022 03:00:22 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.1674898130279725 on epoch=112
05/31/2022 03:00:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=112
05/31/2022 03:00:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=113
05/31/2022 03:00:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=113
05/31/2022 03:00:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=114
05/31/2022 03:00:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=114
05/31/2022 03:00:46 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.14023004524665153 on epoch=114
05/31/2022 03:00:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=114
05/31/2022 03:00:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=115
05/31/2022 03:00:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=115
05/31/2022 03:00:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=116
05/31/2022 03:01:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=116
05/31/2022 03:01:11 - INFO - __main__ - Global step 2800 Train loss 0.19 Classification-F1 0.1854941634352516 on epoch=116
05/31/2022 03:01:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=117
05/31/2022 03:01:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=117
05/31/2022 03:01:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=117
05/31/2022 03:01:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=118
05/31/2022 03:01:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=118
05/31/2022 03:01:36 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.110412681842097 on epoch=118
05/31/2022 03:01:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=119
05/31/2022 03:01:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=119
05/31/2022 03:01:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=119
05/31/2022 03:01:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=120
05/31/2022 03:01:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=120
05/31/2022 03:02:00 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.2070349440664283 on epoch=120
05/31/2022 03:02:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=121
05/31/2022 03:02:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=121
05/31/2022 03:02:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=122
05/31/2022 03:02:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=122
05/31/2022 03:02:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=122
05/31/2022 03:02:25 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.11502738136796525 on epoch=122
05/31/2022 03:02:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=123
05/31/2022 03:02:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=123
05/31/2022 03:02:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=124
05/31/2022 03:02:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=124
05/31/2022 03:02:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=124
05/31/2022 03:02:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:02:40 - INFO - __main__ - Printing 3 examples
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:02:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:02:40 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:02:40 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:02:40 - INFO - __main__ - Printing 3 examples
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:02:40 - INFO - __main__ - ['entailment']
05/31/2022 03:02:40 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:02:40 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:02:41 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:02:49 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.11256532712228916 on epoch=124
05/31/2022 03:02:49 - INFO - __main__ - save last model!
05/31/2022 03:02:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 03:02:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 03:02:49 - INFO - __main__ - Printing 3 examples
05/31/2022 03:02:49 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 03:02:49 - INFO - __main__ - ['contradiction']
05/31/2022 03:02:49 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 03:02:49 - INFO - __main__ - ['entailment']
05/31/2022 03:02:49 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 03:02:49 - INFO - __main__ - ['contradiction']
05/31/2022 03:02:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:02:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:02:51 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 03:02:59 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:02:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:02:59 - INFO - __main__ - Starting training!
05/31/2022 03:03:21 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_21_0.5_8_predictions.txt
05/31/2022 03:03:21 - INFO - __main__ - Classification-F1 on test data: 0.0686
05/31/2022 03:03:21 - INFO - __main__ - prefix=anli_128_21, lr=0.5, bsz=8, dev_performance=0.4993453552164482, test_performance=0.0685990262624668
05/31/2022 03:03:21 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.4, bsz=8 ...
05/31/2022 03:03:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:03:22 - INFO - __main__ - Printing 3 examples
05/31/2022 03:03:22 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:03:22 - INFO - __main__ - ['entailment']
05/31/2022 03:03:22 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:03:22 - INFO - __main__ - ['entailment']
05/31/2022 03:03:22 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:03:22 - INFO - __main__ - ['entailment']
05/31/2022 03:03:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:03:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:03:23 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:03:23 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:03:23 - INFO - __main__ - Printing 3 examples
05/31/2022 03:03:23 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:03:23 - INFO - __main__ - ['entailment']
05/31/2022 03:03:23 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:03:23 - INFO - __main__ - ['entailment']
05/31/2022 03:03:23 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:03:23 - INFO - __main__ - ['entailment']
05/31/2022 03:03:23 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:03:23 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:03:24 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:03:40 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:03:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:03:41 - INFO - __main__ - Starting training!
05/31/2022 03:03:44 - INFO - __main__ - Step 10 Global step 10 Train loss 0.58 on epoch=0
05/31/2022 03:03:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=0
05/31/2022 03:03:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=1
05/31/2022 03:03:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=1
05/31/2022 03:03:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=2
05/31/2022 03:04:05 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.1676489849377865 on epoch=2
05/31/2022 03:04:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1676489849377865 on epoch=2, global_step=50
05/31/2022 03:04:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=2
05/31/2022 03:04:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=2
05/31/2022 03:04:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=3
05/31/2022 03:04:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=3
05/31/2022 03:04:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=4
05/31/2022 03:04:28 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 03:04:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=4
05/31/2022 03:04:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=4
05/31/2022 03:04:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=5
05/31/2022 03:04:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=5
05/31/2022 03:04:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/31/2022 03:04:51 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 03:04:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=6
05/31/2022 03:04:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/31/2022 03:04:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=7
05/31/2022 03:05:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=7
05/31/2022 03:05:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=8
05/31/2022 03:05:14 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 03:05:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=8
05/31/2022 03:05:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=9
05/31/2022 03:05:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=9
05/31/2022 03:05:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=9
05/31/2022 03:05:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=10
05/31/2022 03:05:38 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 03:05:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=10
05/31/2022 03:05:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=11
05/31/2022 03:05:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=11
05/31/2022 03:05:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=12
05/31/2022 03:05:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=12
05/31/2022 03:06:03 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.18836044746975922 on epoch=12
05/31/2022 03:06:03 - INFO - __main__ - Saving model with best Classification-F1: 0.1676489849377865 -> 0.18836044746975922 on epoch=12, global_step=300
05/31/2022 03:06:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=12
05/31/2022 03:06:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=13
05/31/2022 03:06:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=13
05/31/2022 03:06:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=14
05/31/2022 03:06:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=14
05/31/2022 03:06:27 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.19385251958031624 on epoch=14
05/31/2022 03:06:27 - INFO - __main__ - Saving model with best Classification-F1: 0.18836044746975922 -> 0.19385251958031624 on epoch=14, global_step=350
05/31/2022 03:06:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=14
05/31/2022 03:06:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=15
05/31/2022 03:06:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=15
05/31/2022 03:06:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=16
05/31/2022 03:06:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=16
05/31/2022 03:06:52 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.35739851954839885 on epoch=16
05/31/2022 03:06:52 - INFO - __main__ - Saving model with best Classification-F1: 0.19385251958031624 -> 0.35739851954839885 on epoch=16, global_step=400
05/31/2022 03:06:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=17
05/31/2022 03:06:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=17
05/31/2022 03:07:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=17
05/31/2022 03:07:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=18
05/31/2022 03:07:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=18
05/31/2022 03:07:16 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.2941304852565591 on epoch=18
05/31/2022 03:07:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=19
05/31/2022 03:07:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=19
05/31/2022 03:07:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=19
05/31/2022 03:07:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/31/2022 03:07:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=20
05/31/2022 03:07:41 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.32618612283169246 on epoch=20
05/31/2022 03:07:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=21
05/31/2022 03:07:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=21
05/31/2022 03:07:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=22
05/31/2022 03:07:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=22
05/31/2022 03:07:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/31/2022 03:08:06 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.3230934611827394 on epoch=22
05/31/2022 03:08:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=23
05/31/2022 03:08:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=23
05/31/2022 03:08:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=24
05/31/2022 03:08:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=24
05/31/2022 03:08:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/31/2022 03:08:30 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.26192266160007655 on epoch=24
05/31/2022 03:08:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/31/2022 03:08:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=25
05/31/2022 03:08:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 03:08:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=26
05/31/2022 03:08:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=27
05/31/2022 03:08:55 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.41185732052605123 on epoch=27
05/31/2022 03:08:55 - INFO - __main__ - Saving model with best Classification-F1: 0.35739851954839885 -> 0.41185732052605123 on epoch=27, global_step=650
05/31/2022 03:08:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=27
05/31/2022 03:09:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=27
05/31/2022 03:09:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=28
05/31/2022 03:09:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=28
05/31/2022 03:09:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=29
05/31/2022 03:09:19 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.3199944746397239 on epoch=29
05/31/2022 03:09:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=29
05/31/2022 03:09:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=29
05/31/2022 03:09:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=30
05/31/2022 03:09:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=30
05/31/2022 03:09:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=31
05/31/2022 03:09:44 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.38049613784907904 on epoch=31
05/31/2022 03:09:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=31
05/31/2022 03:09:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/31/2022 03:09:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 03:09:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=32
05/31/2022 03:09:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=33
05/31/2022 03:10:06 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.3641927799157047 on epoch=33
05/31/2022 03:10:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=33
05/31/2022 03:10:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=34
05/31/2022 03:10:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=34
05/31/2022 03:10:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=34
05/31/2022 03:10:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=35
05/31/2022 03:10:29 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.32299015013237303 on epoch=35
05/31/2022 03:10:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=35
05/31/2022 03:10:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=36
05/31/2022 03:10:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=36
05/31/2022 03:10:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/31/2022 03:10:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=37
05/31/2022 03:10:51 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.32193252617447427 on epoch=37
05/31/2022 03:10:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/31/2022 03:10:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=38
05/31/2022 03:10:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=38
05/31/2022 03:11:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=39
05/31/2022 03:11:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=39
05/31/2022 03:11:13 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.35064327485380115 on epoch=39
05/31/2022 03:11:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=39
05/31/2022 03:11:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=40
05/31/2022 03:11:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=40
05/31/2022 03:11:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=41
05/31/2022 03:11:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=41
05/31/2022 03:11:35 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.3944106378183457 on epoch=41
05/31/2022 03:11:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=42
05/31/2022 03:11:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=42
05/31/2022 03:11:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=42
05/31/2022 03:11:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=43
05/31/2022 03:11:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=43
05/31/2022 03:11:58 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.3979238949535979 on epoch=43
05/31/2022 03:12:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=44
05/31/2022 03:12:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=44
05/31/2022 03:12:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/31/2022 03:12:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=45
05/31/2022 03:12:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=45
05/31/2022 03:12:20 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.36469535353644195 on epoch=45
05/31/2022 03:12:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=46
05/31/2022 03:12:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=46
05/31/2022 03:12:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=47
05/31/2022 03:12:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=47
05/31/2022 03:12:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=47
05/31/2022 03:12:42 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3540813540813541 on epoch=47
05/31/2022 03:12:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=48
05/31/2022 03:12:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=48
05/31/2022 03:12:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=49
05/31/2022 03:12:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=49
05/31/2022 03:12:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=49
05/31/2022 03:13:05 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.41748366013071897 on epoch=49
05/31/2022 03:13:05 - INFO - __main__ - Saving model with best Classification-F1: 0.41185732052605123 -> 0.41748366013071897 on epoch=49, global_step=1200
05/31/2022 03:13:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=50
05/31/2022 03:13:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=50
05/31/2022 03:13:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=51
05/31/2022 03:13:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=51
05/31/2022 03:13:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=52
05/31/2022 03:13:27 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.40578002244668915 on epoch=52
05/31/2022 03:13:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=52
05/31/2022 03:13:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=52
05/31/2022 03:13:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=53
05/31/2022 03:13:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=53
05/31/2022 03:13:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=54
05/31/2022 03:13:50 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.4201530168666319 on epoch=54
05/31/2022 03:13:50 - INFO - __main__ - Saving model with best Classification-F1: 0.41748366013071897 -> 0.4201530168666319 on epoch=54, global_step=1300
05/31/2022 03:13:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=54
05/31/2022 03:13:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=54
05/31/2022 03:13:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=55
05/31/2022 03:14:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=55
05/31/2022 03:14:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=56
05/31/2022 03:14:14 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.4049610886783457 on epoch=56
05/31/2022 03:14:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=56
05/31/2022 03:14:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=57
05/31/2022 03:14:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=57
05/31/2022 03:14:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=57
05/31/2022 03:14:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=58
05/31/2022 03:14:36 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.39182073636022174 on epoch=58
05/31/2022 03:14:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=58
05/31/2022 03:14:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=59
05/31/2022 03:14:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=59
05/31/2022 03:14:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=59
05/31/2022 03:14:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=60
05/31/2022 03:14:58 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.36818181818181817 on epoch=60
05/31/2022 03:15:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=60
05/31/2022 03:15:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=61
05/31/2022 03:15:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=61
05/31/2022 03:15:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=62
05/31/2022 03:15:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=62
05/31/2022 03:15:21 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.36519026400729154 on epoch=62
05/31/2022 03:15:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=62
05/31/2022 03:15:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=63
05/31/2022 03:15:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=63
05/31/2022 03:15:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=64
05/31/2022 03:15:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=64
05/31/2022 03:15:43 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.39732414808152255 on epoch=64
05/31/2022 03:15:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=64
05/31/2022 03:15:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=65
05/31/2022 03:15:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=65
05/31/2022 03:15:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=66
05/31/2022 03:15:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=66
05/31/2022 03:16:05 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.4269200685916224 on epoch=66
05/31/2022 03:16:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4201530168666319 -> 0.4269200685916224 on epoch=66, global_step=1600
05/31/2022 03:16:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=67
05/31/2022 03:16:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=67
05/31/2022 03:16:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=67
05/31/2022 03:16:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=68
05/31/2022 03:16:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=68
05/31/2022 03:16:26 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.41515000391080575 on epoch=68
05/31/2022 03:16:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=69
05/31/2022 03:16:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=69
05/31/2022 03:16:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=69
05/31/2022 03:16:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=70
05/31/2022 03:16:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=70
05/31/2022 03:16:49 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.4256053671695601 on epoch=70
05/31/2022 03:16:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=71
05/31/2022 03:16:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=71
05/31/2022 03:16:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=72
05/31/2022 03:16:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=72
05/31/2022 03:17:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=72
05/31/2022 03:17:12 - INFO - __main__ - Global step 1750 Train loss 0.31 Classification-F1 0.42641987952964744 on epoch=72
05/31/2022 03:17:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=73
05/31/2022 03:17:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=73
05/31/2022 03:17:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=74
05/31/2022 03:17:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=74
05/31/2022 03:17:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=74
05/31/2022 03:17:34 - INFO - __main__ - Global step 1800 Train loss 0.32 Classification-F1 0.38170203250997004 on epoch=74
05/31/2022 03:17:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=75
05/31/2022 03:17:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=75
05/31/2022 03:17:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=76
05/31/2022 03:17:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=76
05/31/2022 03:17:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=77
05/31/2022 03:17:57 - INFO - __main__ - Global step 1850 Train loss 0.30 Classification-F1 0.41116048526004034 on epoch=77
05/31/2022 03:17:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=77
05/31/2022 03:18:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=77
05/31/2022 03:18:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=78
05/31/2022 03:18:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=78
05/31/2022 03:18:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=79
05/31/2022 03:18:20 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.44744181550890083 on epoch=79
05/31/2022 03:18:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4269200685916224 -> 0.44744181550890083 on epoch=79, global_step=1900
05/31/2022 03:18:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=79
05/31/2022 03:18:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=79
05/31/2022 03:18:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=80
05/31/2022 03:18:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=80
05/31/2022 03:18:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=81
05/31/2022 03:18:43 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.42302541874708827 on epoch=81
05/31/2022 03:18:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=81
05/31/2022 03:18:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=82
05/31/2022 03:18:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=82
05/31/2022 03:18:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=82
05/31/2022 03:18:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=83
05/31/2022 03:19:06 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.43899082243630566 on epoch=83
05/31/2022 03:19:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=83
05/31/2022 03:19:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=84
05/31/2022 03:19:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=84
05/31/2022 03:19:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=84
05/31/2022 03:19:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=85
05/31/2022 03:19:29 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.4225727130297023 on epoch=85
05/31/2022 03:19:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=85
05/31/2022 03:19:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=86
05/31/2022 03:19:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=86
05/31/2022 03:19:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=87
05/31/2022 03:19:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=87
05/31/2022 03:19:51 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.43975536680555044 on epoch=87
05/31/2022 03:19:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=87
05/31/2022 03:19:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.28 on epoch=88
05/31/2022 03:19:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=88
05/31/2022 03:20:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=89
05/31/2022 03:20:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=89
05/31/2022 03:20:14 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.46835326082081313 on epoch=89
05/31/2022 03:20:14 - INFO - __main__ - Saving model with best Classification-F1: 0.44744181550890083 -> 0.46835326082081313 on epoch=89, global_step=2150
05/31/2022 03:20:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=89
05/31/2022 03:20:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=90
05/31/2022 03:20:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=90
05/31/2022 03:20:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=91
05/31/2022 03:20:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.32 on epoch=91
05/31/2022 03:20:38 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.43350112233318233 on epoch=91
05/31/2022 03:20:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=92
05/31/2022 03:20:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=92
05/31/2022 03:20:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=92
05/31/2022 03:20:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=93
05/31/2022 03:20:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=93
05/31/2022 03:21:01 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.44044481824128584 on epoch=93
05/31/2022 03:21:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=94
05/31/2022 03:21:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=94
05/31/2022 03:21:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=94
05/31/2022 03:21:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=95
05/31/2022 03:21:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=95
05/31/2022 03:21:24 - INFO - __main__ - Global step 2300 Train loss 0.27 Classification-F1 0.4613020527859238 on epoch=95
05/31/2022 03:21:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=96
05/31/2022 03:21:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=96
05/31/2022 03:21:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=97
05/31/2022 03:21:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.28 on epoch=97
05/31/2022 03:21:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=97
05/31/2022 03:21:47 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.49547696699955646 on epoch=97
05/31/2022 03:21:47 - INFO - __main__ - Saving model with best Classification-F1: 0.46835326082081313 -> 0.49547696699955646 on epoch=97, global_step=2350
05/31/2022 03:21:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=98
05/31/2022 03:21:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=98
05/31/2022 03:21:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.24 on epoch=99
05/31/2022 03:21:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=99
05/31/2022 03:22:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=99
05/31/2022 03:22:10 - INFO - __main__ - Global step 2400 Train loss 0.25 Classification-F1 0.45897347103470115 on epoch=99
05/31/2022 03:22:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=100
05/31/2022 03:22:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=100
05/31/2022 03:22:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=101
05/31/2022 03:22:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.29 on epoch=101
05/31/2022 03:22:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.27 on epoch=102
05/31/2022 03:22:34 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.484728770153076 on epoch=102
05/31/2022 03:22:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=102
05/31/2022 03:22:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.26 on epoch=102
05/31/2022 03:22:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=103
05/31/2022 03:22:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=103
05/31/2022 03:22:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.27 on epoch=104
05/31/2022 03:22:59 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.5029150399147713 on epoch=104
05/31/2022 03:22:59 - INFO - __main__ - Saving model with best Classification-F1: 0.49547696699955646 -> 0.5029150399147713 on epoch=104, global_step=2500
05/31/2022 03:23:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=104
05/31/2022 03:23:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.21 on epoch=104
05/31/2022 03:23:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=105
05/31/2022 03:23:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=105
05/31/2022 03:23:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=106
05/31/2022 03:23:23 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.48231303330216185 on epoch=106
05/31/2022 03:23:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.23 on epoch=106
05/31/2022 03:23:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=107
05/31/2022 03:23:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=107
05/31/2022 03:23:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=107
05/31/2022 03:23:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=108
05/31/2022 03:23:47 - INFO - __main__ - Global step 2600 Train loss 0.23 Classification-F1 0.5063019665637134 on epoch=108
05/31/2022 03:23:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5029150399147713 -> 0.5063019665637134 on epoch=108, global_step=2600
05/31/2022 03:23:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=108
05/31/2022 03:23:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=109
05/31/2022 03:23:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=109
05/31/2022 03:23:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.26 on epoch=109
05/31/2022 03:24:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=110
05/31/2022 03:24:10 - INFO - __main__ - Global step 2650 Train loss 0.20 Classification-F1 0.4610657595963345 on epoch=110
05/31/2022 03:24:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=110
05/31/2022 03:24:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=111
05/31/2022 03:24:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=111
05/31/2022 03:24:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.25 on epoch=112
05/31/2022 03:24:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=112
05/31/2022 03:24:34 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.48300653341008354 on epoch=112
05/31/2022 03:24:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.29 on epoch=112
05/31/2022 03:24:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=113
05/31/2022 03:24:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=113
05/31/2022 03:24:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=114
05/31/2022 03:24:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=114
05/31/2022 03:24:58 - INFO - __main__ - Global step 2750 Train loss 0.22 Classification-F1 0.4902766536123833 on epoch=114
05/31/2022 03:25:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=114
05/31/2022 03:25:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=115
05/31/2022 03:25:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=115
05/31/2022 03:25:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.23 on epoch=116
05/31/2022 03:25:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=116
05/31/2022 03:25:23 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.25275753895972475 on epoch=116
05/31/2022 03:25:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=117
05/31/2022 03:25:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=117
05/31/2022 03:25:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=117
05/31/2022 03:25:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=118
05/31/2022 03:25:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=118
05/31/2022 03:25:46 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.49912510411460936 on epoch=118
05/31/2022 03:25:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=119
05/31/2022 03:25:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=119
05/31/2022 03:25:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.27 on epoch=119
05/31/2022 03:25:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=120
05/31/2022 03:25:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=120
05/31/2022 03:26:10 - INFO - __main__ - Global step 2900 Train loss 0.22 Classification-F1 0.5414983229594453 on epoch=120
05/31/2022 03:26:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5063019665637134 -> 0.5414983229594453 on epoch=120, global_step=2900
05/31/2022 03:26:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=121
05/31/2022 03:26:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=121
05/31/2022 03:26:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=122
05/31/2022 03:26:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=122
05/31/2022 03:26:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=122
05/31/2022 03:26:35 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.38290488962969005 on epoch=122
05/31/2022 03:26:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=123
05/31/2022 03:26:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.19 on epoch=123
05/31/2022 03:26:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=124
05/31/2022 03:26:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.25 on epoch=124
05/31/2022 03:26:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=124
05/31/2022 03:26:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:26:49 - INFO - __main__ - Printing 3 examples
05/31/2022 03:26:49 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:26:49 - INFO - __main__ - ['entailment']
05/31/2022 03:26:49 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:26:49 - INFO - __main__ - ['entailment']
05/31/2022 03:26:49 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:26:49 - INFO - __main__ - ['entailment']
05/31/2022 03:26:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:26:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:26:50 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:26:50 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:26:50 - INFO - __main__ - Printing 3 examples
05/31/2022 03:26:50 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:26:50 - INFO - __main__ - ['entailment']
05/31/2022 03:26:50 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:26:50 - INFO - __main__ - ['entailment']
05/31/2022 03:26:50 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:26:50 - INFO - __main__ - ['entailment']
05/31/2022 03:26:50 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:26:50 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:26:51 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:26:59 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.3542620498791025 on epoch=124
05/31/2022 03:26:59 - INFO - __main__ - save last model!
05/31/2022 03:26:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 03:26:59 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 03:26:59 - INFO - __main__ - Printing 3 examples
05/31/2022 03:26:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 03:26:59 - INFO - __main__ - ['contradiction']
05/31/2022 03:26:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 03:26:59 - INFO - __main__ - ['entailment']
05/31/2022 03:26:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 03:26:59 - INFO - __main__ - ['contradiction']
05/31/2022 03:26:59 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:26:59 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:27:00 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 03:27:08 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:27:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:27:09 - INFO - __main__ - Starting training!
05/31/2022 03:27:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_21_0.4_8_predictions.txt
05/31/2022 03:27:29 - INFO - __main__ - Classification-F1 on test data: 0.1932
05/31/2022 03:27:29 - INFO - __main__ - prefix=anli_128_21, lr=0.4, bsz=8, dev_performance=0.5414983229594453, test_performance=0.19316630782746494
05/31/2022 03:27:29 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.3, bsz=8 ...
05/31/2022 03:27:30 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:27:30 - INFO - __main__ - Printing 3 examples
05/31/2022 03:27:30 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:27:30 - INFO - __main__ - ['entailment']
05/31/2022 03:27:30 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:27:30 - INFO - __main__ - ['entailment']
05/31/2022 03:27:30 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:27:30 - INFO - __main__ - ['entailment']
05/31/2022 03:27:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:27:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:27:31 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:27:31 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:27:31 - INFO - __main__ - Printing 3 examples
05/31/2022 03:27:31 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:27:31 - INFO - __main__ - ['entailment']
05/31/2022 03:27:31 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:27:31 - INFO - __main__ - ['entailment']
05/31/2022 03:27:31 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:27:31 - INFO - __main__ - ['entailment']
05/31/2022 03:27:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:27:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:27:31 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:27:49 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:27:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:27:50 - INFO - __main__ - Starting training!
05/31/2022 03:27:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.55 on epoch=0
05/31/2022 03:27:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=0
05/31/2022 03:27:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=1
05/31/2022 03:28:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=1
05/31/2022 03:28:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=2
05/31/2022 03:28:15 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 03:28:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 03:28:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=2
05/31/2022 03:28:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=2
05/31/2022 03:28:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=3
05/31/2022 03:28:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=3
05/31/2022 03:28:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=4
05/31/2022 03:28:40 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 03:28:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=4
05/31/2022 03:28:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=4
05/31/2022 03:28:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=5
05/31/2022 03:28:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=5
05/31/2022 03:28:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=6
05/31/2022 03:29:03 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=6
05/31/2022 03:29:03 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16699282452707112 on epoch=6, global_step=150
05/31/2022 03:29:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=6
05/31/2022 03:29:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/31/2022 03:29:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=7
05/31/2022 03:29:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=7
05/31/2022 03:29:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=8
05/31/2022 03:29:25 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 03:29:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=8
05/31/2022 03:29:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=9
05/31/2022 03:29:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=9
05/31/2022 03:29:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=9
05/31/2022 03:29:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=10
05/31/2022 03:29:48 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 03:29:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=10
05/31/2022 03:29:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=11
05/31/2022 03:29:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=11
05/31/2022 03:29:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=12
05/31/2022 03:30:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/31/2022 03:30:12 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 03:30:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=12
05/31/2022 03:30:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/31/2022 03:30:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=13
05/31/2022 03:30:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=14
05/31/2022 03:30:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=14
05/31/2022 03:30:35 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.1717171717171717 on epoch=14
05/31/2022 03:30:35 - INFO - __main__ - Saving model with best Classification-F1: 0.16699282452707112 -> 0.1717171717171717 on epoch=14, global_step=350
05/31/2022 03:30:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=14
05/31/2022 03:30:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=15
05/31/2022 03:30:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=15
05/31/2022 03:30:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=16
05/31/2022 03:30:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=16
05/31/2022 03:30:59 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.18313051777618705 on epoch=16
05/31/2022 03:30:59 - INFO - __main__ - Saving model with best Classification-F1: 0.1717171717171717 -> 0.18313051777618705 on epoch=16, global_step=400
05/31/2022 03:31:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=17
05/31/2022 03:31:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=17
05/31/2022 03:31:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=17
05/31/2022 03:31:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=18
05/31/2022 03:31:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=18
05/31/2022 03:31:23 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.2188547677377666 on epoch=18
05/31/2022 03:31:23 - INFO - __main__ - Saving model with best Classification-F1: 0.18313051777618705 -> 0.2188547677377666 on epoch=18, global_step=450
05/31/2022 03:31:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=19
05/31/2022 03:31:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=19
05/31/2022 03:31:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=19
05/31/2022 03:31:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=20
05/31/2022 03:31:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=20
05/31/2022 03:31:47 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.19833288965164378 on epoch=20
05/31/2022 03:31:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=21
05/31/2022 03:31:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=21
05/31/2022 03:31:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=22
05/31/2022 03:31:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=22
05/31/2022 03:32:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/31/2022 03:32:11 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.23310424964050422 on epoch=22
05/31/2022 03:32:11 - INFO - __main__ - Saving model with best Classification-F1: 0.2188547677377666 -> 0.23310424964050422 on epoch=22, global_step=550
05/31/2022 03:32:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=23
05/31/2022 03:32:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=23
05/31/2022 03:32:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=24
05/31/2022 03:32:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=24
05/31/2022 03:32:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=24
05/31/2022 03:32:35 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.22324898701728269 on epoch=24
05/31/2022 03:32:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=25
05/31/2022 03:32:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=25
05/31/2022 03:32:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 03:32:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=26
05/31/2022 03:32:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=27
05/31/2022 03:32:59 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.2946906667334262 on epoch=27
05/31/2022 03:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.23310424964050422 -> 0.2946906667334262 on epoch=27, global_step=650
05/31/2022 03:33:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=27
05/31/2022 03:33:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=27
05/31/2022 03:33:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=28
05/31/2022 03:33:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=28
05/31/2022 03:33:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/31/2022 03:33:23 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.24867587748639752 on epoch=29
05/31/2022 03:33:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=29
05/31/2022 03:33:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=29
05/31/2022 03:33:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=30
05/31/2022 03:33:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=30
05/31/2022 03:33:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=31
05/31/2022 03:33:47 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.37740087644516235 on epoch=31
05/31/2022 03:33:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2946906667334262 -> 0.37740087644516235 on epoch=31, global_step=750
05/31/2022 03:33:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=31
05/31/2022 03:33:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=32
05/31/2022 03:33:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=32
05/31/2022 03:33:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/31/2022 03:34:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=33
05/31/2022 03:34:11 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.4143545941624131 on epoch=33
05/31/2022 03:34:11 - INFO - __main__ - Saving model with best Classification-F1: 0.37740087644516235 -> 0.4143545941624131 on epoch=33, global_step=800
05/31/2022 03:34:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=33
05/31/2022 03:34:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=34
05/31/2022 03:34:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=34
05/31/2022 03:34:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=34
05/31/2022 03:34:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=35
05/31/2022 03:34:35 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3163342830009497 on epoch=35
05/31/2022 03:34:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/31/2022 03:34:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=36
05/31/2022 03:34:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/31/2022 03:34:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=37
05/31/2022 03:34:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=37
05/31/2022 03:34:59 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.33390687855242907 on epoch=37
05/31/2022 03:35:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/31/2022 03:35:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=38
05/31/2022 03:35:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=38
05/31/2022 03:35:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=39
05/31/2022 03:35:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=39
05/31/2022 03:35:23 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3814183634523969 on epoch=39
05/31/2022 03:35:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/31/2022 03:35:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=40
05/31/2022 03:35:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=40
05/31/2022 03:35:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=41
05/31/2022 03:35:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=41
05/31/2022 03:35:47 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.4143779708536191 on epoch=41
05/31/2022 03:35:47 - INFO - __main__ - Saving model with best Classification-F1: 0.4143545941624131 -> 0.4143779708536191 on epoch=41, global_step=1000
05/31/2022 03:35:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=42
05/31/2022 03:35:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=42
05/31/2022 03:35:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=42
05/31/2022 03:35:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=43
05/31/2022 03:36:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=43
05/31/2022 03:36:10 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.38390555727394576 on epoch=43
05/31/2022 03:36:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=44
05/31/2022 03:36:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=44
05/31/2022 03:36:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/31/2022 03:36:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=45
05/31/2022 03:36:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=45
05/31/2022 03:36:32 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.3716308772742181 on epoch=45
05/31/2022 03:36:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=46
05/31/2022 03:36:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=46
05/31/2022 03:36:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=47
05/31/2022 03:36:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=47
05/31/2022 03:36:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=47
05/31/2022 03:36:55 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.3820243248814677 on epoch=47
05/31/2022 03:36:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=48
05/31/2022 03:37:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=48
05/31/2022 03:37:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=49
05/31/2022 03:37:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=49
05/31/2022 03:37:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=49
05/31/2022 03:37:16 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.36116959064327486 on epoch=49
05/31/2022 03:37:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=50
05/31/2022 03:37:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=50
05/31/2022 03:37:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=51
05/31/2022 03:37:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=51
05/31/2022 03:37:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=52
05/31/2022 03:37:37 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3790299971965237 on epoch=52
05/31/2022 03:37:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=52
05/31/2022 03:37:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=52
05/31/2022 03:37:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=53
05/31/2022 03:37:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=53
05/31/2022 03:37:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=54
05/31/2022 03:37:59 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.37850655451657644 on epoch=54
05/31/2022 03:38:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=54
05/31/2022 03:38:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=54
05/31/2022 03:38:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=55
05/31/2022 03:38:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=55
05/31/2022 03:38:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=56
05/31/2022 03:38:21 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3815587298488414 on epoch=56
05/31/2022 03:38:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=56
05/31/2022 03:38:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=57
05/31/2022 03:38:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=57
05/31/2022 03:38:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/31/2022 03:38:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=58
05/31/2022 03:38:42 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.383547651618787 on epoch=58
05/31/2022 03:38:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=58
05/31/2022 03:38:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=59
05/31/2022 03:38:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=59
05/31/2022 03:38:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=59
05/31/2022 03:38:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=60
05/31/2022 03:39:04 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.37122679164136235 on epoch=60
05/31/2022 03:39:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=60
05/31/2022 03:39:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/31/2022 03:39:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=61
05/31/2022 03:39:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=62
05/31/2022 03:39:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=62
05/31/2022 03:39:26 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3702584797826651 on epoch=62
05/31/2022 03:39:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=62
05/31/2022 03:39:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=63
05/31/2022 03:39:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=63
05/31/2022 03:39:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=64
05/31/2022 03:39:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/31/2022 03:39:49 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.38535839957381784 on epoch=64
05/31/2022 03:39:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 03:39:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=65
05/31/2022 03:39:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=65
05/31/2022 03:39:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=66
05/31/2022 03:40:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=66
05/31/2022 03:40:11 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3852446451217304 on epoch=66
05/31/2022 03:40:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=67
05/31/2022 03:40:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=67
05/31/2022 03:40:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=67
05/31/2022 03:40:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=68
05/31/2022 03:40:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=68
05/31/2022 03:40:34 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3928957708684164 on epoch=68
05/31/2022 03:40:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=69
05/31/2022 03:40:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=69
05/31/2022 03:40:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=69
05/31/2022 03:40:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=70
05/31/2022 03:40:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=70
05/31/2022 03:40:56 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.38504022613611655 on epoch=70
05/31/2022 03:40:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=71
05/31/2022 03:41:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=71
05/31/2022 03:41:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=72
05/31/2022 03:41:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=72
05/31/2022 03:41:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=72
05/31/2022 03:41:18 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.38390555727394576 on epoch=72
05/31/2022 03:41:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=73
05/31/2022 03:41:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=73
05/31/2022 03:41:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=74
05/31/2022 03:41:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=74
05/31/2022 03:41:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=74
05/31/2022 03:41:40 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.39238294898672255 on epoch=74
05/31/2022 03:41:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=75
05/31/2022 03:41:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=75
05/31/2022 03:41:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=76
05/31/2022 03:41:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.38 on epoch=76
05/31/2022 03:41:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=77
05/31/2022 03:42:03 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.393858212489651 on epoch=77
05/31/2022 03:42:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=77
05/31/2022 03:42:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=77
05/31/2022 03:42:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=78
05/31/2022 03:42:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=78
05/31/2022 03:42:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=79
05/31/2022 03:42:25 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3831857219538379 on epoch=79
05/31/2022 03:42:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=79
05/31/2022 03:42:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=79
05/31/2022 03:42:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=80
05/31/2022 03:42:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=80
05/31/2022 03:42:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=81
05/31/2022 03:42:47 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.39836691560299836 on epoch=81
05/31/2022 03:42:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=81
05/31/2022 03:42:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=82
05/31/2022 03:42:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=82
05/31/2022 03:42:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=82
05/31/2022 03:43:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=83
05/31/2022 03:43:10 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.4138305509210958 on epoch=83
05/31/2022 03:43:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=83
05/31/2022 03:43:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=84
05/31/2022 03:43:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=84
05/31/2022 03:43:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.34 on epoch=84
05/31/2022 03:43:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=85
05/31/2022 03:43:33 - INFO - __main__ - Global step 2050 Train loss 0.34 Classification-F1 0.38774165701174496 on epoch=85
05/31/2022 03:43:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=85
05/31/2022 03:43:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=86
05/31/2022 03:43:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=86
05/31/2022 03:43:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=87
05/31/2022 03:43:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=87
05/31/2022 03:43:55 - INFO - __main__ - Global step 2100 Train loss 0.35 Classification-F1 0.3892453421728425 on epoch=87
05/31/2022 03:43:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.33 on epoch=87
05/31/2022 03:44:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.35 on epoch=88
05/31/2022 03:44:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=88
05/31/2022 03:44:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.35 on epoch=89
05/31/2022 03:44:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=89
05/31/2022 03:44:18 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.39970345658419054 on epoch=89
05/31/2022 03:44:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=89
05/31/2022 03:44:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=90
05/31/2022 03:44:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.33 on epoch=90
05/31/2022 03:44:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=91
05/31/2022 03:44:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=91
05/31/2022 03:44:41 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.3954505686789152 on epoch=91
05/31/2022 03:44:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=92
05/31/2022 03:44:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=92
05/31/2022 03:44:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=92
05/31/2022 03:44:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.32 on epoch=93
05/31/2022 03:44:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.31 on epoch=93
05/31/2022 03:45:05 - INFO - __main__ - Global step 2250 Train loss 0.33 Classification-F1 0.4178062435361705 on epoch=93
05/31/2022 03:45:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4143779708536191 -> 0.4178062435361705 on epoch=93, global_step=2250
05/31/2022 03:45:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=94
05/31/2022 03:45:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=94
05/31/2022 03:45:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.31 on epoch=94
05/31/2022 03:45:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.33 on epoch=95
05/31/2022 03:45:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=95
05/31/2022 03:45:28 - INFO - __main__ - Global step 2300 Train loss 0.34 Classification-F1 0.4115748965402937 on epoch=95
05/31/2022 03:45:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=96
05/31/2022 03:45:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.33 on epoch=96
05/31/2022 03:45:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.34 on epoch=97
05/31/2022 03:45:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.34 on epoch=97
05/31/2022 03:45:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=97
05/31/2022 03:45:51 - INFO - __main__ - Global step 2350 Train loss 0.33 Classification-F1 0.421077467481006 on epoch=97
05/31/2022 03:45:51 - INFO - __main__ - Saving model with best Classification-F1: 0.4178062435361705 -> 0.421077467481006 on epoch=97, global_step=2350
05/31/2022 03:45:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.30 on epoch=98
05/31/2022 03:45:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=98
05/31/2022 03:46:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=99
05/31/2022 03:46:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=99
05/31/2022 03:46:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=99
05/31/2022 03:46:15 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.4210950080515297 on epoch=99
05/31/2022 03:46:15 - INFO - __main__ - Saving model with best Classification-F1: 0.421077467481006 -> 0.4210950080515297 on epoch=99, global_step=2400
05/31/2022 03:46:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.33 on epoch=100
05/31/2022 03:46:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.31 on epoch=100
05/31/2022 03:46:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=101
05/31/2022 03:46:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=101
05/31/2022 03:46:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=102
05/31/2022 03:46:39 - INFO - __main__ - Global step 2450 Train loss 0.32 Classification-F1 0.43211032003116717 on epoch=102
05/31/2022 03:46:39 - INFO - __main__ - Saving model with best Classification-F1: 0.4210950080515297 -> 0.43211032003116717 on epoch=102, global_step=2450
05/31/2022 03:46:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.33 on epoch=102
05/31/2022 03:46:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.32 on epoch=102
05/31/2022 03:46:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=103
05/31/2022 03:46:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=103
05/31/2022 03:46:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.33 on epoch=104
05/31/2022 03:47:03 - INFO - __main__ - Global step 2500 Train loss 0.32 Classification-F1 0.4445249239951227 on epoch=104
05/31/2022 03:47:03 - INFO - __main__ - Saving model with best Classification-F1: 0.43211032003116717 -> 0.4445249239951227 on epoch=104, global_step=2500
05/31/2022 03:47:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=104
05/31/2022 03:47:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=104
05/31/2022 03:47:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.29 on epoch=105
05/31/2022 03:47:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.30 on epoch=105
05/31/2022 03:47:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=106
05/31/2022 03:47:27 - INFO - __main__ - Global step 2550 Train loss 0.31 Classification-F1 0.43779084900329107 on epoch=106
05/31/2022 03:47:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.31 on epoch=106
05/31/2022 03:47:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=107
05/31/2022 03:47:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=107
05/31/2022 03:47:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.26 on epoch=107
05/31/2022 03:47:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=108
05/31/2022 03:47:51 - INFO - __main__ - Global step 2600 Train loss 0.30 Classification-F1 0.44184593688755597 on epoch=108
05/31/2022 03:47:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=108
05/31/2022 03:47:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=109
05/31/2022 03:47:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=109
05/31/2022 03:48:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.32 on epoch=109
05/31/2022 03:48:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=110
05/31/2022 03:48:14 - INFO - __main__ - Global step 2650 Train loss 0.31 Classification-F1 0.4323883550502256 on epoch=110
05/31/2022 03:48:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.31 on epoch=110
05/31/2022 03:48:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.25 on epoch=111
05/31/2022 03:48:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=111
05/31/2022 03:48:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.29 on epoch=112
05/31/2022 03:48:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=112
05/31/2022 03:48:38 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.4357244804751456 on epoch=112
05/31/2022 03:48:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=112
05/31/2022 03:48:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.33 on epoch=113
05/31/2022 03:48:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.29 on epoch=113
05/31/2022 03:48:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=114
05/31/2022 03:48:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=114
05/31/2022 03:49:02 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.43071444417950744 on epoch=114
05/31/2022 03:49:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.30 on epoch=114
05/31/2022 03:49:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.29 on epoch=115
05/31/2022 03:49:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=115
05/31/2022 03:49:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.30 on epoch=116
05/31/2022 03:49:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=116
05/31/2022 03:49:26 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.43132367215442785 on epoch=116
05/31/2022 03:49:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.28 on epoch=117
05/31/2022 03:49:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.33 on epoch=117
05/31/2022 03:49:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.30 on epoch=117
05/31/2022 03:49:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.29 on epoch=118
05/31/2022 03:49:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.30 on epoch=118
05/31/2022 03:49:50 - INFO - __main__ - Global step 2850 Train loss 0.30 Classification-F1 0.4249597831620738 on epoch=118
05/31/2022 03:49:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=119
05/31/2022 03:49:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.28 on epoch=119
05/31/2022 03:49:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.27 on epoch=119
05/31/2022 03:50:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=120
05/31/2022 03:50:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=120
05/31/2022 03:50:14 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.4548581282618261 on epoch=120
05/31/2022 03:50:14 - INFO - __main__ - Saving model with best Classification-F1: 0.4445249239951227 -> 0.4548581282618261 on epoch=120, global_step=2900
05/31/2022 03:50:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=121
05/31/2022 03:50:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=121
05/31/2022 03:50:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.27 on epoch=122
05/31/2022 03:50:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=122
05/31/2022 03:50:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.28 on epoch=122
05/31/2022 03:50:38 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.46354610640324917 on epoch=122
05/31/2022 03:50:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4548581282618261 -> 0.46354610640324917 on epoch=122, global_step=2950
05/31/2022 03:50:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.28 on epoch=123
05/31/2022 03:50:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.25 on epoch=123
05/31/2022 03:50:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.26 on epoch=124
05/31/2022 03:50:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=124
05/31/2022 03:50:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.27 on epoch=124
05/31/2022 03:50:53 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:50:53 - INFO - __main__ - Printing 3 examples
05/31/2022 03:50:53 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:50:53 - INFO - __main__ - ['entailment']
05/31/2022 03:50:53 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:50:53 - INFO - __main__ - ['entailment']
05/31/2022 03:50:53 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:50:53 - INFO - __main__ - ['entailment']
05/31/2022 03:50:53 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:50:53 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:50:54 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:50:54 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:50:54 - INFO - __main__ - Printing 3 examples
05/31/2022 03:50:54 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:50:54 - INFO - __main__ - ['entailment']
05/31/2022 03:50:54 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:50:54 - INFO - __main__ - ['entailment']
05/31/2022 03:50:54 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:50:54 - INFO - __main__ - ['entailment']
05/31/2022 03:50:54 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:50:54 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:50:54 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:51:03 - INFO - __main__ - Global step 3000 Train loss 0.27 Classification-F1 0.4965114250828537 on epoch=124
05/31/2022 03:51:03 - INFO - __main__ - Saving model with best Classification-F1: 0.46354610640324917 -> 0.4965114250828537 on epoch=124, global_step=3000
05/31/2022 03:51:03 - INFO - __main__ - save last model!
05/31/2022 03:51:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 03:51:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 03:51:03 - INFO - __main__ - Printing 3 examples
05/31/2022 03:51:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 03:51:03 - INFO - __main__ - ['contradiction']
05/31/2022 03:51:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 03:51:03 - INFO - __main__ - ['entailment']
05/31/2022 03:51:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 03:51:03 - INFO - __main__ - ['contradiction']
05/31/2022 03:51:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:51:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:51:04 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 03:51:11 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:51:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:51:12 - INFO - __main__ - Starting training!
05/31/2022 03:51:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_21_0.3_8_predictions.txt
05/31/2022 03:51:34 - INFO - __main__ - Classification-F1 on test data: 0.3109
05/31/2022 03:51:34 - INFO - __main__ - prefix=anli_128_21, lr=0.3, bsz=8, dev_performance=0.4965114250828537, test_performance=0.3108705074310507
05/31/2022 03:51:34 - INFO - __main__ - Running ... prefix=anli_128_21, lr=0.2, bsz=8 ...
05/31/2022 03:51:35 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:51:35 - INFO - __main__ - Printing 3 examples
05/31/2022 03:51:35 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/31/2022 03:51:35 - INFO - __main__ - ['entailment']
05/31/2022 03:51:35 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/31/2022 03:51:35 - INFO - __main__ - ['entailment']
05/31/2022 03:51:35 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/31/2022 03:51:35 - INFO - __main__ - ['entailment']
05/31/2022 03:51:35 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:51:35 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:51:36 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 03:51:36 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 03:51:36 - INFO - __main__ - Printing 3 examples
05/31/2022 03:51:36 - INFO - __main__ -  [anli] premise: Terenzo is a "comune" (municipality) in the Province of Parma in the Italian region Emilia-Romagna, located about 100 km west of Bologna and about 30 km southwest of Parma. As of 31 December 2004, it had a population of 1,250 and an area of 72.4 km2 . [SEP] hypothesis: Terenzo has a small population.
05/31/2022 03:51:36 - INFO - __main__ - ['entailment']
05/31/2022 03:51:36 - INFO - __main__ -  [anli] premise: Brett McLaughlin, known professionally as Leland, is an American singer, songwriter, record producer, composer and lecturer. Based in Los Angeles, California, he has worked closely with a range of popular artists, including Troye Sivan, Daya, Capital Cities, Andy Grammer, Hilary Duff and Allie X. [SEP] hypothesis: Leland has worked with at least 6 popular artists.
05/31/2022 03:51:36 - INFO - __main__ - ['entailment']
05/31/2022 03:51:36 - INFO - __main__ -  [anli] premise: Amazon Fire TV refers to two digital media players and microconsoles developed by Amazon.com. It is a small network appliance and entertainment device designed to stream digital audio/video content to a high-definition television. The device also allows users to play video games with the included remote, via a mobile app, or with an optional game controller. [SEP] hypothesis: Amazon Fire TV refers to the digital media players and micro consoles developed by amazon.com
05/31/2022 03:51:36 - INFO - __main__ - ['entailment']
05/31/2022 03:51:36 - INFO - __main__ - Tokenizing Input ...
05/31/2022 03:51:36 - INFO - __main__ - Tokenizing Output ...
05/31/2022 03:51:36 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 03:51:54 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 03:51:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 03:51:55 - INFO - __main__ - Starting training!
05/31/2022 03:51:59 - INFO - __main__ - Step 10 Global step 10 Train loss 0.60 on epoch=0
05/31/2022 03:52:01 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=0
05/31/2022 03:52:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=1
05/31/2022 03:52:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=1
05/31/2022 03:52:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=2
05/31/2022 03:52:20 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 03:52:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 03:52:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=2
05/31/2022 03:52:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=2
05/31/2022 03:52:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=3
05/31/2022 03:52:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=3
05/31/2022 03:52:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=4
05/31/2022 03:52:45 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 03:52:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=4
05/31/2022 03:52:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=4
05/31/2022 03:52:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=5
05/31/2022 03:52:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=5
05/31/2022 03:52:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=6
05/31/2022 03:53:09 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 03:53:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=6
05/31/2022 03:53:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=7
05/31/2022 03:53:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=7
05/31/2022 03:53:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=7
05/31/2022 03:53:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=8
05/31/2022 03:53:33 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 03:53:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=8
05/31/2022 03:53:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=9
05/31/2022 03:53:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=9
05/31/2022 03:53:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=9
05/31/2022 03:53:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=10
05/31/2022 03:53:56 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 03:53:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=10
05/31/2022 03:54:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=11
05/31/2022 03:54:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=11
05/31/2022 03:54:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=12
05/31/2022 03:54:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=12
05/31/2022 03:54:19 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 03:54:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=12
05/31/2022 03:54:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=13
05/31/2022 03:54:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=13
05/31/2022 03:54:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=14
05/31/2022 03:54:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=14
05/31/2022 03:54:42 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 03:54:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=14
05/31/2022 03:54:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=15
05/31/2022 03:54:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=15
05/31/2022 03:54:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=16
05/31/2022 03:54:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=16
05/31/2022 03:55:05 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 03:55:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=17
05/31/2022 03:55:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=17
05/31/2022 03:55:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=17
05/31/2022 03:55:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=18
05/31/2022 03:55:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=18
05/31/2022 03:55:28 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 03:55:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=19
05/31/2022 03:55:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=19
05/31/2022 03:55:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=19
05/31/2022 03:55:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=20
05/31/2022 03:55:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=20
05/31/2022 03:55:51 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.1721607831834019 on epoch=20
05/31/2022 03:55:51 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1721607831834019 on epoch=20, global_step=500
05/31/2022 03:55:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=21
05/31/2022 03:55:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=21
05/31/2022 03:55:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=22
05/31/2022 03:56:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=22
05/31/2022 03:56:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/31/2022 03:56:15 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=22
05/31/2022 03:56:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/31/2022 03:56:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/31/2022 03:56:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/31/2022 03:56:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=24
05/31/2022 03:56:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/31/2022 03:56:38 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.1721607831834019 on epoch=24
05/31/2022 03:56:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=25
05/31/2022 03:56:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=25
05/31/2022 03:56:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 03:56:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=26
05/31/2022 03:56:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.54 on epoch=27
05/31/2022 03:57:02 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.20362589480236537 on epoch=27
05/31/2022 03:57:02 - INFO - __main__ - Saving model with best Classification-F1: 0.1721607831834019 -> 0.20362589480236537 on epoch=27, global_step=650
05/31/2022 03:57:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=27
05/31/2022 03:57:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/31/2022 03:57:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=28
05/31/2022 03:57:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/31/2022 03:57:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=29
05/31/2022 03:57:26 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.22883450914616632 on epoch=29
05/31/2022 03:57:26 - INFO - __main__ - Saving model with best Classification-F1: 0.20362589480236537 -> 0.22883450914616632 on epoch=29, global_step=700
05/31/2022 03:57:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=29
05/31/2022 03:57:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=29
05/31/2022 03:57:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=30
05/31/2022 03:57:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=30
05/31/2022 03:57:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=31
05/31/2022 03:57:51 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.22301814029951159 on epoch=31
05/31/2022 03:57:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=31
05/31/2022 03:57:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/31/2022 03:57:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 03:58:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=32
05/31/2022 03:58:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=33
05/31/2022 03:58:15 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.26004401597637994 on epoch=33
05/31/2022 03:58:15 - INFO - __main__ - Saving model with best Classification-F1: 0.22883450914616632 -> 0.26004401597637994 on epoch=33, global_step=800
05/31/2022 03:58:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/31/2022 03:58:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=34
05/31/2022 03:58:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=34
05/31/2022 03:58:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=34
05/31/2022 03:58:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=35
05/31/2022 03:58:39 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.2085278555866791 on epoch=35
05/31/2022 03:58:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/31/2022 03:58:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=36
05/31/2022 03:58:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=36
05/31/2022 03:58:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=37
05/31/2022 03:58:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=37
05/31/2022 03:59:03 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.2558834550580091 on epoch=37
05/31/2022 03:59:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=37
05/31/2022 03:59:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=38
05/31/2022 03:59:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=38
05/31/2022 03:59:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=39
05/31/2022 03:59:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=39
05/31/2022 03:59:26 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.2740713899066357 on epoch=39
05/31/2022 03:59:26 - INFO - __main__ - Saving model with best Classification-F1: 0.26004401597637994 -> 0.2740713899066357 on epoch=39, global_step=950
05/31/2022 03:59:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 03:59:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=40
05/31/2022 03:59:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=40
05/31/2022 03:59:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=41
05/31/2022 03:59:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=41
05/31/2022 03:59:49 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.2916666666666667 on epoch=41
05/31/2022 03:59:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2740713899066357 -> 0.2916666666666667 on epoch=41, global_step=1000
05/31/2022 03:59:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=42
05/31/2022 03:59:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=42
05/31/2022 03:59:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=42
05/31/2022 04:00:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=43
05/31/2022 04:00:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=43
05/31/2022 04:00:12 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.3039249197165922 on epoch=43
05/31/2022 04:00:12 - INFO - __main__ - Saving model with best Classification-F1: 0.2916666666666667 -> 0.3039249197165922 on epoch=43, global_step=1050
05/31/2022 04:00:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=44
05/31/2022 04:00:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=44
05/31/2022 04:00:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=44
05/31/2022 04:00:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=45
05/31/2022 04:00:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=45
05/31/2022 04:00:35 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.2973898160034232 on epoch=45
05/31/2022 04:00:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=46
05/31/2022 04:00:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=46
05/31/2022 04:00:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=47
05/31/2022 04:00:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=47
05/31/2022 04:00:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=47
05/31/2022 04:00:59 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.3253449527959332 on epoch=47
05/31/2022 04:00:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3039249197165922 -> 0.3253449527959332 on epoch=47, global_step=1150
05/31/2022 04:01:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=48
05/31/2022 04:01:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=48
05/31/2022 04:01:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=49
05/31/2022 04:01:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=49
05/31/2022 04:01:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=49
05/31/2022 04:01:22 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.333345370069452 on epoch=49
05/31/2022 04:01:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3253449527959332 -> 0.333345370069452 on epoch=49, global_step=1200
05/31/2022 04:01:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=50
05/31/2022 04:01:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=50
05/31/2022 04:01:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=51
05/31/2022 04:01:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=51
05/31/2022 04:01:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=52
05/31/2022 04:01:45 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.35416082626887774 on epoch=52
05/31/2022 04:01:45 - INFO - __main__ - Saving model with best Classification-F1: 0.333345370069452 -> 0.35416082626887774 on epoch=52, global_step=1250
05/31/2022 04:01:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=52
05/31/2022 04:01:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=52
05/31/2022 04:01:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=53
05/31/2022 04:01:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=53
05/31/2022 04:01:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=54
05/31/2022 04:02:08 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.3290486564996369 on epoch=54
05/31/2022 04:02:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=54
05/31/2022 04:02:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=54
05/31/2022 04:02:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=55
05/31/2022 04:02:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=55
05/31/2022 04:02:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=56
05/31/2022 04:02:31 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.3509334147851959 on epoch=56
05/31/2022 04:02:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=56
05/31/2022 04:02:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=57
05/31/2022 04:02:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=57
05/31/2022 04:02:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=57
05/31/2022 04:02:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=58
05/31/2022 04:02:55 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.34346055259392055 on epoch=58
05/31/2022 04:02:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=58
05/31/2022 04:03:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=59
05/31/2022 04:03:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=59
05/31/2022 04:03:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=59
05/31/2022 04:03:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=60
05/31/2022 04:03:18 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.34612481766953307 on epoch=60
05/31/2022 04:03:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=60
05/31/2022 04:03:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/31/2022 04:03:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=61
05/31/2022 04:03:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/31/2022 04:03:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=62
05/31/2022 04:03:41 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.3101754385964912 on epoch=62
05/31/2022 04:03:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=62
05/31/2022 04:03:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=63
05/31/2022 04:03:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=63
05/31/2022 04:03:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=64
05/31/2022 04:03:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=64
05/31/2022 04:04:03 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.32959131264216013 on epoch=64
05/31/2022 04:04:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=64
05/31/2022 04:04:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=65
05/31/2022 04:04:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=65
05/31/2022 04:04:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=66
05/31/2022 04:04:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=66
05/31/2022 04:04:26 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.3581320956374208 on epoch=66
05/31/2022 04:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.35416082626887774 -> 0.3581320956374208 on epoch=66, global_step=1600
05/31/2022 04:04:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=67
05/31/2022 04:04:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=67
05/31/2022 04:04:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=67
05/31/2022 04:04:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=68
05/31/2022 04:04:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=68
05/31/2022 04:04:50 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.36009374397532296 on epoch=68
05/31/2022 04:04:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3581320956374208 -> 0.36009374397532296 on epoch=68, global_step=1650
05/31/2022 04:04:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=69
05/31/2022 04:04:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=69
05/31/2022 04:04:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=69
05/31/2022 04:05:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=70
05/31/2022 04:05:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=70
05/31/2022 04:05:12 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.34433002796847195 on epoch=70
05/31/2022 04:05:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=71
05/31/2022 04:05:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=71
05/31/2022 04:05:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=72
05/31/2022 04:05:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.48 on epoch=72
05/31/2022 04:05:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=72
05/31/2022 04:05:36 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.3575421575421575 on epoch=72
05/31/2022 04:05:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=73
05/31/2022 04:05:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=73
05/31/2022 04:05:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=74
05/31/2022 04:05:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=74
05/31/2022 04:05:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=74
05/31/2022 04:05:59 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.35361011753688026 on epoch=74
05/31/2022 04:06:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=75
05/31/2022 04:06:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=75
05/31/2022 04:06:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=76
05/31/2022 04:06:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=76
05/31/2022 04:06:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=77
05/31/2022 04:06:21 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.37186147186147184 on epoch=77
05/31/2022 04:06:21 - INFO - __main__ - Saving model with best Classification-F1: 0.36009374397532296 -> 0.37186147186147184 on epoch=77, global_step=1850
05/31/2022 04:06:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=77
05/31/2022 04:06:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=77
05/31/2022 04:06:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=78
05/31/2022 04:06:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=78
05/31/2022 04:06:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=79
05/31/2022 04:06:44 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.36748185474550416 on epoch=79
05/31/2022 04:06:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=79
05/31/2022 04:06:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=79
05/31/2022 04:06:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=80
05/31/2022 04:06:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=80
05/31/2022 04:06:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=81
05/31/2022 04:07:07 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.37038317487755695 on epoch=81
05/31/2022 04:07:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=81
05/31/2022 04:07:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=82
05/31/2022 04:07:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=82
05/31/2022 04:07:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=82
05/31/2022 04:07:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=83
05/31/2022 04:07:29 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.37303030303030305 on epoch=83
05/31/2022 04:07:30 - INFO - __main__ - Saving model with best Classification-F1: 0.37186147186147184 -> 0.37303030303030305 on epoch=83, global_step=2000
05/31/2022 04:07:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=83
05/31/2022 04:07:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=84
05/31/2022 04:07:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=84
05/31/2022 04:07:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=84
05/31/2022 04:07:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.49 on epoch=85
05/31/2022 04:07:53 - INFO - __main__ - Global step 2050 Train loss 0.39 Classification-F1 0.3677189792083409 on epoch=85
05/31/2022 04:07:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=85
05/31/2022 04:07:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=86
05/31/2022 04:08:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=86
05/31/2022 04:08:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=87
05/31/2022 04:08:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=87
05/31/2022 04:08:15 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.3621233145597935 on epoch=87
05/31/2022 04:08:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/31/2022 04:08:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=88
05/31/2022 04:08:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=88
05/31/2022 04:08:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=89
05/31/2022 04:08:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=89
05/31/2022 04:08:38 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.39954648329570874 on epoch=89
05/31/2022 04:08:38 - INFO - __main__ - Saving model with best Classification-F1: 0.37303030303030305 -> 0.39954648329570874 on epoch=89, global_step=2150
05/31/2022 04:08:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=89
05/31/2022 04:08:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=90
05/31/2022 04:08:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=90
05/31/2022 04:08:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=91
05/31/2022 04:08:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=91
05/31/2022 04:09:01 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.3858337703101234 on epoch=91
05/31/2022 04:09:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=92
05/31/2022 04:09:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=92
05/31/2022 04:09:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=92
05/31/2022 04:09:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=93
05/31/2022 04:09:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=93
05/31/2022 04:09:24 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.4021583707820433 on epoch=93
05/31/2022 04:09:24 - INFO - __main__ - Saving model with best Classification-F1: 0.39954648329570874 -> 0.4021583707820433 on epoch=93, global_step=2250
05/31/2022 04:09:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=94
05/31/2022 04:09:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=94
05/31/2022 04:09:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=94
05/31/2022 04:09:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=95
05/31/2022 04:09:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=95
05/31/2022 04:09:46 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.39978473343402254 on epoch=95
05/31/2022 04:09:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.34 on epoch=96
05/31/2022 04:09:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=96
05/31/2022 04:09:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=97
05/31/2022 04:09:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=97
05/31/2022 04:10:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=97
05/31/2022 04:10:10 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.39442195231668914 on epoch=97
05/31/2022 04:10:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=98
05/31/2022 04:10:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=98
05/31/2022 04:10:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.39 on epoch=99
05/31/2022 04:10:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.41 on epoch=99
05/31/2022 04:10:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=99
05/31/2022 04:10:32 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.3696299069941045 on epoch=99
05/31/2022 04:10:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.36 on epoch=100
05/31/2022 04:10:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=100
05/31/2022 04:10:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=101
05/31/2022 04:10:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=101
05/31/2022 04:10:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=102
05/31/2022 04:10:56 - INFO - __main__ - Global step 2450 Train loss 0.39 Classification-F1 0.40484221321651576 on epoch=102
05/31/2022 04:10:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4021583707820433 -> 0.40484221321651576 on epoch=102, global_step=2450
05/31/2022 04:10:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=102
05/31/2022 04:11:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=102
05/31/2022 04:11:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=103
05/31/2022 04:11:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=103
05/31/2022 04:11:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=104
05/31/2022 04:11:19 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.3919247788767009 on epoch=104
05/31/2022 04:11:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=104
05/31/2022 04:11:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.36 on epoch=104
05/31/2022 04:11:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=105
05/31/2022 04:11:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=105
05/31/2022 04:11:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.38 on epoch=106
05/31/2022 04:11:42 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.4113168491683858 on epoch=106
05/31/2022 04:11:42 - INFO - __main__ - Saving model with best Classification-F1: 0.40484221321651576 -> 0.4113168491683858 on epoch=106, global_step=2550
05/31/2022 04:11:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=106
05/31/2022 04:11:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=107
05/31/2022 04:11:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=107
05/31/2022 04:11:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=107
05/31/2022 04:11:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=108
05/31/2022 04:12:05 - INFO - __main__ - Global step 2600 Train loss 0.35 Classification-F1 0.423452857662905 on epoch=108
05/31/2022 04:12:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4113168491683858 -> 0.423452857662905 on epoch=108, global_step=2600
05/31/2022 04:12:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=108
05/31/2022 04:12:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=109
05/31/2022 04:12:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=109
05/31/2022 04:12:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=109
05/31/2022 04:12:19 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=110
05/31/2022 04:12:29 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.40002394685167325 on epoch=110
05/31/2022 04:12:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.34 on epoch=110
05/31/2022 04:12:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=111
05/31/2022 04:12:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=111
05/31/2022 04:12:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.41 on epoch=112
05/31/2022 04:12:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=112
05/31/2022 04:12:52 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.3860133573229456 on epoch=112
05/31/2022 04:12:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.35 on epoch=112
05/31/2022 04:12:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.36 on epoch=113
05/31/2022 04:13:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.36 on epoch=113
05/31/2022 04:13:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.36 on epoch=114
05/31/2022 04:13:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=114
05/31/2022 04:13:16 - INFO - __main__ - Global step 2750 Train loss 0.36 Classification-F1 0.4082250012568767 on epoch=114
05/31/2022 04:13:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=114
05/31/2022 04:13:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.34 on epoch=115
05/31/2022 04:13:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.34 on epoch=115
05/31/2022 04:13:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.36 on epoch=116
05/31/2022 04:13:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.37 on epoch=116
05/31/2022 04:13:39 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.41012770100737744 on epoch=116
05/31/2022 04:13:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=117
05/31/2022 04:13:44 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.35 on epoch=117
05/31/2022 04:13:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.32 on epoch=117
05/31/2022 04:13:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=118
05/31/2022 04:13:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=118
05/31/2022 04:14:02 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.4155798724014616 on epoch=118
05/31/2022 04:14:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.37 on epoch=119
05/31/2022 04:14:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=119
05/31/2022 04:14:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.33 on epoch=119
05/31/2022 04:14:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=120
05/31/2022 04:14:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.34 on epoch=120
05/31/2022 04:14:26 - INFO - __main__ - Global step 2900 Train loss 0.34 Classification-F1 0.4230893189919138 on epoch=120
05/31/2022 04:14:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=121
05/31/2022 04:14:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=121
05/31/2022 04:14:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=122
05/31/2022 04:14:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.36 on epoch=122
05/31/2022 04:14:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=122
05/31/2022 04:14:50 - INFO - __main__ - Global step 2950 Train loss 0.35 Classification-F1 0.42320283123202834 on epoch=122
05/31/2022 04:14:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.32 on epoch=123
05/31/2022 04:14:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=123
05/31/2022 04:14:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.34 on epoch=124
05/31/2022 04:15:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=124
05/31/2022 04:15:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.30 on epoch=124
05/31/2022 04:15:04 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:15:04 - INFO - __main__ - Printing 3 examples
05/31/2022 04:15:04 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 04:15:04 - INFO - __main__ - ['neutral']
05/31/2022 04:15:04 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 04:15:04 - INFO - __main__ - ['neutral']
05/31/2022 04:15:04 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 04:15:04 - INFO - __main__ - ['neutral']
05/31/2022 04:15:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:15:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:15:05 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 04:15:05 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:15:05 - INFO - __main__ - Printing 3 examples
05/31/2022 04:15:05 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 04:15:05 - INFO - __main__ - ['neutral']
05/31/2022 04:15:05 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 04:15:05 - INFO - __main__ - ['neutral']
05/31/2022 04:15:05 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 04:15:05 - INFO - __main__ - ['neutral']
05/31/2022 04:15:05 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:15:05 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:15:05 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 04:15:13 - INFO - __main__ - Global step 3000 Train loss 0.34 Classification-F1 0.41838934843629677 on epoch=124
05/31/2022 04:15:13 - INFO - __main__ - save last model!
05/31/2022 04:15:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 04:15:13 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 04:15:13 - INFO - __main__ - Printing 3 examples
05/31/2022 04:15:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 04:15:13 - INFO - __main__ - ['contradiction']
05/31/2022 04:15:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 04:15:13 - INFO - __main__ - ['entailment']
05/31/2022 04:15:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 04:15:13 - INFO - __main__ - ['contradiction']
05/31/2022 04:15:13 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:15:14 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:15:15 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 04:15:22 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 04:15:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 04:15:23 - INFO - __main__ - Starting training!
05/31/2022 04:15:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_21_0.2_8_predictions.txt
05/31/2022 04:15:43 - INFO - __main__ - Classification-F1 on test data: 0.2545
05/31/2022 04:15:43 - INFO - __main__ - prefix=anli_128_21, lr=0.2, bsz=8, dev_performance=0.423452857662905, test_performance=0.25445652047781175
05/31/2022 04:15:43 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.5, bsz=8 ...
05/31/2022 04:15:44 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:15:44 - INFO - __main__ - Printing 3 examples
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:15:44 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:15:44 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 04:15:44 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:15:44 - INFO - __main__ - Printing 3 examples
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 04:15:44 - INFO - __main__ - ['neutral']
05/31/2022 04:15:44 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:15:45 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:15:45 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 04:16:04 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 04:16:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 04:16:05 - INFO - __main__ - Starting training!
05/31/2022 04:16:08 - INFO - __main__ - Step 10 Global step 10 Train loss 0.55 on epoch=0
05/31/2022 04:16:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=0
05/31/2022 04:16:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=1
05/31/2022 04:16:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=1
05/31/2022 04:16:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=2
05/31/2022 04:16:29 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 04:16:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 04:16:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=2
05/31/2022 04:16:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=2
05/31/2022 04:16:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=3
05/31/2022 04:16:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/31/2022 04:16:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=4
05/31/2022 04:16:53 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.17244846656611368 on epoch=4
05/31/2022 04:16:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17244846656611368 on epoch=4, global_step=100
05/31/2022 04:16:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=4
05/31/2022 04:16:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=4
05/31/2022 04:17:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=5
05/31/2022 04:17:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=5
05/31/2022 04:17:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=6
05/31/2022 04:17:18 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 04:17:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=6
05/31/2022 04:17:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=7
05/31/2022 04:17:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=7
05/31/2022 04:17:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/31/2022 04:17:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=8
05/31/2022 04:17:43 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 04:17:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=8
05/31/2022 04:17:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=9
05/31/2022 04:17:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=9
05/31/2022 04:17:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=9
05/31/2022 04:17:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/31/2022 04:18:07 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 04:18:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=10
05/31/2022 04:18:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=11
05/31/2022 04:18:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=11
05/31/2022 04:18:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=12
05/31/2022 04:18:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=12
05/31/2022 04:18:32 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.3034048749506231 on epoch=12
05/31/2022 04:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.17244846656611368 -> 0.3034048749506231 on epoch=12, global_step=300
05/31/2022 04:18:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=12
05/31/2022 04:18:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/31/2022 04:18:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=13
05/31/2022 04:18:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=14
05/31/2022 04:18:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=14
05/31/2022 04:18:57 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.4534106793402293 on epoch=14
05/31/2022 04:18:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3034048749506231 -> 0.4534106793402293 on epoch=14, global_step=350
05/31/2022 04:19:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=14
05/31/2022 04:19:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=15
05/31/2022 04:19:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=15
05/31/2022 04:19:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=16
05/31/2022 04:19:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=16
05/31/2022 04:19:21 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 04:19:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=17
05/31/2022 04:19:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=17
05/31/2022 04:19:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=17
05/31/2022 04:19:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=18
05/31/2022 04:19:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=18
05/31/2022 04:19:46 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.17782710198613258 on epoch=18
05/31/2022 04:19:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=19
05/31/2022 04:19:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=19
05/31/2022 04:19:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=19
05/31/2022 04:19:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/31/2022 04:19:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=20
05/31/2022 04:20:10 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.28992275437562237 on epoch=20
05/31/2022 04:20:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=21
05/31/2022 04:20:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=21
05/31/2022 04:20:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=22
05/31/2022 04:20:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=22
05/31/2022 04:20:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/31/2022 04:20:35 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.27177057322581083 on epoch=22
05/31/2022 04:20:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=23
05/31/2022 04:20:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=23
05/31/2022 04:20:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/31/2022 04:20:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=24
05/31/2022 04:20:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=24
05/31/2022 04:21:00 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.2644832395517722 on epoch=24
05/31/2022 04:21:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=25
05/31/2022 04:21:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=25
05/31/2022 04:21:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=26
05/31/2022 04:21:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=26
05/31/2022 04:21:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=27
05/31/2022 04:21:24 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.2085278555866791 on epoch=27
05/31/2022 04:21:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=27
05/31/2022 04:21:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/31/2022 04:21:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=28
05/31/2022 04:21:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/31/2022 04:21:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=29
05/31/2022 04:21:49 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.35881352987889614 on epoch=29
05/31/2022 04:21:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=29
05/31/2022 04:21:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/31/2022 04:21:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=30
05/31/2022 04:21:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=30
05/31/2022 04:22:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=31
05/31/2022 04:22:13 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.2698085419734904 on epoch=31
05/31/2022 04:22:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=31
05/31/2022 04:22:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/31/2022 04:22:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=32
05/31/2022 04:22:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/31/2022 04:22:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=33
05/31/2022 04:22:38 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.2976971175842611 on epoch=33
05/31/2022 04:22:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=33
05/31/2022 04:22:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=34
05/31/2022 04:22:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=34
05/31/2022 04:22:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=34
05/31/2022 04:22:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=35
05/31/2022 04:23:03 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.319955806872329 on epoch=35
05/31/2022 04:23:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=35
05/31/2022 04:23:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=36
05/31/2022 04:23:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=36
05/31/2022 04:23:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=37
05/31/2022 04:23:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=37
05/31/2022 04:23:27 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3275414331825275 on epoch=37
05/31/2022 04:23:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=37
05/31/2022 04:23:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=38
05/31/2022 04:23:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=38
05/31/2022 04:23:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=39
05/31/2022 04:23:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=39
05/31/2022 04:23:52 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.3728791244987882 on epoch=39
05/31/2022 04:23:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=39
05/31/2022 04:23:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=40
05/31/2022 04:24:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=40
05/31/2022 04:24:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=41
05/31/2022 04:24:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=41
05/31/2022 04:24:16 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3263689514021247 on epoch=41
05/31/2022 04:24:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=42
05/31/2022 04:24:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=42
05/31/2022 04:24:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=42
05/31/2022 04:24:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=43
05/31/2022 04:24:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/31/2022 04:24:41 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.33752642958983303 on epoch=43
05/31/2022 04:24:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=44
05/31/2022 04:24:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=44
05/31/2022 04:24:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/31/2022 04:24:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=45
05/31/2022 04:24:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=45
05/31/2022 04:25:05 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3608261400099426 on epoch=45
05/31/2022 04:25:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=46
05/31/2022 04:25:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=46
05/31/2022 04:25:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=47
05/31/2022 04:25:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=47
05/31/2022 04:25:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=47
05/31/2022 04:25:30 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3958640692535476 on epoch=47
05/31/2022 04:25:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=48
05/31/2022 04:25:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=48
05/31/2022 04:25:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=49
05/31/2022 04:25:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=49
05/31/2022 04:25:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=49
05/31/2022 04:25:54 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.373849765258216 on epoch=49
05/31/2022 04:25:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=50
05/31/2022 04:26:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=50
05/31/2022 04:26:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=51
05/31/2022 04:26:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=51
05/31/2022 04:26:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=52
05/31/2022 04:26:19 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.37353423263206226 on epoch=52
05/31/2022 04:26:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=52
05/31/2022 04:26:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=52
05/31/2022 04:26:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=53
05/31/2022 04:26:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=53
05/31/2022 04:26:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=54
05/31/2022 04:26:44 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.4128836168174482 on epoch=54
05/31/2022 04:26:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=54
05/31/2022 04:26:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=54
05/31/2022 04:26:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=55
05/31/2022 04:26:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=55
05/31/2022 04:26:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=56
05/31/2022 04:27:08 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.37548248960210406 on epoch=56
05/31/2022 04:27:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=56
05/31/2022 04:27:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=57
05/31/2022 04:27:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=57
05/31/2022 04:27:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=57
05/31/2022 04:27:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=58
05/31/2022 04:27:32 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3840015889767134 on epoch=58
05/31/2022 04:27:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=58
05/31/2022 04:27:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=59
05/31/2022 04:27:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=59
05/31/2022 04:27:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=59
05/31/2022 04:27:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=60
05/31/2022 04:27:56 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.38856895048698786 on epoch=60
05/31/2022 04:27:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=60
05/31/2022 04:28:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=61
05/31/2022 04:28:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=61
05/31/2022 04:28:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=62
05/31/2022 04:28:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=62
05/31/2022 04:28:20 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.38223688672463146 on epoch=62
05/31/2022 04:28:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=62
05/31/2022 04:28:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=63
05/31/2022 04:28:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=63
05/31/2022 04:28:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=64
05/31/2022 04:28:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=64
05/31/2022 04:28:43 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.38510225284339455 on epoch=64
05/31/2022 04:28:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 04:28:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=65
05/31/2022 04:28:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=65
05/31/2022 04:28:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=66
05/31/2022 04:28:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=66
05/31/2022 04:29:06 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.3763782276397762 on epoch=66
05/31/2022 04:29:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=67
05/31/2022 04:29:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=67
05/31/2022 04:29:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/31/2022 04:29:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=68
05/31/2022 04:29:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=68
05/31/2022 04:29:30 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.37433303453280403 on epoch=68
05/31/2022 04:29:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=69
05/31/2022 04:29:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=69
05/31/2022 04:29:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=69
05/31/2022 04:29:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=70
05/31/2022 04:29:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=70
05/31/2022 04:29:52 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.3796046912630214 on epoch=70
05/31/2022 04:29:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=71
05/31/2022 04:29:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=71
05/31/2022 04:30:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=72
05/31/2022 04:30:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=72
05/31/2022 04:30:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=72
05/31/2022 04:30:15 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.36987303259025217 on epoch=72
05/31/2022 04:30:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=73
05/31/2022 04:30:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=73
05/31/2022 04:30:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=74
05/31/2022 04:30:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=74
05/31/2022 04:30:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=74
05/31/2022 04:30:38 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.39083353402071097 on epoch=74
05/31/2022 04:30:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=75
05/31/2022 04:30:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=75
05/31/2022 04:30:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=76
05/31/2022 04:30:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=76
05/31/2022 04:30:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=77
05/31/2022 04:31:01 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.37732420685311946 on epoch=77
05/31/2022 04:31:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=77
05/31/2022 04:31:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=77
05/31/2022 04:31:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=78
05/31/2022 04:31:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=78
05/31/2022 04:31:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=79
05/31/2022 04:31:24 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.36909618538924166 on epoch=79
05/31/2022 04:31:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=79
05/31/2022 04:31:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=79
05/31/2022 04:31:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=80
05/31/2022 04:31:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=80
05/31/2022 04:31:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=81
05/31/2022 04:31:46 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.3816806722689076 on epoch=81
05/31/2022 04:31:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=81
05/31/2022 04:31:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=82
05/31/2022 04:31:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=82
05/31/2022 04:31:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=82
05/31/2022 04:31:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=83
05/31/2022 04:32:09 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.371950015514372 on epoch=83
05/31/2022 04:32:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=83
05/31/2022 04:32:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=84
05/31/2022 04:32:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=84
05/31/2022 04:32:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=84
05/31/2022 04:32:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=85
05/31/2022 04:32:32 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.3962546274877454 on epoch=85
05/31/2022 04:32:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=85
05/31/2022 04:32:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.30 on epoch=86
05/31/2022 04:32:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=86
05/31/2022 04:32:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=87
05/31/2022 04:32:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=87
05/31/2022 04:32:55 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.3911140740881332 on epoch=87
05/31/2022 04:32:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=87
05/31/2022 04:33:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=88
05/31/2022 04:33:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=88
05/31/2022 04:33:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=89
05/31/2022 04:33:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=89
05/31/2022 04:33:19 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.39155381777911763 on epoch=89
05/31/2022 04:33:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=89
05/31/2022 04:33:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=90
05/31/2022 04:33:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=90
05/31/2022 04:33:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=91
05/31/2022 04:33:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=91
05/31/2022 04:33:42 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.38563053328593133 on epoch=91
05/31/2022 04:33:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=92
05/31/2022 04:33:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=92
05/31/2022 04:33:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=92
05/31/2022 04:33:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=93
05/31/2022 04:33:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.31 on epoch=93
05/31/2022 04:34:05 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.39263683521235393 on epoch=93
05/31/2022 04:34:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.27 on epoch=94
05/31/2022 04:34:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=94
05/31/2022 04:34:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=94
05/31/2022 04:34:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=95
05/31/2022 04:34:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=95
05/31/2022 04:34:27 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.3936668183952448 on epoch=95
05/31/2022 04:34:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=96
05/31/2022 04:34:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.23 on epoch=96
05/31/2022 04:34:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=97
05/31/2022 04:34:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.22 on epoch=97
05/31/2022 04:34:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=97
05/31/2022 04:34:51 - INFO - __main__ - Global step 2350 Train loss 0.25 Classification-F1 0.3914843703241866 on epoch=97
05/31/2022 04:34:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.27 on epoch=98
05/31/2022 04:34:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=98
05/31/2022 04:34:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.33 on epoch=99
05/31/2022 04:35:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=99
05/31/2022 04:35:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=99
05/31/2022 04:35:14 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.3930851993857514 on epoch=99
05/31/2022 04:35:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=100
05/31/2022 04:35:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=100
05/31/2022 04:35:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=101
05/31/2022 04:35:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=101
05/31/2022 04:35:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=102
05/31/2022 04:35:38 - INFO - __main__ - Global step 2450 Train loss 0.26 Classification-F1 0.42984197230060756 on epoch=102
05/31/2022 04:35:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=102
05/31/2022 04:35:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=102
05/31/2022 04:35:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=103
05/31/2022 04:35:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=103
05/31/2022 04:35:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=104
05/31/2022 04:36:01 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.395848989085704 on epoch=104
05/31/2022 04:36:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=104
05/31/2022 04:36:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=104
05/31/2022 04:36:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.25 on epoch=105
05/31/2022 04:36:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=105
05/31/2022 04:36:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=106
05/31/2022 04:36:24 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.3994026824124665 on epoch=106
05/31/2022 04:36:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=106
05/31/2022 04:36:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=107
05/31/2022 04:36:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=107
05/31/2022 04:36:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.29 on epoch=107
05/31/2022 04:36:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=108
05/31/2022 04:36:47 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.3973212023827619 on epoch=108
05/31/2022 04:36:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=108
05/31/2022 04:36:53 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.23 on epoch=109
05/31/2022 04:36:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=109
05/31/2022 04:36:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=109
05/31/2022 04:37:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=110
05/31/2022 04:37:11 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.4264455181637516 on epoch=110
05/31/2022 04:37:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=110
05/31/2022 04:37:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=111
05/31/2022 04:37:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=111
05/31/2022 04:37:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.29 on epoch=112
05/31/2022 04:37:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=112
05/31/2022 04:37:35 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.3430856844839324 on epoch=112
05/31/2022 04:37:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.24 on epoch=112
05/31/2022 04:37:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=113
05/31/2022 04:37:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.23 on epoch=113
05/31/2022 04:37:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.23 on epoch=114
05/31/2022 04:37:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=114
05/31/2022 04:37:58 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.3258592880482332 on epoch=114
05/31/2022 04:38:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=114
05/31/2022 04:38:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=115
05/31/2022 04:38:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.27 on epoch=115
05/31/2022 04:38:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=116
05/31/2022 04:38:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=116
05/31/2022 04:38:22 - INFO - __main__ - Global step 2800 Train loss 0.24 Classification-F1 0.4093983390693303 on epoch=116
05/31/2022 04:38:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=117
05/31/2022 04:38:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=117
05/31/2022 04:38:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=117
05/31/2022 04:38:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=118
05/31/2022 04:38:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=118
05/31/2022 04:38:45 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.40895545334048006 on epoch=118
05/31/2022 04:38:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=119
05/31/2022 04:38:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=119
05/31/2022 04:38:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=119
05/31/2022 04:38:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=120
05/31/2022 04:38:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=120
05/31/2022 04:39:09 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.4566621818693087 on epoch=120
05/31/2022 04:39:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4534106793402293 -> 0.4566621818693087 on epoch=120, global_step=2900
05/31/2022 04:39:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=121
05/31/2022 04:39:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=121
05/31/2022 04:39:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.27 on epoch=122
05/31/2022 04:39:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=122
05/31/2022 04:39:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=122
05/31/2022 04:39:34 - INFO - __main__ - Global step 2950 Train loss 0.22 Classification-F1 0.39279332733992606 on epoch=122
05/31/2022 04:39:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=123
05/31/2022 04:39:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=123
05/31/2022 04:39:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=124
05/31/2022 04:39:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=124
05/31/2022 04:39:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=124
05/31/2022 04:39:48 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:39:48 - INFO - __main__ - Printing 3 examples
05/31/2022 04:39:48 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 04:39:48 - INFO - __main__ - ['neutral']
05/31/2022 04:39:48 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 04:39:48 - INFO - __main__ - ['neutral']
05/31/2022 04:39:48 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 04:39:48 - INFO - __main__ - ['neutral']
05/31/2022 04:39:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:39:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:39:49 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 04:39:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:39:49 - INFO - __main__ - Printing 3 examples
05/31/2022 04:39:49 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 04:39:49 - INFO - __main__ - ['neutral']
05/31/2022 04:39:49 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 04:39:49 - INFO - __main__ - ['neutral']
05/31/2022 04:39:49 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 04:39:49 - INFO - __main__ - ['neutral']
05/31/2022 04:39:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:39:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:39:50 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 04:39:58 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.4055514103072797 on epoch=124
05/31/2022 04:39:58 - INFO - __main__ - save last model!
05/31/2022 04:39:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 04:39:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 04:39:58 - INFO - __main__ - Printing 3 examples
05/31/2022 04:39:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 04:39:58 - INFO - __main__ - ['contradiction']
05/31/2022 04:39:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 04:39:58 - INFO - __main__ - ['entailment']
05/31/2022 04:39:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 04:39:58 - INFO - __main__ - ['contradiction']
05/31/2022 04:39:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:39:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:39:59 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 04:40:06 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 04:40:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 04:40:06 - INFO - __main__ - Starting training!
05/31/2022 04:40:27 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_42_0.5_8_predictions.txt
05/31/2022 04:40:27 - INFO - __main__ - Classification-F1 on test data: 0.2543
05/31/2022 04:40:27 - INFO - __main__ - prefix=anli_128_42, lr=0.5, bsz=8, dev_performance=0.4566621818693087, test_performance=0.25431522455543976
05/31/2022 04:40:27 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.4, bsz=8 ...
05/31/2022 04:40:28 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:40:28 - INFO - __main__ - Printing 3 examples
05/31/2022 04:40:28 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 04:40:28 - INFO - __main__ - ['neutral']
05/31/2022 04:40:28 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 04:40:28 - INFO - __main__ - ['neutral']
05/31/2022 04:40:28 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 04:40:28 - INFO - __main__ - ['neutral']
05/31/2022 04:40:28 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:40:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:40:29 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 04:40:29 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 04:40:29 - INFO - __main__ - Printing 3 examples
05/31/2022 04:40:29 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 04:40:29 - INFO - __main__ - ['neutral']
05/31/2022 04:40:29 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 04:40:29 - INFO - __main__ - ['neutral']
05/31/2022 04:40:29 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 04:40:29 - INFO - __main__ - ['neutral']
05/31/2022 04:40:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 04:40:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 04:40:30 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 04:40:46 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 04:40:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 04:40:47 - INFO - __main__ - Starting training!
05/31/2022 04:40:50 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=0
05/31/2022 04:40:53 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=0
05/31/2022 04:40:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=1
05/31/2022 04:40:58 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=1
05/31/2022 04:41:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=2
05/31/2022 04:41:11 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 04:41:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 04:41:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=2
05/31/2022 04:41:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=2
05/31/2022 04:41:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=3
05/31/2022 04:41:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/31/2022 04:41:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=4
05/31/2022 04:41:34 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.1872259257074312 on epoch=4
05/31/2022 04:41:34 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1872259257074312 on epoch=4, global_step=100
05/31/2022 04:41:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=4
05/31/2022 04:41:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=4
05/31/2022 04:41:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=5
05/31/2022 04:41:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=5
05/31/2022 04:41:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=6
05/31/2022 04:41:56 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 04:41:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=6
05/31/2022 04:42:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=7
05/31/2022 04:42:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=7
05/31/2022 04:42:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=7
05/31/2022 04:42:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=8
05/31/2022 04:42:19 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 04:42:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=8
05/31/2022 04:42:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=9
05/31/2022 04:42:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=9
05/31/2022 04:42:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=9
05/31/2022 04:42:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=10
05/31/2022 04:42:41 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.17782710198613258 on epoch=10
05/31/2022 04:42:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=10
05/31/2022 04:42:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/31/2022 04:42:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=11
05/31/2022 04:42:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=12
05/31/2022 04:42:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=12
05/31/2022 04:43:05 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.1721607831834019 on epoch=12
05/31/2022 04:43:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=12
05/31/2022 04:43:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=13
05/31/2022 04:43:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=13
05/31/2022 04:43:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=14
05/31/2022 04:43:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=14
05/31/2022 04:43:28 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.18836044746975922 on epoch=14
05/31/2022 04:43:28 - INFO - __main__ - Saving model with best Classification-F1: 0.1872259257074312 -> 0.18836044746975922 on epoch=14, global_step=350
05/31/2022 04:43:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=14
05/31/2022 04:43:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=15
05/31/2022 04:43:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=15
05/31/2022 04:43:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=16
05/31/2022 04:43:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=16
05/31/2022 04:43:52 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 04:43:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=17
05/31/2022 04:43:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=17
05/31/2022 04:44:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=17
05/31/2022 04:44:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=18
05/31/2022 04:44:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=18
05/31/2022 04:44:16 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 04:44:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/31/2022 04:44:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=19
05/31/2022 04:44:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=19
05/31/2022 04:44:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=20
05/31/2022 04:44:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=20
05/31/2022 04:44:39 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.28104247319434267 on epoch=20
05/31/2022 04:44:39 - INFO - __main__ - Saving model with best Classification-F1: 0.18836044746975922 -> 0.28104247319434267 on epoch=20, global_step=500
05/31/2022 04:44:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=21
05/31/2022 04:44:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=21
05/31/2022 04:44:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=22
05/31/2022 04:44:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=22
05/31/2022 04:44:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=22
05/31/2022 04:45:04 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.24988662131519276 on epoch=22
05/31/2022 04:45:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/31/2022 04:45:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=23
05/31/2022 04:45:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=24
05/31/2022 04:45:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/31/2022 04:45:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=24
05/31/2022 04:45:28 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.2683372946373544 on epoch=24
05/31/2022 04:45:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=25
05/31/2022 04:45:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=25
05/31/2022 04:45:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=26
05/31/2022 04:45:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=26
05/31/2022 04:45:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=27
05/31/2022 04:45:52 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.19337297879808002 on epoch=27
05/31/2022 04:45:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=27
05/31/2022 04:45:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=27
05/31/2022 04:45:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=28
05/31/2022 04:46:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=28
05/31/2022 04:46:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=29
05/31/2022 04:46:15 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.24430001885811795 on epoch=29
05/31/2022 04:46:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=29
05/31/2022 04:46:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/31/2022 04:46:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=30
05/31/2022 04:46:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=30
05/31/2022 04:46:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=31
05/31/2022 04:46:38 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.2134443944604877 on epoch=31
05/31/2022 04:46:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=31
05/31/2022 04:46:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/31/2022 04:46:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=32
05/31/2022 04:46:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/31/2022 04:46:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=33
05/31/2022 04:47:01 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.2933247753530167 on epoch=33
05/31/2022 04:47:01 - INFO - __main__ - Saving model with best Classification-F1: 0.28104247319434267 -> 0.2933247753530167 on epoch=33, global_step=800
05/31/2022 04:47:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=33
05/31/2022 04:47:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=34
05/31/2022 04:47:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=34
05/31/2022 04:47:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=34
05/31/2022 04:47:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=35
05/31/2022 04:47:24 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.30279516061743844 on epoch=35
05/31/2022 04:47:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2933247753530167 -> 0.30279516061743844 on epoch=35, global_step=850
05/31/2022 04:47:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=35
05/31/2022 04:47:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=36
05/31/2022 04:47:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=36
05/31/2022 04:47:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=37
05/31/2022 04:47:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=37
05/31/2022 04:47:46 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.292699161901304 on epoch=37
05/31/2022 04:47:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=37
05/31/2022 04:47:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=38
05/31/2022 04:47:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=38
05/31/2022 04:47:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=39
05/31/2022 04:47:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=39
05/31/2022 04:48:08 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3646329277611238 on epoch=39
05/31/2022 04:48:08 - INFO - __main__ - Saving model with best Classification-F1: 0.30279516061743844 -> 0.3646329277611238 on epoch=39, global_step=950
05/31/2022 04:48:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 04:48:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=40
05/31/2022 04:48:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=40
05/31/2022 04:48:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=41
05/31/2022 04:48:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=41
05/31/2022 04:48:31 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.33579666330758035 on epoch=41
05/31/2022 04:48:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=42
05/31/2022 04:48:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=42
05/31/2022 04:48:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=42
05/31/2022 04:48:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=43
05/31/2022 04:48:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=43
05/31/2022 04:48:53 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.3330112721417069 on epoch=43
05/31/2022 04:48:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=44
05/31/2022 04:48:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=44
05/31/2022 04:49:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/31/2022 04:49:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=45
05/31/2022 04:49:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=45
05/31/2022 04:49:15 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3991848716742907 on epoch=45
05/31/2022 04:49:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3646329277611238 -> 0.3991848716742907 on epoch=45, global_step=1100
05/31/2022 04:49:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=46
05/31/2022 04:49:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=46
05/31/2022 04:49:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=47
05/31/2022 04:49:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=47
05/31/2022 04:49:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=47
05/31/2022 04:49:37 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.33899710235040437 on epoch=47
05/31/2022 04:49:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=48
05/31/2022 04:49:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=48
05/31/2022 04:49:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=49
05/31/2022 04:49:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=49
05/31/2022 04:49:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=49
05/31/2022 04:49:59 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3610362208416364 on epoch=49
05/31/2022 04:50:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=50
05/31/2022 04:50:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=50
05/31/2022 04:50:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=51
05/31/2022 04:50:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=51
05/31/2022 04:50:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=52
05/31/2022 04:50:22 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.3375383131454445 on epoch=52
05/31/2022 04:50:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=52
05/31/2022 04:50:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=52
05/31/2022 04:50:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=53
05/31/2022 04:50:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=53
05/31/2022 04:50:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=54
05/31/2022 04:50:45 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.3802650453692114 on epoch=54
05/31/2022 04:50:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=54
05/31/2022 04:50:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=54
05/31/2022 04:50:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=55
05/31/2022 04:50:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=55
05/31/2022 04:50:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=56
05/31/2022 04:51:09 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3470285841609371 on epoch=56
05/31/2022 04:51:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=56
05/31/2022 04:51:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=57
05/31/2022 04:51:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=57
05/31/2022 04:51:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=57
05/31/2022 04:51:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=58
05/31/2022 04:51:31 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.38433279790588387 on epoch=58
05/31/2022 04:51:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=58
05/31/2022 04:51:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=59
05/31/2022 04:51:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=59
05/31/2022 04:51:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=59
05/31/2022 04:51:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.35 on epoch=60
05/31/2022 04:51:54 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.37711864854721994 on epoch=60
05/31/2022 04:51:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=60
05/31/2022 04:51:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=61
05/31/2022 04:52:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=61
05/31/2022 04:52:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=62
05/31/2022 04:52:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=62
05/31/2022 04:52:17 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.36556913094498517 on epoch=62
05/31/2022 04:52:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=62
05/31/2022 04:52:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=63
05/31/2022 04:52:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=63
05/31/2022 04:52:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=64
05/31/2022 04:52:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=64
05/31/2022 04:52:39 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3723917004643507 on epoch=64
05/31/2022 04:52:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=64
05/31/2022 04:52:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=65
05/31/2022 04:52:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=65
05/31/2022 04:52:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=66
05/31/2022 04:52:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=66
05/31/2022 04:53:01 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3880735964601673 on epoch=66
05/31/2022 04:53:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=67
05/31/2022 04:53:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=67
05/31/2022 04:53:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=67
05/31/2022 04:53:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=68
05/31/2022 04:53:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=68
05/31/2022 04:53:23 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.37374796179827624 on epoch=68
05/31/2022 04:53:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=69
05/31/2022 04:53:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=69
05/31/2022 04:53:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=69
05/31/2022 04:53:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=70
05/31/2022 04:53:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=70
05/31/2022 04:53:45 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.3903527351803214 on epoch=70
05/31/2022 04:53:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=71
05/31/2022 04:53:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=71
05/31/2022 04:53:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=72
05/31/2022 04:53:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=72
05/31/2022 04:53:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=72
05/31/2022 04:54:07 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.3896748664416883 on epoch=72
05/31/2022 04:54:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=73
05/31/2022 04:54:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=73
05/31/2022 04:54:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=74
05/31/2022 04:54:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=74
05/31/2022 04:54:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=74
05/31/2022 04:54:30 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.3950845883243294 on epoch=74
05/31/2022 04:54:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=75
05/31/2022 04:54:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=75
05/31/2022 04:54:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=76
05/31/2022 04:54:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=76
05/31/2022 04:54:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=77
05/31/2022 04:54:52 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.3798059577787529 on epoch=77
05/31/2022 04:54:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=77
05/31/2022 04:54:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=77
05/31/2022 04:55:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=78
05/31/2022 04:55:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=78
05/31/2022 04:55:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=79
05/31/2022 04:55:15 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.3911129471211933 on epoch=79
05/31/2022 04:55:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=79
05/31/2022 04:55:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=79
05/31/2022 04:55:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=80
05/31/2022 04:55:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=80
05/31/2022 04:55:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=81
05/31/2022 04:55:38 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3847690074524766 on epoch=81
05/31/2022 04:55:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=81
05/31/2022 04:55:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=82
05/31/2022 04:55:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=82
05/31/2022 04:55:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=82
05/31/2022 04:55:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=83
05/31/2022 04:56:01 - INFO - __main__ - Global step 2000 Train loss 0.29 Classification-F1 0.38915887850467284 on epoch=83
05/31/2022 04:56:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=83
05/31/2022 04:56:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.30 on epoch=84
05/31/2022 04:56:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=84
05/31/2022 04:56:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.33 on epoch=84
05/31/2022 04:56:14 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=85
05/31/2022 04:56:24 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.4137576186591856 on epoch=85
05/31/2022 04:56:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3991848716742907 -> 0.4137576186591856 on epoch=85, global_step=2050
05/31/2022 04:56:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.33 on epoch=85
05/31/2022 04:56:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.31 on epoch=86
05/31/2022 04:56:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=86
05/31/2022 04:56:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=87
05/31/2022 04:56:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=87
05/31/2022 04:56:47 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.4211859027629381 on epoch=87
05/31/2022 04:56:47 - INFO - __main__ - Saving model with best Classification-F1: 0.4137576186591856 -> 0.4211859027629381 on epoch=87, global_step=2100
05/31/2022 04:56:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=87
05/31/2022 04:56:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=88
05/31/2022 04:56:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=88
05/31/2022 04:56:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.29 on epoch=89
05/31/2022 04:57:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=89
05/31/2022 04:57:11 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.4268965998960023 on epoch=89
05/31/2022 04:57:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4211859027629381 -> 0.4268965998960023 on epoch=89, global_step=2150
05/31/2022 04:57:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=89
05/31/2022 04:57:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.33 on epoch=90
05/31/2022 04:57:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=90
05/31/2022 04:57:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=91
05/31/2022 04:57:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=91
05/31/2022 04:57:35 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.4190965395656227 on epoch=91
05/31/2022 04:57:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.35 on epoch=92
05/31/2022 04:57:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=92
05/31/2022 04:57:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.33 on epoch=92
05/31/2022 04:57:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=93
05/31/2022 04:57:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=93
05/31/2022 04:57:58 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.4041569931499674 on epoch=93
05/31/2022 04:58:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=94
05/31/2022 04:58:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=94
05/31/2022 04:58:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=94
05/31/2022 04:58:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=95
05/31/2022 04:58:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=95
05/31/2022 04:58:22 - INFO - __main__ - Global step 2300 Train loss 0.31 Classification-F1 0.4321986208675377 on epoch=95
05/31/2022 04:58:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4268965998960023 -> 0.4321986208675377 on epoch=95, global_step=2300
05/31/2022 04:58:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=96
05/31/2022 04:58:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.32 on epoch=96
05/31/2022 04:58:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=97
05/31/2022 04:58:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=97
05/31/2022 04:58:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.26 on epoch=97
05/31/2022 04:58:46 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.4467247992629697 on epoch=97
05/31/2022 04:58:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4321986208675377 -> 0.4467247992629697 on epoch=97, global_step=2350
05/31/2022 04:58:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.31 on epoch=98
05/31/2022 04:58:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.27 on epoch=98
05/31/2022 04:58:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.29 on epoch=99
05/31/2022 04:58:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=99
05/31/2022 04:59:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=99
05/31/2022 04:59:10 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.4578753710137023 on epoch=99
05/31/2022 04:59:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4467247992629697 -> 0.4578753710137023 on epoch=99, global_step=2400
05/31/2022 04:59:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=100
05/31/2022 04:59:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=100
05/31/2022 04:59:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=101
05/31/2022 04:59:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.26 on epoch=101
05/31/2022 04:59:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.31 on epoch=102
05/31/2022 04:59:34 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.45655623350906466 on epoch=102
05/31/2022 04:59:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.26 on epoch=102
05/31/2022 04:59:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=102
05/31/2022 04:59:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=103
05/31/2022 04:59:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=103
05/31/2022 04:59:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=104
05/31/2022 04:59:57 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.4511377511377512 on epoch=104
05/31/2022 05:00:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=104
05/31/2022 05:00:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.30 on epoch=104
05/31/2022 05:00:06 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=105
05/31/2022 05:00:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.31 on epoch=105
05/31/2022 05:00:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.25 on epoch=106
05/31/2022 05:00:21 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.4313009089296069 on epoch=106
05/31/2022 05:00:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.30 on epoch=106
05/31/2022 05:00:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.31 on epoch=107
05/31/2022 05:00:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=107
05/31/2022 05:00:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=107
05/31/2022 05:00:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.25 on epoch=108
05/31/2022 05:00:45 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.45355017865694913 on epoch=108
05/31/2022 05:00:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.25 on epoch=108
05/31/2022 05:00:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.29 on epoch=109
05/31/2022 05:00:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=109
05/31/2022 05:00:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=109
05/31/2022 05:00:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.29 on epoch=110
05/31/2022 05:01:09 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.4262787973447872 on epoch=110
05/31/2022 05:01:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.28 on epoch=110
05/31/2022 05:01:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=111
05/31/2022 05:01:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=111
05/31/2022 05:01:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.30 on epoch=112
05/31/2022 05:01:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=112
05/31/2022 05:01:33 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.2677393423541394 on epoch=112
05/31/2022 05:01:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=112
05/31/2022 05:01:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.23 on epoch=113
05/31/2022 05:01:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=113
05/31/2022 05:01:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.28 on epoch=114
05/31/2022 05:01:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=114
05/31/2022 05:01:57 - INFO - __main__ - Global step 2750 Train loss 0.23 Classification-F1 0.4593503381382169 on epoch=114
05/31/2022 05:01:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4578753710137023 -> 0.4593503381382169 on epoch=114, global_step=2750
05/31/2022 05:01:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.24 on epoch=114
05/31/2022 05:02:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=115
05/31/2022 05:02:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=115
05/31/2022 05:02:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=116
05/31/2022 05:02:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.26 on epoch=116
05/31/2022 05:02:21 - INFO - __main__ - Global step 2800 Train loss 0.25 Classification-F1 0.4482978085717812 on epoch=116
05/31/2022 05:02:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.33 on epoch=117
05/31/2022 05:02:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=117
05/31/2022 05:02:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=117
05/31/2022 05:02:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=118
05/31/2022 05:02:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=118
05/31/2022 05:02:44 - INFO - __main__ - Global step 2850 Train loss 0.23 Classification-F1 0.4260458170905932 on epoch=118
05/31/2022 05:02:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=119
05/31/2022 05:02:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=119
05/31/2022 05:02:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=119
05/31/2022 05:02:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.22 on epoch=120
05/31/2022 05:02:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.25 on epoch=120
05/31/2022 05:03:08 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.34411571946172737 on epoch=120
05/31/2022 05:03:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=121
05/31/2022 05:03:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.24 on epoch=121
05/31/2022 05:03:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.25 on epoch=122
05/31/2022 05:03:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=122
05/31/2022 05:03:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=122
05/31/2022 05:03:32 - INFO - __main__ - Global step 2950 Train loss 0.24 Classification-F1 0.3309329064509787 on epoch=122
05/31/2022 05:03:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=123
05/31/2022 05:03:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.30 on epoch=123
05/31/2022 05:03:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.27 on epoch=124
05/31/2022 05:03:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=124
05/31/2022 05:03:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=124
05/31/2022 05:03:47 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:03:47 - INFO - __main__ - Printing 3 examples
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:03:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:03:47 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:03:47 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:03:47 - INFO - __main__ - Printing 3 examples
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 05:03:47 - INFO - __main__ - ['neutral']
05/31/2022 05:03:47 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:03:47 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:03:48 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:03:56 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.3454594489700531 on epoch=124
05/31/2022 05:03:56 - INFO - __main__ - save last model!
05/31/2022 05:03:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 05:03:56 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 05:03:56 - INFO - __main__ - Printing 3 examples
05/31/2022 05:03:56 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 05:03:56 - INFO - __main__ - ['contradiction']
05/31/2022 05:03:56 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 05:03:56 - INFO - __main__ - ['entailment']
05/31/2022 05:03:56 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 05:03:56 - INFO - __main__ - ['contradiction']
05/31/2022 05:03:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:03:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:03:58 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 05:04:06 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:04:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:04:06 - INFO - __main__ - Starting training!
05/31/2022 05:04:26 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_42_0.4_8_predictions.txt
05/31/2022 05:04:26 - INFO - __main__ - Classification-F1 on test data: 0.1336
05/31/2022 05:04:26 - INFO - __main__ - prefix=anli_128_42, lr=0.4, bsz=8, dev_performance=0.4593503381382169, test_performance=0.1336241249167828
05/31/2022 05:04:26 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.3, bsz=8 ...
05/31/2022 05:04:27 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:04:27 - INFO - __main__ - Printing 3 examples
05/31/2022 05:04:27 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 05:04:27 - INFO - __main__ - ['neutral']
05/31/2022 05:04:27 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 05:04:27 - INFO - __main__ - ['neutral']
05/31/2022 05:04:27 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 05:04:27 - INFO - __main__ - ['neutral']
05/31/2022 05:04:27 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:04:27 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:04:28 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:04:28 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:04:28 - INFO - __main__ - Printing 3 examples
05/31/2022 05:04:28 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 05:04:28 - INFO - __main__ - ['neutral']
05/31/2022 05:04:28 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 05:04:28 - INFO - __main__ - ['neutral']
05/31/2022 05:04:28 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 05:04:28 - INFO - __main__ - ['neutral']
05/31/2022 05:04:28 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:04:28 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:04:28 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:04:44 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:04:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:04:45 - INFO - __main__ - Starting training!
05/31/2022 05:04:49 - INFO - __main__ - Step 10 Global step 10 Train loss 0.54 on epoch=0
05/31/2022 05:04:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=0
05/31/2022 05:04:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=1
05/31/2022 05:04:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=1
05/31/2022 05:05:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=2
05/31/2022 05:05:10 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 05:05:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 05:05:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=2
05/31/2022 05:05:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=2
05/31/2022 05:05:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=3
05/31/2022 05:05:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=3
05/31/2022 05:05:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=4
05/31/2022 05:05:33 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.18313051777618705 on epoch=4
05/31/2022 05:05:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18313051777618705 on epoch=4, global_step=100
05/31/2022 05:05:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=4
05/31/2022 05:05:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=4
05/31/2022 05:05:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=5
05/31/2022 05:05:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=5
05/31/2022 05:05:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=6
05/31/2022 05:05:55 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 05:05:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=6
05/31/2022 05:06:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=7
05/31/2022 05:06:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=7
05/31/2022 05:06:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=7
05/31/2022 05:06:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=8
05/31/2022 05:06:19 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 05:06:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=8
05/31/2022 05:06:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=9
05/31/2022 05:06:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=9
05/31/2022 05:06:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/31/2022 05:06:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/31/2022 05:06:44 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.17244846656611368 on epoch=10
05/31/2022 05:06:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=10
05/31/2022 05:06:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=11
05/31/2022 05:06:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=11
05/31/2022 05:06:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=12
05/31/2022 05:06:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=12
05/31/2022 05:07:08 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.17277719006599165 on epoch=12
05/31/2022 05:07:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=12
05/31/2022 05:07:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=13
05/31/2022 05:07:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=13
05/31/2022 05:07:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=14
05/31/2022 05:07:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=14
05/31/2022 05:07:32 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.19351857314219414 on epoch=14
05/31/2022 05:07:32 - INFO - __main__ - Saving model with best Classification-F1: 0.18313051777618705 -> 0.19351857314219414 on epoch=14, global_step=350
05/31/2022 05:07:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=14
05/31/2022 05:07:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/31/2022 05:07:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=15
05/31/2022 05:07:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=16
05/31/2022 05:07:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=16
05/31/2022 05:07:55 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16699282452707112 on epoch=16
05/31/2022 05:07:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=17
05/31/2022 05:08:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=17
05/31/2022 05:08:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=17
05/31/2022 05:08:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=18
05/31/2022 05:08:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 05:08:19 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=18
05/31/2022 05:08:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=19
05/31/2022 05:08:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=19
05/31/2022 05:08:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=19
05/31/2022 05:08:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=20
05/31/2022 05:08:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=20
05/31/2022 05:08:43 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.26181377441003684 on epoch=20
05/31/2022 05:08:43 - INFO - __main__ - Saving model with best Classification-F1: 0.19351857314219414 -> 0.26181377441003684 on epoch=20, global_step=500
05/31/2022 05:08:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=21
05/31/2022 05:08:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=21
05/31/2022 05:08:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=22
05/31/2022 05:08:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=22
05/31/2022 05:08:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=22
05/31/2022 05:09:08 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.20866072987602582 on epoch=22
05/31/2022 05:09:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/31/2022 05:09:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=23
05/31/2022 05:09:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=24
05/31/2022 05:09:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/31/2022 05:09:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=24
05/31/2022 05:09:32 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.18981282050236745 on epoch=24
05/31/2022 05:09:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=25
05/31/2022 05:09:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=25
05/31/2022 05:09:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=26
05/31/2022 05:09:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=26
05/31/2022 05:09:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=27
05/31/2022 05:09:56 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.1896988993358147 on epoch=27
05/31/2022 05:09:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=27
05/31/2022 05:10:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=27
05/31/2022 05:10:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=28
05/31/2022 05:10:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=28
05/31/2022 05:10:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=29
05/31/2022 05:10:20 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.22803487592219984 on epoch=29
05/31/2022 05:10:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=29
05/31/2022 05:10:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=29
05/31/2022 05:10:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=30
05/31/2022 05:10:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=30
05/31/2022 05:10:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=31
05/31/2022 05:10:44 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.2038800705467372 on epoch=31
05/31/2022 05:10:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=31
05/31/2022 05:10:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/31/2022 05:10:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 05:10:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=32
05/31/2022 05:10:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=33
05/31/2022 05:11:08 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.2530018192844148 on epoch=33
05/31/2022 05:11:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=33
05/31/2022 05:11:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=34
05/31/2022 05:11:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=34
05/31/2022 05:11:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=34
05/31/2022 05:11:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=35
05/31/2022 05:11:32 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.2088644602970413 on epoch=35
05/31/2022 05:11:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=35
05/31/2022 05:11:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=36
05/31/2022 05:11:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=36
05/31/2022 05:11:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=37
05/31/2022 05:11:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=37
05/31/2022 05:11:57 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.2372053872053872 on epoch=37
05/31/2022 05:11:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=37
05/31/2022 05:12:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=38
05/31/2022 05:12:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=38
05/31/2022 05:12:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=39
05/31/2022 05:12:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=39
05/31/2022 05:12:21 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.28608464512362614 on epoch=39
05/31/2022 05:12:21 - INFO - __main__ - Saving model with best Classification-F1: 0.26181377441003684 -> 0.28608464512362614 on epoch=39, global_step=950
05/31/2022 05:12:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=39
05/31/2022 05:12:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=40
05/31/2022 05:12:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=40
05/31/2022 05:12:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=41
05/31/2022 05:12:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=41
05/31/2022 05:12:45 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.26688370265266576 on epoch=41
05/31/2022 05:12:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/31/2022 05:12:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=42
05/31/2022 05:12:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=42
05/31/2022 05:12:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=43
05/31/2022 05:12:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=43
05/31/2022 05:13:09 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.29615797429448787 on epoch=43
05/31/2022 05:13:09 - INFO - __main__ - Saving model with best Classification-F1: 0.28608464512362614 -> 0.29615797429448787 on epoch=43, global_step=1050
05/31/2022 05:13:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=44
05/31/2022 05:13:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=44
05/31/2022 05:13:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=44
05/31/2022 05:13:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=45
05/31/2022 05:13:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=45
05/31/2022 05:13:33 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.3294215563878485 on epoch=45
05/31/2022 05:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.29615797429448787 -> 0.3294215563878485 on epoch=45, global_step=1100
05/31/2022 05:13:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=46
05/31/2022 05:13:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=46
05/31/2022 05:13:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=47
05/31/2022 05:13:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=47
05/31/2022 05:13:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=47
05/31/2022 05:13:58 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.3429925221480276 on epoch=47
05/31/2022 05:13:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3294215563878485 -> 0.3429925221480276 on epoch=47, global_step=1150
05/31/2022 05:14:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=48
05/31/2022 05:14:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=48
05/31/2022 05:14:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/31/2022 05:14:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=49
05/31/2022 05:14:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=49
05/31/2022 05:14:22 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.33172847136000017 on epoch=49
05/31/2022 05:14:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=50
05/31/2022 05:14:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=50
05/31/2022 05:14:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=51
05/31/2022 05:14:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=51
05/31/2022 05:14:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=52
05/31/2022 05:14:45 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.3061465180731602 on epoch=52
05/31/2022 05:14:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=52
05/31/2022 05:14:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=52
05/31/2022 05:14:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=53
05/31/2022 05:14:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=53
05/31/2022 05:14:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=54
05/31/2022 05:15:08 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3282498184458969 on epoch=54
05/31/2022 05:15:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=54
05/31/2022 05:15:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=54
05/31/2022 05:15:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=55
05/31/2022 05:15:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=55
05/31/2022 05:15:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=56
05/31/2022 05:15:32 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3109975748316782 on epoch=56
05/31/2022 05:15:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=56
05/31/2022 05:15:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=57
05/31/2022 05:15:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=57
05/31/2022 05:15:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=57
05/31/2022 05:15:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=58
05/31/2022 05:15:56 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.34540879907876265 on epoch=58
05/31/2022 05:15:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3429925221480276 -> 0.34540879907876265 on epoch=58, global_step=1400
05/31/2022 05:15:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=58
05/31/2022 05:16:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=59
05/31/2022 05:16:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=59
05/31/2022 05:16:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=59
05/31/2022 05:16:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=60
05/31/2022 05:16:19 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.348667149889875 on epoch=60
05/31/2022 05:16:19 - INFO - __main__ - Saving model with best Classification-F1: 0.34540879907876265 -> 0.348667149889875 on epoch=60, global_step=1450
05/31/2022 05:16:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=60
05/31/2022 05:16:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=61
05/31/2022 05:16:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=61
05/31/2022 05:16:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/31/2022 05:16:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=62
05/31/2022 05:16:42 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.35266183741520063 on epoch=62
05/31/2022 05:16:42 - INFO - __main__ - Saving model with best Classification-F1: 0.348667149889875 -> 0.35266183741520063 on epoch=62, global_step=1500
05/31/2022 05:16:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=62
05/31/2022 05:16:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=63
05/31/2022 05:16:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=63
05/31/2022 05:16:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=64
05/31/2022 05:16:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=64
05/31/2022 05:17:06 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.36659509073302177 on epoch=64
05/31/2022 05:17:06 - INFO - __main__ - Saving model with best Classification-F1: 0.35266183741520063 -> 0.36659509073302177 on epoch=64, global_step=1550
05/31/2022 05:17:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=64
05/31/2022 05:17:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=65
05/31/2022 05:17:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=65
05/31/2022 05:17:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=66
05/31/2022 05:17:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=66
05/31/2022 05:17:29 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3575557840937423 on epoch=66
05/31/2022 05:17:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=67
05/31/2022 05:17:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=67
05/31/2022 05:17:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/31/2022 05:17:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=68
05/31/2022 05:17:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=68
05/31/2022 05:17:52 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.34186585030809685 on epoch=68
05/31/2022 05:17:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=69
05/31/2022 05:17:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=69
05/31/2022 05:18:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=69
05/31/2022 05:18:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=70
05/31/2022 05:18:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=70
05/31/2022 05:18:16 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.4140504242783755 on epoch=70
05/31/2022 05:18:16 - INFO - __main__ - Saving model with best Classification-F1: 0.36659509073302177 -> 0.4140504242783755 on epoch=70, global_step=1700
05/31/2022 05:18:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=71
05/31/2022 05:18:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=71
05/31/2022 05:18:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=72
05/31/2022 05:18:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=72
05/31/2022 05:18:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=72
05/31/2022 05:18:41 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.4079330628707816 on epoch=72
05/31/2022 05:18:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=73
05/31/2022 05:18:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=73
05/31/2022 05:18:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=74
05/31/2022 05:18:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=74
05/31/2022 05:18:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=74
05/31/2022 05:19:05 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.44753470143717405 on epoch=74
05/31/2022 05:19:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4140504242783755 -> 0.44753470143717405 on epoch=74, global_step=1800
05/31/2022 05:19:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=75
05/31/2022 05:19:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=75
05/31/2022 05:19:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=76
05/31/2022 05:19:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=76
05/31/2022 05:19:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=77
05/31/2022 05:19:29 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.35353149241013293 on epoch=77
05/31/2022 05:19:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=77
05/31/2022 05:19:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=77
05/31/2022 05:19:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=78
05/31/2022 05:19:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=78
05/31/2022 05:19:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=79
05/31/2022 05:19:52 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3896616040500933 on epoch=79
05/31/2022 05:19:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=79
05/31/2022 05:19:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=79
05/31/2022 05:20:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=80
05/31/2022 05:20:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=80
05/31/2022 05:20:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=81
05/31/2022 05:20:16 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.35868077067223486 on epoch=81
05/31/2022 05:20:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=81
05/31/2022 05:20:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=82
05/31/2022 05:20:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=82
05/31/2022 05:20:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=82
05/31/2022 05:20:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=83
05/31/2022 05:20:38 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.39690235252009237 on epoch=83
05/31/2022 05:20:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=83
05/31/2022 05:20:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=84
05/31/2022 05:20:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=84
05/31/2022 05:20:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=84
05/31/2022 05:20:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.32 on epoch=85
05/31/2022 05:21:00 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.37402190923317685 on epoch=85
05/31/2022 05:21:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=85
05/31/2022 05:21:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=86
05/31/2022 05:21:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=86
05/31/2022 05:21:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=87
05/31/2022 05:21:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=87
05/31/2022 05:21:23 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.39212325098030815 on epoch=87
05/31/2022 05:21:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=87
05/31/2022 05:21:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=88
05/31/2022 05:21:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.37 on epoch=88
05/31/2022 05:21:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=89
05/31/2022 05:21:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=89
05/31/2022 05:21:46 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.39476207693831555 on epoch=89
05/31/2022 05:21:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=89
05/31/2022 05:21:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.28 on epoch=90
05/31/2022 05:21:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=90
05/31/2022 05:21:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=91
05/31/2022 05:21:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=91
05/31/2022 05:22:08 - INFO - __main__ - Global step 2200 Train loss 0.34 Classification-F1 0.39013617960986385 on epoch=91
05/31/2022 05:22:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=92
05/31/2022 05:22:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=92
05/31/2022 05:22:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=92
05/31/2022 05:22:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=93
05/31/2022 05:22:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.26 on epoch=93
05/31/2022 05:22:29 - INFO - __main__ - Global step 2250 Train loss 0.32 Classification-F1 0.391025641025641 on epoch=93
05/31/2022 05:22:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.35 on epoch=94
05/31/2022 05:22:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.28 on epoch=94
05/31/2022 05:22:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=94
05/31/2022 05:22:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.33 on epoch=95
05/31/2022 05:22:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=95
05/31/2022 05:22:52 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.3928997687324911 on epoch=95
05/31/2022 05:22:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.33 on epoch=96
05/31/2022 05:22:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=96
05/31/2022 05:23:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=97
05/31/2022 05:23:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.28 on epoch=97
05/31/2022 05:23:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=97
05/31/2022 05:23:14 - INFO - __main__ - Global step 2350 Train loss 0.34 Classification-F1 0.3880843661665579 on epoch=97
05/31/2022 05:23:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=98
05/31/2022 05:23:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=98
05/31/2022 05:23:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.34 on epoch=99
05/31/2022 05:23:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.31 on epoch=99
05/31/2022 05:23:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.28 on epoch=99
05/31/2022 05:23:36 - INFO - __main__ - Global step 2400 Train loss 0.30 Classification-F1 0.3748289606758284 on epoch=99
05/31/2022 05:23:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.34 on epoch=100
05/31/2022 05:23:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.28 on epoch=100
05/31/2022 05:23:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=101
05/31/2022 05:23:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=101
05/31/2022 05:23:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=102
05/31/2022 05:23:59 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.38241187655404413 on epoch=102
05/31/2022 05:24:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.26 on epoch=102
05/31/2022 05:24:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.36 on epoch=102
05/31/2022 05:24:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=103
05/31/2022 05:24:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.28 on epoch=103
05/31/2022 05:24:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.31 on epoch=104
05/31/2022 05:24:22 - INFO - __main__ - Global step 2500 Train loss 0.31 Classification-F1 0.38294712472972675 on epoch=104
05/31/2022 05:24:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.30 on epoch=104
05/31/2022 05:24:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=104
05/31/2022 05:24:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=105
05/31/2022 05:24:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=105
05/31/2022 05:24:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.30 on epoch=106
05/31/2022 05:24:44 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.3704705464114377 on epoch=106
05/31/2022 05:24:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.35 on epoch=106
05/31/2022 05:24:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.32 on epoch=107
05/31/2022 05:24:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.29 on epoch=107
05/31/2022 05:24:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=107
05/31/2022 05:24:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.27 on epoch=108
05/31/2022 05:25:07 - INFO - __main__ - Global step 2600 Train loss 0.32 Classification-F1 0.37666130369070255 on epoch=108
05/31/2022 05:25:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=108
05/31/2022 05:25:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.31 on epoch=109
05/31/2022 05:25:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=109
05/31/2022 05:25:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.32 on epoch=109
05/31/2022 05:25:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=110
05/31/2022 05:25:29 - INFO - __main__ - Global step 2650 Train loss 0.32 Classification-F1 0.3777953285653694 on epoch=110
05/31/2022 05:25:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.30 on epoch=110
05/31/2022 05:25:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.27 on epoch=111
05/31/2022 05:25:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.27 on epoch=111
05/31/2022 05:25:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=112
05/31/2022 05:25:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=112
05/31/2022 05:25:51 - INFO - __main__ - Global step 2700 Train loss 0.28 Classification-F1 0.381388697251926 on epoch=112
05/31/2022 05:25:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=112
05/31/2022 05:25:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.32 on epoch=113
05/31/2022 05:25:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=113
05/31/2022 05:26:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=114
05/31/2022 05:26:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=114
05/31/2022 05:26:13 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.4006558581793349 on epoch=114
05/31/2022 05:26:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.32 on epoch=114
05/31/2022 05:26:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=115
05/31/2022 05:26:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=115
05/31/2022 05:26:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.26 on epoch=116
05/31/2022 05:26:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.28 on epoch=116
05/31/2022 05:26:35 - INFO - __main__ - Global step 2800 Train loss 0.29 Classification-F1 0.39620741735190473 on epoch=116
05/31/2022 05:26:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=117
05/31/2022 05:26:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=117
05/31/2022 05:26:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.29 on epoch=117
05/31/2022 05:26:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.24 on epoch=118
05/31/2022 05:26:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.24 on epoch=118
05/31/2022 05:26:57 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.3782080157080157 on epoch=118
05/31/2022 05:27:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.29 on epoch=119
05/31/2022 05:27:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.27 on epoch=119
05/31/2022 05:27:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=119
05/31/2022 05:27:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.28 on epoch=120
05/31/2022 05:27:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.30 on epoch=120
05/31/2022 05:27:19 - INFO - __main__ - Global step 2900 Train loss 0.28 Classification-F1 0.39644912691444745 on epoch=120
05/31/2022 05:27:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=121
05/31/2022 05:27:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=121
05/31/2022 05:27:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=122
05/31/2022 05:27:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.21 on epoch=122
05/31/2022 05:27:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=122
05/31/2022 05:27:41 - INFO - __main__ - Global step 2950 Train loss 0.27 Classification-F1 0.3963466925231631 on epoch=122
05/31/2022 05:27:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=123
05/31/2022 05:27:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=123
05/31/2022 05:27:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.31 on epoch=124
05/31/2022 05:27:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.29 on epoch=124
05/31/2022 05:27:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.30 on epoch=124
05/31/2022 05:27:56 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:27:56 - INFO - __main__ - Printing 3 examples
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:27:56 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:27:56 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:27:56 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:27:56 - INFO - __main__ - Printing 3 examples
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 05:27:56 - INFO - __main__ - ['neutral']
05/31/2022 05:27:56 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:27:57 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:27:57 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:28:03 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.38860398860398854 on epoch=124
05/31/2022 05:28:03 - INFO - __main__ - save last model!
05/31/2022 05:28:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 05:28:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 05:28:03 - INFO - __main__ - Printing 3 examples
05/31/2022 05:28:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 05:28:03 - INFO - __main__ - ['contradiction']
05/31/2022 05:28:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 05:28:03 - INFO - __main__ - ['entailment']
05/31/2022 05:28:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 05:28:03 - INFO - __main__ - ['contradiction']
05/31/2022 05:28:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:28:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:28:05 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 05:28:13 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:28:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:28:14 - INFO - __main__ - Starting training!
05/31/2022 05:28:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_42_0.3_8_predictions.txt
05/31/2022 05:28:29 - INFO - __main__ - Classification-F1 on test data: 0.3149
05/31/2022 05:28:29 - INFO - __main__ - prefix=anli_128_42, lr=0.3, bsz=8, dev_performance=0.44753470143717405, test_performance=0.314901590241594
05/31/2022 05:28:29 - INFO - __main__ - Running ... prefix=anli_128_42, lr=0.2, bsz=8 ...
05/31/2022 05:28:30 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:28:30 - INFO - __main__ - Printing 3 examples
05/31/2022 05:28:30 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/31/2022 05:28:30 - INFO - __main__ - ['neutral']
05/31/2022 05:28:30 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/31/2022 05:28:30 - INFO - __main__ - ['neutral']
05/31/2022 05:28:30 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/31/2022 05:28:30 - INFO - __main__ - ['neutral']
05/31/2022 05:28:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:28:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:28:31 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:28:31 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:28:31 - INFO - __main__ - Printing 3 examples
05/31/2022 05:28:31 - INFO - __main__ -  [anli] premise: Arlette Roxburgh is a Trinidadian American singer and songwriter. She was born in Trinidad. She is best known for singing The Star-Spangled Banner before every New Jersey Devils home game started. When the Nets were in New Jersey, she also sang the national anthem before their home games at the time as well. [SEP] hypothesis: Arlette Roxburgh has only ever sung for the New Jersey Devils.
05/31/2022 05:28:31 - INFO - __main__ - ['neutral']
05/31/2022 05:28:31 - INFO - __main__ -  [anli] premise: Storm Keating (born 27 October 1981) is an Australian-born fashion designer, brand ambassador, producer-director and blogger, now based in London. Keating has worked on a number of Australian and British television programmes such as "The Apprentice Australia", "Masterchef Australia", "The X Factor", "The Voice Australia", and "The Voice UK". She is the wife of Ronan Keating. [SEP] hypothesis: Storm Keaton learned about fashion while studying abroad in the United States.
05/31/2022 05:28:31 - INFO - __main__ - ['neutral']
05/31/2022 05:28:31 - INFO - __main__ -  [anli] premise: Piton is a Pilsner beer brand from the island of Saint Lucia, brewed by Windward & Leeward Brewing Limited, which is owned by Heineken. The beer was named for the Gros Piton and Petit Piton mountains on the island. It was first brewed on October 7, 1992. [SEP] hypothesis: Heineken plans to stop production of Piton in the near future
05/31/2022 05:28:31 - INFO - __main__ - ['neutral']
05/31/2022 05:28:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:28:31 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:28:31 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:28:49 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:28:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:28:50 - INFO - __main__ - Starting training!
05/31/2022 05:28:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=0
05/31/2022 05:28:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.45 on epoch=0
05/31/2022 05:28:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=1
05/31/2022 05:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=1
05/31/2022 05:29:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=2
05/31/2022 05:29:15 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 05:29:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 05:29:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=2
05/31/2022 05:29:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=2
05/31/2022 05:29:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=3
05/31/2022 05:29:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=3
05/31/2022 05:29:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=4
05/31/2022 05:29:40 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 05:29:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=4
05/31/2022 05:29:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=4
05/31/2022 05:29:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=5
05/31/2022 05:29:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=5
05/31/2022 05:29:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=6
05/31/2022 05:30:03 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 05:30:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=6
05/31/2022 05:30:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.57 on epoch=7
05/31/2022 05:30:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=7
05/31/2022 05:30:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=7
05/31/2022 05:30:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=8
05/31/2022 05:30:27 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 05:30:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=8
05/31/2022 05:30:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=9
05/31/2022 05:30:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=9
05/31/2022 05:30:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=9
05/31/2022 05:30:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=10
05/31/2022 05:30:50 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 05:30:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=10
05/31/2022 05:30:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=11
05/31/2022 05:30:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=11
05/31/2022 05:31:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=12
05/31/2022 05:31:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=12
05/31/2022 05:31:13 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 05:31:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=12
05/31/2022 05:31:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=13
05/31/2022 05:31:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=13
05/31/2022 05:31:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=14
05/31/2022 05:31:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=14
05/31/2022 05:31:37 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.1823630221799558 on epoch=14
05/31/2022 05:31:37 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1823630221799558 on epoch=14, global_step=350
05/31/2022 05:31:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=14
05/31/2022 05:31:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=15
05/31/2022 05:31:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=15
05/31/2022 05:31:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=16
05/31/2022 05:31:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=16
05/31/2022 05:32:00 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 05:32:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=17
05/31/2022 05:32:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=17
05/31/2022 05:32:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=17
05/31/2022 05:32:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=18
05/31/2022 05:32:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=18
05/31/2022 05:32:24 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 05:32:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=19
05/31/2022 05:32:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=19
05/31/2022 05:32:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=19
05/31/2022 05:32:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=20
05/31/2022 05:32:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=20
05/31/2022 05:32:48 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.2330566372890619 on epoch=20
05/31/2022 05:32:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1823630221799558 -> 0.2330566372890619 on epoch=20, global_step=500
05/31/2022 05:32:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=21
05/31/2022 05:32:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=21
05/31/2022 05:32:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=22
05/31/2022 05:32:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=22
05/31/2022 05:33:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=22
05/31/2022 05:33:13 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.24646231387505035 on epoch=22
05/31/2022 05:33:13 - INFO - __main__ - Saving model with best Classification-F1: 0.2330566372890619 -> 0.24646231387505035 on epoch=22, global_step=550
05/31/2022 05:33:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=23
05/31/2022 05:33:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=23
05/31/2022 05:33:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=24
05/31/2022 05:33:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=24
05/31/2022 05:33:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=24
05/31/2022 05:33:37 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.18902702300305219 on epoch=24
05/31/2022 05:33:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=25
05/31/2022 05:33:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=25
05/31/2022 05:33:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=26
05/31/2022 05:33:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=26
05/31/2022 05:33:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=27
05/31/2022 05:34:01 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.17815711967301798 on epoch=27
05/31/2022 05:34:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/31/2022 05:34:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=27
05/31/2022 05:34:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=28
05/31/2022 05:34:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=28
05/31/2022 05:34:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=29
05/31/2022 05:34:26 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.23126461492041941 on epoch=29
05/31/2022 05:34:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=29
05/31/2022 05:34:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=29
05/31/2022 05:34:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=30
05/31/2022 05:34:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=30
05/31/2022 05:34:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=31
05/31/2022 05:34:50 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=31
05/31/2022 05:34:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=31
05/31/2022 05:34:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=32
05/31/2022 05:34:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=32
05/31/2022 05:35:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=32
05/31/2022 05:35:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=33
05/31/2022 05:35:14 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.19337297879808002 on epoch=33
05/31/2022 05:35:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=33
05/31/2022 05:35:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=34
05/31/2022 05:35:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=34
05/31/2022 05:35:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=34
05/31/2022 05:35:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=35
05/31/2022 05:35:38 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.21862773165378377 on epoch=35
05/31/2022 05:35:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=35
05/31/2022 05:35:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=36
05/31/2022 05:35:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=36
05/31/2022 05:35:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=37
05/31/2022 05:35:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=37
05/31/2022 05:36:03 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.21346498065708183 on epoch=37
05/31/2022 05:36:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=37
05/31/2022 05:36:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=38
05/31/2022 05:36:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=38
05/31/2022 05:36:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=39
05/31/2022 05:36:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=39
05/31/2022 05:36:27 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.2443414993809385 on epoch=39
05/31/2022 05:36:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=39
05/31/2022 05:36:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=40
05/31/2022 05:36:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=40
05/31/2022 05:36:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=41
05/31/2022 05:36:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=41
05/31/2022 05:36:52 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.24464907914385262 on epoch=41
05/31/2022 05:36:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.48 on epoch=42
05/31/2022 05:36:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=42
05/31/2022 05:37:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=42
05/31/2022 05:37:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=43
05/31/2022 05:37:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=43
05/31/2022 05:37:16 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.2405798715521401 on epoch=43
05/31/2022 05:37:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=44
05/31/2022 05:37:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=44
05/31/2022 05:37:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=44
05/31/2022 05:37:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=45
05/31/2022 05:37:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=45
05/31/2022 05:37:41 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.26495767060672964 on epoch=45
05/31/2022 05:37:41 - INFO - __main__ - Saving model with best Classification-F1: 0.24646231387505035 -> 0.26495767060672964 on epoch=45, global_step=1100
05/31/2022 05:37:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=46
05/31/2022 05:37:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=46
05/31/2022 05:37:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=47
05/31/2022 05:37:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=47
05/31/2022 05:37:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=47
05/31/2022 05:38:06 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.2686175698103479 on epoch=47
05/31/2022 05:38:06 - INFO - __main__ - Saving model with best Classification-F1: 0.26495767060672964 -> 0.2686175698103479 on epoch=47, global_step=1150
05/31/2022 05:38:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=48
05/31/2022 05:38:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=48
05/31/2022 05:38:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=49
05/31/2022 05:38:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=49
05/31/2022 05:38:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=49
05/31/2022 05:38:31 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.2801297925656394 on epoch=49
05/31/2022 05:38:31 - INFO - __main__ - Saving model with best Classification-F1: 0.2686175698103479 -> 0.2801297925656394 on epoch=49, global_step=1200
05/31/2022 05:38:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=50
05/31/2022 05:38:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=50
05/31/2022 05:38:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=51
05/31/2022 05:38:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=51
05/31/2022 05:38:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=52
05/31/2022 05:38:56 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.25314246762099524 on epoch=52
05/31/2022 05:38:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=52
05/31/2022 05:39:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=52
05/31/2022 05:39:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=53
05/31/2022 05:39:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=53
05/31/2022 05:39:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=54
05/31/2022 05:39:20 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.30246409366278676 on epoch=54
05/31/2022 05:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2801297925656394 -> 0.30246409366278676 on epoch=54, global_step=1300
05/31/2022 05:39:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=54
05/31/2022 05:39:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=54
05/31/2022 05:39:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=55
05/31/2022 05:39:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=55
05/31/2022 05:39:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=56
05/31/2022 05:39:45 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.26061983608513867 on epoch=56
05/31/2022 05:39:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=56
05/31/2022 05:39:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=57
05/31/2022 05:39:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=57
05/31/2022 05:39:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=57
05/31/2022 05:39:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=58
05/31/2022 05:40:10 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.2792518863102818 on epoch=58
05/31/2022 05:40:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=58
05/31/2022 05:40:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.54 on epoch=59
05/31/2022 05:40:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=59
05/31/2022 05:40:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=59
05/31/2022 05:40:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.45 on epoch=60
05/31/2022 05:40:34 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.2883263009845289 on epoch=60
05/31/2022 05:40:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=60
05/31/2022 05:40:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/31/2022 05:40:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=61
05/31/2022 05:40:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=62
05/31/2022 05:40:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=62
05/31/2022 05:40:59 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.28502708936809357 on epoch=62
05/31/2022 05:41:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=62
05/31/2022 05:41:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=63
05/31/2022 05:41:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=63
05/31/2022 05:41:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=64
05/31/2022 05:41:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=64
05/31/2022 05:41:23 - INFO - __main__ - Global step 1550 Train loss 0.46 Classification-F1 0.3234815176681322 on epoch=64
05/31/2022 05:41:23 - INFO - __main__ - Saving model with best Classification-F1: 0.30246409366278676 -> 0.3234815176681322 on epoch=64, global_step=1550
05/31/2022 05:41:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=64
05/31/2022 05:41:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=65
05/31/2022 05:41:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=65
05/31/2022 05:41:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=66
05/31/2022 05:41:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=66
05/31/2022 05:41:47 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.31060984130152713 on epoch=66
05/31/2022 05:41:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=67
05/31/2022 05:41:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=67
05/31/2022 05:41:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=67
05/31/2022 05:41:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=68
05/31/2022 05:42:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=68
05/31/2022 05:42:11 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.30578794504395507 on epoch=68
05/31/2022 05:42:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=69
05/31/2022 05:42:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=69
05/31/2022 05:42:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=69
05/31/2022 05:42:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=70
05/31/2022 05:42:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=70
05/31/2022 05:42:35 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.3263447079236553 on epoch=70
05/31/2022 05:42:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3234815176681322 -> 0.3263447079236553 on epoch=70, global_step=1700
05/31/2022 05:42:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=71
05/31/2022 05:42:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=71
05/31/2022 05:42:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=72
05/31/2022 05:42:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=72
05/31/2022 05:42:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=72
05/31/2022 05:42:59 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.34261201126032165 on epoch=72
05/31/2022 05:42:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3263447079236553 -> 0.34261201126032165 on epoch=72, global_step=1750
05/31/2022 05:43:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=73
05/31/2022 05:43:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=73
05/31/2022 05:43:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=74
05/31/2022 05:43:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=74
05/31/2022 05:43:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=74
05/31/2022 05:43:22 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.30904396024947106 on epoch=74
05/31/2022 05:43:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=75
05/31/2022 05:43:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=75
05/31/2022 05:43:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=76
05/31/2022 05:43:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=76
05/31/2022 05:43:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=77
05/31/2022 05:43:45 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3114942528735632 on epoch=77
05/31/2022 05:43:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=77
05/31/2022 05:43:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=77
05/31/2022 05:43:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=78
05/31/2022 05:43:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=78
05/31/2022 05:43:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=79
05/31/2022 05:44:07 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.32496154157558094 on epoch=79
05/31/2022 05:44:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=79
05/31/2022 05:44:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=79
05/31/2022 05:44:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=80
05/31/2022 05:44:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=80
05/31/2022 05:44:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=81
05/31/2022 05:44:29 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3153273148833309 on epoch=81
05/31/2022 05:44:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=81
05/31/2022 05:44:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=82
05/31/2022 05:44:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=82
05/31/2022 05:44:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=82
05/31/2022 05:44:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=83
05/31/2022 05:44:51 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.3137215927913602 on epoch=83
05/31/2022 05:44:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=83
05/31/2022 05:44:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=84
05/31/2022 05:44:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=84
05/31/2022 05:45:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=84
05/31/2022 05:45:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=85
05/31/2022 05:45:13 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.3391752986466643 on epoch=85
05/31/2022 05:45:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=85
05/31/2022 05:45:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=86
05/31/2022 05:45:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.38 on epoch=86
05/31/2022 05:45:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=87
05/31/2022 05:45:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=87
05/31/2022 05:45:36 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.3257775606578733 on epoch=87
05/31/2022 05:45:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=87
05/31/2022 05:45:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=88
05/31/2022 05:45:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=88
05/31/2022 05:45:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=89
05/31/2022 05:45:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=89
05/31/2022 05:45:58 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.3667487006969514 on epoch=89
05/31/2022 05:45:58 - INFO - __main__ - Saving model with best Classification-F1: 0.34261201126032165 -> 0.3667487006969514 on epoch=89, global_step=2150
05/31/2022 05:46:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=89
05/31/2022 05:46:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=90
05/31/2022 05:46:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=90
05/31/2022 05:46:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=91
05/31/2022 05:46:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=91
05/31/2022 05:46:21 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.3391752986466643 on epoch=91
05/31/2022 05:46:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=92
05/31/2022 05:46:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.39 on epoch=92
05/31/2022 05:46:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=92
05/31/2022 05:46:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=93
05/31/2022 05:46:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=93
05/31/2022 05:46:43 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.31227103955278807 on epoch=93
05/31/2022 05:46:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=94
05/31/2022 05:46:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.31 on epoch=94
05/31/2022 05:46:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=94
05/31/2022 05:46:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=95
05/31/2022 05:46:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=95
05/31/2022 05:47:05 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.36591047061462073 on epoch=95
05/31/2022 05:47:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=96
05/31/2022 05:47:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=96
05/31/2022 05:47:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=97
05/31/2022 05:47:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.31 on epoch=97
05/31/2022 05:47:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=97
05/31/2022 05:47:27 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.34277890225026786 on epoch=97
05/31/2022 05:47:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.34 on epoch=98
05/31/2022 05:47:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=98
05/31/2022 05:47:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=99
05/31/2022 05:47:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=99
05/31/2022 05:47:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=99
05/31/2022 05:47:49 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.35401709401709397 on epoch=99
05/31/2022 05:47:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=100
05/31/2022 05:47:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=100
05/31/2022 05:47:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=101
05/31/2022 05:47:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.39 on epoch=101
05/31/2022 05:48:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=102
05/31/2022 05:48:11 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.35594884688396133 on epoch=102
05/31/2022 05:48:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.32 on epoch=102
05/31/2022 05:48:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=102
05/31/2022 05:48:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=103
05/31/2022 05:48:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=103
05/31/2022 05:48:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=104
05/31/2022 05:48:34 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.352132073422396 on epoch=104
05/31/2022 05:48:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=104
05/31/2022 05:48:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=104
05/31/2022 05:48:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=105
05/31/2022 05:48:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=105
05/31/2022 05:48:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.31 on epoch=106
05/31/2022 05:48:56 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.3468341494831561 on epoch=106
05/31/2022 05:48:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.35 on epoch=106
05/31/2022 05:49:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=107
05/31/2022 05:49:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.38 on epoch=107
05/31/2022 05:49:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=107
05/31/2022 05:49:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.36 on epoch=108
05/31/2022 05:49:19 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.3764838443722614 on epoch=108
05/31/2022 05:49:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3667487006969514 -> 0.3764838443722614 on epoch=108, global_step=2600
05/31/2022 05:49:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=108
05/31/2022 05:49:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=109
05/31/2022 05:49:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.34 on epoch=109
05/31/2022 05:49:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.37 on epoch=109
05/31/2022 05:49:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=110
05/31/2022 05:49:42 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.36737759594902447 on epoch=110
05/31/2022 05:49:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=110
05/31/2022 05:49:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.39 on epoch=111
05/31/2022 05:49:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.32 on epoch=111
05/31/2022 05:49:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.35 on epoch=112
05/31/2022 05:49:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.32 on epoch=112
05/31/2022 05:50:05 - INFO - __main__ - Global step 2700 Train loss 0.35 Classification-F1 0.35957064632748387 on epoch=112
05/31/2022 05:50:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.35 on epoch=112
05/31/2022 05:50:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=113
05/31/2022 05:50:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.38 on epoch=113
05/31/2022 05:50:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=114
05/31/2022 05:50:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=114
05/31/2022 05:50:27 - INFO - __main__ - Global step 2750 Train loss 0.36 Classification-F1 0.3593890283529178 on epoch=114
05/31/2022 05:50:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=114
05/31/2022 05:50:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.33 on epoch=115
05/31/2022 05:50:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=115
05/31/2022 05:50:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=116
05/31/2022 05:50:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.31 on epoch=116
05/31/2022 05:50:50 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.36457137040533 on epoch=116
05/31/2022 05:50:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=117
05/31/2022 05:50:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.33 on epoch=117
05/31/2022 05:50:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=117
05/31/2022 05:51:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.35 on epoch=118
05/31/2022 05:51:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.30 on epoch=118
05/31/2022 05:51:13 - INFO - __main__ - Global step 2850 Train loss 0.35 Classification-F1 0.3640617074561329 on epoch=118
05/31/2022 05:51:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=119
05/31/2022 05:51:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=119
05/31/2022 05:51:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=119
05/31/2022 05:51:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=120
05/31/2022 05:51:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=120
05/31/2022 05:51:36 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.3758937843049992 on epoch=120
05/31/2022 05:51:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=121
05/31/2022 05:51:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.34 on epoch=121
05/31/2022 05:51:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=122
05/31/2022 05:51:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.31 on epoch=122
05/31/2022 05:51:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=122
05/31/2022 05:52:00 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.36589112281643094 on epoch=122
05/31/2022 05:52:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.30 on epoch=123
05/31/2022 05:52:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.34 on epoch=123
05/31/2022 05:52:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=124
05/31/2022 05:52:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.32 on epoch=124
05/31/2022 05:52:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=124
05/31/2022 05:52:15 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:52:15 - INFO - __main__ - Printing 3 examples
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:52:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:52:15 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:52:15 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:52:15 - INFO - __main__ - Printing 3 examples
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 05:52:15 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:15 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:52:15 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:52:16 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:52:24 - INFO - __main__ - Global step 3000 Train loss 0.34 Classification-F1 0.37015580643031626 on epoch=124
05/31/2022 05:52:24 - INFO - __main__ - save last model!
05/31/2022 05:52:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 05:52:24 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 05:52:24 - INFO - __main__ - Printing 3 examples
05/31/2022 05:52:24 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 05:52:24 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:24 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 05:52:24 - INFO - __main__ - ['entailment']
05/31/2022 05:52:24 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 05:52:24 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:24 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:52:24 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:52:25 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 05:52:32 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:52:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:52:33 - INFO - __main__ - Starting training!
05/31/2022 05:52:52 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_42_0.2_8_predictions.txt
05/31/2022 05:52:52 - INFO - __main__ - Classification-F1 on test data: 0.2734
05/31/2022 05:52:53 - INFO - __main__ - prefix=anli_128_42, lr=0.2, bsz=8, dev_performance=0.3764838443722614, test_performance=0.27338443984514993
05/31/2022 05:52:53 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.5, bsz=8 ...
05/31/2022 05:52:54 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:52:54 - INFO - __main__ - Printing 3 examples
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:52:54 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:52:54 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 05:52:54 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 05:52:54 - INFO - __main__ - Printing 3 examples
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 05:52:54 - INFO - __main__ - ['contradiction']
05/31/2022 05:52:54 - INFO - __main__ - Tokenizing Input ...
05/31/2022 05:52:55 - INFO - __main__ - Tokenizing Output ...
05/31/2022 05:52:55 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 05:53:12 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 05:53:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 05:53:13 - INFO - __main__ - Starting training!
05/31/2022 05:53:16 - INFO - __main__ - Step 10 Global step 10 Train loss 0.54 on epoch=0
05/31/2022 05:53:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=0
05/31/2022 05:53:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
05/31/2022 05:53:24 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=1
05/31/2022 05:53:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=2
05/31/2022 05:53:38 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 05:53:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 05:53:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=2
05/31/2022 05:53:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=2
05/31/2022 05:53:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=3
05/31/2022 05:53:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=3
05/31/2022 05:53:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=4
05/31/2022 05:54:01 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 05:54:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=4
05/31/2022 05:54:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=4
05/31/2022 05:54:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=5
05/31/2022 05:54:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=5
05/31/2022 05:54:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=6
05/31/2022 05:54:26 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 05:54:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=6
05/31/2022 05:54:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=7
05/31/2022 05:54:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=7
05/31/2022 05:54:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=7
05/31/2022 05:54:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=8
05/31/2022 05:54:51 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 05:54:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=8
05/31/2022 05:54:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=9
05/31/2022 05:54:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=9
05/31/2022 05:55:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=9
05/31/2022 05:55:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=10
05/31/2022 05:55:16 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.3862023764910027 on epoch=10
05/31/2022 05:55:16 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.3862023764910027 on epoch=10, global_step=250
05/31/2022 05:55:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=10
05/31/2022 05:55:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=11
05/31/2022 05:55:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=11
05/31/2022 05:55:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=12
05/31/2022 05:55:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=12
05/31/2022 05:55:41 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.2381664172331708 on epoch=12
05/31/2022 05:55:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=12
05/31/2022 05:55:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=13
05/31/2022 05:55:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=13
05/31/2022 05:55:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=14
05/31/2022 05:55:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=14
05/31/2022 05:56:06 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 05:56:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=14
05/31/2022 05:56:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=15
05/31/2022 05:56:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=15
05/31/2022 05:56:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/31/2022 05:56:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=16
05/31/2022 05:56:31 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.2680392724465673 on epoch=16
05/31/2022 05:56:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=17
05/31/2022 05:56:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=17
05/31/2022 05:56:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=17
05/31/2022 05:56:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=18
05/31/2022 05:56:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 05:56:56 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.21025945731828086 on epoch=18
05/31/2022 05:56:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=19
05/31/2022 05:57:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=19
05/31/2022 05:57:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=19
05/31/2022 05:57:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=20
05/31/2022 05:57:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=20
05/31/2022 05:57:19 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.24460698943457562 on epoch=20
05/31/2022 05:57:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=21
05/31/2022 05:57:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=21
05/31/2022 05:57:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=22
05/31/2022 05:57:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=22
05/31/2022 05:57:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=22
05/31/2022 05:57:42 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.2824363565104306 on epoch=22
05/31/2022 05:57:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=23
05/31/2022 05:57:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=23
05/31/2022 05:57:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=24
05/31/2022 05:57:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=24
05/31/2022 05:57:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=24
05/31/2022 05:58:06 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.3745783550735468 on epoch=24
05/31/2022 05:58:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=25
05/31/2022 05:58:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=25
05/31/2022 05:58:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 05:58:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=26
05/31/2022 05:58:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=27
05/31/2022 05:58:29 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.27810735218142624 on epoch=27
05/31/2022 05:58:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=27
05/31/2022 05:58:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=27
05/31/2022 05:58:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=28
05/31/2022 05:58:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=28
05/31/2022 05:58:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=29
05/31/2022 05:58:53 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.3122970913668588 on epoch=29
05/31/2022 05:58:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=29
05/31/2022 05:58:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=29
05/31/2022 05:59:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=30
05/31/2022 05:59:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=30
05/31/2022 05:59:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=31
05/31/2022 05:59:17 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.3458068079752779 on epoch=31
05/31/2022 05:59:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=31
05/31/2022 05:59:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=32
05/31/2022 05:59:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=32
05/31/2022 05:59:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=32
05/31/2022 05:59:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=33
05/31/2022 05:59:40 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.3410456814430325 on epoch=33
05/31/2022 05:59:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=33
05/31/2022 05:59:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=34
05/31/2022 05:59:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=34
05/31/2022 05:59:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=34
05/31/2022 05:59:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=35
05/31/2022 06:00:03 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.40755774080671703 on epoch=35
05/31/2022 06:00:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3862023764910027 -> 0.40755774080671703 on epoch=35, global_step=850
05/31/2022 06:00:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=35
05/31/2022 06:00:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=36
05/31/2022 06:00:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=36
05/31/2022 06:00:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=37
05/31/2022 06:00:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=37
05/31/2022 06:00:26 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.3471345029239766 on epoch=37
05/31/2022 06:00:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=37
05/31/2022 06:00:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=38
05/31/2022 06:00:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=38
05/31/2022 06:00:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=39
05/31/2022 06:00:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=39
05/31/2022 06:00:49 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3770205329838357 on epoch=39
05/31/2022 06:00:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=39
05/31/2022 06:00:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=40
05/31/2022 06:00:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=40
05/31/2022 06:00:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=41
05/31/2022 06:01:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=41
05/31/2022 06:01:12 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.36892333649688913 on epoch=41
05/31/2022 06:01:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=42
05/31/2022 06:01:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=42
05/31/2022 06:01:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=42
05/31/2022 06:01:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=43
05/31/2022 06:01:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=43
05/31/2022 06:01:35 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.36886876427106313 on epoch=43
05/31/2022 06:01:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=44
05/31/2022 06:01:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=44
05/31/2022 06:01:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=44
05/31/2022 06:01:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=45
05/31/2022 06:01:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=45
05/31/2022 06:01:58 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.37003291881734723 on epoch=45
05/31/2022 06:02:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=46
05/31/2022 06:02:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=46
05/31/2022 06:02:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=47
05/31/2022 06:02:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=47
05/31/2022 06:02:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=47
05/31/2022 06:02:20 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.4160626195509916 on epoch=47
05/31/2022 06:02:20 - INFO - __main__ - Saving model with best Classification-F1: 0.40755774080671703 -> 0.4160626195509916 on epoch=47, global_step=1150
05/31/2022 06:02:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=48
05/31/2022 06:02:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=48
05/31/2022 06:02:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=49
05/31/2022 06:02:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=49
05/31/2022 06:02:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=49
05/31/2022 06:02:43 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.411928811928812 on epoch=49
05/31/2022 06:02:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=50
05/31/2022 06:02:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=50
05/31/2022 06:02:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=51
05/31/2022 06:02:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=51
05/31/2022 06:02:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=52
05/31/2022 06:03:06 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3811729060898496 on epoch=52
05/31/2022 06:03:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=52
05/31/2022 06:03:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=52
05/31/2022 06:03:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=53
05/31/2022 06:03:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=53
05/31/2022 06:03:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=54
05/31/2022 06:03:28 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.3880223655531996 on epoch=54
05/31/2022 06:03:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=54
05/31/2022 06:03:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=54
05/31/2022 06:03:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=55
05/31/2022 06:03:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=55
05/31/2022 06:03:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=56
05/31/2022 06:03:50 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.37122679164136235 on epoch=56
05/31/2022 06:03:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=56
05/31/2022 06:03:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=57
05/31/2022 06:03:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=57
05/31/2022 06:04:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=57
05/31/2022 06:04:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=58
05/31/2022 06:04:13 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.37569254185692547 on epoch=58
05/31/2022 06:04:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=58
05/31/2022 06:04:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=59
05/31/2022 06:04:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=59
05/31/2022 06:04:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=59
05/31/2022 06:04:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=60
05/31/2022 06:04:36 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.4243997404282933 on epoch=60
05/31/2022 06:04:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4160626195509916 -> 0.4243997404282933 on epoch=60, global_step=1450
05/31/2022 06:04:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=60
05/31/2022 06:04:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=61
05/31/2022 06:04:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=61
05/31/2022 06:04:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=62
05/31/2022 06:04:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=62
05/31/2022 06:04:59 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.4100186051878971 on epoch=62
05/31/2022 06:05:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=62
05/31/2022 06:05:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=63
05/31/2022 06:05:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=63
05/31/2022 06:05:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=64
05/31/2022 06:05:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=64
05/31/2022 06:05:23 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.425445898704129 on epoch=64
05/31/2022 06:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4243997404282933 -> 0.425445898704129 on epoch=64, global_step=1550
05/31/2022 06:05:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=64
05/31/2022 06:05:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=65
05/31/2022 06:05:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=65
05/31/2022 06:05:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=66
05/31/2022 06:05:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=66
05/31/2022 06:05:46 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.4318498865333759 on epoch=66
05/31/2022 06:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.425445898704129 -> 0.4318498865333759 on epoch=66, global_step=1600
05/31/2022 06:05:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=67
05/31/2022 06:05:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=67
05/31/2022 06:05:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=67
05/31/2022 06:05:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=68
05/31/2022 06:06:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=68
05/31/2022 06:06:11 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.42215105686854226 on epoch=68
05/31/2022 06:06:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=69
05/31/2022 06:06:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=69
05/31/2022 06:06:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=69
05/31/2022 06:06:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=70
05/31/2022 06:06:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=70
05/31/2022 06:06:35 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.4128560139716185 on epoch=70
05/31/2022 06:06:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=71
05/31/2022 06:06:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=71
05/31/2022 06:06:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=72
05/31/2022 06:06:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=72
05/31/2022 06:06:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=72
05/31/2022 06:06:59 - INFO - __main__ - Global step 1750 Train loss 0.26 Classification-F1 0.4232517711178583 on epoch=72
05/31/2022 06:07:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=73
05/31/2022 06:07:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=73
05/31/2022 06:07:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=74
05/31/2022 06:07:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=74
05/31/2022 06:07:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=74
05/31/2022 06:07:22 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.45902930572247386 on epoch=74
05/31/2022 06:07:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4318498865333759 -> 0.45902930572247386 on epoch=74, global_step=1800
05/31/2022 06:07:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.31 on epoch=75
05/31/2022 06:07:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=75
05/31/2022 06:07:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=76
05/31/2022 06:07:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=76
05/31/2022 06:07:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=77
05/31/2022 06:07:46 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.4282024073036568 on epoch=77
05/31/2022 06:07:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=77
05/31/2022 06:07:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=77
05/31/2022 06:07:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=78
05/31/2022 06:07:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=78
05/31/2022 06:07:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=79
05/31/2022 06:08:09 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.44962971568476157 on epoch=79
05/31/2022 06:08:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=79
05/31/2022 06:08:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=79
05/31/2022 06:08:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=80
05/31/2022 06:08:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=80
05/31/2022 06:08:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=81
05/31/2022 06:08:33 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.45939616245619136 on epoch=81
05/31/2022 06:08:33 - INFO - __main__ - Saving model with best Classification-F1: 0.45902930572247386 -> 0.45939616245619136 on epoch=81, global_step=1950
05/31/2022 06:08:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=81
05/31/2022 06:08:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=82
05/31/2022 06:08:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=82
05/31/2022 06:08:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=82
05/31/2022 06:08:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.29 on epoch=83
05/31/2022 06:08:56 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.4480622799397615 on epoch=83
05/31/2022 06:08:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=83
05/31/2022 06:09:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=84
05/31/2022 06:09:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=84
05/31/2022 06:09:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=84
05/31/2022 06:09:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=85
05/31/2022 06:09:20 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.4654789975098009 on epoch=85
05/31/2022 06:09:20 - INFO - __main__ - Saving model with best Classification-F1: 0.45939616245619136 -> 0.4654789975098009 on epoch=85, global_step=2050
05/31/2022 06:09:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=85
05/31/2022 06:09:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=86
05/31/2022 06:09:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=86
05/31/2022 06:09:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=87
05/31/2022 06:09:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=87
05/31/2022 06:09:43 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.4348709267879813 on epoch=87
05/31/2022 06:09:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.28 on epoch=87
05/31/2022 06:09:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=88
05/31/2022 06:09:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=88
05/31/2022 06:09:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=89
05/31/2022 06:09:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=89
05/31/2022 06:10:06 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.4383110457600556 on epoch=89
05/31/2022 06:10:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.28 on epoch=89
05/31/2022 06:10:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=90
05/31/2022 06:10:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=90
05/31/2022 06:10:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=91
05/31/2022 06:10:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=91
05/31/2022 06:10:30 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.43512955015100935 on epoch=91
05/31/2022 06:10:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=92
05/31/2022 06:10:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=92
05/31/2022 06:10:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=92
05/31/2022 06:10:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=93
05/31/2022 06:10:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=93
05/31/2022 06:10:53 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.43156219207234797 on epoch=93
05/31/2022 06:10:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.29 on epoch=94
05/31/2022 06:10:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=94
05/31/2022 06:11:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=94
05/31/2022 06:11:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=95
05/31/2022 06:11:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=95
05/31/2022 06:11:17 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.46084540560735593 on epoch=95
05/31/2022 06:11:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.29 on epoch=96
05/31/2022 06:11:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=96
05/31/2022 06:11:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=97
05/31/2022 06:11:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=97
05/31/2022 06:11:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.26 on epoch=97
05/31/2022 06:11:41 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.44819126689347355 on epoch=97
05/31/2022 06:11:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=98
05/31/2022 06:11:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=98
05/31/2022 06:11:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=99
05/31/2022 06:11:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=99
05/31/2022 06:11:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.27 on epoch=99
05/31/2022 06:12:04 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.4505367987928144 on epoch=99
05/31/2022 06:12:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.27 on epoch=100
05/31/2022 06:12:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=100
05/31/2022 06:12:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=101
05/31/2022 06:12:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=101
05/31/2022 06:12:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=102
05/31/2022 06:12:29 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.48343578792044567 on epoch=102
05/31/2022 06:12:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4654789975098009 -> 0.48343578792044567 on epoch=102, global_step=2450
05/31/2022 06:12:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.29 on epoch=102
05/31/2022 06:12:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=102
05/31/2022 06:12:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.28 on epoch=103
05/31/2022 06:12:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=103
05/31/2022 06:12:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=104
05/31/2022 06:12:53 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.45263849423092345 on epoch=104
05/31/2022 06:12:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=104
05/31/2022 06:12:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=104
05/31/2022 06:13:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=105
05/31/2022 06:13:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=105
05/31/2022 06:13:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.24 on epoch=106
05/31/2022 06:13:17 - INFO - __main__ - Global step 2550 Train loss 0.24 Classification-F1 0.4846836430290395 on epoch=106
05/31/2022 06:13:17 - INFO - __main__ - Saving model with best Classification-F1: 0.48343578792044567 -> 0.4846836430290395 on epoch=106, global_step=2550
05/31/2022 06:13:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=106
05/31/2022 06:13:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.25 on epoch=107
05/31/2022 06:13:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.29 on epoch=107
05/31/2022 06:13:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=107
05/31/2022 06:13:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.25 on epoch=108
05/31/2022 06:13:42 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.5075249363351712 on epoch=108
05/31/2022 06:13:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4846836430290395 -> 0.5075249363351712 on epoch=108, global_step=2600
05/31/2022 06:13:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=108
05/31/2022 06:13:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=109
05/31/2022 06:13:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=109
05/31/2022 06:13:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=109
05/31/2022 06:13:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=110
05/31/2022 06:14:06 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.5262299765302784 on epoch=110
05/31/2022 06:14:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5075249363351712 -> 0.5262299765302784 on epoch=110, global_step=2650
05/31/2022 06:14:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=110
05/31/2022 06:14:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=111
05/31/2022 06:14:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=111
05/31/2022 06:14:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=112
05/31/2022 06:14:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=112
05/31/2022 06:14:31 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.5376155306557319 on epoch=112
05/31/2022 06:14:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5262299765302784 -> 0.5376155306557319 on epoch=112, global_step=2700
05/31/2022 06:14:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=112
05/31/2022 06:14:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=113
05/31/2022 06:14:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=113
05/31/2022 06:14:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=114
05/31/2022 06:14:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=114
05/31/2022 06:14:55 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.5017327058245221 on epoch=114
05/31/2022 06:14:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=114
05/31/2022 06:15:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=115
05/31/2022 06:15:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=115
05/31/2022 06:15:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=116
05/31/2022 06:15:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=116
05/31/2022 06:15:20 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.4874800901923593 on epoch=116
05/31/2022 06:15:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=117
05/31/2022 06:15:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=117
05/31/2022 06:15:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=117
05/31/2022 06:15:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.24 on epoch=118
05/31/2022 06:15:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=118
05/31/2022 06:15:44 - INFO - __main__ - Global step 2850 Train loss 0.21 Classification-F1 0.5408458267391975 on epoch=118
05/31/2022 06:15:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5376155306557319 -> 0.5408458267391975 on epoch=118, global_step=2850
05/31/2022 06:15:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=119
05/31/2022 06:15:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=119
05/31/2022 06:15:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=119
05/31/2022 06:15:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=120
05/31/2022 06:15:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=120
05/31/2022 06:16:09 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.48540022069433836 on epoch=120
05/31/2022 06:16:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=121
05/31/2022 06:16:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=121
05/31/2022 06:16:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=122
05/31/2022 06:16:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.22 on epoch=122
05/31/2022 06:16:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=122
05/31/2022 06:16:33 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.5365215678449551 on epoch=122
05/31/2022 06:16:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=123
05/31/2022 06:16:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=123
05/31/2022 06:16:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=124
05/31/2022 06:16:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=124
05/31/2022 06:16:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.23 on epoch=124
05/31/2022 06:16:48 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:16:48 - INFO - __main__ - Printing 3 examples
05/31/2022 06:16:48 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 06:16:48 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:48 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 06:16:48 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:48 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 06:16:48 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:48 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:16:48 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:16:49 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 06:16:49 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:16:49 - INFO - __main__ - Printing 3 examples
05/31/2022 06:16:49 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 06:16:49 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:49 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 06:16:49 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:49 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 06:16:49 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:49 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:16:49 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:16:49 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 06:16:58 - INFO - __main__ - Global step 3000 Train loss 0.19 Classification-F1 0.5583486207686376 on epoch=124
05/31/2022 06:16:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5408458267391975 -> 0.5583486207686376 on epoch=124, global_step=3000
05/31/2022 06:16:58 - INFO - __main__ - save last model!
05/31/2022 06:16:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 06:16:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 06:16:58 - INFO - __main__ - Printing 3 examples
05/31/2022 06:16:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 06:16:58 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 06:16:58 - INFO - __main__ - ['entailment']
05/31/2022 06:16:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 06:16:58 - INFO - __main__ - ['contradiction']
05/31/2022 06:16:58 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:16:58 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:16:59 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 06:17:07 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 06:17:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 06:17:08 - INFO - __main__ - Starting training!
05/31/2022 06:17:28 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_87_0.5_8_predictions.txt
05/31/2022 06:17:28 - INFO - __main__ - Classification-F1 on test data: 0.3646
05/31/2022 06:17:29 - INFO - __main__ - prefix=anli_128_87, lr=0.5, bsz=8, dev_performance=0.5583486207686376, test_performance=0.3645759962594002
05/31/2022 06:17:29 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.4, bsz=8 ...
05/31/2022 06:17:30 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:17:30 - INFO - __main__ - Printing 3 examples
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:17:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:17:30 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 06:17:30 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:17:30 - INFO - __main__ - Printing 3 examples
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 06:17:30 - INFO - __main__ - ['contradiction']
05/31/2022 06:17:30 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:17:30 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:17:31 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 06:17:47 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 06:17:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 06:17:48 - INFO - __main__ - Starting training!
05/31/2022 06:17:51 - INFO - __main__ - Step 10 Global step 10 Train loss 0.56 on epoch=0
05/31/2022 06:17:54 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=0
05/31/2022 06:17:57 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=1
05/31/2022 06:17:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=1
05/31/2022 06:18:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=2
05/31/2022 06:18:13 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 06:18:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 06:18:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=2
05/31/2022 06:18:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=2
05/31/2022 06:18:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=3
05/31/2022 06:18:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=3
05/31/2022 06:18:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=4
05/31/2022 06:18:35 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 06:18:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=4
05/31/2022 06:18:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=4
05/31/2022 06:18:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=5
05/31/2022 06:18:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=5
05/31/2022 06:18:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=6
05/31/2022 06:18:58 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 06:19:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=6
05/31/2022 06:19:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/31/2022 06:19:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=7
05/31/2022 06:19:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=7
05/31/2022 06:19:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=8
05/31/2022 06:19:22 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 06:19:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=8
05/31/2022 06:19:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=9
05/31/2022 06:19:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=9
05/31/2022 06:19:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=9
05/31/2022 06:19:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=10
05/31/2022 06:19:45 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.26310036209605797 on epoch=10
05/31/2022 06:19:45 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26310036209605797 on epoch=10, global_step=250
05/31/2022 06:19:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=10
05/31/2022 06:19:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=11
05/31/2022 06:19:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=11
05/31/2022 06:19:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=12
05/31/2022 06:19:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=12
05/31/2022 06:20:09 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 06:20:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=12
05/31/2022 06:20:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=13
05/31/2022 06:20:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=13
05/31/2022 06:20:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=14
05/31/2022 06:20:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=14
05/31/2022 06:20:34 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16699282452707112 on epoch=14
05/31/2022 06:20:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=14
05/31/2022 06:20:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=15
05/31/2022 06:20:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=15
05/31/2022 06:20:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=16
05/31/2022 06:20:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=16
05/31/2022 06:20:58 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=16
05/31/2022 06:21:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=17
05/31/2022 06:21:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=17
05/31/2022 06:21:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=17
05/31/2022 06:21:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/31/2022 06:21:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=18
05/31/2022 06:21:22 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16732026143790854 on epoch=18
05/31/2022 06:21:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=19
05/31/2022 06:21:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=19
05/31/2022 06:21:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=19
05/31/2022 06:21:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=20
05/31/2022 06:21:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=20
05/31/2022 06:21:46 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.20354479892433688 on epoch=20
05/31/2022 06:21:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=21
05/31/2022 06:21:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=21
05/31/2022 06:21:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=22
05/31/2022 06:21:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=22
05/31/2022 06:21:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=22
05/31/2022 06:22:09 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=22
05/31/2022 06:22:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=23
05/31/2022 06:22:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=23
05/31/2022 06:22:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=24
05/31/2022 06:22:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=24
05/31/2022 06:22:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=24
05/31/2022 06:22:33 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.32369177298689084 on epoch=24
05/31/2022 06:22:33 - INFO - __main__ - Saving model with best Classification-F1: 0.26310036209605797 -> 0.32369177298689084 on epoch=24, global_step=600
05/31/2022 06:22:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=25
05/31/2022 06:22:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=25
05/31/2022 06:22:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=26
05/31/2022 06:22:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=26
05/31/2022 06:22:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=27
05/31/2022 06:22:56 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.27810735218142624 on epoch=27
05/31/2022 06:22:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=27
05/31/2022 06:23:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=27
05/31/2022 06:23:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=28
05/31/2022 06:23:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=28
05/31/2022 06:23:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=29
05/31/2022 06:23:18 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.25802278857268673 on epoch=29
05/31/2022 06:23:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=29
05/31/2022 06:23:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=29
05/31/2022 06:23:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=30
05/31/2022 06:23:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=30
05/31/2022 06:23:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=31
05/31/2022 06:23:41 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.35407535051564637 on epoch=31
05/31/2022 06:23:41 - INFO - __main__ - Saving model with best Classification-F1: 0.32369177298689084 -> 0.35407535051564637 on epoch=31, global_step=750
05/31/2022 06:23:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=31
05/31/2022 06:23:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=32
05/31/2022 06:23:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 06:23:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=32
05/31/2022 06:23:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=33
05/31/2022 06:24:04 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.3051016494054469 on epoch=33
05/31/2022 06:24:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=33
05/31/2022 06:24:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=34
05/31/2022 06:24:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=34
05/31/2022 06:24:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=34
05/31/2022 06:24:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=35
05/31/2022 06:24:28 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.36615997170853865 on epoch=35
05/31/2022 06:24:28 - INFO - __main__ - Saving model with best Classification-F1: 0.35407535051564637 -> 0.36615997170853865 on epoch=35, global_step=850
05/31/2022 06:24:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=35
05/31/2022 06:24:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=36
05/31/2022 06:24:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=36
05/31/2022 06:24:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=37
05/31/2022 06:24:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=37
05/31/2022 06:24:51 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3753505144372638 on epoch=37
05/31/2022 06:24:51 - INFO - __main__ - Saving model with best Classification-F1: 0.36615997170853865 -> 0.3753505144372638 on epoch=37, global_step=900
05/31/2022 06:24:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=37
05/31/2022 06:24:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=38
05/31/2022 06:24:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=38
05/31/2022 06:25:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=39
05/31/2022 06:25:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=39
05/31/2022 06:25:14 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.34894540412144975 on epoch=39
05/31/2022 06:25:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 06:25:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=40
05/31/2022 06:25:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=40
05/31/2022 06:25:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=41
05/31/2022 06:25:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=41
05/31/2022 06:25:37 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3766977469631249 on epoch=41
05/31/2022 06:25:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3753505144372638 -> 0.3766977469631249 on epoch=41, global_step=1000
05/31/2022 06:25:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=42
05/31/2022 06:25:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=42
05/31/2022 06:25:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=42
05/31/2022 06:25:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=43
05/31/2022 06:25:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=43
05/31/2022 06:26:00 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.3245608918246397 on epoch=43
05/31/2022 06:26:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=44
05/31/2022 06:26:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=44
05/31/2022 06:26:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=44
05/31/2022 06:26:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=45
05/31/2022 06:26:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=45
05/31/2022 06:26:24 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.39106582663325934 on epoch=45
05/31/2022 06:26:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3766977469631249 -> 0.39106582663325934 on epoch=45, global_step=1100
05/31/2022 06:26:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=46
05/31/2022 06:26:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=46
05/31/2022 06:26:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=47
05/31/2022 06:26:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=47
05/31/2022 06:26:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=47
05/31/2022 06:26:47 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.39952339955609323 on epoch=47
05/31/2022 06:26:47 - INFO - __main__ - Saving model with best Classification-F1: 0.39106582663325934 -> 0.39952339955609323 on epoch=47, global_step=1150
05/31/2022 06:26:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=48
05/31/2022 06:26:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=48
05/31/2022 06:26:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=49
05/31/2022 06:26:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=49
05/31/2022 06:27:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=49
05/31/2022 06:27:11 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.4181236791634298 on epoch=49
05/31/2022 06:27:11 - INFO - __main__ - Saving model with best Classification-F1: 0.39952339955609323 -> 0.4181236791634298 on epoch=49, global_step=1200
05/31/2022 06:27:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=50
05/31/2022 06:27:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=50
05/31/2022 06:27:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=51
05/31/2022 06:27:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=51
05/31/2022 06:27:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=52
05/31/2022 06:27:35 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.3500385856053428 on epoch=52
05/31/2022 06:27:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=52
05/31/2022 06:27:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=52
05/31/2022 06:27:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=53
05/31/2022 06:27:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=53
05/31/2022 06:27:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=54
05/31/2022 06:27:59 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.38992106442383917 on epoch=54
05/31/2022 06:28:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=54
05/31/2022 06:28:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=54
05/31/2022 06:28:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=55
05/31/2022 06:28:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=55
05/31/2022 06:28:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=56
05/31/2022 06:28:23 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.3821002917023202 on epoch=56
05/31/2022 06:28:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=56
05/31/2022 06:28:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=57
05/31/2022 06:28:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=57
05/31/2022 06:28:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=57
05/31/2022 06:28:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=58
05/31/2022 06:28:47 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.40042416966786715 on epoch=58
05/31/2022 06:28:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=58
05/31/2022 06:28:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=59
05/31/2022 06:28:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=59
05/31/2022 06:28:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=59
05/31/2022 06:29:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=60
05/31/2022 06:29:11 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.45063827180347205 on epoch=60
05/31/2022 06:29:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4181236791634298 -> 0.45063827180347205 on epoch=60, global_step=1450
05/31/2022 06:29:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=60
05/31/2022 06:29:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/31/2022 06:29:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=61
05/31/2022 06:29:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=62
05/31/2022 06:29:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=62
05/31/2022 06:29:35 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.46993753573232117 on epoch=62
05/31/2022 06:29:35 - INFO - __main__ - Saving model with best Classification-F1: 0.45063827180347205 -> 0.46993753573232117 on epoch=62, global_step=1500
05/31/2022 06:29:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=62
05/31/2022 06:29:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=63
05/31/2022 06:29:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=63
05/31/2022 06:29:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=64
05/31/2022 06:29:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=64
05/31/2022 06:29:59 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.4344721790782284 on epoch=64
05/31/2022 06:30:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=64
05/31/2022 06:30:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=65
05/31/2022 06:30:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=65
05/31/2022 06:30:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=66
05/31/2022 06:30:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=66
05/31/2022 06:30:23 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.4586829889005554 on epoch=66
05/31/2022 06:30:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=67
05/31/2022 06:30:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=67
05/31/2022 06:30:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=67
05/31/2022 06:30:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=68
05/31/2022 06:30:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=68
05/31/2022 06:30:47 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.4600115079799671 on epoch=68
05/31/2022 06:30:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=69
05/31/2022 06:30:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=69
05/31/2022 06:30:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=69
05/31/2022 06:30:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=70
05/31/2022 06:31:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=70
05/31/2022 06:31:11 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.4650607830987578 on epoch=70
05/31/2022 06:31:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=71
05/31/2022 06:31:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=71
05/31/2022 06:31:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=72
05/31/2022 06:31:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=72
05/31/2022 06:31:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=72
05/31/2022 06:31:35 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.43450124846715593 on epoch=72
05/31/2022 06:31:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=73
05/31/2022 06:31:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=73
05/31/2022 06:31:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=74
05/31/2022 06:31:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=74
05/31/2022 06:31:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=74
05/31/2022 06:31:59 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.4895755761522826 on epoch=74
05/31/2022 06:31:59 - INFO - __main__ - Saving model with best Classification-F1: 0.46993753573232117 -> 0.4895755761522826 on epoch=74, global_step=1800
05/31/2022 06:32:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=75
05/31/2022 06:32:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=75
05/31/2022 06:32:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=76
05/31/2022 06:32:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=76
05/31/2022 06:32:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=77
05/31/2022 06:32:23 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.39166832677590185 on epoch=77
05/31/2022 06:32:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=77
05/31/2022 06:32:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=77
05/31/2022 06:32:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=78
05/31/2022 06:32:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=78
05/31/2022 06:32:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=79
05/31/2022 06:32:47 - INFO - __main__ - Global step 1900 Train loss 0.30 Classification-F1 0.44319049482481104 on epoch=79
05/31/2022 06:32:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=79
05/31/2022 06:32:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=79
05/31/2022 06:32:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=80
05/31/2022 06:32:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=80
05/31/2022 06:33:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=81
05/31/2022 06:33:11 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.4493448122215922 on epoch=81
05/31/2022 06:33:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=81
05/31/2022 06:33:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=82
05/31/2022 06:33:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=82
05/31/2022 06:33:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=82
05/31/2022 06:33:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=83
05/31/2022 06:33:35 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.4520487449058878 on epoch=83
05/31/2022 06:33:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=83
05/31/2022 06:33:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=84
05/31/2022 06:33:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=84
05/31/2022 06:33:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.30 on epoch=84
05/31/2022 06:33:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=85
05/31/2022 06:33:59 - INFO - __main__ - Global step 2050 Train loss 0.30 Classification-F1 0.4692076692076692 on epoch=85
05/31/2022 06:34:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.26 on epoch=85
05/31/2022 06:34:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=86
05/31/2022 06:34:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=86
05/31/2022 06:34:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=87
05/31/2022 06:34:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=87
05/31/2022 06:34:23 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.48130892954613375 on epoch=87
05/31/2022 06:34:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=87
05/31/2022 06:34:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=88
05/31/2022 06:34:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.26 on epoch=88
05/31/2022 06:34:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=89
05/31/2022 06:34:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.29 on epoch=89
05/31/2022 06:34:46 - INFO - __main__ - Global step 2150 Train loss 0.28 Classification-F1 0.4564316156038844 on epoch=89
05/31/2022 06:34:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=89
05/31/2022 06:34:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=90
05/31/2022 06:34:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=90
05/31/2022 06:34:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=91
05/31/2022 06:34:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=91
05/31/2022 06:35:10 - INFO - __main__ - Global step 2200 Train loss 0.28 Classification-F1 0.4759085420178371 on epoch=91
05/31/2022 06:35:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=92
05/31/2022 06:35:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.24 on epoch=92
05/31/2022 06:35:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=92
05/31/2022 06:35:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.28 on epoch=93
05/31/2022 06:35:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=93
05/31/2022 06:35:34 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.5009157994786674 on epoch=93
05/31/2022 06:35:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4895755761522826 -> 0.5009157994786674 on epoch=93, global_step=2250
05/31/2022 06:35:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.28 on epoch=94
05/31/2022 06:35:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=94
05/31/2022 06:35:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=94
05/31/2022 06:35:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=95
05/31/2022 06:35:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=95
05/31/2022 06:35:57 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.46560372507403297 on epoch=95
05/31/2022 06:36:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=96
05/31/2022 06:36:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=96
05/31/2022 06:36:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.30 on epoch=97
05/31/2022 06:36:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.26 on epoch=97
05/31/2022 06:36:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=97
05/31/2022 06:36:21 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.4824742463426273 on epoch=97
05/31/2022 06:36:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.32 on epoch=98
05/31/2022 06:36:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=98
05/31/2022 06:36:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.32 on epoch=99
05/31/2022 06:36:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=99
05/31/2022 06:36:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=99
05/31/2022 06:36:45 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.5111238497737013 on epoch=99
05/31/2022 06:36:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5009157994786674 -> 0.5111238497737013 on epoch=99, global_step=2400
05/31/2022 06:36:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=100
05/31/2022 06:36:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=100
05/31/2022 06:36:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.26 on epoch=101
05/31/2022 06:36:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=101
05/31/2022 06:36:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.22 on epoch=102
05/31/2022 06:37:09 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.4656579803638627 on epoch=102
05/31/2022 06:37:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.31 on epoch=102
05/31/2022 06:37:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=102
05/31/2022 06:37:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.23 on epoch=103
05/31/2022 06:37:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=103
05/31/2022 06:37:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.24 on epoch=104
05/31/2022 06:37:32 - INFO - __main__ - Global step 2500 Train loss 0.24 Classification-F1 0.4686258058679093 on epoch=104
05/31/2022 06:37:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.23 on epoch=104
05/31/2022 06:37:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=104
05/31/2022 06:37:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.25 on epoch=105
05/31/2022 06:37:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=105
05/31/2022 06:37:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=106
05/31/2022 06:37:56 - INFO - __main__ - Global step 2550 Train loss 0.23 Classification-F1 0.45610887639766123 on epoch=106
05/31/2022 06:37:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=106
05/31/2022 06:38:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=107
05/31/2022 06:38:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.25 on epoch=107
05/31/2022 06:38:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=107
05/31/2022 06:38:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.29 on epoch=108
05/31/2022 06:38:19 - INFO - __main__ - Global step 2600 Train loss 0.25 Classification-F1 0.45221775900939565 on epoch=108
05/31/2022 06:38:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=108
05/31/2022 06:38:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=109
05/31/2022 06:38:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=109
05/31/2022 06:38:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.22 on epoch=109
05/31/2022 06:38:33 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.25 on epoch=110
05/31/2022 06:38:43 - INFO - __main__ - Global step 2650 Train loss 0.23 Classification-F1 0.4849216356927692 on epoch=110
05/31/2022 06:38:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.26 on epoch=110
05/31/2022 06:38:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=111
05/31/2022 06:38:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=111
05/31/2022 06:38:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=112
05/31/2022 06:38:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=112
05/31/2022 06:39:07 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.4679318931287435 on epoch=112
05/31/2022 06:39:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=112
05/31/2022 06:39:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.24 on epoch=113
05/31/2022 06:39:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=113
05/31/2022 06:39:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=114
05/31/2022 06:39:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=114
05/31/2022 06:39:30 - INFO - __main__ - Global step 2750 Train loss 0.23 Classification-F1 0.4401910284486393 on epoch=114
05/31/2022 06:39:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.21 on epoch=114
05/31/2022 06:39:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.26 on epoch=115
05/31/2022 06:39:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=115
05/31/2022 06:39:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.26 on epoch=116
05/31/2022 06:39:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=116
05/31/2022 06:39:54 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.48572628199191126 on epoch=116
05/31/2022 06:39:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=117
05/31/2022 06:39:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=117
05/31/2022 06:40:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=117
05/31/2022 06:40:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.26 on epoch=118
05/31/2022 06:40:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=118
05/31/2022 06:40:18 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.5134210897915166 on epoch=118
05/31/2022 06:40:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5111238497737013 -> 0.5134210897915166 on epoch=118, global_step=2850
05/31/2022 06:40:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.24 on epoch=119
05/31/2022 06:40:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=119
05/31/2022 06:40:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=119
05/31/2022 06:40:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=120
05/31/2022 06:40:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.21 on epoch=120
05/31/2022 06:40:43 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.5159053089733449 on epoch=120
05/31/2022 06:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5134210897915166 -> 0.5159053089733449 on epoch=120, global_step=2900
05/31/2022 06:40:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=121
05/31/2022 06:40:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=121
05/31/2022 06:40:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.24 on epoch=122
05/31/2022 06:40:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=122
05/31/2022 06:40:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=122
05/31/2022 06:41:07 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.4994153298132009 on epoch=122
05/31/2022 06:41:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.26 on epoch=123
05/31/2022 06:41:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=123
05/31/2022 06:41:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=124
05/31/2022 06:41:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=124
05/31/2022 06:41:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.18 on epoch=124
05/31/2022 06:41:21 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:41:21 - INFO - __main__ - Printing 3 examples
05/31/2022 06:41:21 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 06:41:21 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:21 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 06:41:21 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:21 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 06:41:21 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:21 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:41:21 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:41:22 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 06:41:22 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:41:22 - INFO - __main__ - Printing 3 examples
05/31/2022 06:41:22 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 06:41:22 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:22 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 06:41:22 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:22 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 06:41:22 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:22 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:41:22 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:41:22 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 06:41:31 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.5451911830859201 on epoch=124
05/31/2022 06:41:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5159053089733449 -> 0.5451911830859201 on epoch=124, global_step=3000
05/31/2022 06:41:31 - INFO - __main__ - save last model!
05/31/2022 06:41:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 06:41:31 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 06:41:31 - INFO - __main__ - Printing 3 examples
05/31/2022 06:41:31 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 06:41:31 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:31 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 06:41:31 - INFO - __main__ - ['entailment']
05/31/2022 06:41:31 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 06:41:31 - INFO - __main__ - ['contradiction']
05/31/2022 06:41:31 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:41:32 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:41:33 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 06:41:40 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 06:41:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 06:41:41 - INFO - __main__ - Starting training!
05/31/2022 06:42:02 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_87_0.4_8_predictions.txt
05/31/2022 06:42:02 - INFO - __main__ - Classification-F1 on test data: 0.3696
05/31/2022 06:42:02 - INFO - __main__ - prefix=anli_128_87, lr=0.4, bsz=8, dev_performance=0.5451911830859201, test_performance=0.36962508826137735
05/31/2022 06:42:02 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.3, bsz=8 ...
05/31/2022 06:42:03 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:42:03 - INFO - __main__ - Printing 3 examples
05/31/2022 06:42:03 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 06:42:03 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:03 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 06:42:03 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:03 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 06:42:03 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:03 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:42:03 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:42:04 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 06:42:04 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 06:42:04 - INFO - __main__ - Printing 3 examples
05/31/2022 06:42:04 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 06:42:04 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:04 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 06:42:04 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:04 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 06:42:04 - INFO - __main__ - ['contradiction']
05/31/2022 06:42:04 - INFO - __main__ - Tokenizing Input ...
05/31/2022 06:42:04 - INFO - __main__ - Tokenizing Output ...
05/31/2022 06:42:04 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 06:42:20 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 06:42:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 06:42:21 - INFO - __main__ - Starting training!
05/31/2022 06:42:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.57 on epoch=0
05/31/2022 06:42:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=0
05/31/2022 06:42:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
05/31/2022 06:42:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=1
05/31/2022 06:42:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=2
05/31/2022 06:42:46 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 06:42:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 06:42:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=2
05/31/2022 06:42:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=2
05/31/2022 06:42:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=3
05/31/2022 06:42:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/31/2022 06:42:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=4
05/31/2022 06:43:09 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 06:43:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=4
05/31/2022 06:43:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=4
05/31/2022 06:43:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=5
05/31/2022 06:43:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=5
05/31/2022 06:43:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=6
05/31/2022 06:43:32 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 06:43:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=6
05/31/2022 06:43:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=7
05/31/2022 06:43:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=7
05/31/2022 06:43:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=7
05/31/2022 06:43:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=8
05/31/2022 06:43:54 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 06:43:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=8
05/31/2022 06:43:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=9
05/31/2022 06:44:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=9
05/31/2022 06:44:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=9
05/31/2022 06:44:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=10
05/31/2022 06:44:17 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.1721607831834019 on epoch=10
05/31/2022 06:44:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1721607831834019 on epoch=10, global_step=250
05/31/2022 06:44:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=10
05/31/2022 06:44:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=11
05/31/2022 06:44:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=11
05/31/2022 06:44:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=12
05/31/2022 06:44:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=12
05/31/2022 06:44:40 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 06:44:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=12
05/31/2022 06:44:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=13
05/31/2022 06:44:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=13
05/31/2022 06:44:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=14
05/31/2022 06:44:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=14
05/31/2022 06:45:04 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.23124139218150674 on epoch=14
05/31/2022 06:45:04 - INFO - __main__ - Saving model with best Classification-F1: 0.1721607831834019 -> 0.23124139218150674 on epoch=14, global_step=350
05/31/2022 06:45:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=14
05/31/2022 06:45:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=15
05/31/2022 06:45:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=15
05/31/2022 06:45:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=16
05/31/2022 06:45:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=16
05/31/2022 06:45:28 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16699282452707112 on epoch=16
05/31/2022 06:45:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=17
05/31/2022 06:45:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=17
05/31/2022 06:45:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=17
05/31/2022 06:45:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=18
05/31/2022 06:45:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 06:45:52 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 06:45:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=19
05/31/2022 06:45:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=19
05/31/2022 06:46:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=19
05/31/2022 06:46:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=20
05/31/2022 06:46:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=20
05/31/2022 06:46:16 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/31/2022 06:46:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=21
05/31/2022 06:46:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=21
05/31/2022 06:46:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=22
05/31/2022 06:46:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=22
05/31/2022 06:46:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=22
05/31/2022 06:46:40 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=22
05/31/2022 06:46:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=23
05/31/2022 06:46:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=23
05/31/2022 06:46:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=24
05/31/2022 06:46:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=24
05/31/2022 06:46:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=24
05/31/2022 06:47:04 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.2574483886694278 on epoch=24
05/31/2022 06:47:04 - INFO - __main__ - Saving model with best Classification-F1: 0.23124139218150674 -> 0.2574483886694278 on epoch=24, global_step=600
05/31/2022 06:47:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=25
05/31/2022 06:47:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=25
05/31/2022 06:47:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=26
05/31/2022 06:47:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=26
05/31/2022 06:47:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=27
05/31/2022 06:47:28 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.2416269176832557 on epoch=27
05/31/2022 06:47:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=27
05/31/2022 06:47:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=27
05/31/2022 06:47:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=28
05/31/2022 06:47:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=28
05/31/2022 06:47:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=29
05/31/2022 06:47:52 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.2134443944604877 on epoch=29
05/31/2022 06:47:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=29
05/31/2022 06:47:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=29
05/31/2022 06:47:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=30
05/31/2022 06:48:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=30
05/31/2022 06:48:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=31
05/31/2022 06:48:16 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.24665314401622718 on epoch=31
05/31/2022 06:48:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=31
05/31/2022 06:48:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=32
05/31/2022 06:48:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=32
05/31/2022 06:48:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=32
05/31/2022 06:48:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=33
05/31/2022 06:48:40 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.2631371389135364 on epoch=33
05/31/2022 06:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2574483886694278 -> 0.2631371389135364 on epoch=33, global_step=800
05/31/2022 06:48:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=33
05/31/2022 06:48:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=34
05/31/2022 06:48:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=34
05/31/2022 06:48:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=34
05/31/2022 06:48:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=35
05/31/2022 06:49:04 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.34622327727833496 on epoch=35
05/31/2022 06:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.2631371389135364 -> 0.34622327727833496 on epoch=35, global_step=850
05/31/2022 06:49:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=35
05/31/2022 06:49:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=36
05/31/2022 06:49:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=36
05/31/2022 06:49:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=37
05/31/2022 06:49:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=37
05/31/2022 06:49:28 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.30500363490065824 on epoch=37
05/31/2022 06:49:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=37
05/31/2022 06:49:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=38
05/31/2022 06:49:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=38
05/31/2022 06:49:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=39
05/31/2022 06:49:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=39
05/31/2022 06:49:52 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.3164110101885702 on epoch=39
05/31/2022 06:49:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=39
05/31/2022 06:49:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=40
05/31/2022 06:50:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=40
05/31/2022 06:50:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=41
05/31/2022 06:50:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=41
05/31/2022 06:50:16 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.32310444186618564 on epoch=41
05/31/2022 06:50:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=42
05/31/2022 06:50:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=42
05/31/2022 06:50:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=42
05/31/2022 06:50:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=43
05/31/2022 06:50:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=43
05/31/2022 06:50:40 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.3380940614446916 on epoch=43
05/31/2022 06:50:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=44
05/31/2022 06:50:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=44
05/31/2022 06:50:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/31/2022 06:50:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=45
05/31/2022 06:50:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=45
05/31/2022 06:51:04 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.3221597507185483 on epoch=45
05/31/2022 06:51:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=46
05/31/2022 06:51:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=46
05/31/2022 06:51:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=47
05/31/2022 06:51:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=47
05/31/2022 06:51:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=47
05/31/2022 06:51:28 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.3654485420431475 on epoch=47
05/31/2022 06:51:28 - INFO - __main__ - Saving model with best Classification-F1: 0.34622327727833496 -> 0.3654485420431475 on epoch=47, global_step=1150
05/31/2022 06:51:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=48
05/31/2022 06:51:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=48
05/31/2022 06:51:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=49
05/31/2022 06:51:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=49
05/31/2022 06:51:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=49
05/31/2022 06:51:51 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.35620511954042405 on epoch=49
05/31/2022 06:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=50
05/31/2022 06:51:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=50
05/31/2022 06:51:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=51
05/31/2022 06:52:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=51
05/31/2022 06:52:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=52
05/31/2022 06:52:15 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3536175862430016 on epoch=52
05/31/2022 06:52:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=52
05/31/2022 06:52:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=52
05/31/2022 06:52:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=53
05/31/2022 06:52:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=53
05/31/2022 06:52:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=54
05/31/2022 06:52:39 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.2908178016093494 on epoch=54
05/31/2022 06:52:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=54
05/31/2022 06:52:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=54
05/31/2022 06:52:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=55
05/31/2022 06:52:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=55
05/31/2022 06:52:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=56
05/31/2022 06:53:02 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3678027014841871 on epoch=56
05/31/2022 06:53:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3654485420431475 -> 0.3678027014841871 on epoch=56, global_step=1350
05/31/2022 06:53:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=56
05/31/2022 06:53:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=57
05/31/2022 06:53:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=57
05/31/2022 06:53:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=57
05/31/2022 06:53:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=58
05/31/2022 06:53:27 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.3550986443460861 on epoch=58
05/31/2022 06:53:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=58
05/31/2022 06:53:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=59
05/31/2022 06:53:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=59
05/31/2022 06:53:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=59
05/31/2022 06:53:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=60
05/31/2022 06:53:51 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.411935914552737 on epoch=60
05/31/2022 06:53:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3678027014841871 -> 0.411935914552737 on epoch=60, global_step=1450
05/31/2022 06:53:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=60
05/31/2022 06:53:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=61
05/31/2022 06:53:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=61
05/31/2022 06:54:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=62
05/31/2022 06:54:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=62
05/31/2022 06:54:16 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.4035572790509274 on epoch=62
05/31/2022 06:54:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=62
05/31/2022 06:54:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=63
05/31/2022 06:54:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=63
05/31/2022 06:54:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=64
05/31/2022 06:54:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=64
05/31/2022 06:54:40 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.3605391608886206 on epoch=64
05/31/2022 06:54:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=64
05/31/2022 06:54:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=65
05/31/2022 06:54:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=65
05/31/2022 06:54:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=66
05/31/2022 06:54:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=66
05/31/2022 06:55:05 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.38442871211374935 on epoch=66
05/31/2022 06:55:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=67
05/31/2022 06:55:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=67
05/31/2022 06:55:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=67
05/31/2022 06:55:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=68
05/31/2022 06:55:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=68
05/31/2022 06:55:30 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3485472345768054 on epoch=68
05/31/2022 06:55:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=69
05/31/2022 06:55:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=69
05/31/2022 06:55:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=69
05/31/2022 06:55:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=70
05/31/2022 06:55:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=70
05/31/2022 06:55:55 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3743996490751271 on epoch=70
05/31/2022 06:55:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=71
05/31/2022 06:56:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=71
05/31/2022 06:56:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=72
05/31/2022 06:56:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=72
05/31/2022 06:56:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=72
05/31/2022 06:56:19 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.39260958397573303 on epoch=72
05/31/2022 06:56:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=73
05/31/2022 06:56:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=73
05/31/2022 06:56:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=74
05/31/2022 06:56:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=74
05/31/2022 06:56:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=74
05/31/2022 06:56:43 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.4048700215867152 on epoch=74
05/31/2022 06:56:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=75
05/31/2022 06:56:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=75
05/31/2022 06:56:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=76
05/31/2022 06:56:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=76
05/31/2022 06:56:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=77
05/31/2022 06:57:07 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.3889882959536323 on epoch=77
05/31/2022 06:57:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=77
05/31/2022 06:57:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=77
05/31/2022 06:57:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=78
05/31/2022 06:57:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=78
05/31/2022 06:57:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=79
05/31/2022 06:57:31 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3887139286010981 on epoch=79
05/31/2022 06:57:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=79
05/31/2022 06:57:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=79
05/31/2022 06:57:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=80
05/31/2022 06:57:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=80
05/31/2022 06:57:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=81
05/31/2022 06:57:56 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.3879604242748305 on epoch=81
05/31/2022 06:57:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.31 on epoch=81
05/31/2022 06:58:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=82
05/31/2022 06:58:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=82
05/31/2022 06:58:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=82
05/31/2022 06:58:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=83
05/31/2022 06:58:20 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.3836153243280623 on epoch=83
05/31/2022 06:58:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=83
05/31/2022 06:58:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.33 on epoch=84
05/31/2022 06:58:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.34 on epoch=84
05/31/2022 06:58:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=84
05/31/2022 06:58:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=85
05/31/2022 06:58:44 - INFO - __main__ - Global step 2050 Train loss 0.34 Classification-F1 0.39792117346843464 on epoch=85
05/31/2022 06:58:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.34 on epoch=85
05/31/2022 06:58:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=86
05/31/2022 06:58:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=86
05/31/2022 06:58:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=87
05/31/2022 06:58:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=87
05/31/2022 06:59:08 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.40670442407531615 on epoch=87
05/31/2022 06:59:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.33 on epoch=87
05/31/2022 06:59:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=88
05/31/2022 06:59:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=88
05/31/2022 06:59:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=89
05/31/2022 06:59:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=89
05/31/2022 06:59:32 - INFO - __main__ - Global step 2150 Train loss 0.33 Classification-F1 0.40725776179120904 on epoch=89
05/31/2022 06:59:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=89
05/31/2022 06:59:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.31 on epoch=90
05/31/2022 06:59:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.34 on epoch=90
05/31/2022 06:59:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.32 on epoch=91
05/31/2022 06:59:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=91
05/31/2022 06:59:57 - INFO - __main__ - Global step 2200 Train loss 0.31 Classification-F1 0.4395558593439812 on epoch=91
05/31/2022 06:59:57 - INFO - __main__ - Saving model with best Classification-F1: 0.411935914552737 -> 0.4395558593439812 on epoch=91, global_step=2200
05/31/2022 06:59:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=92
05/31/2022 07:00:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=92
05/31/2022 07:00:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.30 on epoch=92
05/31/2022 07:00:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.30 on epoch=93
05/31/2022 07:00:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=93
05/31/2022 07:00:21 - INFO - __main__ - Global step 2250 Train loss 0.31 Classification-F1 0.43882179953025724 on epoch=93
05/31/2022 07:00:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.38 on epoch=94
05/31/2022 07:00:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.32 on epoch=94
05/31/2022 07:00:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=94
05/31/2022 07:00:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=95
05/31/2022 07:00:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=95
05/31/2022 07:00:45 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.4240286914853144 on epoch=95
05/31/2022 07:00:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.30 on epoch=96
05/31/2022 07:00:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=96
05/31/2022 07:00:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=97
05/31/2022 07:00:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.32 on epoch=97
05/31/2022 07:00:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=97
05/31/2022 07:01:10 - INFO - __main__ - Global step 2350 Train loss 0.30 Classification-F1 0.42726017943409245 on epoch=97
05/31/2022 07:01:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.30 on epoch=98
05/31/2022 07:01:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.31 on epoch=98
05/31/2022 07:01:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=99
05/31/2022 07:01:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.30 on epoch=99
05/31/2022 07:01:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=99
05/31/2022 07:01:34 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.4528936788736409 on epoch=99
05/31/2022 07:01:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4395558593439812 -> 0.4528936788736409 on epoch=99, global_step=2400
05/31/2022 07:01:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.29 on epoch=100
05/31/2022 07:01:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.32 on epoch=100
05/31/2022 07:01:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.32 on epoch=101
05/31/2022 07:01:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.32 on epoch=101
05/31/2022 07:01:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.27 on epoch=102
05/31/2022 07:01:59 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.44910218498101456 on epoch=102
05/31/2022 07:02:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.33 on epoch=102
05/31/2022 07:02:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=102
05/31/2022 07:02:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=103
05/31/2022 07:02:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=103
05/31/2022 07:02:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.36 on epoch=104
05/31/2022 07:02:23 - INFO - __main__ - Global step 2500 Train loss 0.30 Classification-F1 0.42788492759270086 on epoch=104
05/31/2022 07:02:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.29 on epoch=104
05/31/2022 07:02:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.28 on epoch=104
05/31/2022 07:02:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=105
05/31/2022 07:02:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.29 on epoch=105
05/31/2022 07:02:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=106
05/31/2022 07:02:48 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.41549956958514 on epoch=106
05/31/2022 07:02:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=106
05/31/2022 07:02:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=107
05/31/2022 07:02:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.30 on epoch=107
05/31/2022 07:02:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=107
05/31/2022 07:03:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.29 on epoch=108
05/31/2022 07:03:12 - INFO - __main__ - Global step 2600 Train loss 0.29 Classification-F1 0.4441567381584279 on epoch=108
05/31/2022 07:03:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=108
05/31/2022 07:03:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.29 on epoch=109
05/31/2022 07:03:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.29 on epoch=109
05/31/2022 07:03:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.33 on epoch=109
05/31/2022 07:03:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=110
05/31/2022 07:03:37 - INFO - __main__ - Global step 2650 Train loss 0.30 Classification-F1 0.4655116416181384 on epoch=110
05/31/2022 07:03:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4528936788736409 -> 0.4655116416181384 on epoch=110, global_step=2650
05/31/2022 07:03:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.26 on epoch=110
05/31/2022 07:03:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.28 on epoch=111
05/31/2022 07:03:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=111
05/31/2022 07:03:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=112
05/31/2022 07:03:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=112
05/31/2022 07:04:02 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.46429019472316596 on epoch=112
05/31/2022 07:04:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.23 on epoch=112
05/31/2022 07:04:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.28 on epoch=113
05/31/2022 07:04:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.29 on epoch=113
05/31/2022 07:04:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.30 on epoch=114
05/31/2022 07:04:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=114
05/31/2022 07:04:26 - INFO - __main__ - Global step 2750 Train loss 0.27 Classification-F1 0.45855097427284236 on epoch=114
05/31/2022 07:04:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=114
05/31/2022 07:04:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.25 on epoch=115
05/31/2022 07:04:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=115
05/31/2022 07:04:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.30 on epoch=116
05/31/2022 07:04:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.26 on epoch=116
05/31/2022 07:04:50 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.4624321532950974 on epoch=116
05/31/2022 07:04:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.28 on epoch=117
05/31/2022 07:04:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.27 on epoch=117
05/31/2022 07:04:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.26 on epoch=117
05/31/2022 07:05:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=118
05/31/2022 07:05:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.25 on epoch=118
05/31/2022 07:05:15 - INFO - __main__ - Global step 2850 Train loss 0.28 Classification-F1 0.5034338852923818 on epoch=118
05/31/2022 07:05:15 - INFO - __main__ - Saving model with best Classification-F1: 0.4655116416181384 -> 0.5034338852923818 on epoch=118, global_step=2850
05/31/2022 07:05:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=119
05/31/2022 07:05:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=119
05/31/2022 07:05:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=119
05/31/2022 07:05:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.25 on epoch=120
05/31/2022 07:05:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=120
05/31/2022 07:05:39 - INFO - __main__ - Global step 2900 Train loss 0.26 Classification-F1 0.4797287571409293 on epoch=120
05/31/2022 07:05:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=121
05/31/2022 07:05:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=121
05/31/2022 07:05:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.22 on epoch=122
05/31/2022 07:05:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=122
05/31/2022 07:05:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=122
05/31/2022 07:06:04 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.439784346081667 on epoch=122
05/31/2022 07:06:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.30 on epoch=123
05/31/2022 07:06:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.26 on epoch=123
05/31/2022 07:06:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.26 on epoch=124
05/31/2022 07:06:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.29 on epoch=124
05/31/2022 07:06:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=124
05/31/2022 07:06:19 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 07:06:19 - INFO - __main__ - Printing 3 examples
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:06:19 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:06:19 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 07:06:19 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 07:06:19 - INFO - __main__ - Printing 3 examples
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 07:06:19 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:19 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:06:20 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:06:20 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 07:06:29 - INFO - __main__ - Global step 3000 Train loss 0.28 Classification-F1 0.5027458492975735 on epoch=124
05/31/2022 07:06:29 - INFO - __main__ - save last model!
05/31/2022 07:06:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 07:06:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 07:06:29 - INFO - __main__ - Printing 3 examples
05/31/2022 07:06:29 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 07:06:29 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:29 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 07:06:29 - INFO - __main__ - ['entailment']
05/31/2022 07:06:29 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 07:06:29 - INFO - __main__ - ['contradiction']
05/31/2022 07:06:29 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:06:29 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:06:30 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 07:06:36 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 07:06:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 07:06:37 - INFO - __main__ - Starting training!
05/31/2022 07:07:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_87_0.3_8_predictions.txt
05/31/2022 07:07:00 - INFO - __main__ - Classification-F1 on test data: 0.3452
05/31/2022 07:07:00 - INFO - __main__ - prefix=anli_128_87, lr=0.3, bsz=8, dev_performance=0.5034338852923818, test_performance=0.34517709404237645
05/31/2022 07:07:00 - INFO - __main__ - Running ... prefix=anli_128_87, lr=0.2, bsz=8 ...
05/31/2022 07:07:01 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 07:07:01 - INFO - __main__ - Printing 3 examples
05/31/2022 07:07:01 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/31/2022 07:07:01 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:01 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/31/2022 07:07:01 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:01 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/31/2022 07:07:01 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:01 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:07:01 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:07:02 - INFO - __main__ - Loaded 384 examples from train data
05/31/2022 07:07:02 - INFO - __main__ - Start tokenizing ... 384 instances
05/31/2022 07:07:02 - INFO - __main__ - Printing 3 examples
05/31/2022 07:07:02 - INFO - __main__ -  [anli] premise: Never Shout Never is an EP by Never Shout Never which was released on December 8, 2009. The physical release is sold exclusively at Hot Topic. The EP features two songs from his then upcoming Sire Records full-length debut, "What is Love?", one song that is a B-side from "The Summer EP" and one live track. [SEP] hypothesis:  The EP features two live tracks. 
05/31/2022 07:07:02 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:02 - INFO - __main__ -  [anli] premise: The Hound of the Baskervilles is a 1978 British comedy film spoofing "The Hound of the Baskervilles" by Sir Arthur Conan Doyle. It starred Peter Cook as Sherlock Holmes and Dudley Moore as Dr. Watson. A number of other well-known British comedy actors appeared in the film including Terry-Thomas (in his final screen appearance), Kenneth Williams and Denholm Elliott. [SEP] hypothesis: The Hound of the Baskervilles is a american food dish.
05/31/2022 07:07:02 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:02 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts is most famous for his music career.
05/31/2022 07:07:02 - INFO - __main__ - ['contradiction']
05/31/2022 07:07:02 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:07:02 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:07:03 - INFO - __main__ - Loaded 384 examples from dev data
05/31/2022 07:07:20 - INFO - __main__ - load prompt embedding from ckpt
05/31/2022 07:07:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/31/2022 07:07:21 - INFO - __main__ - Starting training!
05/31/2022 07:07:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.58 on epoch=0
05/31/2022 07:07:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=0
05/31/2022 07:07:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=1
05/31/2022 07:07:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=1
05/31/2022 07:07:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=2
05/31/2022 07:07:46 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=2
05/31/2022 07:07:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=2, global_step=50
05/31/2022 07:07:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=2
05/31/2022 07:07:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=2
05/31/2022 07:07:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=3
05/31/2022 07:07:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=3
05/31/2022 07:08:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=4
05/31/2022 07:08:10 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/31/2022 07:08:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=4
05/31/2022 07:08:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=4
05/31/2022 07:08:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=5
05/31/2022 07:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=5
05/31/2022 07:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=6
05/31/2022 07:08:34 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=6
05/31/2022 07:08:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=6
05/31/2022 07:08:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=7
05/31/2022 07:08:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=7
05/31/2022 07:08:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=7
05/31/2022 07:08:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=8
05/31/2022 07:08:57 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=8
05/31/2022 07:09:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=8
05/31/2022 07:09:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=9
05/31/2022 07:09:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=9
05/31/2022 07:09:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=9
05/31/2022 07:09:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=10
05/31/2022 07:09:20 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=10
05/31/2022 07:09:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=10
05/31/2022 07:09:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=11
05/31/2022 07:09:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=11
05/31/2022 07:09:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=12
05/31/2022 07:09:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=12
05/31/2022 07:09:43 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/31/2022 07:09:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=12
05/31/2022 07:09:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=13
05/31/2022 07:09:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=13
05/31/2022 07:09:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=14
05/31/2022 07:09:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=14
05/31/2022 07:10:06 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=14
05/31/2022 07:10:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=14
05/31/2022 07:10:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=15
05/31/2022 07:10:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=15
05/31/2022 07:10:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=16
05/31/2022 07:10:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=16
05/31/2022 07:10:31 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/31/2022 07:10:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=17
05/31/2022 07:10:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=17
05/31/2022 07:10:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=17
05/31/2022 07:10:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=18
05/31/2022 07:10:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=18
05/31/2022 07:10:54 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=18
05/31/2022 07:10:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=19
05/31/2022 07:10:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=19
05/31/2022 07:11:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=19
05/31/2022 07:11:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=20
05/31/2022 07:11:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=20
05/31/2022 07:11:18 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=20
05/31/2022 07:11:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=21
05/31/2022 07:11:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=21
05/31/2022 07:11:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=22
05/31/2022 07:11:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=22
05/31/2022 07:11:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=22
05/31/2022 07:11:43 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=22
05/31/2022 07:11:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=23
05/31/2022 07:11:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=23
05/31/2022 07:11:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=24
05/31/2022 07:11:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=24
05/31/2022 07:11:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=24
05/31/2022 07:12:07 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/31/2022 07:12:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=25
05/31/2022 07:12:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=25
05/31/2022 07:12:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=26
05/31/2022 07:12:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=26
05/31/2022 07:12:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=27
05/31/2022 07:12:31 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.183246178197191 on epoch=27
05/31/2022 07:12:31 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.183246178197191 on epoch=27, global_step=650
05/31/2022 07:12:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=27
05/31/2022 07:12:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=27
05/31/2022 07:12:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=28
05/31/2022 07:12:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=28
05/31/2022 07:12:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=29
05/31/2022 07:12:56 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/31/2022 07:12:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=29
05/31/2022 07:13:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=29
05/31/2022 07:13:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=30
05/31/2022 07:13:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=30
05/31/2022 07:13:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=31
05/31/2022 07:13:20 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=31
05/31/2022 07:13:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=31
05/31/2022 07:13:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=32
05/31/2022 07:13:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=32
05/31/2022 07:13:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=32
05/31/2022 07:13:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=33
05/31/2022 07:13:44 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/31/2022 07:13:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=33
05/31/2022 07:13:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=34
05/31/2022 07:13:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=34
05/31/2022 07:13:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=34
05/31/2022 07:13:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=35
05/31/2022 07:14:08 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.26585769940907467 on epoch=35
05/31/2022 07:14:08 - INFO - __main__ - Saving model with best Classification-F1: 0.183246178197191 -> 0.26585769940907467 on epoch=35, global_step=850
05/31/2022 07:14:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=35
05/31/2022 07:14:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=36
05/31/2022 07:14:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=36
05/31/2022 07:14:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=37
05/31/2022 07:14:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=37
05/31/2022 07:14:32 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.1881810228266921 on epoch=37
05/31/2022 07:14:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=37
05/31/2022 07:14:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=38
05/31/2022 07:14:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=38
05/31/2022 07:14:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=39
05/31/2022 07:14:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=39
05/31/2022 07:14:56 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.22641026734319705 on epoch=39
05/31/2022 07:14:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=39
05/31/2022 07:15:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=40
05/31/2022 07:15:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=40
05/31/2022 07:15:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=41
05/31/2022 07:15:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=41
05/31/2022 07:15:19 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.24014336917562726 on epoch=41
05/31/2022 07:15:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=42
05/31/2022 07:15:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=42
05/31/2022 07:15:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=42
05/31/2022 07:15:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=43
05/31/2022 07:15:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=43
05/31/2022 07:15:42 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.2416269176832557 on epoch=43
05/31/2022 07:15:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.51 on epoch=44
05/31/2022 07:15:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=44
05/31/2022 07:15:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=44
05/31/2022 07:15:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=45
05/31/2022 07:15:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=45
05/31/2022 07:16:04 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.2416269176832557 on epoch=45
05/31/2022 07:16:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=46
05/31/2022 07:16:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=46
05/31/2022 07:16:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=47
05/31/2022 07:16:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=47
05/31/2022 07:16:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=47
05/31/2022 07:16:26 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.30330423403412715 on epoch=47
05/31/2022 07:16:26 - INFO - __main__ - Saving model with best Classification-F1: 0.26585769940907467 -> 0.30330423403412715 on epoch=47, global_step=1150
05/31/2022 07:16:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=48
05/31/2022 07:16:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=48
05/31/2022 07:16:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=49
05/31/2022 07:16:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=49
05/31/2022 07:16:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=49
05/31/2022 07:16:48 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.32677400119260586 on epoch=49
05/31/2022 07:16:48 - INFO - __main__ - Saving model with best Classification-F1: 0.30330423403412715 -> 0.32677400119260586 on epoch=49, global_step=1200
05/31/2022 07:16:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=50
05/31/2022 07:16:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=50
05/31/2022 07:16:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=51
05/31/2022 07:16:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=51
05/31/2022 07:17:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=52
05/31/2022 07:17:10 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.3097453103553454 on epoch=52
05/31/2022 07:17:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=52
05/31/2022 07:17:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=52
05/31/2022 07:17:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=53
05/31/2022 07:17:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=53
05/31/2022 07:17:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.49 on epoch=54
05/31/2022 07:17:32 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.34166706655789625 on epoch=54
05/31/2022 07:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.32677400119260586 -> 0.34166706655789625 on epoch=54, global_step=1300
05/31/2022 07:17:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=54
05/31/2022 07:17:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=54
05/31/2022 07:17:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=55
05/31/2022 07:17:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=55
05/31/2022 07:17:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=56
05/31/2022 07:17:54 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3133183949226283 on epoch=56
05/31/2022 07:17:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=56
05/31/2022 07:17:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=57
05/31/2022 07:18:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=57
05/31/2022 07:18:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=57
05/31/2022 07:18:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=58
05/31/2022 07:18:15 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.29918900769667855 on epoch=58
05/31/2022 07:18:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=58
05/31/2022 07:18:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=59
05/31/2022 07:18:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=59
05/31/2022 07:18:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=59
05/31/2022 07:18:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=60
05/31/2022 07:18:37 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.34598554598554604 on epoch=60
05/31/2022 07:18:37 - INFO - __main__ - Saving model with best Classification-F1: 0.34166706655789625 -> 0.34598554598554604 on epoch=60, global_step=1450
05/31/2022 07:18:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=60
05/31/2022 07:18:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=61
05/31/2022 07:18:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=61
05/31/2022 07:18:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=62
05/31/2022 07:18:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=62
05/31/2022 07:18:59 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.3122970913668588 on epoch=62
05/31/2022 07:19:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=62
05/31/2022 07:19:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=63
05/31/2022 07:19:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=63
05/31/2022 07:19:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=64
05/31/2022 07:19:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=64
05/31/2022 07:19:20 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3546576879910213 on epoch=64
05/31/2022 07:19:20 - INFO - __main__ - Saving model with best Classification-F1: 0.34598554598554604 -> 0.3546576879910213 on epoch=64, global_step=1550
05/31/2022 07:19:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=64
05/31/2022 07:19:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=65
05/31/2022 07:19:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=65
05/31/2022 07:19:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=66
05/31/2022 07:19:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=66
05/31/2022 07:19:42 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.350543950231317 on epoch=66
05/31/2022 07:19:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=67
05/31/2022 07:19:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=67
05/31/2022 07:19:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=67
05/31/2022 07:19:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=68
05/31/2022 07:19:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=68
05/31/2022 07:20:04 - INFO - __main__ - Global step 1650 Train loss 0.40 Classification-F1 0.3411813003949891 on epoch=68
05/31/2022 07:20:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=69
05/31/2022 07:20:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=69
05/31/2022 07:20:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=69
05/31/2022 07:20:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=70
05/31/2022 07:20:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=70
05/31/2022 07:20:25 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3448242876445337 on epoch=70
05/31/2022 07:20:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=71
05/31/2022 07:20:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=71
05/31/2022 07:20:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=72
05/31/2022 07:20:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=72
05/31/2022 07:20:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=72
05/31/2022 07:20:47 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.32210031347962387 on epoch=72
05/31/2022 07:20:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=73
05/31/2022 07:20:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=73
05/31/2022 07:20:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=74
05/31/2022 07:20:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=74
05/31/2022 07:21:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=74
05/31/2022 07:21:09 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.36919725155019273 on epoch=74
05/31/2022 07:21:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3546576879910213 -> 0.36919725155019273 on epoch=74, global_step=1800
05/31/2022 07:21:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=75
05/31/2022 07:21:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=75
05/31/2022 07:21:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=76
05/31/2022 07:21:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.46 on epoch=76
05/31/2022 07:21:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=77
05/31/2022 07:21:31 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.324999594662516 on epoch=77
05/31/2022 07:21:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=77
05/31/2022 07:21:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=77
05/31/2022 07:21:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=78
05/31/2022 07:21:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=78
05/31/2022 07:21:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=79
05/31/2022 07:21:54 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.33434105923188895 on epoch=79
05/31/2022 07:21:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=79
05/31/2022 07:21:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=79
05/31/2022 07:22:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=80
05/31/2022 07:22:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=80
05/31/2022 07:22:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=81
05/31/2022 07:22:16 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.343777445465119 on epoch=81
05/31/2022 07:22:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=81
05/31/2022 07:22:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=82
05/31/2022 07:22:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=82
05/31/2022 07:22:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=82
05/31/2022 07:22:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=83
05/31/2022 07:22:39 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.338399018924183 on epoch=83
05/31/2022 07:22:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=83
05/31/2022 07:22:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=84
05/31/2022 07:22:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=84
05/31/2022 07:22:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=84
05/31/2022 07:22:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=85
05/31/2022 07:23:02 - INFO - __main__ - Global step 2050 Train loss 0.39 Classification-F1 0.37410348754886574 on epoch=85
05/31/2022 07:23:02 - INFO - __main__ - Saving model with best Classification-F1: 0.36919725155019273 -> 0.37410348754886574 on epoch=85, global_step=2050
05/31/2022 07:23:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=85
05/31/2022 07:23:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=86
05/31/2022 07:23:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.38 on epoch=86
05/31/2022 07:23:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=87
05/31/2022 07:23:15 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.41 on epoch=87
05/31/2022 07:23:25 - INFO - __main__ - Global step 2100 Train loss 0.39 Classification-F1 0.4051195242900702 on epoch=87
05/31/2022 07:23:25 - INFO - __main__ - Saving model with best Classification-F1: 0.37410348754886574 -> 0.4051195242900702 on epoch=87, global_step=2100
05/31/2022 07:23:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=87
05/31/2022 07:23:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=88
05/31/2022 07:23:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=88
05/31/2022 07:23:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=89
05/31/2022 07:23:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=89
05/31/2022 07:23:47 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.3720662545898224 on epoch=89
05/31/2022 07:23:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.35 on epoch=89
05/31/2022 07:23:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=90
05/31/2022 07:23:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=90
05/31/2022 07:23:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=91
05/31/2022 07:24:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=91
05/31/2022 07:24:10 - INFO - __main__ - Global step 2200 Train loss 0.39 Classification-F1 0.4118267697215066 on epoch=91
05/31/2022 07:24:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4051195242900702 -> 0.4118267697215066 on epoch=91, global_step=2200
05/31/2022 07:24:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=92
05/31/2022 07:24:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.34 on epoch=92
05/31/2022 07:24:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=92
05/31/2022 07:24:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=93
05/31/2022 07:24:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.34 on epoch=93
05/31/2022 07:24:33 - INFO - __main__ - Global step 2250 Train loss 0.36 Classification-F1 0.3813196182003522 on epoch=93
05/31/2022 07:24:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=94
05/31/2022 07:24:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=94
05/31/2022 07:24:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=94
05/31/2022 07:24:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=95
05/31/2022 07:24:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.34 on epoch=95
05/31/2022 07:24:55 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3743495106861796 on epoch=95
05/31/2022 07:24:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=96
05/31/2022 07:25:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=96
05/31/2022 07:25:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=97
05/31/2022 07:25:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=97
05/31/2022 07:25:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=97
05/31/2022 07:25:18 - INFO - __main__ - Global step 2350 Train loss 0.36 Classification-F1 0.38171361753179794 on epoch=97
05/31/2022 07:25:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=98
05/31/2022 07:25:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=98
05/31/2022 07:25:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.38 on epoch=99
05/31/2022 07:25:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=99
05/31/2022 07:25:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=99
05/31/2022 07:25:41 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.3927992195597829 on epoch=99
05/31/2022 07:25:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.34 on epoch=100
05/31/2022 07:25:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=100
05/31/2022 07:25:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.37 on epoch=101
05/31/2022 07:25:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.35 on epoch=101
05/31/2022 07:25:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=102
05/31/2022 07:26:04 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.37711864854721994 on epoch=102
05/31/2022 07:26:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=102
05/31/2022 07:26:09 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=102
05/31/2022 07:26:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=103
05/31/2022 07:26:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=103
05/31/2022 07:26:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.33 on epoch=104
05/31/2022 07:26:27 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.37127701062366963 on epoch=104
05/31/2022 07:26:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=104
05/31/2022 07:26:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=104
05/31/2022 07:26:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=105
05/31/2022 07:26:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=105
05/31/2022 07:26:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=106
05/31/2022 07:26:49 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.4026593561477283 on epoch=106
05/31/2022 07:26:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.31 on epoch=106
05/31/2022 07:26:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=107
05/31/2022 07:26:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=107
05/31/2022 07:27:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=107
05/31/2022 07:27:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=108
05/31/2022 07:27:12 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.37897237212305707 on epoch=108
05/31/2022 07:27:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.31 on epoch=108
05/31/2022 07:27:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.35 on epoch=109
05/31/2022 07:27:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=109
05/31/2022 07:27:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=109
05/31/2022 07:27:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=110
05/31/2022 07:27:35 - INFO - __main__ - Global step 2650 Train loss 0.34 Classification-F1 0.40118206221164004 on epoch=110
05/31/2022 07:27:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=110
05/31/2022 07:27:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=111
05/31/2022 07:27:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=111
05/31/2022 07:27:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=112
05/31/2022 07:27:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.36 on epoch=112
05/31/2022 07:27:57 - INFO - __main__ - Global step 2700 Train loss 0.33 Classification-F1 0.40331448776981205 on epoch=112
05/31/2022 07:28:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=112
05/31/2022 07:28:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.29 on epoch=113
05/31/2022 07:28:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.33 on epoch=113
05/31/2022 07:28:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.36 on epoch=114
05/31/2022 07:28:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=114
05/31/2022 07:28:20 - INFO - __main__ - Global step 2750 Train loss 0.34 Classification-F1 0.420475326924302 on epoch=114
05/31/2022 07:28:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4118267697215066 -> 0.420475326924302 on epoch=114, global_step=2750
05/31/2022 07:28:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.34 on epoch=114
05/31/2022 07:28:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.36 on epoch=115
05/31/2022 07:28:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.33 on epoch=115
05/31/2022 07:28:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=116
05/31/2022 07:28:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=116
05/31/2022 07:28:44 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.41771496854004414 on epoch=116
05/31/2022 07:28:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.36 on epoch=117
05/31/2022 07:28:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.34 on epoch=117
05/31/2022 07:28:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=117
05/31/2022 07:28:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=118
05/31/2022 07:28:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.31 on epoch=118
05/31/2022 07:29:07 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.3990642374476092 on epoch=118
05/31/2022 07:29:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=119
05/31/2022 07:29:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=119
05/31/2022 07:29:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=119
05/31/2022 07:29:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=120
05/31/2022 07:29:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.37 on epoch=120
05/31/2022 07:29:30 - INFO - __main__ - Global step 2900 Train loss 0.35 Classification-F1 0.3919207379014235 on epoch=120
05/31/2022 07:29:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.30 on epoch=121
05/31/2022 07:29:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=121
05/31/2022 07:29:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=122
05/31/2022 07:29:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.33 on epoch=122
05/31/2022 07:29:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=122
05/31/2022 07:29:54 - INFO - __main__ - Global step 2950 Train loss 0.34 Classification-F1 0.4062020946697092 on epoch=122
05/31/2022 07:29:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.27 on epoch=123
05/31/2022 07:29:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.27 on epoch=123
05/31/2022 07:30:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=124
05/31/2022 07:30:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.30 on epoch=124
05/31/2022 07:30:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=124
05/31/2022 07:30:18 - INFO - __main__ - Global step 3000 Train loss 0.31 Classification-F1 0.41596407882904235 on epoch=124
05/31/2022 07:30:18 - INFO - __main__ - save last model!
05/31/2022 07:30:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/31/2022 07:30:18 - INFO - __main__ - Start tokenizing ... 1000 instances
05/31/2022 07:30:18 - INFO - __main__ - Printing 3 examples
05/31/2022 07:30:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/31/2022 07:30:18 - INFO - __main__ - ['contradiction']
05/31/2022 07:30:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/31/2022 07:30:18 - INFO - __main__ - ['entailment']
05/31/2022 07:30:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/31/2022 07:30:18 - INFO - __main__ - ['contradiction']
05/31/2022 07:30:18 - INFO - __main__ - Tokenizing Input ...
05/31/2022 07:30:18 - INFO - __main__ - Tokenizing Output ...
05/31/2022 07:30:19 - INFO - __main__ - Loaded 1000 examples from test data
05/31/2022 07:30:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-anli/anli_128_87_0.2_8_predictions.txt
05/31/2022 07:30:46 - INFO - __main__ - Classification-F1 on test data: 0.2820
05/31/2022 07:30:47 - INFO - __main__ - prefix=anli_128_87, lr=0.2, bsz=8, dev_performance=0.420475326924302, test_performance=0.28197707251493465
