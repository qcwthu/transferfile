05/28/2022 19:19:13 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/28/2022 19:19:13 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa
05/28/2022 19:19:13 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/28/2022 19:19:13 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa
05/28/2022 19:19:14 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/28/2022 19:19:14 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/28/2022 19:19:14 - INFO - __main__ - args.device: cuda:0
05/28/2022 19:19:14 - INFO - __main__ - Using 2 gpus
05/28/2022 19:19:14 - INFO - __main__ - args.device: cuda:1
05/28/2022 19:19:14 - INFO - __main__ - Using 2 gpus
05/28/2022 19:19:14 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/28/2022 19:19:14 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/28/2022 19:19:19 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.5, bsz=8 ...
05/28/2022 19:19:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:19:20 - INFO - __main__ - Printing 3 examples
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:19:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:19:20 - INFO - __main__ - Printing 3 examples
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:19:20 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:19:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:19:20 - INFO - __main__ - Printing 3 examples
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:19:20 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:19:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:19:20 - INFO - __main__ - Printing 3 examples
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:19:20 - INFO - __main__ - ['false']
05/28/2022 19:19:20 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:19:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:19:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:19:21 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:19:21 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:19:39 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:19:39 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:19:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:19:39 - INFO - __main__ - Starting training!
05/28/2022 19:19:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:19:44 - INFO - __main__ - Starting training!
05/28/2022 19:19:47 - INFO - __main__ - Step 10 Global step 10 Train loss 2.08 on epoch=0
05/28/2022 19:19:50 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=1
05/28/2022 19:19:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=1
05/28/2022 19:19:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=2
05/28/2022 19:19:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
05/28/2022 19:20:03 - INFO - __main__ - Global step 50 Train loss 0.76 Classification-F1 0.4431057563587684 on epoch=3
05/28/2022 19:20:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4431057563587684 on epoch=3, global_step=50
05/28/2022 19:20:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=3
05/28/2022 19:20:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/28/2022 19:20:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=4
05/28/2022 19:20:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/28/2022 19:20:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=6
05/28/2022 19:20:22 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.512661771528211 on epoch=6
05/28/2022 19:20:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4431057563587684 -> 0.512661771528211 on epoch=6, global_step=100
05/28/2022 19:20:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/28/2022 19:20:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/28/2022 19:20:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/28/2022 19:20:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=8
05/28/2022 19:20:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
05/28/2022 19:20:40 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3531737612590892 on epoch=9
05/28/2022 19:20:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/28/2022 19:20:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.34 on epoch=10
05/28/2022 19:20:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=11
05/28/2022 19:20:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
05/28/2022 19:20:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
05/28/2022 19:20:58 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.38904444235647845 on epoch=12
05/28/2022 19:21:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=13
05/28/2022 19:21:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=13
05/28/2022 19:21:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
05/28/2022 19:21:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=14
05/28/2022 19:21:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
05/28/2022 19:21:17 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.6222099243681978 on epoch=15
05/28/2022 19:21:17 - INFO - __main__ - Saving model with best Classification-F1: 0.512661771528211 -> 0.6222099243681978 on epoch=15, global_step=250
05/28/2022 19:21:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
05/28/2022 19:21:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/28/2022 19:21:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/28/2022 19:21:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=18
05/28/2022 19:21:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
05/28/2022 19:21:35 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.6679021497405486 on epoch=18
05/28/2022 19:21:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6222099243681978 -> 0.6679021497405486 on epoch=18, global_step=300
05/28/2022 19:21:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/28/2022 19:21:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=19
05/28/2022 19:21:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=20
05/28/2022 19:21:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
05/28/2022 19:21:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=21
05/28/2022 19:21:53 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.6644717103289277 on epoch=21
05/28/2022 19:21:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/28/2022 19:21:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
05/28/2022 19:22:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.33 on epoch=23
05/28/2022 19:22:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=24
05/28/2022 19:22:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
05/28/2022 19:22:12 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6091734192292851 on epoch=24
05/28/2022 19:22:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=25
05/28/2022 19:22:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
05/28/2022 19:22:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=26
05/28/2022 19:22:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=27
05/28/2022 19:22:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/28/2022 19:22:30 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.7221432393181992 on epoch=28
05/28/2022 19:22:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6679021497405486 -> 0.7221432393181992 on epoch=28, global_step=450
05/28/2022 19:22:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=28
05/28/2022 19:22:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=29
05/28/2022 19:22:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
05/28/2022 19:22:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=30
05/28/2022 19:22:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=31
05/28/2022 19:22:47 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.7311588831233012 on epoch=31
05/28/2022 19:22:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7221432393181992 -> 0.7311588831233012 on epoch=31, global_step=500
05/28/2022 19:22:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=31
05/28/2022 19:22:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=32
05/28/2022 19:22:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=33
05/28/2022 19:22:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=33
05/28/2022 19:23:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/28/2022 19:23:05 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.7457795431976166 on epoch=34
05/28/2022 19:23:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7311588831233012 -> 0.7457795431976166 on epoch=34, global_step=550
05/28/2022 19:23:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=34
05/28/2022 19:23:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=35
05/28/2022 19:23:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=36
05/28/2022 19:23:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=36
05/28/2022 19:23:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=37
05/28/2022 19:23:22 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.7342290076335878 on epoch=37
05/28/2022 19:23:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=38
05/28/2022 19:23:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=38
05/28/2022 19:23:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=39
05/28/2022 19:23:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=39
05/28/2022 19:23:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/28/2022 19:23:40 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.6947425474254743 on epoch=40
05/28/2022 19:23:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/28/2022 19:23:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
05/28/2022 19:23:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=42
05/28/2022 19:23:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=43
05/28/2022 19:23:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=43
05/28/2022 19:23:57 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.6753094546213495 on epoch=43
05/28/2022 19:24:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=44
05/28/2022 19:24:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=44
05/28/2022 19:24:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=45
05/28/2022 19:24:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=46
05/28/2022 19:24:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=46
05/28/2022 19:24:14 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.7245280698518108 on epoch=46
05/28/2022 19:24:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=47
05/28/2022 19:24:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=48
05/28/2022 19:24:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=48
05/28/2022 19:24:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=49
05/28/2022 19:24:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=49
05/28/2022 19:24:31 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.7730912647472339 on epoch=49
05/28/2022 19:24:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7457795431976166 -> 0.7730912647472339 on epoch=49, global_step=800
05/28/2022 19:24:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/28/2022 19:24:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/28/2022 19:24:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
05/28/2022 19:24:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=52
05/28/2022 19:24:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/28/2022 19:24:49 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.7283904387443051 on epoch=53
05/28/2022 19:24:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=53
05/28/2022 19:24:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/28/2022 19:24:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=54
05/28/2022 19:24:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/28/2022 19:25:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
05/28/2022 19:25:06 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.7848903827056757 on epoch=56
05/28/2022 19:25:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7730912647472339 -> 0.7848903827056757 on epoch=56, global_step=900
05/28/2022 19:25:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/28/2022 19:25:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/28/2022 19:25:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
05/28/2022 19:25:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/28/2022 19:25:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/28/2022 19:25:23 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.7563252272169 on epoch=59
05/28/2022 19:25:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=59
05/28/2022 19:25:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=60
05/28/2022 19:25:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=61
05/28/2022 19:25:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/28/2022 19:25:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=62
05/28/2022 19:25:41 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.7538123368594587 on epoch=62
05/28/2022 19:25:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/28/2022 19:25:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=63
05/28/2022 19:25:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/28/2022 19:25:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=64
05/28/2022 19:25:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=65
05/28/2022 19:25:58 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.718998933701311 on epoch=65
05/28/2022 19:26:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=66
05/28/2022 19:26:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/28/2022 19:26:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/28/2022 19:26:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=68
05/28/2022 19:26:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=68
05/28/2022 19:26:16 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7676557909148245 on epoch=68
05/28/2022 19:26:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/28/2022 19:26:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=69
05/28/2022 19:26:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=70
05/28/2022 19:26:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/28/2022 19:26:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=71
05/28/2022 19:26:33 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.6851466414380321 on epoch=71
05/28/2022 19:26:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=72
05/28/2022 19:26:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=73
05/28/2022 19:26:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/28/2022 19:26:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=74
05/28/2022 19:26:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/28/2022 19:26:50 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.7769318963540472 on epoch=74
05/28/2022 19:26:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=75
05/28/2022 19:26:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/28/2022 19:26:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=76
05/28/2022 19:27:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=77
05/28/2022 19:27:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
05/28/2022 19:27:08 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.6913006029285098 on epoch=78
05/28/2022 19:27:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=78
05/28/2022 19:27:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=79
05/28/2022 19:27:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=79
05/28/2022 19:27:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=80
05/28/2022 19:27:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=81
05/28/2022 19:27:25 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.7499847402795581 on epoch=81
05/28/2022 19:27:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=81
05/28/2022 19:27:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/28/2022 19:27:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
05/28/2022 19:27:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=83
05/28/2022 19:27:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/28/2022 19:27:42 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.7487578973195117 on epoch=84
05/28/2022 19:27:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=84
05/28/2022 19:27:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=85
05/28/2022 19:27:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=86
05/28/2022 19:27:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/28/2022 19:27:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=87
05/28/2022 19:28:00 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.6923076923076923 on epoch=87
05/28/2022 19:28:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/28/2022 19:28:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=88
05/28/2022 19:28:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=89
05/28/2022 19:28:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=89
05/28/2022 19:28:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=90
05/28/2022 19:28:18 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.6788559242300697 on epoch=90
05/28/2022 19:28:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
05/28/2022 19:28:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/28/2022 19:28:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=92
05/28/2022 19:28:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
05/28/2022 19:28:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/28/2022 19:28:35 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.6553846153846153 on epoch=93
05/28/2022 19:28:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=94
05/28/2022 19:28:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=94
05/28/2022 19:28:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=95
05/28/2022 19:28:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=96
05/28/2022 19:28:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=96
05/28/2022 19:28:52 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.6521739130434783 on epoch=96
05/28/2022 19:28:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=97
05/28/2022 19:28:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=98
05/28/2022 19:29:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=98
05/28/2022 19:29:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/28/2022 19:29:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=99
05/28/2022 19:29:09 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7497556207233627 on epoch=99
05/28/2022 19:29:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=100
05/28/2022 19:29:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/28/2022 19:29:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=101
05/28/2022 19:29:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/28/2022 19:29:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=103
05/28/2022 19:29:27 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7343101343101344 on epoch=103
05/28/2022 19:29:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=103
05/28/2022 19:29:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=104
05/28/2022 19:29:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/28/2022 19:29:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=105
05/28/2022 19:29:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=106
05/28/2022 19:29:44 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.7566092130282771 on epoch=106
05/28/2022 19:29:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/28/2022 19:29:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=107
05/28/2022 19:29:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=108
05/28/2022 19:29:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/28/2022 19:29:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=109
05/28/2022 19:30:01 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.7446873801304181 on epoch=109
05/28/2022 19:30:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/28/2022 19:30:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=110
05/28/2022 19:30:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=111
05/28/2022 19:30:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=111
05/28/2022 19:30:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=112
05/28/2022 19:30:18 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.723251791450457 on epoch=112
05/28/2022 19:30:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=113
05/28/2022 19:30:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=113
05/28/2022 19:30:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
05/28/2022 19:30:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/28/2022 19:30:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=115
05/28/2022 19:30:36 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.675 on epoch=115
05/28/2022 19:30:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=116
05/28/2022 19:30:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
05/28/2022 19:30:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/28/2022 19:30:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=118
05/28/2022 19:30:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=118
05/28/2022 19:30:53 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.7269649895664272 on epoch=118
05/28/2022 19:30:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=119
05/28/2022 19:30:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/28/2022 19:31:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=120
05/28/2022 19:31:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=121
05/28/2022 19:31:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/28/2022 19:31:11 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.698497580850522 on epoch=121
05/28/2022 19:31:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/28/2022 19:31:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
05/28/2022 19:31:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
05/28/2022 19:31:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/28/2022 19:31:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=124
05/28/2022 19:31:28 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.7575757575757577 on epoch=124
05/28/2022 19:31:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
05/28/2022 19:31:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=126
05/28/2022 19:31:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=126
05/28/2022 19:31:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=127
05/28/2022 19:31:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=128
05/28/2022 19:31:46 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.7269649895664272 on epoch=128
05/28/2022 19:31:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/28/2022 19:31:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=129
05/28/2022 19:31:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=129
05/28/2022 19:31:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=130
05/28/2022 19:31:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=131
05/28/2022 19:32:03 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7357612731278211 on epoch=131
05/28/2022 19:32:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/28/2022 19:32:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=132
05/28/2022 19:32:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=133
05/28/2022 19:32:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=133
05/28/2022 19:32:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=134
05/28/2022 19:32:21 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.6949233335981568 on epoch=134
05/28/2022 19:32:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/28/2022 19:32:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=135
05/28/2022 19:32:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/28/2022 19:32:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=136
05/28/2022 19:32:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=137
05/28/2022 19:32:38 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7515367195082496 on epoch=137
05/28/2022 19:32:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
05/28/2022 19:32:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=138
05/28/2022 19:32:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=139
05/28/2022 19:32:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=139
05/28/2022 19:32:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=140
05/28/2022 19:32:55 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.7306828709155714 on epoch=140
05/28/2022 19:32:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/28/2022 19:33:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=141
05/28/2022 19:33:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=142
05/28/2022 19:33:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/28/2022 19:33:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=143
05/28/2022 19:33:13 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.6519403082870576 on epoch=143
05/28/2022 19:33:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=144
05/28/2022 19:33:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/28/2022 19:33:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=145
05/28/2022 19:33:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/28/2022 19:33:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/28/2022 19:33:30 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.6635897435897435 on epoch=146
05/28/2022 19:33:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
05/28/2022 19:33:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=148
05/28/2022 19:33:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/28/2022 19:33:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=149
05/28/2022 19:33:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
05/28/2022 19:33:47 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7449691173540545 on epoch=149
05/28/2022 19:33:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=150
05/28/2022 19:33:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=151
05/28/2022 19:33:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
05/28/2022 19:33:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=152
05/28/2022 19:33:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
05/28/2022 19:34:03 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7446873801304181 on epoch=153
05/28/2022 19:34:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/28/2022 19:34:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/28/2022 19:34:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=154
05/28/2022 19:34:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/28/2022 19:34:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/28/2022 19:34:21 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7182389937106919 on epoch=156
05/28/2022 19:34:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=156
05/28/2022 19:34:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/28/2022 19:34:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/28/2022 19:34:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=158
05/28/2022 19:34:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
05/28/2022 19:34:38 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.7576793893129771 on epoch=159
05/28/2022 19:34:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=159
05/28/2022 19:34:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=160
05/28/2022 19:34:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/28/2022 19:34:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=161
05/28/2022 19:34:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/28/2022 19:34:55 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.72591431719087 on epoch=162
05/28/2022 19:34:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
05/28/2022 19:35:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/28/2022 19:35:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/28/2022 19:35:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/28/2022 19:35:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/28/2022 19:35:13 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.6610694910875743 on epoch=165
05/28/2022 19:35:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=166
05/28/2022 19:35:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=166
05/28/2022 19:35:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=167
05/28/2022 19:35:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
05/28/2022 19:35:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/28/2022 19:35:30 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7313914337170151 on epoch=168
05/28/2022 19:35:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
05/28/2022 19:35:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/28/2022 19:35:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/28/2022 19:35:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/28/2022 19:35:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/28/2022 19:35:47 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7299365464250287 on epoch=171
05/28/2022 19:35:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/28/2022 19:35:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
05/28/2022 19:35:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/28/2022 19:35:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
05/28/2022 19:36:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
05/28/2022 19:36:04 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7528162214354683 on epoch=174
05/28/2022 19:36:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=175
05/28/2022 19:36:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/28/2022 19:36:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/28/2022 19:36:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/28/2022 19:36:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/28/2022 19:36:21 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7350561374898075 on epoch=178
05/28/2022 19:36:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/28/2022 19:36:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/28/2022 19:36:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/28/2022 19:36:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/28/2022 19:36:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
05/28/2022 19:36:39 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7357357357357357 on epoch=181
05/28/2022 19:36:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/28/2022 19:36:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/28/2022 19:36:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/28/2022 19:36:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=183
05/28/2022 19:36:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/28/2022 19:36:55 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7056531459597573 on epoch=184
05/28/2022 19:36:58 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/28/2022 19:37:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/28/2022 19:37:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/28/2022 19:37:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/28/2022 19:37:08 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/28/2022 19:37:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:37:09 - INFO - __main__ - Printing 3 examples
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:37:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:37:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:37:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:37:09 - INFO - __main__ - Printing 3 examples
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:37:09 - INFO - __main__ - ['false']
05/28/2022 19:37:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:37:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:37:10 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:37:12 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7850742623376226 on epoch=187
05/28/2022 19:37:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7848903827056757 -> 0.7850742623376226 on epoch=187, global_step=3000
05/28/2022 19:37:12 - INFO - __main__ - save last model!
05/28/2022 19:37:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 19:37:12 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 19:37:12 - INFO - __main__ - Printing 3 examples
05/28/2022 19:37:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 19:37:12 - INFO - __main__ - ['false']
05/28/2022 19:37:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 19:37:12 - INFO - __main__ - ['false']
05/28/2022 19:37:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 19:37:12 - INFO - __main__ - ['false']
05/28/2022 19:37:12 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:37:14 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:37:16 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 19:37:25 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:37:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:37:26 - INFO - __main__ - Starting training!
05/28/2022 19:38:05 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.5_8_predictions.txt
05/28/2022 19:38:05 - INFO - __main__ - Classification-F1 on test data: 0.5414
05/28/2022 19:38:06 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.5, bsz=8, dev_performance=0.7850742623376226, test_performance=0.5414278032701252
05/28/2022 19:38:06 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.4, bsz=8 ...
05/28/2022 19:38:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:38:07 - INFO - __main__ - Printing 3 examples
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:38:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:38:07 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:38:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:38:07 - INFO - __main__ - Printing 3 examples
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:38:07 - INFO - __main__ - ['false']
05/28/2022 19:38:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:38:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:38:08 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:38:23 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:38:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:38:24 - INFO - __main__ - Starting training!
05/28/2022 19:38:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.24 on epoch=0
05/28/2022 19:38:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=1
05/28/2022 19:38:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
05/28/2022 19:38:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=2
05/28/2022 19:38:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
05/28/2022 19:38:43 - INFO - __main__ - Global step 50 Train loss 0.85 Classification-F1 0.4623177662466024 on epoch=3
05/28/2022 19:38:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4623177662466024 on epoch=3, global_step=50
05/28/2022 19:38:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
05/28/2022 19:38:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=4
05/28/2022 19:38:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
05/28/2022 19:38:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
05/28/2022 19:38:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=6
05/28/2022 19:39:01 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.4944691508661518 on epoch=6
05/28/2022 19:39:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4623177662466024 -> 0.4944691508661518 on epoch=6, global_step=100
05/28/2022 19:39:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
05/28/2022 19:39:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/28/2022 19:39:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=8
05/28/2022 19:39:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=8
05/28/2022 19:39:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
05/28/2022 19:39:19 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3601804002630836 on epoch=9
05/28/2022 19:39:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=9
05/28/2022 19:39:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
05/28/2022 19:39:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=11
05/28/2022 19:39:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/28/2022 19:39:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=12
05/28/2022 19:39:38 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.38904444235647845 on epoch=12
05/28/2022 19:39:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/28/2022 19:39:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
05/28/2022 19:39:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=14
05/28/2022 19:39:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/28/2022 19:39:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=15
05/28/2022 19:39:56 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.5615288471924329 on epoch=15
05/28/2022 19:39:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4944691508661518 -> 0.5615288471924329 on epoch=15, global_step=250
05/28/2022 19:39:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=16
05/28/2022 19:40:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/28/2022 19:40:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/28/2022 19:40:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/28/2022 19:40:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/28/2022 19:40:14 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6185267685690146 on epoch=18
05/28/2022 19:40:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5615288471924329 -> 0.6185267685690146 on epoch=18, global_step=300
05/28/2022 19:40:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
05/28/2022 19:40:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/28/2022 19:40:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
05/28/2022 19:40:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=21
05/28/2022 19:40:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=21
05/28/2022 19:40:32 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6483516483516483 on epoch=21
05/28/2022 19:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6185267685690146 -> 0.6483516483516483 on epoch=21, global_step=350
05/28/2022 19:40:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=22
05/28/2022 19:40:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=23
05/28/2022 19:40:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
05/28/2022 19:40:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
05/28/2022 19:40:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/28/2022 19:40:51 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6294151708164448 on epoch=24
05/28/2022 19:40:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=25
05/28/2022 19:40:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=26
05/28/2022 19:40:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=26
05/28/2022 19:41:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
05/28/2022 19:41:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=28
05/28/2022 19:41:08 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.7176470588235293 on epoch=28
05/28/2022 19:41:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6483516483516483 -> 0.7176470588235293 on epoch=28, global_step=450
05/28/2022 19:41:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=28
05/28/2022 19:41:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/28/2022 19:41:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/28/2022 19:41:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/28/2022 19:41:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.32 on epoch=31
05/28/2022 19:41:26 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.6989912742125131 on epoch=31
05/28/2022 19:41:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=31
05/28/2022 19:41:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/28/2022 19:41:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/28/2022 19:41:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=33
05/28/2022 19:41:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=34
05/28/2022 19:41:44 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.7224487302823461 on epoch=34
05/28/2022 19:41:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7176470588235293 -> 0.7224487302823461 on epoch=34, global_step=550
05/28/2022 19:41:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=34
05/28/2022 19:41:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=35
05/28/2022 19:41:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/28/2022 19:41:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=36
05/28/2022 19:41:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=37
05/28/2022 19:42:01 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.720777325447423 on epoch=37
05/28/2022 19:42:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=38
05/28/2022 19:42:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/28/2022 19:42:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=39
05/28/2022 19:42:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=39
05/28/2022 19:42:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=40
05/28/2022 19:42:18 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.671560940841055 on epoch=40
05/28/2022 19:42:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/28/2022 19:42:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=41
05/28/2022 19:42:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=42
05/28/2022 19:42:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/28/2022 19:42:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=43
05/28/2022 19:42:35 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.6842105263157895 on epoch=43
05/28/2022 19:42:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=44
05/28/2022 19:42:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=44
05/28/2022 19:42:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=45
05/28/2022 19:42:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=46
05/28/2022 19:42:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=46
05/28/2022 19:42:53 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.7098039215686275 on epoch=46
05/28/2022 19:42:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
05/28/2022 19:42:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=48
05/28/2022 19:43:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/28/2022 19:43:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=49
05/28/2022 19:43:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=49
05/28/2022 19:43:10 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.7456240923335629 on epoch=49
05/28/2022 19:43:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7224487302823461 -> 0.7456240923335629 on epoch=49, global_step=800
05/28/2022 19:43:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/28/2022 19:43:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=51
05/28/2022 19:43:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=51
05/28/2022 19:43:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=52
05/28/2022 19:43:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=53
05/28/2022 19:43:27 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.7227551209852094 on epoch=53
05/28/2022 19:43:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=53
05/28/2022 19:43:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=54
05/28/2022 19:43:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
05/28/2022 19:43:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=55
05/28/2022 19:43:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
05/28/2022 19:43:44 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.7456240923335629 on epoch=56
05/28/2022 19:43:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=56
05/28/2022 19:43:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=57
05/28/2022 19:43:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
05/28/2022 19:43:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=58
05/28/2022 19:43:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=59
05/28/2022 19:44:02 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.7371220132726408 on epoch=59
05/28/2022 19:44:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/28/2022 19:44:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=60
05/28/2022 19:44:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/28/2022 19:44:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=61
05/28/2022 19:44:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=62
05/28/2022 19:44:19 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.6832824367708088 on epoch=62
05/28/2022 19:44:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
05/28/2022 19:44:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
05/28/2022 19:44:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/28/2022 19:44:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=64
05/28/2022 19:44:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=65
05/28/2022 19:44:36 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.6851466414380321 on epoch=65
05/28/2022 19:44:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=66
05/28/2022 19:44:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
05/28/2022 19:44:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=67
05/28/2022 19:44:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=68
05/28/2022 19:44:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/28/2022 19:44:53 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.5952364933055239 on epoch=68
05/28/2022 19:44:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=69
05/28/2022 19:44:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=69
05/28/2022 19:45:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=70
05/28/2022 19:45:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=71
05/28/2022 19:45:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/28/2022 19:45:10 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.6811242414564036 on epoch=71
05/28/2022 19:45:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=72
05/28/2022 19:45:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=73
05/28/2022 19:45:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=73
05/28/2022 19:45:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=74
05/28/2022 19:45:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=74
05/28/2022 19:45:28 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.7440275662620949 on epoch=74
05/28/2022 19:45:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=75
05/28/2022 19:45:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/28/2022 19:45:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=76
05/28/2022 19:45:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=77
05/28/2022 19:45:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=78
05/28/2022 19:45:45 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.7371220132726408 on epoch=78
05/28/2022 19:45:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
05/28/2022 19:45:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/28/2022 19:45:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=79
05/28/2022 19:45:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/28/2022 19:45:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/28/2022 19:46:02 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.736986301369863 on epoch=81
05/28/2022 19:46:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=81
05/28/2022 19:46:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/28/2022 19:46:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/28/2022 19:46:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/28/2022 19:46:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
05/28/2022 19:46:19 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.7283904387443051 on epoch=84
05/28/2022 19:46:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=84
05/28/2022 19:46:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
05/28/2022 19:46:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
05/28/2022 19:46:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=86
05/28/2022 19:46:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=87
05/28/2022 19:46:37 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.7226435536294691 on epoch=87
05/28/2022 19:46:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/28/2022 19:46:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/28/2022 19:46:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
05/28/2022 19:46:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/28/2022 19:46:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=90
05/28/2022 19:46:54 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.6519403082870576 on epoch=90
05/28/2022 19:46:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=91
05/28/2022 19:46:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
05/28/2022 19:47:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=92
05/28/2022 19:47:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=93
05/28/2022 19:47:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/28/2022 19:47:11 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.7138133551668214 on epoch=93
05/28/2022 19:47:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=94
05/28/2022 19:47:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=94
05/28/2022 19:47:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/28/2022 19:47:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=96
05/28/2022 19:47:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
05/28/2022 19:47:28 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.7477832512315271 on epoch=96
05/28/2022 19:47:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7456240923335629 -> 0.7477832512315271 on epoch=96, global_step=1550
05/28/2022 19:47:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=97
05/28/2022 19:47:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=98
05/28/2022 19:47:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/28/2022 19:47:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=99
05/28/2022 19:47:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/28/2022 19:47:45 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7080684214637194 on epoch=99
05/28/2022 19:47:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=100
05/28/2022 19:47:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/28/2022 19:47:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=101
05/28/2022 19:47:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/28/2022 19:47:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=103
05/28/2022 19:48:02 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.7326974924805585 on epoch=103
05/28/2022 19:48:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/28/2022 19:48:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=104
05/28/2022 19:48:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/28/2022 19:48:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=105
05/28/2022 19:48:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/28/2022 19:48:20 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.641958041958042 on epoch=106
05/28/2022 19:48:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=106
05/28/2022 19:48:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=107
05/28/2022 19:48:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/28/2022 19:48:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/28/2022 19:48:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=109
05/28/2022 19:48:37 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.7012929675181331 on epoch=109
05/28/2022 19:48:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/28/2022 19:48:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/28/2022 19:48:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
05/28/2022 19:48:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/28/2022 19:48:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/28/2022 19:48:54 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.7167391338226814 on epoch=112
05/28/2022 19:48:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=113
05/28/2022 19:48:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=113
05/28/2022 19:49:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=114
05/28/2022 19:49:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=114
05/28/2022 19:49:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
05/28/2022 19:49:12 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.7012929675181331 on epoch=115
05/28/2022 19:49:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
05/28/2022 19:49:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=116
05/28/2022 19:49:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=117
05/28/2022 19:49:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=118
05/28/2022 19:49:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/28/2022 19:49:29 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.6728302192024589 on epoch=118
05/28/2022 19:49:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=119
05/28/2022 19:49:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=119
05/28/2022 19:49:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
05/28/2022 19:49:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/28/2022 19:49:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/28/2022 19:49:46 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.7117117117117117 on epoch=121
05/28/2022 19:49:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/28/2022 19:49:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/28/2022 19:49:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
05/28/2022 19:49:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=124
05/28/2022 19:49:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=124
05/28/2022 19:50:04 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.6682823823007151 on epoch=124
05/28/2022 19:50:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
05/28/2022 19:50:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=126
05/28/2022 19:50:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=126
05/28/2022 19:50:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=127
05/28/2022 19:50:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=128
05/28/2022 19:50:21 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.6669839456818327 on epoch=128
05/28/2022 19:50:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=128
05/28/2022 19:50:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=129
05/28/2022 19:50:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=129
05/28/2022 19:50:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=130
05/28/2022 19:50:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/28/2022 19:50:39 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.7087599544937428 on epoch=131
05/28/2022 19:50:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=131
05/28/2022 19:50:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=132
05/28/2022 19:50:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=133
05/28/2022 19:50:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
05/28/2022 19:50:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/28/2022 19:50:56 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.714833838727644 on epoch=134
05/28/2022 19:50:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/28/2022 19:51:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
05/28/2022 19:51:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=136
05/28/2022 19:51:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=136
05/28/2022 19:51:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=137
05/28/2022 19:51:13 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.6635897435897435 on epoch=137
05/28/2022 19:51:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/28/2022 19:51:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/28/2022 19:51:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=139
05/28/2022 19:51:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/28/2022 19:51:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=140
05/28/2022 19:51:31 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.649343324470228 on epoch=140
05/28/2022 19:51:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/28/2022 19:51:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/28/2022 19:51:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/28/2022 19:51:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/28/2022 19:51:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=143
05/28/2022 19:51:49 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.6575940697768109 on epoch=143
05/28/2022 19:51:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=144
05/28/2022 19:51:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/28/2022 19:51:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/28/2022 19:51:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/28/2022 19:52:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
05/28/2022 19:52:06 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.6949233335981568 on epoch=146
05/28/2022 19:52:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=147
05/28/2022 19:52:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=148
05/28/2022 19:52:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=148
05/28/2022 19:52:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
05/28/2022 19:52:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/28/2022 19:52:23 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.72591431719087 on epoch=149
05/28/2022 19:52:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=150
05/28/2022 19:52:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
05/28/2022 19:52:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/28/2022 19:52:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
05/28/2022 19:52:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/28/2022 19:52:40 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.6610694910875743 on epoch=153
05/28/2022 19:52:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
05/28/2022 19:52:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/28/2022 19:52:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/28/2022 19:52:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/28/2022 19:52:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=156
05/28/2022 19:52:57 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.6705912629479509 on epoch=156
05/28/2022 19:53:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=156
05/28/2022 19:53:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/28/2022 19:53:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=158
05/28/2022 19:53:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=158
05/28/2022 19:53:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
05/28/2022 19:53:15 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.696811786441802 on epoch=159
05/28/2022 19:53:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=159
05/28/2022 19:53:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/28/2022 19:53:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=161
05/28/2022 19:53:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/28/2022 19:53:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
05/28/2022 19:53:32 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.7012929675181331 on epoch=162
05/28/2022 19:53:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
05/28/2022 19:53:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/28/2022 19:53:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/28/2022 19:53:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/28/2022 19:53:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/28/2022 19:53:49 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.7290161892901619 on epoch=165
05/28/2022 19:53:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=166
05/28/2022 19:53:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
05/28/2022 19:53:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/28/2022 19:53:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/28/2022 19:54:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/28/2022 19:54:06 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.6583333333333333 on epoch=168
05/28/2022 19:54:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/28/2022 19:54:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/28/2022 19:54:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/28/2022 19:54:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/28/2022 19:54:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
05/28/2022 19:54:24 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7216526871699285 on epoch=171
05/28/2022 19:54:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/28/2022 19:54:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/28/2022 19:54:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/28/2022 19:54:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
05/28/2022 19:54:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=174
05/28/2022 19:54:41 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7203993416093651 on epoch=174
05/28/2022 19:54:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/28/2022 19:54:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
05/28/2022 19:54:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=176
05/28/2022 19:54:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
05/28/2022 19:54:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/28/2022 19:54:58 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7092351746092688 on epoch=178
05/28/2022 19:55:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
05/28/2022 19:55:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/28/2022 19:55:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
05/28/2022 19:55:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/28/2022 19:55:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
05/28/2022 19:55:16 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.7117117117117117 on epoch=181
05/28/2022 19:55:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=181
05/28/2022 19:55:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/28/2022 19:55:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/28/2022 19:55:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
05/28/2022 19:55:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/28/2022 19:55:33 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.698497580850522 on epoch=184
05/28/2022 19:55:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=184
05/28/2022 19:55:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/28/2022 19:55:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=186
05/28/2022 19:55:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/28/2022 19:55:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/28/2022 19:55:46 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:55:46 - INFO - __main__ - Printing 3 examples
05/28/2022 19:55:46 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:55:46 - INFO - __main__ - ['false']
05/28/2022 19:55:46 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:55:46 - INFO - __main__ - ['false']
05/28/2022 19:55:46 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:55:46 - INFO - __main__ - ['false']
05/28/2022 19:55:46 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:55:46 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:55:47 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:55:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:55:47 - INFO - __main__ - Printing 3 examples
05/28/2022 19:55:47 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:55:47 - INFO - __main__ - ['false']
05/28/2022 19:55:47 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:55:47 - INFO - __main__ - ['false']
05/28/2022 19:55:47 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:55:47 - INFO - __main__ - ['false']
05/28/2022 19:55:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:55:47 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:55:47 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:55:50 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7012929675181331 on epoch=187
05/28/2022 19:55:50 - INFO - __main__ - save last model!
05/28/2022 19:55:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 19:55:50 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 19:55:50 - INFO - __main__ - Printing 3 examples
05/28/2022 19:55:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 19:55:50 - INFO - __main__ - ['false']
05/28/2022 19:55:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 19:55:50 - INFO - __main__ - ['false']
05/28/2022 19:55:50 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 19:55:50 - INFO - __main__ - ['false']
05/28/2022 19:55:50 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:55:51 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:55:54 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 19:56:02 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:56:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:56:03 - INFO - __main__ - Starting training!
05/28/2022 19:56:44 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.4_8_predictions.txt
05/28/2022 19:56:44 - INFO - __main__ - Classification-F1 on test data: 0.4516
05/28/2022 19:56:44 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.4, bsz=8, dev_performance=0.7477832512315271, test_performance=0.45158939824033023
05/28/2022 19:56:44 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.3, bsz=8 ...
05/28/2022 19:56:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:56:45 - INFO - __main__ - Printing 3 examples
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:56:45 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:56:45 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 19:56:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 19:56:45 - INFO - __main__ - Printing 3 examples
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 19:56:45 - INFO - __main__ - ['false']
05/28/2022 19:56:45 - INFO - __main__ - Tokenizing Input ...
05/28/2022 19:56:45 - INFO - __main__ - Tokenizing Output ...
05/28/2022 19:56:46 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 19:57:01 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 19:57:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 19:57:02 - INFO - __main__ - Starting training!
05/28/2022 19:57:05 - INFO - __main__ - Step 10 Global step 10 Train loss 2.10 on epoch=0
05/28/2022 19:57:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=1
05/28/2022 19:57:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
05/28/2022 19:57:13 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=2
05/28/2022 19:57:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=3
05/28/2022 19:57:21 - INFO - __main__ - Global step 50 Train loss 0.82 Classification-F1 0.4336574574265576 on epoch=3
05/28/2022 19:57:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4336574574265576 on epoch=3, global_step=50
05/28/2022 19:57:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=3
05/28/2022 19:57:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
05/28/2022 19:57:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/28/2022 19:57:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
05/28/2022 19:57:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/28/2022 19:57:39 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3491235113073732 on epoch=6
05/28/2022 19:57:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=6
05/28/2022 19:57:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=7
05/28/2022 19:57:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
05/28/2022 19:57:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=8
05/28/2022 19:57:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
05/28/2022 19:57:58 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.33469557799511973 on epoch=9
05/28/2022 19:58:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/28/2022 19:58:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
05/28/2022 19:58:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
05/28/2022 19:58:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/28/2022 19:58:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
05/28/2022 19:58:16 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3328595119639896 on epoch=12
05/28/2022 19:58:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
05/28/2022 19:58:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=13
05/28/2022 19:58:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/28/2022 19:58:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=14
05/28/2022 19:58:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
05/28/2022 19:58:34 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.33652312599681017 on epoch=15
05/28/2022 19:58:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
05/28/2022 19:58:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=16
05/28/2022 19:58:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
05/28/2022 19:58:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/28/2022 19:58:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/28/2022 19:58:52 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.580591324585445 on epoch=18
05/28/2022 19:58:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4336574574265576 -> 0.580591324585445 on epoch=18, global_step=300
05/28/2022 19:58:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/28/2022 19:58:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
05/28/2022 19:59:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 19:59:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=21
05/28/2022 19:59:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
05/28/2022 19:59:10 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.578502640571606 on epoch=21
05/28/2022 19:59:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=22
05/28/2022 19:59:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/28/2022 19:59:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/28/2022 19:59:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/28/2022 19:59:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/28/2022 19:59:29 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.5630522088353414 on epoch=24
05/28/2022 19:59:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
05/28/2022 19:59:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
05/28/2022 19:59:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=26
05/28/2022 19:59:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
05/28/2022 19:59:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/28/2022 19:59:47 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.6485052736691006 on epoch=28
05/28/2022 19:59:47 - INFO - __main__ - Saving model with best Classification-F1: 0.580591324585445 -> 0.6485052736691006 on epoch=28, global_step=450
05/28/2022 19:59:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/28/2022 19:59:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=29
05/28/2022 19:59:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
05/28/2022 19:59:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=30
05/28/2022 19:59:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=31
05/28/2022 20:00:05 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.6682823823007151 on epoch=31
05/28/2022 20:00:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6485052736691006 -> 0.6682823823007151 on epoch=31, global_step=500
05/28/2022 20:00:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
05/28/2022 20:00:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
05/28/2022 20:00:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=33
05/28/2022 20:00:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=33
05/28/2022 20:00:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
05/28/2022 20:00:23 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.6514296829410621 on epoch=34
05/28/2022 20:00:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/28/2022 20:00:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=35
05/28/2022 20:00:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/28/2022 20:00:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/28/2022 20:00:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/28/2022 20:00:41 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6958897600380138 on epoch=37
05/28/2022 20:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6682823823007151 -> 0.6958897600380138 on epoch=37, global_step=600
05/28/2022 20:00:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=38
05/28/2022 20:00:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/28/2022 20:00:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=39
05/28/2022 20:00:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=39
05/28/2022 20:00:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/28/2022 20:00:59 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.6912884859031307 on epoch=40
05/28/2022 20:01:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/28/2022 20:01:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/28/2022 20:01:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=42
05/28/2022 20:01:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=43
05/28/2022 20:01:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/28/2022 20:01:16 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.6977943460702081 on epoch=43
05/28/2022 20:01:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6958897600380138 -> 0.6977943460702081 on epoch=43, global_step=700
05/28/2022 20:01:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=44
05/28/2022 20:01:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=44
05/28/2022 20:01:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/28/2022 20:01:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=46
05/28/2022 20:01:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=46
05/28/2022 20:01:35 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.7183201907207042 on epoch=46
05/28/2022 20:01:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6977943460702081 -> 0.7183201907207042 on epoch=46, global_step=750
05/28/2022 20:01:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/28/2022 20:01:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
05/28/2022 20:01:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=48
05/28/2022 20:01:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=49
05/28/2022 20:01:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=49
05/28/2022 20:01:52 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7179063360881542 on epoch=49
05/28/2022 20:01:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/28/2022 20:01:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=51
05/28/2022 20:02:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=51
05/28/2022 20:02:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=52
05/28/2022 20:02:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
05/28/2022 20:02:10 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.7297719032539354 on epoch=53
05/28/2022 20:02:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7183201907207042 -> 0.7297719032539354 on epoch=53, global_step=850
05/28/2022 20:02:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=53
05/28/2022 20:02:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
05/28/2022 20:02:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=54
05/28/2022 20:02:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
05/28/2022 20:02:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=56
05/28/2022 20:02:27 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7297719032539354 on epoch=56
05/28/2022 20:02:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=56
05/28/2022 20:02:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=57
05/28/2022 20:02:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
05/28/2022 20:02:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=58
05/28/2022 20:02:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=59
05/28/2022 20:02:44 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.7252039501932159 on epoch=59
05/28/2022 20:02:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=59
05/28/2022 20:02:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=60
05/28/2022 20:02:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=61
05/28/2022 20:02:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=61
05/28/2022 20:02:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=62
05/28/2022 20:03:01 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.7576793893129771 on epoch=62
05/28/2022 20:03:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7297719032539354 -> 0.7576793893129771 on epoch=62, global_step=1000
05/28/2022 20:03:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=63
05/28/2022 20:03:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=63
05/28/2022 20:03:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/28/2022 20:03:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=64
05/28/2022 20:03:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=65
05/28/2022 20:03:18 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.6956956956956957 on epoch=65
05/28/2022 20:03:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=66
05/28/2022 20:03:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=66
05/28/2022 20:03:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=67
05/28/2022 20:03:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.26 on epoch=68
05/28/2022 20:03:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/28/2022 20:03:34 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.7106000774293457 on epoch=68
05/28/2022 20:03:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=69
05/28/2022 20:03:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=69
05/28/2022 20:03:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=70
05/28/2022 20:03:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=71
05/28/2022 20:03:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/28/2022 20:03:51 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.6857493583305434 on epoch=71
05/28/2022 20:03:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=72
05/28/2022 20:03:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=73
05/28/2022 20:03:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=73
05/28/2022 20:04:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=74
05/28/2022 20:04:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=74
05/28/2022 20:04:07 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.7812366477446133 on epoch=74
05/28/2022 20:04:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7576793893129771 -> 0.7812366477446133 on epoch=74, global_step=1200
05/28/2022 20:04:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=75
05/28/2022 20:04:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=76
05/28/2022 20:04:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
05/28/2022 20:04:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
05/28/2022 20:04:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
05/28/2022 20:04:23 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.6842105263157895 on epoch=78
05/28/2022 20:04:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/28/2022 20:04:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=79
05/28/2022 20:04:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=79
05/28/2022 20:04:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=80
05/28/2022 20:04:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=81
05/28/2022 20:04:40 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.7577533577533577 on epoch=81
05/28/2022 20:04:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=81
05/28/2022 20:04:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=82
05/28/2022 20:04:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=83
05/28/2022 20:04:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=83
05/28/2022 20:04:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=84
05/28/2022 20:04:56 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.7484647506755098 on epoch=84
05/28/2022 20:04:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=84
05/28/2022 20:05:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=85
05/28/2022 20:05:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
05/28/2022 20:05:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=86
05/28/2022 20:05:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=87
05/28/2022 20:05:13 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.7316065371569533 on epoch=87
05/28/2022 20:05:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=88
05/28/2022 20:05:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=88
05/28/2022 20:05:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=89
05/28/2022 20:05:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=89
05/28/2022 20:05:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/28/2022 20:05:30 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.6811242414564036 on epoch=90
05/28/2022 20:05:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=91
05/28/2022 20:05:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=91
05/28/2022 20:05:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
05/28/2022 20:05:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=93
05/28/2022 20:05:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/28/2022 20:05:46 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.703999366437 on epoch=93
05/28/2022 20:05:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=94
05/28/2022 20:05:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
05/28/2022 20:05:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/28/2022 20:05:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=96
05/28/2022 20:05:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=96
05/28/2022 20:06:03 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.703999366437 on epoch=96
05/28/2022 20:06:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/28/2022 20:06:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/28/2022 20:06:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=98
05/28/2022 20:06:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=99
05/28/2022 20:06:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=99
05/28/2022 20:06:20 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.7682546988876103 on epoch=99
05/28/2022 20:06:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=100
05/28/2022 20:06:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=101
05/28/2022 20:06:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=101
05/28/2022 20:06:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/28/2022 20:06:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/28/2022 20:06:36 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.6913509740465764 on epoch=103
05/28/2022 20:06:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/28/2022 20:06:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=104
05/28/2022 20:06:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
05/28/2022 20:06:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=105
05/28/2022 20:06:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/28/2022 20:06:52 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.7552883132901633 on epoch=106
05/28/2022 20:06:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=106
05/28/2022 20:06:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/28/2022 20:07:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
05/28/2022 20:07:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/28/2022 20:07:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
05/28/2022 20:07:09 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.7332960146786709 on epoch=109
05/28/2022 20:07:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/28/2022 20:07:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
05/28/2022 20:07:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=111
05/28/2022 20:07:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=111
05/28/2022 20:07:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=112
05/28/2022 20:07:26 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.7343863724351529 on epoch=112
05/28/2022 20:07:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=113
05/28/2022 20:07:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
05/28/2022 20:07:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
05/28/2022 20:07:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=114
05/28/2022 20:07:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=115
05/28/2022 20:07:42 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7093661305581835 on epoch=115
05/28/2022 20:07:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/28/2022 20:07:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/28/2022 20:07:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
05/28/2022 20:07:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
05/28/2022 20:07:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/28/2022 20:07:59 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.6964102564102563 on epoch=118
05/28/2022 20:08:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=119
05/28/2022 20:08:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=119
05/28/2022 20:08:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/28/2022 20:08:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
05/28/2022 20:08:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/28/2022 20:08:16 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.7338588297360622 on epoch=121
05/28/2022 20:08:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=122
05/28/2022 20:08:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=123
05/28/2022 20:08:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=123
05/28/2022 20:08:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=124
05/28/2022 20:08:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=124
05/28/2022 20:08:33 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7548801581418334 on epoch=124
05/28/2022 20:08:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
05/28/2022 20:08:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=126
05/28/2022 20:08:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=126
05/28/2022 20:08:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=127
05/28/2022 20:08:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
05/28/2022 20:08:49 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.769104945348926 on epoch=128
05/28/2022 20:08:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
05/28/2022 20:08:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/28/2022 20:08:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=129
05/28/2022 20:08:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/28/2022 20:09:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/28/2022 20:09:06 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.729605467536502 on epoch=131
05/28/2022 20:09:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
05/28/2022 20:09:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
05/28/2022 20:09:13 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/28/2022 20:09:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
05/28/2022 20:09:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/28/2022 20:09:22 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7597797160305814 on epoch=134
05/28/2022 20:09:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=134
05/28/2022 20:09:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=135
05/28/2022 20:09:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
05/28/2022 20:09:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=136
05/28/2022 20:09:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/28/2022 20:09:39 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.6923076923076923 on epoch=137
05/28/2022 20:09:42 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
05/28/2022 20:09:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/28/2022 20:09:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/28/2022 20:09:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=139
05/28/2022 20:09:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=140
05/28/2022 20:09:56 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.7101886792452831 on epoch=140
05/28/2022 20:09:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/28/2022 20:10:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=141
05/28/2022 20:10:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=142
05/28/2022 20:10:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/28/2022 20:10:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=143
05/28/2022 20:10:14 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.6736188296829029 on epoch=143
05/28/2022 20:10:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
05/28/2022 20:10:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=144
05/28/2022 20:10:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=145
05/28/2022 20:10:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/28/2022 20:10:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
05/28/2022 20:10:31 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.7138133551668214 on epoch=146
05/28/2022 20:10:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
05/28/2022 20:10:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
05/28/2022 20:10:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/28/2022 20:10:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
05/28/2022 20:10:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/28/2022 20:10:48 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.7238403451995685 on epoch=149
05/28/2022 20:10:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/28/2022 20:10:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/28/2022 20:10:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=151
05/28/2022 20:10:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/28/2022 20:11:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/28/2022 20:11:05 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7270275355955591 on epoch=153
05/28/2022 20:11:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=153
05/28/2022 20:11:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
05/28/2022 20:11:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/28/2022 20:11:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/28/2022 20:11:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
05/28/2022 20:11:23 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.740676671809497 on epoch=156
05/28/2022 20:11:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
05/28/2022 20:11:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=157
05/28/2022 20:11:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=158
05/28/2022 20:11:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/28/2022 20:11:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/28/2022 20:11:40 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.7758353176127197 on epoch=159
05/28/2022 20:11:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
05/28/2022 20:11:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/28/2022 20:11:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/28/2022 20:11:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=161
05/28/2022 20:11:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
05/28/2022 20:11:57 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7165991902834008 on epoch=162
05/28/2022 20:12:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=163
05/28/2022 20:12:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=163
05/28/2022 20:12:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/28/2022 20:12:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
05/28/2022 20:12:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
05/28/2022 20:12:14 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.7597797160305814 on epoch=165
05/28/2022 20:12:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=166
05/28/2022 20:12:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/28/2022 20:12:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/28/2022 20:12:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/28/2022 20:12:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
05/28/2022 20:12:31 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.6905970510031424 on epoch=168
05/28/2022 20:12:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
05/28/2022 20:12:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
05/28/2022 20:12:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
05/28/2022 20:12:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
05/28/2022 20:12:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/28/2022 20:12:48 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.7075956596230726 on epoch=171
05/28/2022 20:12:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/28/2022 20:12:53 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/28/2022 20:12:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/28/2022 20:12:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/28/2022 20:13:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=174
05/28/2022 20:13:05 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.740676671809497 on epoch=174
05/28/2022 20:13:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
05/28/2022 20:13:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=176
05/28/2022 20:13:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
05/28/2022 20:13:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=177
05/28/2022 20:13:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/28/2022 20:13:23 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.740676671809497 on epoch=178
05/28/2022 20:13:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
05/28/2022 20:13:28 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=179
05/28/2022 20:13:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=179
05/28/2022 20:13:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
05/28/2022 20:13:35 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
05/28/2022 20:13:40 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.6939117126596203 on epoch=181
05/28/2022 20:13:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
05/28/2022 20:13:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
05/28/2022 20:13:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=183
05/28/2022 20:13:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
05/28/2022 20:13:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
05/28/2022 20:13:57 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.7326974924805585 on epoch=184
05/28/2022 20:14:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
05/28/2022 20:14:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/28/2022 20:14:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/28/2022 20:14:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/28/2022 20:14:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/28/2022 20:14:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:14:11 - INFO - __main__ - Printing 3 examples
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:14:11 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:14:11 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:14:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:14:11 - INFO - __main__ - Printing 3 examples
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 20:14:11 - INFO - __main__ - ['false']
05/28/2022 20:14:11 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:14:12 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:14:12 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:14:15 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7290161892901619 on epoch=187
05/28/2022 20:14:15 - INFO - __main__ - save last model!
05/28/2022 20:14:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 20:14:15 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 20:14:15 - INFO - __main__ - Printing 3 examples
05/28/2022 20:14:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 20:14:15 - INFO - __main__ - ['false']
05/28/2022 20:14:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 20:14:15 - INFO - __main__ - ['false']
05/28/2022 20:14:15 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 20:14:15 - INFO - __main__ - ['false']
05/28/2022 20:14:15 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:14:16 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:14:19 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 20:14:27 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:14:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:14:28 - INFO - __main__ - Starting training!
05/28/2022 20:15:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.3_8_predictions.txt
05/28/2022 20:15:08 - INFO - __main__ - Classification-F1 on test data: 0.4788
05/28/2022 20:15:08 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.3, bsz=8, dev_performance=0.7812366477446133, test_performance=0.4787595015824482
05/28/2022 20:15:08 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.2, bsz=8 ...
05/28/2022 20:15:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:15:09 - INFO - __main__ - Printing 3 examples
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:15:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:15:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:15:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:15:09 - INFO - __main__ - Printing 3 examples
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
05/28/2022 20:15:09 - INFO - __main__ - ['false']
05/28/2022 20:15:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:15:10 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:15:10 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:15:26 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:15:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:15:26 - INFO - __main__ - Starting training!
05/28/2022 20:15:29 - INFO - __main__ - Step 10 Global step 10 Train loss 2.78 on epoch=0
05/28/2022 20:15:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.12 on epoch=1
05/28/2022 20:15:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=1
05/28/2022 20:15:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=2
05/28/2022 20:15:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/28/2022 20:15:45 - INFO - __main__ - Global step 50 Train loss 1.11 Classification-F1 0.3511520737327189 on epoch=3
05/28/2022 20:15:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3511520737327189 on epoch=3, global_step=50
05/28/2022 20:15:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=3
05/28/2022 20:15:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
05/28/2022 20:15:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
05/28/2022 20:15:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
05/28/2022 20:15:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
05/28/2022 20:16:03 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3894441713547072 on epoch=6
05/28/2022 20:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3511520737327189 -> 0.3894441713547072 on epoch=6, global_step=100
05/28/2022 20:16:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=6
05/28/2022 20:16:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=7
05/28/2022 20:16:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
05/28/2022 20:16:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=8
05/28/2022 20:16:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=9
05/28/2022 20:16:21 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.32453825857519786 on epoch=9
05/28/2022 20:16:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=9
05/28/2022 20:16:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
05/28/2022 20:16:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/28/2022 20:16:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=11
05/28/2022 20:16:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
05/28/2022 20:16:39 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.33159268929503916 on epoch=12
05/28/2022 20:16:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/28/2022 20:16:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/28/2022 20:16:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/28/2022 20:16:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/28/2022 20:16:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/28/2022 20:16:58 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.38942814354650845 on epoch=15
05/28/2022 20:17:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=16
05/28/2022 20:17:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=16
05/28/2022 20:17:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=17
05/28/2022 20:17:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
05/28/2022 20:17:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
05/28/2022 20:17:16 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.5143399174199308 on epoch=18
05/28/2022 20:17:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3894441713547072 -> 0.5143399174199308 on epoch=18, global_step=300
05/28/2022 20:17:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
05/28/2022 20:17:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/28/2022 20:17:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=20
05/28/2022 20:17:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=21
05/28/2022 20:17:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=21
05/28/2022 20:17:34 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.5158730158730159 on epoch=21
05/28/2022 20:17:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5143399174199308 -> 0.5158730158730159 on epoch=21, global_step=350
05/28/2022 20:17:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=22
05/28/2022 20:17:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/28/2022 20:17:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/28/2022 20:17:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=24
05/28/2022 20:17:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/28/2022 20:17:53 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.4178268345443452 on epoch=24
05/28/2022 20:17:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=25
05/28/2022 20:17:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=26
05/28/2022 20:18:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 20:18:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=27
05/28/2022 20:18:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/28/2022 20:18:11 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.5292881534016286 on epoch=28
05/28/2022 20:18:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5158730158730159 -> 0.5292881534016286 on epoch=28, global_step=450
05/28/2022 20:18:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=28
05/28/2022 20:18:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=29
05/28/2022 20:18:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/28/2022 20:18:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=30
05/28/2022 20:18:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
05/28/2022 20:18:29 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.5460992907801419 on epoch=31
05/28/2022 20:18:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5292881534016286 -> 0.5460992907801419 on epoch=31, global_step=500
05/28/2022 20:18:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/28/2022 20:18:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
05/28/2022 20:18:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/28/2022 20:18:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/28/2022 20:18:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
05/28/2022 20:18:47 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.5473032714412025 on epoch=34
05/28/2022 20:18:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5460992907801419 -> 0.5473032714412025 on epoch=34, global_step=550
05/28/2022 20:18:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=34
05/28/2022 20:18:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/28/2022 20:18:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
05/28/2022 20:18:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/28/2022 20:19:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/28/2022 20:19:06 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.45827145827145827 on epoch=37
05/28/2022 20:19:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/28/2022 20:19:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/28/2022 20:19:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
05/28/2022 20:19:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=39
05/28/2022 20:19:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
05/28/2022 20:19:24 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.620450523697929 on epoch=40
05/28/2022 20:19:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5473032714412025 -> 0.620450523697929 on epoch=40, global_step=650
05/28/2022 20:19:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=41
05/28/2022 20:19:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/28/2022 20:19:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/28/2022 20:19:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/28/2022 20:19:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/28/2022 20:19:42 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6757763027389945 on epoch=43
05/28/2022 20:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.620450523697929 -> 0.6757763027389945 on epoch=43, global_step=700
05/28/2022 20:19:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
05/28/2022 20:19:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/28/2022 20:19:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=45
05/28/2022 20:19:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/28/2022 20:19:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/28/2022 20:20:00 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.6705882352941177 on epoch=46
05/28/2022 20:20:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
05/28/2022 20:20:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
05/28/2022 20:20:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
05/28/2022 20:20:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=49
05/28/2022 20:20:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/28/2022 20:20:19 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.7044248892217437 on epoch=49
05/28/2022 20:20:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6757763027389945 -> 0.7044248892217437 on epoch=49, global_step=800
05/28/2022 20:20:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=50
05/28/2022 20:20:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=51
05/28/2022 20:20:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
05/28/2022 20:20:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/28/2022 20:20:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/28/2022 20:20:37 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.70267131242741 on epoch=53
05/28/2022 20:20:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/28/2022 20:20:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=54
05/28/2022 20:20:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=54
05/28/2022 20:20:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=55
05/28/2022 20:20:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
05/28/2022 20:20:55 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.7042103803669639 on epoch=56
05/28/2022 20:20:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=56
05/28/2022 20:21:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
05/28/2022 20:21:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
05/28/2022 20:21:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
05/28/2022 20:21:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=59
05/28/2022 20:21:13 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.7057335969469861 on epoch=59
05/28/2022 20:21:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7044248892217437 -> 0.7057335969469861 on epoch=59, global_step=950
05/28/2022 20:21:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
05/28/2022 20:21:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
05/28/2022 20:21:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/28/2022 20:21:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/28/2022 20:21:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=62
05/28/2022 20:21:30 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.7185954198473283 on epoch=62
05/28/2022 20:21:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7057335969469861 -> 0.7185954198473283 on epoch=62, global_step=1000
05/28/2022 20:21:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=63
05/28/2022 20:21:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
05/28/2022 20:21:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
05/28/2022 20:21:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=64
05/28/2022 20:21:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
05/28/2022 20:21:47 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7223130394927814 on epoch=65
05/28/2022 20:21:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7185954198473283 -> 0.7223130394927814 on epoch=65, global_step=1050
05/28/2022 20:21:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
05/28/2022 20:21:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/28/2022 20:21:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=67
05/28/2022 20:21:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=68
05/28/2022 20:21:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=68
05/28/2022 20:22:04 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.7009161901248231 on epoch=68
05/28/2022 20:22:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=69
05/28/2022 20:22:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=69
05/28/2022 20:22:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/28/2022 20:22:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=71
05/28/2022 20:22:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/28/2022 20:22:21 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.7060219564851251 on epoch=71
05/28/2022 20:22:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=72
05/28/2022 20:22:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
05/28/2022 20:22:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
05/28/2022 20:22:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=74
05/28/2022 20:22:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=74
05/28/2022 20:22:38 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.7302670759081955 on epoch=74
05/28/2022 20:22:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7223130394927814 -> 0.7302670759081955 on epoch=74, global_step=1200
05/28/2022 20:22:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
05/28/2022 20:22:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
05/28/2022 20:22:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=76
05/28/2022 20:22:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=77
05/28/2022 20:22:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=78
05/28/2022 20:22:55 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.7176470588235293 on epoch=78
05/28/2022 20:22:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=78
05/28/2022 20:23:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=79
05/28/2022 20:23:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=79
05/28/2022 20:23:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=80
05/28/2022 20:23:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
05/28/2022 20:23:12 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.7343587865470305 on epoch=81
05/28/2022 20:23:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7302670759081955 -> 0.7343587865470305 on epoch=81, global_step=1300
05/28/2022 20:23:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=81
05/28/2022 20:23:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=82
05/28/2022 20:23:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=83
05/28/2022 20:23:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=83
05/28/2022 20:23:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=84
05/28/2022 20:23:29 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.7147349300117538 on epoch=84
05/28/2022 20:23:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=84
05/28/2022 20:23:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
05/28/2022 20:23:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=86
05/28/2022 20:23:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/28/2022 20:23:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/28/2022 20:23:46 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.7343587865470305 on epoch=87
05/28/2022 20:23:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
05/28/2022 20:23:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=88
05/28/2022 20:23:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=89
05/28/2022 20:23:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=89
05/28/2022 20:23:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=90
05/28/2022 20:24:03 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.7158186864014802 on epoch=90
05/28/2022 20:24:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=91
05/28/2022 20:24:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=91
05/28/2022 20:24:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=92
05/28/2022 20:24:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=93
05/28/2022 20:24:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=93
05/28/2022 20:24:20 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.6682823823007151 on epoch=93
05/28/2022 20:24:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.33 on epoch=94
05/28/2022 20:24:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
05/28/2022 20:24:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
05/28/2022 20:24:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=96
05/28/2022 20:24:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=96
05/28/2022 20:24:37 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.7080684214637194 on epoch=96
05/28/2022 20:24:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=97
05/28/2022 20:24:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=98
05/28/2022 20:24:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=98
05/28/2022 20:24:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
05/28/2022 20:24:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=99
05/28/2022 20:24:54 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.7460588764936591 on epoch=99
05/28/2022 20:24:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7343587865470305 -> 0.7460588764936591 on epoch=99, global_step=1600
05/28/2022 20:24:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=100
05/28/2022 20:24:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
05/28/2022 20:25:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=101
05/28/2022 20:25:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=102
05/28/2022 20:25:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
05/28/2022 20:25:11 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.7381813741203767 on epoch=103
05/28/2022 20:25:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=103
05/28/2022 20:25:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=104
05/28/2022 20:25:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=104
05/28/2022 20:25:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
05/28/2022 20:25:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=106
05/28/2022 20:25:28 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.7204019222367847 on epoch=106
05/28/2022 20:25:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=106
05/28/2022 20:25:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=107
05/28/2022 20:25:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=108
05/28/2022 20:25:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=108
05/28/2022 20:25:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=109
05/28/2022 20:25:45 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.7327437975927291 on epoch=109
05/28/2022 20:25:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
05/28/2022 20:25:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/28/2022 20:25:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/28/2022 20:25:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
05/28/2022 20:25:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=112
05/28/2022 20:26:02 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7326974924805585 on epoch=112
05/28/2022 20:26:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
05/28/2022 20:26:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=113
05/28/2022 20:26:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
05/28/2022 20:26:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/28/2022 20:26:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=115
05/28/2022 20:26:19 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7167391338226814 on epoch=115
05/28/2022 20:26:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=116
05/28/2022 20:26:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/28/2022 20:26:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=117
05/28/2022 20:26:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/28/2022 20:26:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/28/2022 20:26:36 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.6799999999999999 on epoch=118
05/28/2022 20:26:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=119
05/28/2022 20:26:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
05/28/2022 20:26:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
05/28/2022 20:26:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=121
05/28/2022 20:26:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/28/2022 20:26:53 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.7204019222367847 on epoch=121
05/28/2022 20:26:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
05/28/2022 20:26:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=123
05/28/2022 20:27:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=123
05/28/2022 20:27:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=124
05/28/2022 20:27:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
05/28/2022 20:27:10 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.7357612731278211 on epoch=124
05/28/2022 20:27:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
05/28/2022 20:27:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=126
05/28/2022 20:27:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=126
05/28/2022 20:27:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
05/28/2022 20:27:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
05/28/2022 20:27:27 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.734879047839864 on epoch=128
05/28/2022 20:27:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=128
05/28/2022 20:27:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=129
05/28/2022 20:27:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=129
05/28/2022 20:27:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/28/2022 20:27:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=131
05/28/2022 20:27:44 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.7306764032427748 on epoch=131
05/28/2022 20:27:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
05/28/2022 20:27:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=132
05/28/2022 20:27:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/28/2022 20:27:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=133
05/28/2022 20:27:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
05/28/2022 20:28:02 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.72591431719087 on epoch=134
05/28/2022 20:28:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/28/2022 20:28:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.22 on epoch=135
05/28/2022 20:28:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=136
05/28/2022 20:28:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/28/2022 20:28:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=137
05/28/2022 20:28:19 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.7240646723424198 on epoch=137
05/28/2022 20:28:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=138
05/28/2022 20:28:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.24 on epoch=138
05/28/2022 20:28:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/28/2022 20:28:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=139
05/28/2022 20:28:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=140
05/28/2022 20:28:36 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.703999366437 on epoch=140
05/28/2022 20:28:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/28/2022 20:28:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=141
05/28/2022 20:28:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
05/28/2022 20:28:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/28/2022 20:28:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
05/28/2022 20:28:53 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.6740955603899766 on epoch=143
05/28/2022 20:28:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=144
05/28/2022 20:28:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/28/2022 20:29:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=145
05/28/2022 20:29:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=146
05/28/2022 20:29:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/28/2022 20:29:10 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.6882051282051281 on epoch=146
05/28/2022 20:29:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=147
05/28/2022 20:29:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=148
05/28/2022 20:29:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/28/2022 20:29:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=149
05/28/2022 20:29:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/28/2022 20:29:27 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.7402693230031361 on epoch=149
05/28/2022 20:29:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/28/2022 20:29:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=151
05/28/2022 20:29:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=151
05/28/2022 20:29:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=152
05/28/2022 20:29:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=153
05/28/2022 20:29:44 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.7353372320891262 on epoch=153
05/28/2022 20:29:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
05/28/2022 20:29:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/28/2022 20:29:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=154
05/28/2022 20:29:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=155
05/28/2022 20:29:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.20 on epoch=156
05/28/2022 20:30:02 - INFO - __main__ - Global step 2500 Train loss 0.23 Classification-F1 0.723251791450457 on epoch=156
05/28/2022 20:30:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/28/2022 20:30:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/28/2022 20:30:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=158
05/28/2022 20:30:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
05/28/2022 20:30:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=159
05/28/2022 20:30:18 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7185288424312815 on epoch=159
05/28/2022 20:30:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=159
05/28/2022 20:30:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
05/28/2022 20:30:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/28/2022 20:30:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/28/2022 20:30:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=162
05/28/2022 20:30:36 - INFO - __main__ - Global step 2600 Train loss 0.21 Classification-F1 0.6860377358490566 on epoch=162
05/28/2022 20:30:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/28/2022 20:30:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=163
05/28/2022 20:30:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=164
05/28/2022 20:30:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
05/28/2022 20:30:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
05/28/2022 20:30:52 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.6761133603238867 on epoch=165
05/28/2022 20:30:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
05/28/2022 20:30:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=166
05/28/2022 20:31:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/28/2022 20:31:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.22 on epoch=168
05/28/2022 20:31:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=168
05/28/2022 20:31:09 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.6860377358490566 on epoch=168
05/28/2022 20:31:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/28/2022 20:31:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
05/28/2022 20:31:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=170
05/28/2022 20:31:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=171
05/28/2022 20:31:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.20 on epoch=171
05/28/2022 20:31:26 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.7029417299128145 on epoch=171
05/28/2022 20:31:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=172
05/28/2022 20:31:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/28/2022 20:31:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/28/2022 20:31:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/28/2022 20:31:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
05/28/2022 20:31:43 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7269649895664272 on epoch=174
05/28/2022 20:31:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=175
05/28/2022 20:31:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=176
05/28/2022 20:31:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=176
05/28/2022 20:31:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=177
05/28/2022 20:31:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/28/2022 20:32:00 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.7158186864014802 on epoch=178
05/28/2022 20:32:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=178
05/28/2022 20:32:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=179
05/28/2022 20:32:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=179
05/28/2022 20:32:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=180
05/28/2022 20:32:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
05/28/2022 20:32:17 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.7173733886893376 on epoch=181
05/28/2022 20:32:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=181
05/28/2022 20:32:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=182
05/28/2022 20:32:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/28/2022 20:32:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/28/2022 20:32:29 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=184
05/28/2022 20:32:34 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.7199858267473926 on epoch=184
05/28/2022 20:32:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=184
05/28/2022 20:32:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=185
05/28/2022 20:32:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/28/2022 20:32:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/28/2022 20:32:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
05/28/2022 20:32:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:32:47 - INFO - __main__ - Printing 3 examples
05/28/2022 20:32:47 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 20:32:47 - INFO - __main__ - ['false']
05/28/2022 20:32:47 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 20:32:47 - INFO - __main__ - ['false']
05/28/2022 20:32:47 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 20:32:47 - INFO - __main__ - ['false']
05/28/2022 20:32:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:32:48 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:32:48 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:32:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:32:48 - INFO - __main__ - Printing 3 examples
05/28/2022 20:32:48 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 20:32:48 - INFO - __main__ - ['false']
05/28/2022 20:32:48 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 20:32:48 - INFO - __main__ - ['false']
05/28/2022 20:32:48 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 20:32:48 - INFO - __main__ - ['false']
05/28/2022 20:32:48 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:32:48 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:32:48 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:32:51 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.7129118979952377 on epoch=187
05/28/2022 20:32:51 - INFO - __main__ - save last model!
05/28/2022 20:32:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 20:32:51 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 20:32:51 - INFO - __main__ - Printing 3 examples
05/28/2022 20:32:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 20:32:51 - INFO - __main__ - ['false']
05/28/2022 20:32:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 20:32:51 - INFO - __main__ - ['false']
05/28/2022 20:32:51 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 20:32:51 - INFO - __main__ - ['false']
05/28/2022 20:32:51 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:32:52 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:32:55 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 20:33:03 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:33:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:33:04 - INFO - __main__ - Starting training!
05/28/2022 20:33:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.2_8_predictions.txt
05/28/2022 20:33:43 - INFO - __main__ - Classification-F1 on test data: 0.5109
05/28/2022 20:33:43 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.2, bsz=8, dev_performance=0.7460588764936591, test_performance=0.510875775379108
05/28/2022 20:33:43 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.5, bsz=8 ...
05/28/2022 20:33:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:33:44 - INFO - __main__ - Printing 3 examples
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:33:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:33:44 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:33:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:33:44 - INFO - __main__ - Printing 3 examples
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 20:33:44 - INFO - __main__ - ['false']
05/28/2022 20:33:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:33:45 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:33:45 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:34:04 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:34:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:34:05 - INFO - __main__ - Starting training!
05/28/2022 20:34:08 - INFO - __main__ - Step 10 Global step 10 Train loss 2.27 on epoch=0
05/28/2022 20:34:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=1
05/28/2022 20:34:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
05/28/2022 20:34:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=2
05/28/2022 20:34:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
05/28/2022 20:34:25 - INFO - __main__ - Global step 50 Train loss 0.82 Classification-F1 0.5564081935763352 on epoch=3
05/28/2022 20:34:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5564081935763352 on epoch=3, global_step=50
05/28/2022 20:34:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
05/28/2022 20:34:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
05/28/2022 20:34:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
05/28/2022 20:34:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=5
05/28/2022 20:34:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=6
05/28/2022 20:34:44 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.42235512098475536 on epoch=6
05/28/2022 20:34:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/28/2022 20:34:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
05/28/2022 20:34:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
05/28/2022 20:34:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/28/2022 20:34:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
05/28/2022 20:35:03 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
05/28/2022 20:35:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=9
05/28/2022 20:35:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/28/2022 20:35:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/28/2022 20:35:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
05/28/2022 20:35:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
05/28/2022 20:35:22 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.5451204995945969 on epoch=12
05/28/2022 20:35:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
05/28/2022 20:35:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/28/2022 20:35:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=14
05/28/2022 20:35:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
05/28/2022 20:35:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
05/28/2022 20:35:41 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.6049382716049383 on epoch=15
05/28/2022 20:35:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5564081935763352 -> 0.6049382716049383 on epoch=15, global_step=250
05/28/2022 20:35:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=16
05/28/2022 20:35:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/28/2022 20:35:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/28/2022 20:35:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
05/28/2022 20:35:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/28/2022 20:36:00 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.5812125811619867 on epoch=18
05/28/2022 20:36:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=19
05/28/2022 20:36:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=19
05/28/2022 20:36:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 20:36:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=21
05/28/2022 20:36:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/28/2022 20:36:19 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.6796092796092795 on epoch=21
05/28/2022 20:36:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6049382716049383 -> 0.6796092796092795 on epoch=21, global_step=350
05/28/2022 20:36:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=22
05/28/2022 20:36:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=23
05/28/2022 20:36:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/28/2022 20:36:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=24
05/28/2022 20:36:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=24
05/28/2022 20:36:38 - INFO - __main__ - Global step 400 Train loss 0.33 Classification-F1 0.680034564167451 on epoch=24
05/28/2022 20:36:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6796092796092795 -> 0.680034564167451 on epoch=24, global_step=400
05/28/2022 20:36:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=25
05/28/2022 20:36:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=26
05/28/2022 20:36:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=26
05/28/2022 20:36:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=27
05/28/2022 20:36:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=28
05/28/2022 20:36:57 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.7083743842364532 on epoch=28
05/28/2022 20:36:57 - INFO - __main__ - Saving model with best Classification-F1: 0.680034564167451 -> 0.7083743842364532 on epoch=28, global_step=450
05/28/2022 20:36:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=28
05/28/2022 20:37:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=29
05/28/2022 20:37:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.30 on epoch=29
05/28/2022 20:37:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=30
05/28/2022 20:37:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=31
05/28/2022 20:37:15 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.7148045843698018 on epoch=31
05/28/2022 20:37:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7083743842364532 -> 0.7148045843698018 on epoch=31, global_step=500
05/28/2022 20:37:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=31
05/28/2022 20:37:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
05/28/2022 20:37:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=33
05/28/2022 20:37:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=33
05/28/2022 20:37:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=34
05/28/2022 20:37:34 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.6946415463665281 on epoch=34
05/28/2022 20:37:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=34
05/28/2022 20:37:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=35
05/28/2022 20:37:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=36
05/28/2022 20:37:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/28/2022 20:37:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=37
05/28/2022 20:37:51 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.7181306581844874 on epoch=37
05/28/2022 20:37:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7148045843698018 -> 0.7181306581844874 on epoch=37, global_step=600
05/28/2022 20:37:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=38
05/28/2022 20:37:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=38
05/28/2022 20:37:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=39
05/28/2022 20:38:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=39
05/28/2022 20:38:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/28/2022 20:38:09 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.6144578313253012 on epoch=40
05/28/2022 20:38:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=41
05/28/2022 20:38:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=41
05/28/2022 20:38:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=42
05/28/2022 20:38:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=43
05/28/2022 20:38:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=43
05/28/2022 20:38:27 - INFO - __main__ - Global step 700 Train loss 0.28 Classification-F1 0.6694246496723898 on epoch=43
05/28/2022 20:38:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=44
05/28/2022 20:38:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=44
05/28/2022 20:38:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=45
05/28/2022 20:38:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=46
05/28/2022 20:38:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=46
05/28/2022 20:38:44 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.7063492063492063 on epoch=46
05/28/2022 20:38:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=47
05/28/2022 20:38:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=48
05/28/2022 20:38:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=48
05/28/2022 20:38:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=49
05/28/2022 20:38:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/28/2022 20:39:02 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.7226181574007661 on epoch=49
05/28/2022 20:39:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7181306581844874 -> 0.7226181574007661 on epoch=49, global_step=800
05/28/2022 20:39:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/28/2022 20:39:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=51
05/28/2022 20:39:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=51
05/28/2022 20:39:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=52
05/28/2022 20:39:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=53
05/28/2022 20:39:20 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.678826481374252 on epoch=53
05/28/2022 20:39:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=53
05/28/2022 20:39:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=54
05/28/2022 20:39:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=54
05/28/2022 20:39:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=55
05/28/2022 20:39:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/28/2022 20:39:37 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.7101886792452831 on epoch=56
05/28/2022 20:39:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/28/2022 20:39:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/28/2022 20:39:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/28/2022 20:39:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/28/2022 20:39:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=59
05/28/2022 20:39:55 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.6975527426160337 on epoch=59
05/28/2022 20:39:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=59
05/28/2022 20:40:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/28/2022 20:40:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=61
05/28/2022 20:40:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=61
05/28/2022 20:40:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=62
05/28/2022 20:40:13 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7030587113688727 on epoch=62
05/28/2022 20:40:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=63
05/28/2022 20:40:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=63
05/28/2022 20:40:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/28/2022 20:40:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/28/2022 20:40:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=65
05/28/2022 20:40:31 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.6694246496723898 on epoch=65
05/28/2022 20:40:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/28/2022 20:40:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=66
05/28/2022 20:40:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=67
05/28/2022 20:40:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/28/2022 20:40:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=68
05/28/2022 20:40:48 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.6124604723137994 on epoch=68
05/28/2022 20:40:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=69
05/28/2022 20:40:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=69
05/28/2022 20:40:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=70
05/28/2022 20:40:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/28/2022 20:41:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/28/2022 20:41:06 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.6764729407624258 on epoch=71
05/28/2022 20:41:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=72
05/28/2022 20:41:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=73
05/28/2022 20:41:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/28/2022 20:41:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/28/2022 20:41:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/28/2022 20:41:24 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.7323986964274735 on epoch=74
05/28/2022 20:41:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7226181574007661 -> 0.7323986964274735 on epoch=74, global_step=1200
05/28/2022 20:41:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/28/2022 20:41:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=76
05/28/2022 20:41:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=76
05/28/2022 20:41:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/28/2022 20:41:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/28/2022 20:41:41 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.6215722424302822 on epoch=78
05/28/2022 20:41:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=78
05/28/2022 20:41:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=79
05/28/2022 20:41:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=79
05/28/2022 20:41:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=80
05/28/2022 20:41:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=81
05/28/2022 20:41:59 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.618239660657476 on epoch=81
05/28/2022 20:42:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/28/2022 20:42:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=82
05/28/2022 20:42:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=83
05/28/2022 20:42:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=83
05/28/2022 20:42:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=84
05/28/2022 20:42:16 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.6479818814203673 on epoch=84
05/28/2022 20:42:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=84
05/28/2022 20:42:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=85
05/28/2022 20:42:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=86
05/28/2022 20:42:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=86
05/28/2022 20:42:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=87
05/28/2022 20:42:33 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7080684214637194 on epoch=87
05/28/2022 20:42:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=88
05/28/2022 20:42:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=88
05/28/2022 20:42:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
05/28/2022 20:42:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=89
05/28/2022 20:42:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=90
05/28/2022 20:42:51 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.6729479399552858 on epoch=90
05/28/2022 20:42:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
05/28/2022 20:42:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=91
05/28/2022 20:42:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=92
05/28/2022 20:43:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=93
05/28/2022 20:43:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/28/2022 20:43:09 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.6652500817260543 on epoch=93
05/28/2022 20:43:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=94
05/28/2022 20:43:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/28/2022 20:43:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/28/2022 20:43:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=96
05/28/2022 20:43:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=96
05/28/2022 20:43:26 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.6717948717948719 on epoch=96
05/28/2022 20:43:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=97
05/28/2022 20:43:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=98
05/28/2022 20:43:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=98
05/28/2022 20:43:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=99
05/28/2022 20:43:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
05/28/2022 20:43:44 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.7262893081761006 on epoch=99
05/28/2022 20:43:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=100
05/28/2022 20:43:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=101
05/28/2022 20:43:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=101
05/28/2022 20:43:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=102
05/28/2022 20:43:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=103
05/28/2022 20:44:01 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.5829260345389378 on epoch=103
05/28/2022 20:44:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/28/2022 20:44:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=104
05/28/2022 20:44:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=104
05/28/2022 20:44:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=105
05/28/2022 20:44:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=106
05/28/2022 20:44:19 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.6917494822354584 on epoch=106
05/28/2022 20:44:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=106
05/28/2022 20:44:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=107
05/28/2022 20:44:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=108
05/28/2022 20:44:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=108
05/28/2022 20:44:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=109
05/28/2022 20:44:36 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.6623560445216496 on epoch=109
05/28/2022 20:44:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/28/2022 20:44:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=110
05/28/2022 20:44:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/28/2022 20:44:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
05/28/2022 20:44:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
05/28/2022 20:44:54 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.7277277277277278 on epoch=112
05/28/2022 20:44:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=113
05/28/2022 20:44:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=113
05/28/2022 20:45:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=114
05/28/2022 20:45:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=114
05/28/2022 20:45:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=115
05/28/2022 20:45:11 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.6764729407624258 on epoch=115
05/28/2022 20:45:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
05/28/2022 20:45:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=116
05/28/2022 20:45:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/28/2022 20:45:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=118
05/28/2022 20:45:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=118
05/28/2022 20:45:29 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.6775870040014667 on epoch=118
05/28/2022 20:45:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=119
05/28/2022 20:45:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=119
05/28/2022 20:45:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=120
05/28/2022 20:45:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=121
05/28/2022 20:45:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=121
05/28/2022 20:45:46 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6623560445216496 on epoch=121
05/28/2022 20:45:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=122
05/28/2022 20:45:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/28/2022 20:45:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=123
05/28/2022 20:45:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/28/2022 20:45:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/28/2022 20:46:04 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.6064468137862633 on epoch=124
05/28/2022 20:46:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
05/28/2022 20:46:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=126
05/28/2022 20:46:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=126
05/28/2022 20:46:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=127
05/28/2022 20:46:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=128
05/28/2022 20:46:22 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.5885088919288647 on epoch=128
05/28/2022 20:46:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=128
05/28/2022 20:46:27 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/28/2022 20:46:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=129
05/28/2022 20:46:32 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=130
05/28/2022 20:46:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=131
05/28/2022 20:46:40 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.6514296829410621 on epoch=131
05/28/2022 20:46:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=131
05/28/2022 20:46:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=132
05/28/2022 20:46:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=133
05/28/2022 20:46:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
05/28/2022 20:46:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=134
05/28/2022 20:46:57 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6705912629479509 on epoch=134
05/28/2022 20:47:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/28/2022 20:47:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=135
05/28/2022 20:47:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=136
05/28/2022 20:47:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/28/2022 20:47:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=137
05/28/2022 20:47:15 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.643130389677754 on epoch=137
05/28/2022 20:47:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
05/28/2022 20:47:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/28/2022 20:47:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
05/28/2022 20:47:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=139
05/28/2022 20:47:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
05/28/2022 20:47:33 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.6740955603899766 on epoch=140
05/28/2022 20:47:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=141
05/28/2022 20:47:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=141
05/28/2022 20:47:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/28/2022 20:47:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
05/28/2022 20:47:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=143
05/28/2022 20:47:51 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.6367372902089348 on epoch=143
05/28/2022 20:47:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/28/2022 20:47:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=144
05/28/2022 20:47:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/28/2022 20:48:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=146
05/28/2022 20:48:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/28/2022 20:48:08 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.6416666666666666 on epoch=146
05/28/2022 20:48:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=147
05/28/2022 20:48:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=148
05/28/2022 20:48:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=148
05/28/2022 20:48:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/28/2022 20:48:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/28/2022 20:48:26 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6416666666666666 on epoch=149
05/28/2022 20:48:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=150
05/28/2022 20:48:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=151
05/28/2022 20:48:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/28/2022 20:48:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=152
05/28/2022 20:48:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=153
05/28/2022 20:48:44 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.643130389677754 on epoch=153
05/28/2022 20:48:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=153
05/28/2022 20:48:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/28/2022 20:48:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/28/2022 20:48:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=155
05/28/2022 20:48:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=156
05/28/2022 20:49:02 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6575940697768109 on epoch=156
05/28/2022 20:49:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/28/2022 20:49:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/28/2022 20:49:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
05/28/2022 20:49:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=158
05/28/2022 20:49:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=159
05/28/2022 20:49:20 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6740514387573211 on epoch=159
05/28/2022 20:49:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
05/28/2022 20:49:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=160
05/28/2022 20:49:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
05/28/2022 20:49:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=161
05/28/2022 20:49:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=162
05/28/2022 20:49:37 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.686787955827441 on epoch=162
05/28/2022 20:49:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/28/2022 20:49:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/28/2022 20:49:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
05/28/2022 20:49:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=164
05/28/2022 20:49:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/28/2022 20:49:55 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.6317750898986596 on epoch=165
05/28/2022 20:49:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
05/28/2022 20:50:00 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=166
05/28/2022 20:50:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/28/2022 20:50:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/28/2022 20:50:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
05/28/2022 20:50:12 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5690011281784257 on epoch=168
05/28/2022 20:50:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=169
05/28/2022 20:50:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=169
05/28/2022 20:50:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=170
05/28/2022 20:50:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/28/2022 20:50:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/28/2022 20:50:30 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6115838775644832 on epoch=171
05/28/2022 20:50:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/28/2022 20:50:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/28/2022 20:50:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/28/2022 20:50:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=174
05/28/2022 20:50:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/28/2022 20:50:48 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7000702960243692 on epoch=174
05/28/2022 20:50:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/28/2022 20:50:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/28/2022 20:50:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
05/28/2022 20:50:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
05/28/2022 20:51:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/28/2022 20:51:06 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.6283864233171862 on epoch=178
05/28/2022 20:51:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/28/2022 20:51:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/28/2022 20:51:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
05/28/2022 20:51:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/28/2022 20:51:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=181
05/28/2022 20:51:23 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.6301549776727082 on epoch=181
05/28/2022 20:51:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/28/2022 20:51:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=182
05/28/2022 20:51:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/28/2022 20:51:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
05/28/2022 20:51:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/28/2022 20:51:41 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.6249084249084249 on epoch=184
05/28/2022 20:51:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/28/2022 20:51:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=185
05/28/2022 20:51:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/28/2022 20:51:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/28/2022 20:51:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/28/2022 20:51:55 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:51:55 - INFO - __main__ - Printing 3 examples
05/28/2022 20:51:55 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 20:51:55 - INFO - __main__ - ['false']
05/28/2022 20:51:55 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 20:51:55 - INFO - __main__ - ['false']
05/28/2022 20:51:55 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 20:51:55 - INFO - __main__ - ['false']
05/28/2022 20:51:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:51:56 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:51:56 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:51:56 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:51:56 - INFO - __main__ - Printing 3 examples
05/28/2022 20:51:56 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 20:51:56 - INFO - __main__ - ['false']
05/28/2022 20:51:56 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 20:51:56 - INFO - __main__ - ['false']
05/28/2022 20:51:56 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 20:51:56 - INFO - __main__ - ['false']
05/28/2022 20:51:56 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:51:56 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:51:56 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:51:58 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6267232237539766 on epoch=187
05/28/2022 20:51:58 - INFO - __main__ - save last model!
05/28/2022 20:51:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 20:51:59 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 20:51:59 - INFO - __main__ - Printing 3 examples
05/28/2022 20:51:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 20:51:59 - INFO - __main__ - ['false']
05/28/2022 20:51:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 20:51:59 - INFO - __main__ - ['false']
05/28/2022 20:51:59 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 20:51:59 - INFO - __main__ - ['false']
05/28/2022 20:51:59 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:52:00 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:52:02 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 20:52:12 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:52:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:52:12 - INFO - __main__ - Starting training!
05/28/2022 20:52:54 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.5_8_predictions.txt
05/28/2022 20:52:54 - INFO - __main__ - Classification-F1 on test data: 0.3876
05/28/2022 20:52:54 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.5, bsz=8, dev_performance=0.7323986964274735, test_performance=0.3875854793347573
05/28/2022 20:52:54 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.4, bsz=8 ...
05/28/2022 20:52:55 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:52:55 - INFO - __main__ - Printing 3 examples
05/28/2022 20:52:55 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 20:52:55 - INFO - __main__ - ['false']
05/28/2022 20:52:55 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 20:52:55 - INFO - __main__ - ['false']
05/28/2022 20:52:55 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 20:52:55 - INFO - __main__ - ['false']
05/28/2022 20:52:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:52:55 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:52:56 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 20:52:56 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 20:52:56 - INFO - __main__ - Printing 3 examples
05/28/2022 20:52:56 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 20:52:56 - INFO - __main__ - ['false']
05/28/2022 20:52:56 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 20:52:56 - INFO - __main__ - ['false']
05/28/2022 20:52:56 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 20:52:56 - INFO - __main__ - ['false']
05/28/2022 20:52:56 - INFO - __main__ - Tokenizing Input ...
05/28/2022 20:52:56 - INFO - __main__ - Tokenizing Output ...
05/28/2022 20:52:56 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 20:53:12 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 20:53:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 20:53:13 - INFO - __main__ - Starting training!
05/28/2022 20:53:16 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=0
05/28/2022 20:53:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=1
05/28/2022 20:53:21 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=1
05/28/2022 20:53:24 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=2
05/28/2022 20:53:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=3
05/28/2022 20:53:32 - INFO - __main__ - Global step 50 Train loss 0.93 Classification-F1 0.4205724632634585 on epoch=3
05/28/2022 20:53:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4205724632634585 on epoch=3, global_step=50
05/28/2022 20:53:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/28/2022 20:53:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=4
05/28/2022 20:53:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=4
05/28/2022 20:53:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
05/28/2022 20:53:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
05/28/2022 20:53:51 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.4941980806623596 on epoch=6
05/28/2022 20:53:51 - INFO - __main__ - Saving model with best Classification-F1: 0.4205724632634585 -> 0.4941980806623596 on epoch=6, global_step=100
05/28/2022 20:53:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/28/2022 20:53:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=7
05/28/2022 20:53:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=8
05/28/2022 20:54:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=8
05/28/2022 20:54:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
05/28/2022 20:54:10 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.33159268929503916 on epoch=9
05/28/2022 20:54:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=9
05/28/2022 20:54:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=10
05/28/2022 20:54:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
05/28/2022 20:54:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=11
05/28/2022 20:54:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
05/28/2022 20:54:29 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.38827397679264397 on epoch=12
05/28/2022 20:54:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/28/2022 20:54:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/28/2022 20:54:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=14
05/28/2022 20:54:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/28/2022 20:54:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/28/2022 20:54:48 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5846954392408936 on epoch=15
05/28/2022 20:54:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4941980806623596 -> 0.5846954392408936 on epoch=15, global_step=250
05/28/2022 20:54:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/28/2022 20:54:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
05/28/2022 20:54:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=17
05/28/2022 20:54:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=18
05/28/2022 20:55:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.34 on epoch=18
05/28/2022 20:55:07 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.5796387520525451 on epoch=18
05/28/2022 20:55:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=19
05/28/2022 20:55:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=19
05/28/2022 20:55:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 20:55:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=21
05/28/2022 20:55:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/28/2022 20:55:26 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6757763027389945 on epoch=21
05/28/2022 20:55:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5846954392408936 -> 0.6757763027389945 on epoch=21, global_step=350
05/28/2022 20:55:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=22
05/28/2022 20:55:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=23
05/28/2022 20:55:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=23
05/28/2022 20:55:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/28/2022 20:55:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/28/2022 20:55:45 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.6426169173170886 on epoch=24
05/28/2022 20:55:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=25
05/28/2022 20:55:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.32 on epoch=26
05/28/2022 20:55:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 20:55:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=27
05/28/2022 20:55:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=28
05/28/2022 20:56:04 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.7028347996089932 on epoch=28
05/28/2022 20:56:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6757763027389945 -> 0.7028347996089932 on epoch=28, global_step=450
05/28/2022 20:56:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=28
05/28/2022 20:56:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=29
05/28/2022 20:56:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
05/28/2022 20:56:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.32 on epoch=30
05/28/2022 20:56:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=31
05/28/2022 20:56:22 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.7223130394927814 on epoch=31
05/28/2022 20:56:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7028347996089932 -> 0.7223130394927814 on epoch=31, global_step=500
05/28/2022 20:56:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/28/2022 20:56:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
05/28/2022 20:56:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=33
05/28/2022 20:56:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/28/2022 20:56:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/28/2022 20:56:41 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.7190509312929902 on epoch=34
05/28/2022 20:56:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=34
05/28/2022 20:56:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
05/28/2022 20:56:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=36
05/28/2022 20:56:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=36
05/28/2022 20:56:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=37
05/28/2022 20:56:59 - INFO - __main__ - Global step 600 Train loss 0.32 Classification-F1 0.7183201907207042 on epoch=37
05/28/2022 20:57:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=38
05/28/2022 20:57:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=38
05/28/2022 20:57:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=39
05/28/2022 20:57:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=39
05/28/2022 20:57:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/28/2022 20:57:17 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.7337900660631269 on epoch=40
05/28/2022 20:57:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7223130394927814 -> 0.7337900660631269 on epoch=40, global_step=650
05/28/2022 20:57:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/28/2022 20:57:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=41
05/28/2022 20:57:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=42
05/28/2022 20:57:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/28/2022 20:57:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=43
05/28/2022 20:57:34 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.643130389677754 on epoch=43
05/28/2022 20:57:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=44
05/28/2022 20:57:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=44
05/28/2022 20:57:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/28/2022 20:57:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=46
05/28/2022 20:57:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=46
05/28/2022 20:57:52 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.7227551209852094 on epoch=46
05/28/2022 20:57:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=47
05/28/2022 20:57:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=48
05/28/2022 20:58:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=48
05/28/2022 20:58:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/28/2022 20:58:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=49
05/28/2022 20:58:11 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.7379573752960049 on epoch=49
05/28/2022 20:58:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7337900660631269 -> 0.7379573752960049 on epoch=49, global_step=800
05/28/2022 20:58:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/28/2022 20:58:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/28/2022 20:58:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=51
05/28/2022 20:58:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=52
05/28/2022 20:58:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/28/2022 20:58:29 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.7417935081606455 on epoch=53
05/28/2022 20:58:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7379573752960049 -> 0.7417935081606455 on epoch=53, global_step=850
05/28/2022 20:58:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=53
05/28/2022 20:58:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/28/2022 20:58:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/28/2022 20:58:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=55
05/28/2022 20:58:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/28/2022 20:58:46 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.7320197044334975 on epoch=56
05/28/2022 20:58:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=56
05/28/2022 20:58:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=57
05/28/2022 20:58:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=58
05/28/2022 20:58:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=58
05/28/2022 20:58:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/28/2022 20:59:04 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.7286427528996082 on epoch=59
05/28/2022 20:59:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=59
05/28/2022 20:59:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=60
05/28/2022 20:59:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=61
05/28/2022 20:59:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=61
05/28/2022 20:59:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=62
05/28/2022 20:59:22 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7204019222367847 on epoch=62
05/28/2022 20:59:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/28/2022 20:59:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/28/2022 20:59:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=64
05/28/2022 20:59:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=64
05/28/2022 20:59:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=65
05/28/2022 20:59:40 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.6870616998005532 on epoch=65
05/28/2022 20:59:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=66
05/28/2022 20:59:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=66
05/28/2022 20:59:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=67
05/28/2022 20:59:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/28/2022 20:59:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=68
05/28/2022 20:59:58 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.6974768444586394 on epoch=68
05/28/2022 21:00:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=69
05/28/2022 21:00:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=69
05/28/2022 21:00:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=70
05/28/2022 21:00:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=71
05/28/2022 21:00:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=71
05/28/2022 21:00:16 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.6846634992241365 on epoch=71
05/28/2022 21:00:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/28/2022 21:00:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=73
05/28/2022 21:00:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/28/2022 21:00:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=74
05/28/2022 21:00:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=74
05/28/2022 21:00:34 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.7316065371569533 on epoch=74
05/28/2022 21:00:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=75
05/28/2022 21:00:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
05/28/2022 21:00:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
05/28/2022 21:00:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/28/2022 21:00:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/28/2022 21:00:52 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.6949233335981568 on epoch=78
05/28/2022 21:00:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/28/2022 21:00:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=79
05/28/2022 21:01:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=79
05/28/2022 21:01:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=80
05/28/2022 21:01:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/28/2022 21:01:10 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.7093661305581835 on epoch=81
05/28/2022 21:01:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=81
05/28/2022 21:01:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=82
05/28/2022 21:01:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=83
05/28/2022 21:01:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=83
05/28/2022 21:01:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=84
05/28/2022 21:01:28 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.6823463056965595 on epoch=84
05/28/2022 21:01:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=84
05/28/2022 21:01:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/28/2022 21:01:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/28/2022 21:01:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=86
05/28/2022 21:01:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/28/2022 21:01:47 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.6521739130434783 on epoch=87
05/28/2022 21:01:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=88
05/28/2022 21:01:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=88
05/28/2022 21:01:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=89
05/28/2022 21:01:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
05/28/2022 21:02:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/28/2022 21:02:05 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.7056531459597573 on epoch=90
05/28/2022 21:02:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=91
05/28/2022 21:02:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/28/2022 21:02:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=92
05/28/2022 21:02:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=93
05/28/2022 21:02:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/28/2022 21:02:23 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.626352609592833 on epoch=93
05/28/2022 21:02:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=94
05/28/2022 21:02:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=94
05/28/2022 21:02:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/28/2022 21:02:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=96
05/28/2022 21:02:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=96
05/28/2022 21:02:41 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.7102364213177739 on epoch=96
05/28/2022 21:02:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=97
05/28/2022 21:02:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=98
05/28/2022 21:02:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=98
05/28/2022 21:02:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=99
05/28/2022 21:02:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=99
05/28/2022 21:03:00 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.6941358822009818 on epoch=99
05/28/2022 21:03:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=100
05/28/2022 21:03:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/28/2022 21:03:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
05/28/2022 21:03:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/28/2022 21:03:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
05/28/2022 21:03:18 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.7262893081761006 on epoch=103
05/28/2022 21:03:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=103
05/28/2022 21:03:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=104
05/28/2022 21:03:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/28/2022 21:03:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=105
05/28/2022 21:03:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/28/2022 21:03:36 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.668028269467678 on epoch=106
05/28/2022 21:03:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/28/2022 21:03:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=107
05/28/2022 21:03:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=108
05/28/2022 21:03:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/28/2022 21:03:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=109
05/28/2022 21:03:54 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.6974768444586394 on epoch=109
05/28/2022 21:03:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/28/2022 21:03:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=110
05/28/2022 21:04:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
05/28/2022 21:04:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=111
05/28/2022 21:04:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=112
05/28/2022 21:04:12 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.6893005429575216 on epoch=112
05/28/2022 21:04:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=113
05/28/2022 21:04:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=113
05/28/2022 21:04:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
05/28/2022 21:04:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=114
05/28/2022 21:04:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=115
05/28/2022 21:04:30 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.7270275355955591 on epoch=115
05/28/2022 21:04:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=116
05/28/2022 21:04:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=116
05/28/2022 21:04:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=117
05/28/2022 21:04:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=118
05/28/2022 21:04:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
05/28/2022 21:04:48 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.5741455300029155 on epoch=118
05/28/2022 21:04:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=119
05/28/2022 21:04:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=119
05/28/2022 21:04:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=120
05/28/2022 21:04:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=121
05/28/2022 21:05:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=121
05/28/2022 21:05:07 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.7255124566382845 on epoch=121
05/28/2022 21:05:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/28/2022 21:05:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/28/2022 21:05:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
05/28/2022 21:05:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=124
05/28/2022 21:05:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=124
05/28/2022 21:05:25 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.7030587113688727 on epoch=124
05/28/2022 21:05:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=125
05/28/2022 21:05:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=126
05/28/2022 21:05:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=126
05/28/2022 21:05:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=127
05/28/2022 21:05:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/28/2022 21:05:43 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.626352609592833 on epoch=128
05/28/2022 21:05:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=128
05/28/2022 21:05:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
05/28/2022 21:05:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/28/2022 21:05:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=130
05/28/2022 21:05:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/28/2022 21:06:01 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.7020740669886971 on epoch=131
05/28/2022 21:06:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/28/2022 21:06:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=132
05/28/2022 21:06:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
05/28/2022 21:06:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/28/2022 21:06:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=134
05/28/2022 21:06:20 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7111940891395885 on epoch=134
05/28/2022 21:06:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=134
05/28/2022 21:06:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
05/28/2022 21:06:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=136
05/28/2022 21:06:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=136
05/28/2022 21:06:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=137
05/28/2022 21:06:38 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7121089728359864 on epoch=137
05/28/2022 21:06:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=138
05/28/2022 21:06:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
05/28/2022 21:06:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/28/2022 21:06:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=139
05/28/2022 21:06:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
05/28/2022 21:06:56 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.6728302192024589 on epoch=140
05/28/2022 21:06:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/28/2022 21:07:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/28/2022 21:07:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=142
05/28/2022 21:07:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/28/2022 21:07:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=143
05/28/2022 21:07:15 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.668028269467678 on epoch=143
05/28/2022 21:07:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=144
05/28/2022 21:07:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/28/2022 21:07:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=145
05/28/2022 21:07:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
05/28/2022 21:07:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/28/2022 21:07:33 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.6928540576858473 on epoch=146
05/28/2022 21:07:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
05/28/2022 21:07:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/28/2022 21:07:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=148
05/28/2022 21:07:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=149
05/28/2022 21:07:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
05/28/2022 21:07:51 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.6728302192024589 on epoch=149
05/28/2022 21:07:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/28/2022 21:07:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=151
05/28/2022 21:07:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/28/2022 21:08:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
05/28/2022 21:08:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=153
05/28/2022 21:08:09 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.5917065390749602 on epoch=153
05/28/2022 21:08:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/28/2022 21:08:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/28/2022 21:08:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
05/28/2022 21:08:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/28/2022 21:08:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
05/28/2022 21:08:27 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.6928540576858473 on epoch=156
05/28/2022 21:08:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/28/2022 21:08:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/28/2022 21:08:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
05/28/2022 21:08:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=158
05/28/2022 21:08:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
05/28/2022 21:08:45 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.6284715350616795 on epoch=159
05/28/2022 21:08:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
05/28/2022 21:08:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=160
05/28/2022 21:08:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/28/2022 21:08:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
05/28/2022 21:08:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
05/28/2022 21:09:03 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.7165991902834008 on epoch=162
05/28/2022 21:09:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
05/28/2022 21:09:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/28/2022 21:09:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/28/2022 21:09:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
05/28/2022 21:09:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
05/28/2022 21:09:20 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.683529468428404 on epoch=165
05/28/2022 21:09:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
05/28/2022 21:09:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=166
05/28/2022 21:09:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/28/2022 21:09:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/28/2022 21:09:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=168
05/28/2022 21:09:38 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.6519403082870576 on epoch=168
05/28/2022 21:09:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=169
05/28/2022 21:09:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/28/2022 21:09:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
05/28/2022 21:09:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/28/2022 21:09:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/28/2022 21:09:56 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.6631961927656049 on epoch=171
05/28/2022 21:09:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/28/2022 21:10:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/28/2022 21:10:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
05/28/2022 21:10:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/28/2022 21:10:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/28/2022 21:10:14 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.706646294881589 on epoch=174
05/28/2022 21:10:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/28/2022 21:10:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/28/2022 21:10:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=176
05/28/2022 21:10:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/28/2022 21:10:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/28/2022 21:10:31 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.5604149144939725 on epoch=178
05/28/2022 21:10:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/28/2022 21:10:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/28/2022 21:10:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/28/2022 21:10:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/28/2022 21:10:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/28/2022 21:10:49 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.6763275627309862 on epoch=181
05/28/2022 21:10:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
05/28/2022 21:10:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/28/2022 21:10:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/28/2022 21:10:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
05/28/2022 21:11:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/28/2022 21:11:07 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.6728302192024589 on epoch=184
05/28/2022 21:11:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/28/2022 21:11:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=185
05/28/2022 21:11:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
05/28/2022 21:11:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
05/28/2022 21:11:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/28/2022 21:11:21 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:11:21 - INFO - __main__ - Printing 3 examples
05/28/2022 21:11:21 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 21:11:21 - INFO - __main__ - ['false']
05/28/2022 21:11:21 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 21:11:21 - INFO - __main__ - ['false']
05/28/2022 21:11:21 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 21:11:21 - INFO - __main__ - ['false']
05/28/2022 21:11:21 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:11:21 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:11:22 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:11:22 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:11:22 - INFO - __main__ - Printing 3 examples
05/28/2022 21:11:22 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 21:11:22 - INFO - __main__ - ['false']
05/28/2022 21:11:22 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 21:11:22 - INFO - __main__ - ['false']
05/28/2022 21:11:22 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 21:11:22 - INFO - __main__ - ['false']
05/28/2022 21:11:22 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:11:22 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:11:22 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:11:24 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6334332334332334 on epoch=187
05/28/2022 21:11:24 - INFO - __main__ - save last model!
05/28/2022 21:11:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 21:11:24 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 21:11:24 - INFO - __main__ - Printing 3 examples
05/28/2022 21:11:24 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 21:11:24 - INFO - __main__ - ['false']
05/28/2022 21:11:24 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 21:11:24 - INFO - __main__ - ['false']
05/28/2022 21:11:24 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 21:11:24 - INFO - __main__ - ['false']
05/28/2022 21:11:24 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:11:26 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:11:28 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 21:11:37 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:11:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:11:38 - INFO - __main__ - Starting training!
05/28/2022 21:12:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.4_8_predictions.txt
05/28/2022 21:12:16 - INFO - __main__ - Classification-F1 on test data: 0.3569
05/28/2022 21:12:16 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.4, bsz=8, dev_performance=0.7417935081606455, test_performance=0.3569065705461333
05/28/2022 21:12:16 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.3, bsz=8 ...
05/28/2022 21:12:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:12:17 - INFO - __main__ - Printing 3 examples
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:12:17 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:12:17 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:12:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:12:17 - INFO - __main__ - Printing 3 examples
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 21:12:17 - INFO - __main__ - ['false']
05/28/2022 21:12:17 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:12:18 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:12:18 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:12:33 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:12:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:12:34 - INFO - __main__ - Starting training!
05/28/2022 21:12:37 - INFO - __main__ - Step 10 Global step 10 Train loss 2.93 on epoch=0
05/28/2022 21:12:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.77 on epoch=1
05/28/2022 21:12:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=1
05/28/2022 21:12:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=2
05/28/2022 21:12:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=3
05/28/2022 21:12:54 - INFO - __main__ - Global step 50 Train loss 1.00 Classification-F1 0.33159268929503916 on epoch=3
05/28/2022 21:12:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.33159268929503916 on epoch=3, global_step=50
05/28/2022 21:12:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=3
05/28/2022 21:12:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/28/2022 21:13:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
05/28/2022 21:13:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
05/28/2022 21:13:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=6
05/28/2022 21:13:13 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3741543009835693 on epoch=6
05/28/2022 21:13:13 - INFO - __main__ - Saving model with best Classification-F1: 0.33159268929503916 -> 0.3741543009835693 on epoch=6, global_step=100
05/28/2022 21:13:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/28/2022 21:13:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=7
05/28/2022 21:13:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=8
05/28/2022 21:13:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
05/28/2022 21:13:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=9
05/28/2022 21:13:32 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.49703629703629704 on epoch=9
05/28/2022 21:13:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3741543009835693 -> 0.49703629703629704 on epoch=9, global_step=150
05/28/2022 21:13:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=9
05/28/2022 21:13:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/28/2022 21:13:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/28/2022 21:13:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
05/28/2022 21:13:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=12
05/28/2022 21:13:51 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=12
05/28/2022 21:13:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
05/28/2022 21:13:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=13
05/28/2022 21:13:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
05/28/2022 21:14:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=14
05/28/2022 21:14:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
05/28/2022 21:14:10 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.33159268929503916 on epoch=15
05/28/2022 21:14:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=16
05/28/2022 21:14:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=16
05/28/2022 21:14:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=17
05/28/2022 21:14:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
05/28/2022 21:14:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
05/28/2022 21:14:29 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.6090126119884745 on epoch=18
05/28/2022 21:14:29 - INFO - __main__ - Saving model with best Classification-F1: 0.49703629703629704 -> 0.6090126119884745 on epoch=18, global_step=300
05/28/2022 21:14:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/28/2022 21:14:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=19
05/28/2022 21:14:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 21:14:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
05/28/2022 21:14:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
05/28/2022 21:14:48 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.47178613079752396 on epoch=21
05/28/2022 21:14:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
05/28/2022 21:14:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/28/2022 21:14:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/28/2022 21:14:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/28/2022 21:15:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
05/28/2022 21:15:07 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.4436832412523021 on epoch=24
05/28/2022 21:15:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/28/2022 21:15:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=26
05/28/2022 21:15:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 21:15:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=27
05/28/2022 21:15:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
05/28/2022 21:15:26 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.6355201484538722 on epoch=28
05/28/2022 21:15:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6090126119884745 -> 0.6355201484538722 on epoch=28, global_step=450
05/28/2022 21:15:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=28
05/28/2022 21:15:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=29
05/28/2022 21:15:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/28/2022 21:15:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/28/2022 21:15:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/28/2022 21:15:45 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.6528539892778304 on epoch=31
05/28/2022 21:15:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6355201484538722 -> 0.6528539892778304 on epoch=31, global_step=500
05/28/2022 21:15:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
05/28/2022 21:15:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
05/28/2022 21:15:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
05/28/2022 21:15:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/28/2022 21:15:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=34
05/28/2022 21:16:04 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.597322623828648 on epoch=34
05/28/2022 21:16:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/28/2022 21:16:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=35
05/28/2022 21:16:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=36
05/28/2022 21:16:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/28/2022 21:16:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=37
05/28/2022 21:16:23 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.6445362955308704 on epoch=37
05/28/2022 21:16:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=38
05/28/2022 21:16:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
05/28/2022 21:16:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=39
05/28/2022 21:16:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=39
05/28/2022 21:16:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/28/2022 21:16:42 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.6958353264307868 on epoch=40
05/28/2022 21:16:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6528539892778304 -> 0.6958353264307868 on epoch=40, global_step=650
05/28/2022 21:16:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/28/2022 21:16:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=41
05/28/2022 21:16:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
05/28/2022 21:16:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/28/2022 21:16:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/28/2022 21:17:01 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6946415463665281 on epoch=43
05/28/2022 21:17:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=44
05/28/2022 21:17:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=44
05/28/2022 21:17:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=45
05/28/2022 21:17:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/28/2022 21:17:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=46
05/28/2022 21:17:19 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.7024712503058479 on epoch=46
05/28/2022 21:17:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6958353264307868 -> 0.7024712503058479 on epoch=46, global_step=750
05/28/2022 21:17:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=47
05/28/2022 21:17:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=48
05/28/2022 21:17:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
05/28/2022 21:17:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=49
05/28/2022 21:17:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=49
05/28/2022 21:17:38 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.7091623679685581 on epoch=49
05/28/2022 21:17:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7024712503058479 -> 0.7091623679685581 on epoch=49, global_step=800
05/28/2022 21:17:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/28/2022 21:17:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=51
05/28/2022 21:17:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=51
05/28/2022 21:17:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=52
05/28/2022 21:17:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=53
05/28/2022 21:17:56 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.678247241919717 on epoch=53
05/28/2022 21:17:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=53
05/28/2022 21:18:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
05/28/2022 21:18:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=54
05/28/2022 21:18:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=55
05/28/2022 21:18:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=56
05/28/2022 21:18:14 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.7166574432761483 on epoch=56
05/28/2022 21:18:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7091623679685581 -> 0.7166574432761483 on epoch=56, global_step=900
05/28/2022 21:18:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=56
05/28/2022 21:18:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=57
05/28/2022 21:18:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=58
05/28/2022 21:18:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=58
05/28/2022 21:18:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=59
05/28/2022 21:18:32 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7409065816107465 on epoch=59
05/28/2022 21:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7166574432761483 -> 0.7409065816107465 on epoch=59, global_step=950
05/28/2022 21:18:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/28/2022 21:18:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=60
05/28/2022 21:18:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=61
05/28/2022 21:18:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=61
05/28/2022 21:18:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=62
05/28/2022 21:18:50 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.7248833210513388 on epoch=62
05/28/2022 21:18:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=63
05/28/2022 21:18:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=63
05/28/2022 21:18:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/28/2022 21:19:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=64
05/28/2022 21:19:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=65
05/28/2022 21:19:08 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.7222222222222223 on epoch=65
05/28/2022 21:19:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=66
05/28/2022 21:19:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
05/28/2022 21:19:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
05/28/2022 21:19:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=68
05/28/2022 21:19:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=68
05/28/2022 21:19:26 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.6588694270072406 on epoch=68
05/28/2022 21:19:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/28/2022 21:19:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=69
05/28/2022 21:19:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=70
05/28/2022 21:19:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
05/28/2022 21:19:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
05/28/2022 21:19:44 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.7000702960243692 on epoch=71
05/28/2022 21:19:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=72
05/28/2022 21:19:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=73
05/28/2022 21:19:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=73
05/28/2022 21:19:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/28/2022 21:19:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=74
05/28/2022 21:20:03 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.7142857142857143 on epoch=74
05/28/2022 21:20:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/28/2022 21:20:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/28/2022 21:20:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/28/2022 21:20:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=77
05/28/2022 21:20:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=78
05/28/2022 21:20:21 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.706646294881589 on epoch=78
05/28/2022 21:20:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=78
05/28/2022 21:20:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=79
05/28/2022 21:20:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=79
05/28/2022 21:20:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=80
05/28/2022 21:20:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=81
05/28/2022 21:20:39 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.7085020242914979 on epoch=81
05/28/2022 21:20:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=81
05/28/2022 21:20:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/28/2022 21:20:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=83
05/28/2022 21:20:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/28/2022 21:20:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/28/2022 21:20:56 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.706646294881589 on epoch=84
05/28/2022 21:20:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/28/2022 21:21:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=85
05/28/2022 21:21:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/28/2022 21:21:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=86
05/28/2022 21:21:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
05/28/2022 21:21:14 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7121089728359864 on epoch=87
05/28/2022 21:21:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/28/2022 21:21:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
05/28/2022 21:21:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=89
05/28/2022 21:21:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
05/28/2022 21:21:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/28/2022 21:21:32 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7138133551668214 on epoch=90
05/28/2022 21:21:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
05/28/2022 21:21:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=91
05/28/2022 21:21:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=92
05/28/2022 21:21:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/28/2022 21:21:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=93
05/28/2022 21:21:50 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.6882051282051281 on epoch=93
05/28/2022 21:21:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
05/28/2022 21:21:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=94
05/28/2022 21:21:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=95
05/28/2022 21:22:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/28/2022 21:22:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
05/28/2022 21:22:09 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.6964607663196193 on epoch=96
05/28/2022 21:22:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/28/2022 21:22:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=98
05/28/2022 21:22:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/28/2022 21:22:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=99
05/28/2022 21:22:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=99
05/28/2022 21:22:27 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6976785569845506 on epoch=99
05/28/2022 21:22:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/28/2022 21:22:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/28/2022 21:22:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/28/2022 21:22:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
05/28/2022 21:22:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/28/2022 21:22:45 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.6572278700719068 on epoch=103
05/28/2022 21:22:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/28/2022 21:22:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=104
05/28/2022 21:22:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/28/2022 21:22:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/28/2022 21:22:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=106
05/28/2022 21:23:03 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.6858692844226298 on epoch=106
05/28/2022 21:23:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/28/2022 21:23:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/28/2022 21:23:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=108
05/28/2022 21:23:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/28/2022 21:23:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=109
05/28/2022 21:23:22 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.7255124566382845 on epoch=109
05/28/2022 21:23:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/28/2022 21:23:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
05/28/2022 21:23:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=111
05/28/2022 21:23:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=111
05/28/2022 21:23:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
05/28/2022 21:23:40 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.722005748961993 on epoch=112
05/28/2022 21:23:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
05/28/2022 21:23:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/28/2022 21:23:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=114
05/28/2022 21:23:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/28/2022 21:23:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=115
05/28/2022 21:23:58 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.722943722943723 on epoch=115
05/28/2022 21:24:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/28/2022 21:24:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
05/28/2022 21:24:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/28/2022 21:24:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
05/28/2022 21:24:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/28/2022 21:24:16 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.6846268559942942 on epoch=118
05/28/2022 21:24:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=119
05/28/2022 21:24:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=119
05/28/2022 21:24:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=120
05/28/2022 21:24:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/28/2022 21:24:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=121
05/28/2022 21:24:35 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.6721828211189913 on epoch=121
05/28/2022 21:24:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/28/2022 21:24:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/28/2022 21:24:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=123
05/28/2022 21:24:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=124
05/28/2022 21:24:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=124
05/28/2022 21:24:53 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.7400850307760645 on epoch=124
05/28/2022 21:24:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/28/2022 21:24:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
05/28/2022 21:25:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=126
05/28/2022 21:25:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=127
05/28/2022 21:25:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/28/2022 21:25:11 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.6572278700719068 on epoch=128
05/28/2022 21:25:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=128
05/28/2022 21:25:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=129
05/28/2022 21:25:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=129
05/28/2022 21:25:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=130
05/28/2022 21:25:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/28/2022 21:25:29 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.678491624667678 on epoch=131
05/28/2022 21:25:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/28/2022 21:25:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=132
05/28/2022 21:25:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=133
05/28/2022 21:25:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=133
05/28/2022 21:25:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/28/2022 21:25:47 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.7193294669103043 on epoch=134
05/28/2022 21:25:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
05/28/2022 21:25:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=135
05/28/2022 21:25:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/28/2022 21:25:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=136
05/28/2022 21:25:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=137
05/28/2022 21:26:05 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7128205128205128 on epoch=137
05/28/2022 21:26:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
05/28/2022 21:26:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=138
05/28/2022 21:26:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=139
05/28/2022 21:26:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
05/28/2022 21:26:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
05/28/2022 21:26:22 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.7246963562753037 on epoch=140
05/28/2022 21:26:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=141
05/28/2022 21:26:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
05/28/2022 21:26:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=142
05/28/2022 21:26:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
05/28/2022 21:26:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/28/2022 21:26:40 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.6637772524297347 on epoch=143
05/28/2022 21:26:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
05/28/2022 21:26:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=144
05/28/2022 21:26:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/28/2022 21:26:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=146
05/28/2022 21:26:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
05/28/2022 21:26:58 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.7319626879878165 on epoch=146
05/28/2022 21:27:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
05/28/2022 21:27:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
05/28/2022 21:27:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=148
05/28/2022 21:27:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=149
05/28/2022 21:27:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.16 on epoch=149
05/28/2022 21:27:16 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.6349422047746069 on epoch=149
05/28/2022 21:27:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
05/28/2022 21:27:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/28/2022 21:27:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=151
05/28/2022 21:27:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=152
05/28/2022 21:27:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=153
05/28/2022 21:27:33 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.736986301369863 on epoch=153
05/28/2022 21:27:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=153
05/28/2022 21:27:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/28/2022 21:27:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/28/2022 21:27:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
05/28/2022 21:27:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
05/28/2022 21:27:51 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.7274648446810201 on epoch=156
05/28/2022 21:27:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/28/2022 21:27:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
05/28/2022 21:27:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
05/28/2022 21:28:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/28/2022 21:28:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
05/28/2022 21:28:09 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6687142693334645 on epoch=159
05/28/2022 21:28:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
05/28/2022 21:28:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
05/28/2022 21:28:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/28/2022 21:28:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
05/28/2022 21:28:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
05/28/2022 21:28:26 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.6858692844226298 on epoch=162
05/28/2022 21:28:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=163
05/28/2022 21:28:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=163
05/28/2022 21:28:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/28/2022 21:28:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/28/2022 21:28:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=165
05/28/2022 21:28:44 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.6771012498566678 on epoch=165
05/28/2022 21:28:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
05/28/2022 21:28:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=166
05/28/2022 21:28:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.09 on epoch=167
05/28/2022 21:28:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/28/2022 21:28:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/28/2022 21:29:01 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.6833333333333333 on epoch=168
05/28/2022 21:29:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/28/2022 21:29:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
05/28/2022 21:29:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/28/2022 21:29:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
05/28/2022 21:29:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
05/28/2022 21:29:19 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.7124134057292641 on epoch=171
05/28/2022 21:29:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/28/2022 21:29:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/28/2022 21:29:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
05/28/2022 21:29:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
05/28/2022 21:29:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
05/28/2022 21:29:37 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.655371683740478 on epoch=174
05/28/2022 21:29:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=175
05/28/2022 21:29:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
05/28/2022 21:29:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/28/2022 21:29:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
05/28/2022 21:29:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
05/28/2022 21:29:54 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6672315919374743 on epoch=178
05/28/2022 21:29:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
05/28/2022 21:29:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/28/2022 21:30:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=179
05/28/2022 21:30:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/28/2022 21:30:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
05/28/2022 21:30:12 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.6771012498566678 on epoch=181
05/28/2022 21:30:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=181
05/28/2022 21:30:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/28/2022 21:30:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=183
05/28/2022 21:30:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=183
05/28/2022 21:30:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/28/2022 21:30:30 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7197197197197198 on epoch=184
05/28/2022 21:30:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/28/2022 21:30:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=185
05/28/2022 21:30:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/28/2022 21:30:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=186
05/28/2022 21:30:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/28/2022 21:30:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:30:44 - INFO - __main__ - Printing 3 examples
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:30:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:30:44 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:30:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:30:44 - INFO - __main__ - Printing 3 examples
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 21:30:44 - INFO - __main__ - ['false']
05/28/2022 21:30:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:30:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:30:44 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:30:47 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.6917494822354584 on epoch=187
05/28/2022 21:30:47 - INFO - __main__ - save last model!
05/28/2022 21:30:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 21:30:47 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 21:30:47 - INFO - __main__ - Printing 3 examples
05/28/2022 21:30:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 21:30:47 - INFO - __main__ - ['false']
05/28/2022 21:30:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 21:30:47 - INFO - __main__ - ['false']
05/28/2022 21:30:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 21:30:47 - INFO - __main__ - ['false']
05/28/2022 21:30:47 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:30:49 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:30:51 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 21:31:00 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:31:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:31:00 - INFO - __main__ - Starting training!
05/28/2022 21:31:42 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.3_8_predictions.txt
05/28/2022 21:31:42 - INFO - __main__ - Classification-F1 on test data: 0.4182
05/28/2022 21:31:43 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.3, bsz=8, dev_performance=0.7409065816107465, test_performance=0.4181663351318267
05/28/2022 21:31:43 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.2, bsz=8 ...
05/28/2022 21:31:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:31:44 - INFO - __main__ - Printing 3 examples
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:31:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:31:44 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:31:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:31:44 - INFO - __main__ - Printing 3 examples
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
05/28/2022 21:31:44 - INFO - __main__ - ['false']
05/28/2022 21:31:44 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:31:44 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:31:44 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:32:03 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:32:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:32:04 - INFO - __main__ - Starting training!
05/28/2022 21:32:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.04 on epoch=0
05/28/2022 21:32:10 - INFO - __main__ - Step 20 Global step 20 Train loss 1.15 on epoch=1
05/28/2022 21:32:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=1
05/28/2022 21:32:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=2
05/28/2022 21:32:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
05/28/2022 21:32:24 - INFO - __main__ - Global step 50 Train loss 1.14 Classification-F1 0.36980174762754864 on epoch=3
05/28/2022 21:32:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36980174762754864 on epoch=3, global_step=50
05/28/2022 21:32:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=3
05/28/2022 21:32:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
05/28/2022 21:32:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=4
05/28/2022 21:32:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=5
05/28/2022 21:32:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/28/2022 21:32:43 - INFO - __main__ - Global step 100 Train loss 0.39 Classification-F1 0.3754404987801573 on epoch=6
05/28/2022 21:32:43 - INFO - __main__ - Saving model with best Classification-F1: 0.36980174762754864 -> 0.3754404987801573 on epoch=6, global_step=100
05/28/2022 21:32:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=6
05/28/2022 21:32:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=7
05/28/2022 21:32:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
05/28/2022 21:32:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/28/2022 21:32:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
05/28/2022 21:33:01 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3531737612590892 on epoch=9
05/28/2022 21:33:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=9
05/28/2022 21:33:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=10
05/28/2022 21:33:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=11
05/28/2022 21:33:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=11
05/28/2022 21:33:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
05/28/2022 21:33:20 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=12
05/28/2022 21:33:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
05/28/2022 21:33:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/28/2022 21:33:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=14
05/28/2022 21:33:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
05/28/2022 21:33:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=15
05/28/2022 21:33:39 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3651088894055646 on epoch=15
05/28/2022 21:33:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/28/2022 21:33:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
05/28/2022 21:33:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=17
05/28/2022 21:33:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/28/2022 21:33:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/28/2022 21:33:57 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.46997929606625255 on epoch=18
05/28/2022 21:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3754404987801573 -> 0.46997929606625255 on epoch=18, global_step=300
05/28/2022 21:34:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
05/28/2022 21:34:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=19
05/28/2022 21:34:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
05/28/2022 21:34:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
05/28/2022 21:34:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
05/28/2022 21:34:16 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.5618051222984755 on epoch=21
05/28/2022 21:34:16 - INFO - __main__ - Saving model with best Classification-F1: 0.46997929606625255 -> 0.5618051222984755 on epoch=21, global_step=350
05/28/2022 21:34:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=22
05/28/2022 21:34:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=23
05/28/2022 21:34:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/28/2022 21:34:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/28/2022 21:34:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/28/2022 21:34:35 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.47065225277515776 on epoch=24
05/28/2022 21:34:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/28/2022 21:34:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
05/28/2022 21:34:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 21:34:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=27
05/28/2022 21:34:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/28/2022 21:34:54 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.6191371075092005 on epoch=28
05/28/2022 21:34:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5618051222984755 -> 0.6191371075092005 on epoch=28, global_step=450
05/28/2022 21:34:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
05/28/2022 21:34:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
05/28/2022 21:35:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
05/28/2022 21:35:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=30
05/28/2022 21:35:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=31
05/28/2022 21:35:13 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.6367076631977294 on epoch=31
05/28/2022 21:35:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6191371075092005 -> 0.6367076631977294 on epoch=31, global_step=500
05/28/2022 21:35:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/28/2022 21:35:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
05/28/2022 21:35:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=33
05/28/2022 21:35:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
05/28/2022 21:35:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=34
05/28/2022 21:35:32 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.6652668174196625 on epoch=34
05/28/2022 21:35:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6367076631977294 -> 0.6652668174196625 on epoch=34, global_step=550
05/28/2022 21:35:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/28/2022 21:35:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
05/28/2022 21:35:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/28/2022 21:35:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/28/2022 21:35:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=37
05/28/2022 21:35:51 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.6402427339508145 on epoch=37
05/28/2022 21:35:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/28/2022 21:35:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/28/2022 21:35:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
05/28/2022 21:36:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=39
05/28/2022 21:36:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/28/2022 21:36:09 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.6542464447445777 on epoch=40
05/28/2022 21:36:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=41
05/28/2022 21:36:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=41
05/28/2022 21:36:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
05/28/2022 21:36:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/28/2022 21:36:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/28/2022 21:36:28 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6791979949874687 on epoch=43
05/28/2022 21:36:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6652668174196625 -> 0.6791979949874687 on epoch=43, global_step=700
05/28/2022 21:36:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=44
05/28/2022 21:36:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
05/28/2022 21:36:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/28/2022 21:36:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
05/28/2022 21:36:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=46
05/28/2022 21:36:47 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.6827757125154895 on epoch=46
05/28/2022 21:36:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6791979949874687 -> 0.6827757125154895 on epoch=46, global_step=750
05/28/2022 21:36:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/28/2022 21:36:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=48
05/28/2022 21:36:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
05/28/2022 21:36:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=49
05/28/2022 21:37:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=49
05/28/2022 21:37:06 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.6893156156386819 on epoch=49
05/28/2022 21:37:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6827757125154895 -> 0.6893156156386819 on epoch=49, global_step=800
05/28/2022 21:37:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/28/2022 21:37:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=51
05/28/2022 21:37:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
05/28/2022 21:37:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=52
05/28/2022 21:37:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
05/28/2022 21:37:24 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.6963226571767498 on epoch=53
05/28/2022 21:37:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6893156156386819 -> 0.6963226571767498 on epoch=53, global_step=850
05/28/2022 21:37:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=53
05/28/2022 21:37:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=54
05/28/2022 21:37:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
05/28/2022 21:37:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
05/28/2022 21:37:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/28/2022 21:37:43 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.7062738078847124 on epoch=56
05/28/2022 21:37:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6963226571767498 -> 0.7062738078847124 on epoch=56, global_step=900
05/28/2022 21:37:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/28/2022 21:37:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=57
05/28/2022 21:37:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=58
05/28/2022 21:37:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=58
05/28/2022 21:37:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=59
05/28/2022 21:38:01 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.7303658927508357 on epoch=59
05/28/2022 21:38:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7062738078847124 -> 0.7303658927508357 on epoch=59, global_step=950
05/28/2022 21:38:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=59
05/28/2022 21:38:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=60
05/28/2022 21:38:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=61
05/28/2022 21:38:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=61
05/28/2022 21:38:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=62
05/28/2022 21:38:19 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.7264122137404581 on epoch=62
05/28/2022 21:38:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
05/28/2022 21:38:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=63
05/28/2022 21:38:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=64
05/28/2022 21:38:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
05/28/2022 21:38:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
05/28/2022 21:38:37 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.7261446298673513 on epoch=65
05/28/2022 21:38:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=66
05/28/2022 21:38:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/28/2022 21:38:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=67
05/28/2022 21:38:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=68
05/28/2022 21:38:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=68
05/28/2022 21:38:55 - INFO - __main__ - Global step 1100 Train loss 0.31 Classification-F1 0.6768432371779887 on epoch=68
05/28/2022 21:38:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=69
05/28/2022 21:39:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=69
05/28/2022 21:39:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=70
05/28/2022 21:39:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=71
05/28/2022 21:39:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/28/2022 21:39:12 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.6716716716716716 on epoch=71
05/28/2022 21:39:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=72
05/28/2022 21:39:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=73
05/28/2022 21:39:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=73
05/28/2022 21:39:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=74
05/28/2022 21:39:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
05/28/2022 21:39:30 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7219392047975277 on epoch=74
05/28/2022 21:39:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=75
05/28/2022 21:39:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/28/2022 21:39:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=76
05/28/2022 21:39:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=77
05/28/2022 21:39:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
05/28/2022 21:39:48 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.7125232667251219 on epoch=78
05/28/2022 21:39:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=78
05/28/2022 21:39:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
05/28/2022 21:39:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/28/2022 21:39:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/28/2022 21:40:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=81
05/28/2022 21:40:05 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.6796796796796798 on epoch=81
05/28/2022 21:40:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
05/28/2022 21:40:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/28/2022 21:40:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=83
05/28/2022 21:40:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=83
05/28/2022 21:40:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=84
05/28/2022 21:40:23 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.6995305164319249 on epoch=84
05/28/2022 21:40:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=84
05/28/2022 21:40:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=85
05/28/2022 21:40:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=86
05/28/2022 21:40:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=86
05/28/2022 21:40:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/28/2022 21:40:41 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.7111368730195533 on epoch=87
05/28/2022 21:40:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=88
05/28/2022 21:40:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=88
05/28/2022 21:40:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/28/2022 21:40:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=89
05/28/2022 21:40:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=90
05/28/2022 21:40:58 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.6796705472400413 on epoch=90
05/28/2022 21:41:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/28/2022 21:41:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=91
05/28/2022 21:41:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/28/2022 21:41:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=93
05/28/2022 21:41:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=93
05/28/2022 21:41:16 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.675 on epoch=93
05/28/2022 21:41:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/28/2022 21:41:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=94
05/28/2022 21:41:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
05/28/2022 21:41:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=96
05/28/2022 21:41:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=96
05/28/2022 21:41:34 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.6958897600380138 on epoch=96
05/28/2022 21:41:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=97
05/28/2022 21:41:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=98
05/28/2022 21:41:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=98
05/28/2022 21:41:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=99
05/28/2022 21:41:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
05/28/2022 21:41:52 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.7167391338226814 on epoch=99
05/28/2022 21:41:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
05/28/2022 21:41:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=101
05/28/2022 21:41:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=101
05/28/2022 21:42:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=102
05/28/2022 21:42:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
05/28/2022 21:42:09 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.6988477963097253 on epoch=103
05/28/2022 21:42:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=103
05/28/2022 21:42:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
05/28/2022 21:42:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
05/28/2022 21:42:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/28/2022 21:42:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=106
05/28/2022 21:42:27 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.7035321366531558 on epoch=106
05/28/2022 21:42:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=106
05/28/2022 21:42:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/28/2022 21:42:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=108
05/28/2022 21:42:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=108
05/28/2022 21:42:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/28/2022 21:42:45 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.7035321366531558 on epoch=109
05/28/2022 21:42:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=109
05/28/2022 21:42:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=110
05/28/2022 21:42:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=111
05/28/2022 21:42:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=111
05/28/2022 21:42:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/28/2022 21:43:03 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7277277277277278 on epoch=112
05/28/2022 21:43:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
05/28/2022 21:43:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=113
05/28/2022 21:43:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/28/2022 21:43:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/28/2022 21:43:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
05/28/2022 21:43:20 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.7046153846153846 on epoch=115
05/28/2022 21:43:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/28/2022 21:43:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=116
05/28/2022 21:43:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/28/2022 21:43:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=118
05/28/2022 21:43:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=118
05/28/2022 21:43:38 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.6988477963097253 on epoch=118
05/28/2022 21:43:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=119
05/28/2022 21:43:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=119
05/28/2022 21:43:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=120
05/28/2022 21:43:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=121
05/28/2022 21:43:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=121
05/28/2022 21:43:56 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.714795008912656 on epoch=121
05/28/2022 21:43:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
05/28/2022 21:44:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=123
05/28/2022 21:44:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/28/2022 21:44:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/28/2022 21:44:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=124
05/28/2022 21:44:14 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7313914337170151 on epoch=124
05/28/2022 21:44:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7303658927508357 -> 0.7313914337170151 on epoch=124, global_step=2000
05/28/2022 21:44:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=125
05/28/2022 21:44:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=126
05/28/2022 21:44:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=126
05/28/2022 21:44:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
05/28/2022 21:44:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=128
05/28/2022 21:44:32 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.7238403451995685 on epoch=128
05/28/2022 21:44:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=128
05/28/2022 21:44:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=129
05/28/2022 21:44:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=129
05/28/2022 21:44:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
05/28/2022 21:44:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/28/2022 21:44:50 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.6929261492576023 on epoch=131
05/28/2022 21:44:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/28/2022 21:44:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=132
05/28/2022 21:44:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=133
05/28/2022 21:45:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
05/28/2022 21:45:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=134
05/28/2022 21:45:08 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.74006092322112 on epoch=134
05/28/2022 21:45:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7313914337170151 -> 0.74006092322112 on epoch=134, global_step=2150
05/28/2022 21:45:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=134
05/28/2022 21:45:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/28/2022 21:45:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
05/28/2022 21:45:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
05/28/2022 21:45:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/28/2022 21:45:26 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7138294474608752 on epoch=137
05/28/2022 21:45:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
05/28/2022 21:45:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
05/28/2022 21:45:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/28/2022 21:45:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=139
05/28/2022 21:45:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
05/28/2022 21:45:44 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.7372368815828951 on epoch=140
05/28/2022 21:45:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
05/28/2022 21:45:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/28/2022 21:45:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=142
05/28/2022 21:45:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
05/28/2022 21:45:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
05/28/2022 21:46:01 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.722943722943723 on epoch=143
05/28/2022 21:46:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=144
05/28/2022 21:46:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/28/2022 21:46:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=145
05/28/2022 21:46:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
05/28/2022 21:46:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.26 on epoch=146
05/28/2022 21:46:19 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.722005748961993 on epoch=146
05/28/2022 21:46:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/28/2022 21:46:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
05/28/2022 21:46:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
05/28/2022 21:46:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=149
05/28/2022 21:46:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
05/28/2022 21:46:37 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7274648446810201 on epoch=149
05/28/2022 21:46:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=150
05/28/2022 21:46:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
05/28/2022 21:46:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=151
05/28/2022 21:46:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
05/28/2022 21:46:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/28/2022 21:46:55 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.722005748961993 on epoch=153
05/28/2022 21:46:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/28/2022 21:47:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
05/28/2022 21:47:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/28/2022 21:47:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=155
05/28/2022 21:47:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/28/2022 21:47:13 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7319626879878165 on epoch=156
05/28/2022 21:47:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/28/2022 21:47:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=157
05/28/2022 21:47:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/28/2022 21:47:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=158
05/28/2022 21:47:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/28/2022 21:47:31 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.7265611299759274 on epoch=159
05/28/2022 21:47:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=159
05/28/2022 21:47:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=160
05/28/2022 21:47:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
05/28/2022 21:47:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
05/28/2022 21:47:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=162
05/28/2022 21:47:49 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.751113341278304 on epoch=162
05/28/2022 21:47:49 - INFO - __main__ - Saving model with best Classification-F1: 0.74006092322112 -> 0.751113341278304 on epoch=162, global_step=2600
05/28/2022 21:47:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/28/2022 21:47:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=163
05/28/2022 21:47:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
05/28/2022 21:47:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/28/2022 21:48:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
05/28/2022 21:48:06 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.6504828504828504 on epoch=165
05/28/2022 21:48:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/28/2022 21:48:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/28/2022 21:48:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=167
05/28/2022 21:48:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
05/28/2022 21:48:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/28/2022 21:48:24 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.6833333333333333 on epoch=168
05/28/2022 21:48:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
05/28/2022 21:48:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=169
05/28/2022 21:48:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=170
05/28/2022 21:48:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=171
05/28/2022 21:48:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/28/2022 21:48:42 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.7265611299759274 on epoch=171
05/28/2022 21:48:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
05/28/2022 21:48:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=173
05/28/2022 21:48:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/28/2022 21:48:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/28/2022 21:48:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=174
05/28/2022 21:49:00 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.6893957777238535 on epoch=174
05/28/2022 21:49:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/28/2022 21:49:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
05/28/2022 21:49:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=176
05/28/2022 21:49:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
05/28/2022 21:49:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/28/2022 21:49:18 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.6798284795426122 on epoch=178
05/28/2022 21:49:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=178
05/28/2022 21:49:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/28/2022 21:49:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
05/28/2022 21:49:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=180
05/28/2022 21:49:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
05/28/2022 21:49:36 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7035321366531558 on epoch=181
05/28/2022 21:49:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
05/28/2022 21:49:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
05/28/2022 21:49:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/28/2022 21:49:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
05/28/2022 21:49:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
05/28/2022 21:49:54 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.6833333333333333 on epoch=184
05/28/2022 21:49:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
05/28/2022 21:49:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=185
05/28/2022 21:50:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/28/2022 21:50:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
05/28/2022 21:50:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
05/28/2022 21:50:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:50:09 - INFO - __main__ - Printing 3 examples
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:50:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:50:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:50:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:50:09 - INFO - __main__ - Printing 3 examples
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 21:50:09 - INFO - __main__ - ['false']
05/28/2022 21:50:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:50:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:50:10 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:50:13 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.7183987756468506 on epoch=187
05/28/2022 21:50:13 - INFO - __main__ - save last model!
05/28/2022 21:50:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 21:50:13 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 21:50:13 - INFO - __main__ - Printing 3 examples
05/28/2022 21:50:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 21:50:13 - INFO - __main__ - ['false']
05/28/2022 21:50:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 21:50:13 - INFO - __main__ - ['false']
05/28/2022 21:50:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 21:50:13 - INFO - __main__ - ['false']
05/28/2022 21:50:13 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:50:14 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:50:17 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 21:50:28 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:50:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:50:29 - INFO - __main__ - Starting training!
05/28/2022 21:51:07 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.2_8_predictions.txt
05/28/2022 21:51:07 - INFO - __main__ - Classification-F1 on test data: 0.4295
05/28/2022 21:51:08 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.2, bsz=8, dev_performance=0.751113341278304, test_performance=0.4294819407678504
05/28/2022 21:51:08 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.5, bsz=8 ...
05/28/2022 21:51:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:51:09 - INFO - __main__ - Printing 3 examples
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:51:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:51:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 21:51:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 21:51:09 - INFO - __main__ - Printing 3 examples
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 21:51:09 - INFO - __main__ - ['false']
05/28/2022 21:51:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 21:51:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 21:51:09 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 21:51:28 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 21:51:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 21:51:29 - INFO - __main__ - Starting training!
05/28/2022 21:51:32 - INFO - __main__ - Step 10 Global step 10 Train loss 2.22 on epoch=0
05/28/2022 21:51:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=1
05/28/2022 21:51:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=1
05/28/2022 21:51:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=2
05/28/2022 21:51:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=3
05/28/2022 21:51:48 - INFO - __main__ - Global step 50 Train loss 0.81 Classification-F1 0.36318407960199 on epoch=3
05/28/2022 21:51:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36318407960199 on epoch=3, global_step=50
05/28/2022 21:51:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=3
05/28/2022 21:51:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=4
05/28/2022 21:51:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=4
05/28/2022 21:51:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=5
05/28/2022 21:52:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/28/2022 21:52:07 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=6
05/28/2022 21:52:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/28/2022 21:52:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=7
05/28/2022 21:52:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/28/2022 21:52:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
05/28/2022 21:52:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
05/28/2022 21:52:24 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.5453371221906484 on epoch=9
05/28/2022 21:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.36318407960199 -> 0.5453371221906484 on epoch=9, global_step=150
05/28/2022 21:52:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
05/28/2022 21:52:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
05/28/2022 21:52:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=11
05/28/2022 21:52:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=11
05/28/2022 21:52:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
05/28/2022 21:52:43 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=12
05/28/2022 21:52:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/28/2022 21:52:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/28/2022 21:52:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/28/2022 21:52:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=14
05/28/2022 21:52:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=15
05/28/2022 21:53:01 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.5490462529568675 on epoch=15
05/28/2022 21:53:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5453371221906484 -> 0.5490462529568675 on epoch=15, global_step=250
05/28/2022 21:53:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
05/28/2022 21:53:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=16
05/28/2022 21:53:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/28/2022 21:53:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/28/2022 21:53:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
05/28/2022 21:53:18 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.4599156118143459 on epoch=18
05/28/2022 21:53:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
05/28/2022 21:53:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=19
05/28/2022 21:53:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 21:53:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
05/28/2022 21:53:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/28/2022 21:53:35 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.6263921960211999 on epoch=21
05/28/2022 21:53:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5490462529568675 -> 0.6263921960211999 on epoch=21, global_step=350
05/28/2022 21:53:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=22
05/28/2022 21:53:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=23
05/28/2022 21:53:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/28/2022 21:53:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/28/2022 21:53:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=24
05/28/2022 21:53:51 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.5855185191367195 on epoch=24
05/28/2022 21:53:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=25
05/28/2022 21:53:56 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
05/28/2022 21:53:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=26
05/28/2022 21:54:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/28/2022 21:54:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=28
05/28/2022 21:54:09 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.6267793667174473 on epoch=28
05/28/2022 21:54:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6263921960211999 -> 0.6267793667174473 on epoch=28, global_step=450
05/28/2022 21:54:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=28
05/28/2022 21:54:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=29
05/28/2022 21:54:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=29
05/28/2022 21:54:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=30
05/28/2022 21:54:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=31
05/28/2022 21:54:26 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.6711524345485687 on epoch=31
05/28/2022 21:54:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6267793667174473 -> 0.6711524345485687 on epoch=31, global_step=500
05/28/2022 21:54:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=31
05/28/2022 21:54:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=32
05/28/2022 21:54:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/28/2022 21:54:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=33
05/28/2022 21:54:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/28/2022 21:54:43 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.6827757125154895 on epoch=34
05/28/2022 21:54:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6711524345485687 -> 0.6827757125154895 on epoch=34, global_step=550
05/28/2022 21:54:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.32 on epoch=34
05/28/2022 21:54:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=35
05/28/2022 21:54:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=36
05/28/2022 21:54:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=36
05/28/2022 21:54:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=37
05/28/2022 21:55:01 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.7057335969469861 on epoch=37
05/28/2022 21:55:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6827757125154895 -> 0.7057335969469861 on epoch=37, global_step=600
05/28/2022 21:55:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=38
05/28/2022 21:55:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=38
05/28/2022 21:55:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/28/2022 21:55:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=39
05/28/2022 21:55:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=40
05/28/2022 21:55:18 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.6739360049704877 on epoch=40
05/28/2022 21:55:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=41
05/28/2022 21:55:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=41
05/28/2022 21:55:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=42
05/28/2022 21:55:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=43
05/28/2022 21:55:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=43
05/28/2022 21:55:35 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.643130389677754 on epoch=43
05/28/2022 21:55:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=44
05/28/2022 21:55:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=44
05/28/2022 21:55:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=45
05/28/2022 21:55:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=46
05/28/2022 21:55:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/28/2022 21:55:52 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6805472107963211 on epoch=46
05/28/2022 21:55:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=47
05/28/2022 21:55:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/28/2022 21:56:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/28/2022 21:56:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=49
05/28/2022 21:56:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=49
05/28/2022 21:56:10 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.7029618320610687 on epoch=49
05/28/2022 21:56:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/28/2022 21:56:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=51
05/28/2022 21:56:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/28/2022 21:56:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=52
05/28/2022 21:56:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/28/2022 21:56:27 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.6518218623481782 on epoch=53
05/28/2022 21:56:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/28/2022 21:56:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=54
05/28/2022 21:56:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=54
05/28/2022 21:56:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/28/2022 21:56:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
05/28/2022 21:56:45 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.7091623679685581 on epoch=56
05/28/2022 21:56:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7057335969469861 -> 0.7091623679685581 on epoch=56, global_step=900
05/28/2022 21:56:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=56
05/28/2022 21:56:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=57
05/28/2022 21:56:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/28/2022 21:56:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=58
05/28/2022 21:56:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=59
05/28/2022 21:57:02 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.6835502922459444 on epoch=59
05/28/2022 21:57:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=59
05/28/2022 21:57:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=60
05/28/2022 21:57:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/28/2022 21:57:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/28/2022 21:57:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=62
05/28/2022 21:57:20 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.6437107069155829 on epoch=62
05/28/2022 21:57:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=63
05/28/2022 21:57:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=63
05/28/2022 21:57:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/28/2022 21:57:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=64
05/28/2022 21:57:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/28/2022 21:57:38 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.6045642533260251 on epoch=65
05/28/2022 21:57:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/28/2022 21:57:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/28/2022 21:57:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/28/2022 21:57:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=68
05/28/2022 21:57:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=68
05/28/2022 21:57:56 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.6379756892031461 on epoch=68
05/28/2022 21:57:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=69
05/28/2022 21:58:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=69
05/28/2022 21:58:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=70
05/28/2022 21:58:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
05/28/2022 21:58:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/28/2022 21:58:13 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.6609271523178808 on epoch=71
05/28/2022 21:58:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=72
05/28/2022 21:58:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=73
05/28/2022 21:58:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=73
05/28/2022 21:58:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=74
05/28/2022 21:58:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/28/2022 21:58:30 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.6755386565272496 on epoch=74
05/28/2022 21:58:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=75
05/28/2022 21:58:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=76
05/28/2022 21:58:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=76
05/28/2022 21:58:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=77
05/28/2022 21:58:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/28/2022 21:58:48 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.623406341941811 on epoch=78
05/28/2022 21:58:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=78
05/28/2022 21:58:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
05/28/2022 21:58:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=79
05/28/2022 21:58:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/28/2022 21:59:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=81
05/28/2022 21:59:05 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.6715665816523688 on epoch=81
05/28/2022 21:59:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=81
05/28/2022 21:59:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=82
05/28/2022 21:59:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=83
05/28/2022 21:59:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=83
05/28/2022 21:59:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/28/2022 21:59:23 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.6310129088461234 on epoch=84
05/28/2022 21:59:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=84
05/28/2022 21:59:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=85
05/28/2022 21:59:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=86
05/28/2022 21:59:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=86
05/28/2022 21:59:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/28/2022 21:59:41 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.6447733580018502 on epoch=87
05/28/2022 21:59:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/28/2022 21:59:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=88
05/28/2022 21:59:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/28/2022 21:59:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/28/2022 21:59:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=90
05/28/2022 21:59:59 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.6317750898986596 on epoch=90
05/28/2022 22:00:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=91
05/28/2022 22:00:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=91
05/28/2022 22:00:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=92
05/28/2022 22:00:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=93
05/28/2022 22:00:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/28/2022 22:00:17 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.567855133254898 on epoch=93
05/28/2022 22:00:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/28/2022 22:00:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=94
05/28/2022 22:00:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=95
05/28/2022 22:00:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=96
05/28/2022 22:00:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=96
05/28/2022 22:00:34 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.6577540106951872 on epoch=96
05/28/2022 22:00:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=97
05/28/2022 22:00:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=98
05/28/2022 22:00:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=98
05/28/2022 22:00:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=99
05/28/2022 22:00:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=99
05/28/2022 22:00:51 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.6672461044554068 on epoch=99
05/28/2022 22:00:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=100
05/28/2022 22:00:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=101
05/28/2022 22:00:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/28/2022 22:01:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/28/2022 22:01:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=103
05/28/2022 22:01:09 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.6033338226419402 on epoch=103
05/28/2022 22:01:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=103
05/28/2022 22:01:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=104
05/28/2022 22:01:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=104
05/28/2022 22:01:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=105
05/28/2022 22:01:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/28/2022 22:01:26 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.6542464447445777 on epoch=106
05/28/2022 22:01:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=106
05/28/2022 22:01:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
05/28/2022 22:01:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=108
05/28/2022 22:01:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=108
05/28/2022 22:01:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=109
05/28/2022 22:01:44 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7116360886941226 on epoch=109
05/28/2022 22:01:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7091623679685581 -> 0.7116360886941226 on epoch=109, global_step=1750
05/28/2022 22:01:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=109
05/28/2022 22:01:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=110
05/28/2022 22:01:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=111
05/28/2022 22:01:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=111
05/28/2022 22:01:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=112
05/28/2022 22:02:02 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7304646372167545 on epoch=112
05/28/2022 22:02:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7116360886941226 -> 0.7304646372167545 on epoch=112, global_step=1800
05/28/2022 22:02:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=113
05/28/2022 22:02:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=113
05/28/2022 22:02:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/28/2022 22:02:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=114
05/28/2022 22:02:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=115
05/28/2022 22:02:20 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.6788850174216028 on epoch=115
05/28/2022 22:02:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=116
05/28/2022 22:02:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=116
05/28/2022 22:02:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=117
05/28/2022 22:02:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=118
05/28/2022 22:02:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=118
05/28/2022 22:02:37 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.5503947345985303 on epoch=118
05/28/2022 22:02:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=119
05/28/2022 22:02:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=119
05/28/2022 22:02:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=120
05/28/2022 22:02:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=121
05/28/2022 22:02:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=121
05/28/2022 22:02:55 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.6414565826330533 on epoch=121
05/28/2022 22:02:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=122
05/28/2022 22:03:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/28/2022 22:03:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=123
05/28/2022 22:03:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/28/2022 22:03:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=124
05/28/2022 22:03:13 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.637650389242746 on epoch=124
05/28/2022 22:03:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
05/28/2022 22:03:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=126
05/28/2022 22:03:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=126
05/28/2022 22:03:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=127
05/28/2022 22:03:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=128
05/28/2022 22:03:31 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6507392601053367 on epoch=128
05/28/2022 22:03:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=128
05/28/2022 22:03:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
05/28/2022 22:03:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=129
05/28/2022 22:03:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=130
05/28/2022 22:03:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=131
05/28/2022 22:03:48 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.6696055107213386 on epoch=131
05/28/2022 22:03:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/28/2022 22:03:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
05/28/2022 22:03:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
05/28/2022 22:03:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/28/2022 22:04:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=134
05/28/2022 22:04:06 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.6599190283400811 on epoch=134
05/28/2022 22:04:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=134
05/28/2022 22:04:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=135
05/28/2022 22:04:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=136
05/28/2022 22:04:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=136
05/28/2022 22:04:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/28/2022 22:04:23 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.6080895979373137 on epoch=137
05/28/2022 22:04:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=138
05/28/2022 22:04:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/28/2022 22:04:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/28/2022 22:04:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=139
05/28/2022 22:04:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=140
05/28/2022 22:04:41 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.6542464447445777 on epoch=140
05/28/2022 22:04:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/28/2022 22:04:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=141
05/28/2022 22:04:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=142
05/28/2022 22:04:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=143
05/28/2022 22:04:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
05/28/2022 22:04:59 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.6437107069155829 on epoch=143
05/28/2022 22:05:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=144
05/28/2022 22:05:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/28/2022 22:05:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=145
05/28/2022 22:05:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=146
05/28/2022 22:05:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=146
05/28/2022 22:05:16 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.652905699191173 on epoch=146
05/28/2022 22:05:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
05/28/2022 22:05:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=148
05/28/2022 22:05:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=148
05/28/2022 22:05:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=149
05/28/2022 22:05:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=149
05/28/2022 22:05:34 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6739360049704877 on epoch=149
05/28/2022 22:05:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=150
05/28/2022 22:05:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=151
05/28/2022 22:05:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/28/2022 22:05:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
05/28/2022 22:05:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=153
05/28/2022 22:05:51 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.5652347652347651 on epoch=153
05/28/2022 22:05:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/28/2022 22:05:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=154
05/28/2022 22:05:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=154
05/28/2022 22:06:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=155
05/28/2022 22:06:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/28/2022 22:06:09 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6344945745288406 on epoch=156
05/28/2022 22:06:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
05/28/2022 22:06:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/28/2022 22:06:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
05/28/2022 22:06:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=158
05/28/2022 22:06:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/28/2022 22:06:27 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.606567985116881 on epoch=159
05/28/2022 22:06:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/28/2022 22:06:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=160
05/28/2022 22:06:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/28/2022 22:06:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/28/2022 22:06:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/28/2022 22:06:45 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.6636636636636637 on epoch=162
05/28/2022 22:06:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=163
05/28/2022 22:06:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/28/2022 22:06:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/28/2022 22:06:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/28/2022 22:06:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/28/2022 22:07:02 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.6978864928655724 on epoch=165
05/28/2022 22:07:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
05/28/2022 22:07:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=166
05/28/2022 22:07:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=167
05/28/2022 22:07:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
05/28/2022 22:07:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/28/2022 22:07:20 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.619736502195815 on epoch=168
05/28/2022 22:07:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=169
05/28/2022 22:07:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/28/2022 22:07:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/28/2022 22:07:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=171
05/28/2022 22:07:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=171
05/28/2022 22:07:37 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.6333078686019862 on epoch=171
05/28/2022 22:07:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=172
05/28/2022 22:07:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/28/2022 22:07:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/28/2022 22:07:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
05/28/2022 22:07:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=174
05/28/2022 22:07:55 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6033338226419402 on epoch=174
05/28/2022 22:07:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=175
05/28/2022 22:08:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/28/2022 22:08:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=176
05/28/2022 22:08:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/28/2022 22:08:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/28/2022 22:08:13 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6259893717790228 on epoch=178
05/28/2022 22:08:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/28/2022 22:08:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/28/2022 22:08:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/28/2022 22:08:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/28/2022 22:08:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/28/2022 22:08:30 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.6132993324345835 on epoch=181
05/28/2022 22:08:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
05/28/2022 22:08:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
05/28/2022 22:08:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/28/2022 22:08:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=183
05/28/2022 22:08:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/28/2022 22:08:48 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7016500030669202 on epoch=184
05/28/2022 22:08:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=184
05/28/2022 22:08:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/28/2022 22:08:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/28/2022 22:08:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
05/28/2022 22:09:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/28/2022 22:09:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:09:02 - INFO - __main__ - Printing 3 examples
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:09:02 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:09:02 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:09:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:09:02 - INFO - __main__ - Printing 3 examples
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:09:02 - INFO - __main__ - ['false']
05/28/2022 22:09:02 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:09:02 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:09:02 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:09:06 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6263722317405926 on epoch=187
05/28/2022 22:09:06 - INFO - __main__ - save last model!
05/28/2022 22:09:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 22:09:06 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 22:09:06 - INFO - __main__ - Printing 3 examples
05/28/2022 22:09:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 22:09:06 - INFO - __main__ - ['false']
05/28/2022 22:09:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 22:09:06 - INFO - __main__ - ['false']
05/28/2022 22:09:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 22:09:06 - INFO - __main__ - ['false']
05/28/2022 22:09:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:09:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:09:10 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 22:09:18 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:09:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:09:19 - INFO - __main__ - Starting training!
05/28/2022 22:10:02 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.5_8_predictions.txt
05/28/2022 22:10:02 - INFO - __main__ - Classification-F1 on test data: 0.4175
05/28/2022 22:10:03 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.5, bsz=8, dev_performance=0.7304646372167545, test_performance=0.4175034092383423
05/28/2022 22:10:03 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.4, bsz=8 ...
05/28/2022 22:10:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:10:04 - INFO - __main__ - Printing 3 examples
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:10:04 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:10:04 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:10:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:10:04 - INFO - __main__ - Printing 3 examples
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:10:04 - INFO - __main__ - ['false']
05/28/2022 22:10:04 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:10:04 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:10:04 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:10:23 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:10:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:10:24 - INFO - __main__ - Starting training!
05/28/2022 22:10:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.18 on epoch=0
05/28/2022 22:10:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=1
05/28/2022 22:10:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
05/28/2022 22:10:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=2
05/28/2022 22:10:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
05/28/2022 22:10:43 - INFO - __main__ - Global step 50 Train loss 0.84 Classification-F1 0.5100365941418751 on epoch=3
05/28/2022 22:10:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5100365941418751 on epoch=3, global_step=50
05/28/2022 22:10:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=3
05/28/2022 22:10:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/28/2022 22:10:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=4
05/28/2022 22:10:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/28/2022 22:10:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
05/28/2022 22:11:02 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.518701761329903 on epoch=6
05/28/2022 22:11:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5100365941418751 -> 0.518701761329903 on epoch=6, global_step=100
05/28/2022 22:11:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
05/28/2022 22:11:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=7
05/28/2022 22:11:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=8
05/28/2022 22:11:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/28/2022 22:11:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=9
05/28/2022 22:11:20 - INFO - __main__ - Global step 150 Train loss 0.39 Classification-F1 0.3401530406766009 on epoch=9
05/28/2022 22:11:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
05/28/2022 22:11:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
05/28/2022 22:11:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/28/2022 22:11:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=11
05/28/2022 22:11:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
05/28/2022 22:11:38 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.35693779904306216 on epoch=12
05/28/2022 22:11:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=13
05/28/2022 22:11:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/28/2022 22:11:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
05/28/2022 22:11:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
05/28/2022 22:11:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
05/28/2022 22:11:56 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.444269780672206 on epoch=15
05/28/2022 22:11:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=16
05/28/2022 22:12:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
05/28/2022 22:12:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/28/2022 22:12:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=18
05/28/2022 22:12:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/28/2022 22:12:14 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.6210879682612345 on epoch=18
05/28/2022 22:12:14 - INFO - __main__ - Saving model with best Classification-F1: 0.518701761329903 -> 0.6210879682612345 on epoch=18, global_step=300
05/28/2022 22:12:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
05/28/2022 22:12:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=19
05/28/2022 22:12:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=20
05/28/2022 22:12:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
05/28/2022 22:12:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=21
05/28/2022 22:12:32 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.5670995670995671 on epoch=21
05/28/2022 22:12:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
05/28/2022 22:12:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=23
05/28/2022 22:12:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
05/28/2022 22:12:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=24
05/28/2022 22:12:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/28/2022 22:12:50 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.5778085625700904 on epoch=24
05/28/2022 22:12:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.34 on epoch=25
05/28/2022 22:12:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=26
05/28/2022 22:12:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 22:13:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
05/28/2022 22:13:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=28
05/28/2022 22:13:08 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.6482442748091604 on epoch=28
05/28/2022 22:13:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6210879682612345 -> 0.6482442748091604 on epoch=28, global_step=450
05/28/2022 22:13:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=28
05/28/2022 22:13:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=29
05/28/2022 22:13:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=29
05/28/2022 22:13:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=30
05/28/2022 22:13:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=31
05/28/2022 22:13:26 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.655493026669929 on epoch=31
05/28/2022 22:13:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6482442748091604 -> 0.655493026669929 on epoch=31, global_step=500
05/28/2022 22:13:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.30 on epoch=31
05/28/2022 22:13:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
05/28/2022 22:13:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=33
05/28/2022 22:13:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.31 on epoch=33
05/28/2022 22:13:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=34
05/28/2022 22:13:44 - INFO - __main__ - Global step 550 Train loss 0.32 Classification-F1 0.6793743890518085 on epoch=34
05/28/2022 22:13:44 - INFO - __main__ - Saving model with best Classification-F1: 0.655493026669929 -> 0.6793743890518085 on epoch=34, global_step=550
05/28/2022 22:13:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=34
05/28/2022 22:13:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=35
05/28/2022 22:13:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=36
05/28/2022 22:13:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=36
05/28/2022 22:13:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=37
05/28/2022 22:14:01 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.6874809253494476 on epoch=37
05/28/2022 22:14:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6793743890518085 -> 0.6874809253494476 on epoch=37, global_step=600
05/28/2022 22:14:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=38
05/28/2022 22:14:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=38
05/28/2022 22:14:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/28/2022 22:14:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=39
05/28/2022 22:14:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=40
05/28/2022 22:14:18 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.4504776269482152 on epoch=40
05/28/2022 22:14:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/28/2022 22:14:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=41
05/28/2022 22:14:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=42
05/28/2022 22:14:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=43
05/28/2022 22:14:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=43
05/28/2022 22:14:36 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.671560940841055 on epoch=43
05/28/2022 22:14:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=44
05/28/2022 22:14:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=44
05/28/2022 22:14:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/28/2022 22:14:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=46
05/28/2022 22:14:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=46
05/28/2022 22:14:53 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.6910243678863341 on epoch=46
05/28/2022 22:14:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6874809253494476 -> 0.6910243678863341 on epoch=46, global_step=750
05/28/2022 22:14:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=47
05/28/2022 22:14:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/28/2022 22:15:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
05/28/2022 22:15:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=49
05/28/2022 22:15:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=49
05/28/2022 22:15:11 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.6834730045335897 on epoch=49
05/28/2022 22:15:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/28/2022 22:15:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/28/2022 22:15:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=51
05/28/2022 22:15:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=52
05/28/2022 22:15:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=53
05/28/2022 22:15:28 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.6947425474254743 on epoch=53
05/28/2022 22:15:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6910243678863341 -> 0.6947425474254743 on epoch=53, global_step=850
05/28/2022 22:15:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=53
05/28/2022 22:15:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/28/2022 22:15:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=54
05/28/2022 22:15:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=55
05/28/2022 22:15:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=56
05/28/2022 22:15:45 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.6963226571767498 on epoch=56
05/28/2022 22:15:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6947425474254743 -> 0.6963226571767498 on epoch=56, global_step=900
05/28/2022 22:15:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=56
05/28/2022 22:15:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=57
05/28/2022 22:15:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/28/2022 22:15:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=58
05/28/2022 22:15:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=59
05/28/2022 22:16:03 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.6884478562067029 on epoch=59
05/28/2022 22:16:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=59
05/28/2022 22:16:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=60
05/28/2022 22:16:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
05/28/2022 22:16:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=61
05/28/2022 22:16:15 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=62
05/28/2022 22:16:21 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.6958353264307868 on epoch=62
05/28/2022 22:16:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=63
05/28/2022 22:16:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/28/2022 22:16:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=64
05/28/2022 22:16:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/28/2022 22:16:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/28/2022 22:16:38 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7000702960243692 on epoch=65
05/28/2022 22:16:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6963226571767498 -> 0.7000702960243692 on epoch=65, global_step=1050
05/28/2022 22:16:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=66
05/28/2022 22:16:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=66
05/28/2022 22:16:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
05/28/2022 22:16:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=68
05/28/2022 22:16:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/28/2022 22:16:57 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.5887550200803213 on epoch=68
05/28/2022 22:16:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
05/28/2022 22:17:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/28/2022 22:17:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=70
05/28/2022 22:17:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/28/2022 22:17:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=71
05/28/2022 22:17:15 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.7211200613732258 on epoch=71
05/28/2022 22:17:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7000702960243692 -> 0.7211200613732258 on epoch=71, global_step=1150
05/28/2022 22:17:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/28/2022 22:17:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=73
05/28/2022 22:17:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=73
05/28/2022 22:17:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/28/2022 22:17:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=74
05/28/2022 22:17:32 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7421717634132943 on epoch=74
05/28/2022 22:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7211200613732258 -> 0.7421717634132943 on epoch=74, global_step=1200
05/28/2022 22:17:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/28/2022 22:17:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=76
05/28/2022 22:17:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=76
05/28/2022 22:17:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=77
05/28/2022 22:17:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/28/2022 22:17:50 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.6770734783979817 on epoch=78
05/28/2022 22:17:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=78
05/28/2022 22:17:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/28/2022 22:17:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=79
05/28/2022 22:18:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=80
05/28/2022 22:18:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=81
05/28/2022 22:18:08 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.681566972650407 on epoch=81
05/28/2022 22:18:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=81
05/28/2022 22:18:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=82
05/28/2022 22:18:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=83
05/28/2022 22:18:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=83
05/28/2022 22:18:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/28/2022 22:18:25 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.6740514387573211 on epoch=84
05/28/2022 22:18:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/28/2022 22:18:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=85
05/28/2022 22:18:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/28/2022 22:18:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=86
05/28/2022 22:18:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
05/28/2022 22:18:43 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.7020807795552935 on epoch=87
05/28/2022 22:18:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=88
05/28/2022 22:18:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=88
05/28/2022 22:18:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/28/2022 22:18:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=89
05/28/2022 22:18:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=90
05/28/2022 22:19:00 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.6597289762043701 on epoch=90
05/28/2022 22:19:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=91
05/28/2022 22:19:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=91
05/28/2022 22:19:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=92
05/28/2022 22:19:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
05/28/2022 22:19:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/28/2022 22:19:18 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.6450881571006835 on epoch=93
05/28/2022 22:19:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
05/28/2022 22:19:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=94
05/28/2022 22:19:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=95
05/28/2022 22:19:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=96
05/28/2022 22:19:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=96
05/28/2022 22:19:36 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.6450881571006835 on epoch=96
05/28/2022 22:19:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=97
05/28/2022 22:19:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/28/2022 22:19:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
05/28/2022 22:19:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/28/2022 22:19:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
05/28/2022 22:19:53 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.6824469671989574 on epoch=99
05/28/2022 22:19:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=100
05/28/2022 22:19:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/28/2022 22:20:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=101
05/28/2022 22:20:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=102
05/28/2022 22:20:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=103
05/28/2022 22:20:10 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.6130682478781537 on epoch=103
05/28/2022 22:20:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
05/28/2022 22:20:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=104
05/28/2022 22:20:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/28/2022 22:20:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/28/2022 22:20:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=106
05/28/2022 22:20:28 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.6694246496723898 on epoch=106
05/28/2022 22:20:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=106
05/28/2022 22:20:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
05/28/2022 22:20:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=108
05/28/2022 22:20:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
05/28/2022 22:20:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
05/28/2022 22:20:45 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.7173733886893376 on epoch=109
05/28/2022 22:20:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=109
05/28/2022 22:20:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=110
05/28/2022 22:20:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
05/28/2022 22:20:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=111
05/28/2022 22:20:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=112
05/28/2022 22:21:02 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.649343324470228 on epoch=112
05/28/2022 22:21:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=113
05/28/2022 22:21:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=113
05/28/2022 22:21:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=114
05/28/2022 22:21:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/28/2022 22:21:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
05/28/2022 22:21:20 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.649343324470228 on epoch=115
05/28/2022 22:21:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/28/2022 22:21:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=116
05/28/2022 22:21:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=117
05/28/2022 22:21:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
05/28/2022 22:21:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
05/28/2022 22:21:37 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.6111565488808399 on epoch=118
05/28/2022 22:21:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=119
05/28/2022 22:21:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=119
05/28/2022 22:21:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/28/2022 22:21:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=121
05/28/2022 22:21:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=121
05/28/2022 22:21:55 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.5829864253393665 on epoch=121
05/28/2022 22:21:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
05/28/2022 22:22:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
05/28/2022 22:22:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
05/28/2022 22:22:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=124
05/28/2022 22:22:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/28/2022 22:22:12 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.6964363727142233 on epoch=124
05/28/2022 22:22:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/28/2022 22:22:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=126
05/28/2022 22:22:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
05/28/2022 22:22:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/28/2022 22:22:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
05/28/2022 22:22:30 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.6049382716049383 on epoch=128
05/28/2022 22:22:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
05/28/2022 22:22:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=129
05/28/2022 22:22:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/28/2022 22:22:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
05/28/2022 22:22:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/28/2022 22:22:47 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.7007807751648044 on epoch=131
05/28/2022 22:22:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=131
05/28/2022 22:22:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=132
05/28/2022 22:22:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=133
05/28/2022 22:22:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=133
05/28/2022 22:23:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=134
05/28/2022 22:23:05 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6953087564726795 on epoch=134
05/28/2022 22:23:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/28/2022 22:23:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=135
05/28/2022 22:23:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/28/2022 22:23:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=136
05/28/2022 22:23:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/28/2022 22:23:22 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7416197700024468 on epoch=137
05/28/2022 22:23:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/28/2022 22:23:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=138
05/28/2022 22:23:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=139
05/28/2022 22:23:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
05/28/2022 22:23:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=140
05/28/2022 22:23:40 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.6600918238390043 on epoch=140
05/28/2022 22:23:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/28/2022 22:23:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/28/2022 22:23:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/28/2022 22:23:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/28/2022 22:23:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
05/28/2022 22:23:57 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.6294151708164448 on epoch=143
05/28/2022 22:24:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=144
05/28/2022 22:24:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/28/2022 22:24:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/28/2022 22:24:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=146
05/28/2022 22:24:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=146
05/28/2022 22:24:15 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.6680161943319838 on epoch=146
05/28/2022 22:24:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=147
05/28/2022 22:24:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
05/28/2022 22:24:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=148
05/28/2022 22:24:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=149
05/28/2022 22:24:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
05/28/2022 22:24:32 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.7063492063492063 on epoch=149
05/28/2022 22:24:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/28/2022 22:24:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/28/2022 22:24:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/28/2022 22:24:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/28/2022 22:24:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
05/28/2022 22:24:49 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.7142857142857143 on epoch=153
05/28/2022 22:24:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
05/28/2022 22:24:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=154
05/28/2022 22:24:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=154
05/28/2022 22:24:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/28/2022 22:25:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=156
05/28/2022 22:25:07 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.669937106918239 on epoch=156
05/28/2022 22:25:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
05/28/2022 22:25:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/28/2022 22:25:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=158
05/28/2022 22:25:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=158
05/28/2022 22:25:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/28/2022 22:25:24 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.6884478562067029 on epoch=159
05/28/2022 22:25:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/28/2022 22:25:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=160
05/28/2022 22:25:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/28/2022 22:25:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
05/28/2022 22:25:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
05/28/2022 22:25:42 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.7497556207233627 on epoch=162
05/28/2022 22:25:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7421717634132943 -> 0.7497556207233627 on epoch=162, global_step=2600
05/28/2022 22:25:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
05/28/2022 22:25:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/28/2022 22:25:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/28/2022 22:25:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/28/2022 22:25:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/28/2022 22:26:00 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7111368730195533 on epoch=165
05/28/2022 22:26:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
05/28/2022 22:26:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/28/2022 22:26:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=167
05/28/2022 22:26:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
05/28/2022 22:26:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/28/2022 22:26:19 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.6437107069155829 on epoch=168
05/28/2022 22:26:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/28/2022 22:26:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/28/2022 22:26:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=170
05/28/2022 22:26:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/28/2022 22:26:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/28/2022 22:26:36 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.6913006029285098 on epoch=171
05/28/2022 22:26:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=172
05/28/2022 22:26:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/28/2022 22:26:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=173
05/28/2022 22:26:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=174
05/28/2022 22:26:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/28/2022 22:26:54 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.723251791450457 on epoch=174
05/28/2022 22:26:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/28/2022 22:26:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
05/28/2022 22:27:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/28/2022 22:27:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/28/2022 22:27:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/28/2022 22:27:11 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.6259893717790228 on epoch=178
05/28/2022 22:27:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
05/28/2022 22:27:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
05/28/2022 22:27:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
05/28/2022 22:27:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/28/2022 22:27:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/28/2022 22:27:29 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.7185288424312815 on epoch=181
05/28/2022 22:27:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/28/2022 22:27:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/28/2022 22:27:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/28/2022 22:27:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/28/2022 22:27:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=184
05/28/2022 22:27:47 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7063492063492063 on epoch=184
05/28/2022 22:27:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/28/2022 22:27:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/28/2022 22:27:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/28/2022 22:27:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/28/2022 22:28:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/28/2022 22:28:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:28:01 - INFO - __main__ - Printing 3 examples
05/28/2022 22:28:01 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:28:01 - INFO - __main__ - ['false']
05/28/2022 22:28:01 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:28:01 - INFO - __main__ - ['false']
05/28/2022 22:28:01 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:28:01 - INFO - __main__ - ['false']
05/28/2022 22:28:01 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:28:01 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:28:02 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:28:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:28:02 - INFO - __main__ - Printing 3 examples
05/28/2022 22:28:02 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:28:02 - INFO - __main__ - ['false']
05/28/2022 22:28:02 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:28:02 - INFO - __main__ - ['false']
05/28/2022 22:28:02 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:28:02 - INFO - __main__ - ['false']
05/28/2022 22:28:02 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:28:02 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:28:02 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:28:05 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7282754164936084 on epoch=187
05/28/2022 22:28:05 - INFO - __main__ - save last model!
05/28/2022 22:28:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 22:28:05 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 22:28:05 - INFO - __main__ - Printing 3 examples
05/28/2022 22:28:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 22:28:05 - INFO - __main__ - ['false']
05/28/2022 22:28:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 22:28:05 - INFO - __main__ - ['false']
05/28/2022 22:28:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 22:28:05 - INFO - __main__ - ['false']
05/28/2022 22:28:05 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:28:06 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:28:09 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 22:28:19 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:28:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:28:20 - INFO - __main__ - Starting training!
05/28/2022 22:28:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.4_8_predictions.txt
05/28/2022 22:28:57 - INFO - __main__ - Classification-F1 on test data: 0.4741
05/28/2022 22:28:58 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.4, bsz=8, dev_performance=0.7497556207233627, test_performance=0.4741190903668537
05/28/2022 22:28:58 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.3, bsz=8 ...
05/28/2022 22:28:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:28:59 - INFO - __main__ - Printing 3 examples
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:28:59 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:28:59 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:28:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:28:59 - INFO - __main__ - Printing 3 examples
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:28:59 - INFO - __main__ - ['false']
05/28/2022 22:28:59 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:28:59 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:29:00 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:29:16 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:29:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:29:16 - INFO - __main__ - Starting training!
05/28/2022 22:29:20 - INFO - __main__ - Step 10 Global step 10 Train loss 2.90 on epoch=0
05/28/2022 22:29:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=1
05/28/2022 22:29:25 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=1
05/28/2022 22:29:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=2
05/28/2022 22:29:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=3
05/28/2022 22:29:36 - INFO - __main__ - Global step 50 Train loss 0.97 Classification-F1 0.36318407960199 on epoch=3
05/28/2022 22:29:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36318407960199 on epoch=3, global_step=50
05/28/2022 22:29:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=3
05/28/2022 22:29:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=4
05/28/2022 22:29:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/28/2022 22:29:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
05/28/2022 22:29:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=6
05/28/2022 22:29:55 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.4507616391332332 on epoch=6
05/28/2022 22:29:55 - INFO - __main__ - Saving model with best Classification-F1: 0.36318407960199 -> 0.4507616391332332 on epoch=6, global_step=100
05/28/2022 22:29:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=6
05/28/2022 22:30:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=7
05/28/2022 22:30:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=8
05/28/2022 22:30:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=8
05/28/2022 22:30:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=9
05/28/2022 22:30:13 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3383422492035824 on epoch=9
05/28/2022 22:30:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
05/28/2022 22:30:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=10
05/28/2022 22:30:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=11
05/28/2022 22:30:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/28/2022 22:30:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
05/28/2022 22:30:32 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3486005089058525 on epoch=12
05/28/2022 22:30:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=13
05/28/2022 22:30:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=13
05/28/2022 22:30:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
05/28/2022 22:30:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=14
05/28/2022 22:30:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/28/2022 22:30:51 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.467967832294208 on epoch=15
05/28/2022 22:30:51 - INFO - __main__ - Saving model with best Classification-F1: 0.4507616391332332 -> 0.467967832294208 on epoch=15, global_step=250
05/28/2022 22:30:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=16
05/28/2022 22:30:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/28/2022 22:30:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=17
05/28/2022 22:31:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
05/28/2022 22:31:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
05/28/2022 22:31:09 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.6217049336617144 on epoch=18
05/28/2022 22:31:09 - INFO - __main__ - Saving model with best Classification-F1: 0.467967832294208 -> 0.6217049336617144 on epoch=18, global_step=300
05/28/2022 22:31:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
05/28/2022 22:31:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=19
05/28/2022 22:31:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/28/2022 22:31:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=21
05/28/2022 22:31:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/28/2022 22:31:28 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6156156156156156 on epoch=21
05/28/2022 22:31:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=22
05/28/2022 22:31:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
05/28/2022 22:31:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
05/28/2022 22:31:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=24
05/28/2022 22:31:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/28/2022 22:31:46 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.6000000000000001 on epoch=24
05/28/2022 22:31:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
05/28/2022 22:31:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
05/28/2022 22:31:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
05/28/2022 22:31:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/28/2022 22:31:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=28
05/28/2022 22:32:05 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.6526672833795868 on epoch=28
05/28/2022 22:32:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6217049336617144 -> 0.6526672833795868 on epoch=28, global_step=450
05/28/2022 22:32:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=28
05/28/2022 22:32:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=29
05/28/2022 22:32:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
05/28/2022 22:32:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=30
05/28/2022 22:32:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
05/28/2022 22:32:23 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.6793743890518085 on epoch=31
05/28/2022 22:32:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6526672833795868 -> 0.6793743890518085 on epoch=31, global_step=500
05/28/2022 22:32:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=31
05/28/2022 22:32:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
05/28/2022 22:32:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/28/2022 22:32:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/28/2022 22:32:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/28/2022 22:32:41 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.6711524345485687 on epoch=34
05/28/2022 22:32:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/28/2022 22:32:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=35
05/28/2022 22:32:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/28/2022 22:32:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=36
05/28/2022 22:32:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=37
05/28/2022 22:33:00 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.6862745098039216 on epoch=37
05/28/2022 22:33:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6793743890518085 -> 0.6862745098039216 on epoch=37, global_step=600
05/28/2022 22:33:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/28/2022 22:33:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=38
05/28/2022 22:33:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=39
05/28/2022 22:33:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=39
05/28/2022 22:33:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/28/2022 22:33:18 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.7054085155350978 on epoch=40
05/28/2022 22:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6862745098039216 -> 0.7054085155350978 on epoch=40, global_step=650
05/28/2022 22:33:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/28/2022 22:33:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=41
05/28/2022 22:33:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=42
05/28/2022 22:33:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
05/28/2022 22:33:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=43
05/28/2022 22:33:36 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.661262295343313 on epoch=43
05/28/2022 22:33:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=44
05/28/2022 22:33:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=44
05/28/2022 22:33:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/28/2022 22:33:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/28/2022 22:33:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
05/28/2022 22:33:54 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6941362670101012 on epoch=46
05/28/2022 22:33:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=47
05/28/2022 22:33:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=48
05/28/2022 22:34:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
05/28/2022 22:34:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
05/28/2022 22:34:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/28/2022 22:34:12 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.7104957515740571 on epoch=49
05/28/2022 22:34:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7054085155350978 -> 0.7104957515740571 on epoch=49, global_step=800
05/28/2022 22:34:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=50
05/28/2022 22:34:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=51
05/28/2022 22:34:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=51
05/28/2022 22:34:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/28/2022 22:34:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/28/2022 22:34:29 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.7007807751648044 on epoch=53
05/28/2022 22:34:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=53
05/28/2022 22:34:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=54
05/28/2022 22:34:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
05/28/2022 22:34:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=55
05/28/2022 22:34:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
05/28/2022 22:34:47 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.71410650634112 on epoch=56
05/28/2022 22:34:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7104957515740571 -> 0.71410650634112 on epoch=56, global_step=900
05/28/2022 22:34:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=56
05/28/2022 22:34:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=57
05/28/2022 22:34:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
05/28/2022 22:34:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=58
05/28/2022 22:35:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=59
05/28/2022 22:35:05 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.7146303846564968 on epoch=59
05/28/2022 22:35:05 - INFO - __main__ - Saving model with best Classification-F1: 0.71410650634112 -> 0.7146303846564968 on epoch=59, global_step=950
05/28/2022 22:35:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=59
05/28/2022 22:35:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=60
05/28/2022 22:35:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=61
05/28/2022 22:35:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=61
05/28/2022 22:35:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=62
05/28/2022 22:35:22 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.7107786259541985 on epoch=62
05/28/2022 22:35:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=63
05/28/2022 22:35:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
05/28/2022 22:35:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=64
05/28/2022 22:35:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=64
05/28/2022 22:35:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=65
05/28/2022 22:35:40 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.710919855948239 on epoch=65
05/28/2022 22:35:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=66
05/28/2022 22:35:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=66
05/28/2022 22:35:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=67
05/28/2022 22:35:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=68
05/28/2022 22:35:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=68
05/28/2022 22:35:58 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.649343324470228 on epoch=68
05/28/2022 22:36:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=69
05/28/2022 22:36:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=69
05/28/2022 22:36:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/28/2022 22:36:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/28/2022 22:36:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
05/28/2022 22:36:16 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.7000308356460068 on epoch=71
05/28/2022 22:36:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=72
05/28/2022 22:36:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=73
05/28/2022 22:36:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/28/2022 22:36:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/28/2022 22:36:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=74
05/28/2022 22:36:33 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.7070267795834286 on epoch=74
05/28/2022 22:36:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=75
05/28/2022 22:36:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/28/2022 22:36:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/28/2022 22:36:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=77
05/28/2022 22:36:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=78
05/28/2022 22:36:51 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.6984126984126984 on epoch=78
05/28/2022 22:36:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/28/2022 22:36:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=79
05/28/2022 22:36:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=79
05/28/2022 22:37:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/28/2022 22:37:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=81
05/28/2022 22:37:09 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.6855809383443872 on epoch=81
05/28/2022 22:37:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=81
05/28/2022 22:37:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=82
05/28/2022 22:37:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
05/28/2022 22:37:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/28/2022 22:37:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/28/2022 22:37:26 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.6758092414133927 on epoch=84
05/28/2022 22:37:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=84
05/28/2022 22:37:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=85
05/28/2022 22:37:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
05/28/2022 22:37:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/28/2022 22:37:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=87
05/28/2022 22:37:44 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.6715665816523688 on epoch=87
05/28/2022 22:37:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/28/2022 22:37:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/28/2022 22:37:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/28/2022 22:37:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
05/28/2022 22:37:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=90
05/28/2022 22:38:01 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.6893156156386819 on epoch=90
05/28/2022 22:38:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=91
05/28/2022 22:38:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/28/2022 22:38:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/28/2022 22:38:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/28/2022 22:38:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=93
05/28/2022 22:38:19 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.6382477626000942 on epoch=93
05/28/2022 22:38:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=94
05/28/2022 22:38:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=94
05/28/2022 22:38:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=95
05/28/2022 22:38:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=96
05/28/2022 22:38:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=96
05/28/2022 22:38:37 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.6971357409713574 on epoch=96
05/28/2022 22:38:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
05/28/2022 22:38:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=98
05/28/2022 22:38:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/28/2022 22:38:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=99
05/28/2022 22:38:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/28/2022 22:38:55 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6926108374384237 on epoch=99
05/28/2022 22:38:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/28/2022 22:38:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=101
05/28/2022 22:39:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/28/2022 22:39:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
05/28/2022 22:39:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/28/2022 22:39:12 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.692072170585019 on epoch=103
05/28/2022 22:39:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=103
05/28/2022 22:39:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/28/2022 22:39:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/28/2022 22:39:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=105
05/28/2022 22:39:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=106
05/28/2022 22:39:30 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.6904862844850598 on epoch=106
05/28/2022 22:39:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=106
05/28/2022 22:39:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/28/2022 22:39:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/28/2022 22:39:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/28/2022 22:39:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=109
05/28/2022 22:39:48 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.6848224165069272 on epoch=109
05/28/2022 22:39:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
05/28/2022 22:39:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/28/2022 22:39:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/28/2022 22:39:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/28/2022 22:40:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
05/28/2022 22:40:05 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.6904761904761905 on epoch=112
05/28/2022 22:40:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=113
05/28/2022 22:40:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=113
05/28/2022 22:40:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=114
05/28/2022 22:40:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/28/2022 22:40:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/28/2022 22:40:23 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.6953087564726795 on epoch=115
05/28/2022 22:40:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=116
05/28/2022 22:40:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
05/28/2022 22:40:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
05/28/2022 22:40:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
05/28/2022 22:40:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
05/28/2022 22:40:41 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.6401438378555083 on epoch=118
05/28/2022 22:40:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=119
05/28/2022 22:40:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=119
05/28/2022 22:40:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=120
05/28/2022 22:40:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=121
05/28/2022 22:40:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=121
05/28/2022 22:40:58 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.6868845261243179 on epoch=121
05/28/2022 22:41:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=122
05/28/2022 22:41:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=123
05/28/2022 22:41:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/28/2022 22:41:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
05/28/2022 22:41:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=124
05/28/2022 22:41:16 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.6964363727142233 on epoch=124
05/28/2022 22:41:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
05/28/2022 22:41:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
05/28/2022 22:41:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
05/28/2022 22:41:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=127
05/28/2022 22:41:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/28/2022 22:41:33 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.5862523913186886 on epoch=128
05/28/2022 22:41:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
05/28/2022 22:41:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=129
05/28/2022 22:41:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=129
05/28/2022 22:41:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=130
05/28/2022 22:41:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/28/2022 22:41:51 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.6659027247262541 on epoch=131
05/28/2022 22:41:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=131
05/28/2022 22:41:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/28/2022 22:41:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/28/2022 22:42:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=133
05/28/2022 22:42:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=134
05/28/2022 22:42:08 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.6116846895336889 on epoch=134
05/28/2022 22:42:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/28/2022 22:42:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/28/2022 22:42:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=136
05/28/2022 22:42:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=136
05/28/2022 22:42:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
05/28/2022 22:42:25 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.6967711169566354 on epoch=137
05/28/2022 22:42:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
05/28/2022 22:42:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=138
05/28/2022 22:42:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=139
05/28/2022 22:42:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/28/2022 22:42:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=140
05/28/2022 22:42:43 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.6623818225152935 on epoch=140
05/28/2022 22:42:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=141
05/28/2022 22:42:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=141
05/28/2022 22:42:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/28/2022 22:42:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/28/2022 22:42:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
05/28/2022 22:43:01 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.6317750898986596 on epoch=143
05/28/2022 22:43:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=144
05/28/2022 22:43:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/28/2022 22:43:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/28/2022 22:43:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/28/2022 22:43:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=146
05/28/2022 22:43:18 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.6553846153846153 on epoch=146
05/28/2022 22:43:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=147
05/28/2022 22:43:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/28/2022 22:43:26 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
05/28/2022 22:43:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
05/28/2022 22:43:31 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
05/28/2022 22:43:36 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.6565953369530502 on epoch=149
05/28/2022 22:43:38 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=150
05/28/2022 22:43:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=151
05/28/2022 22:43:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=151
05/28/2022 22:43:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/28/2022 22:43:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/28/2022 22:43:53 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.5553082047390168 on epoch=153
05/28/2022 22:43:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
05/28/2022 22:43:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
05/28/2022 22:44:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
05/28/2022 22:44:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/28/2022 22:44:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
05/28/2022 22:44:11 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.6659027247262541 on epoch=156
05/28/2022 22:44:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
05/28/2022 22:44:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/28/2022 22:44:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
05/28/2022 22:44:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/28/2022 22:44:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/28/2022 22:44:29 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.6752642706131078 on epoch=159
05/28/2022 22:44:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
05/28/2022 22:44:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
05/28/2022 22:44:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/28/2022 22:44:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
05/28/2022 22:44:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/28/2022 22:44:47 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.6904862844850598 on epoch=162
05/28/2022 22:44:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=163
05/28/2022 22:44:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
05/28/2022 22:44:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/28/2022 22:44:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/28/2022 22:44:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/28/2022 22:45:04 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.664771638454168 on epoch=165
05/28/2022 22:45:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
05/28/2022 22:45:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/28/2022 22:45:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/28/2022 22:45:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
05/28/2022 22:45:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
05/28/2022 22:45:22 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.5979833044053228 on epoch=168
05/28/2022 22:45:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=169
05/28/2022 22:45:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/28/2022 22:45:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/28/2022 22:45:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/28/2022 22:45:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/28/2022 22:45:40 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.671560940841055 on epoch=171
05/28/2022 22:45:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
05/28/2022 22:45:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/28/2022 22:45:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=173
05/28/2022 22:45:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/28/2022 22:45:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/28/2022 22:45:57 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.6729479399552858 on epoch=174
05/28/2022 22:46:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/28/2022 22:46:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
05/28/2022 22:46:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/28/2022 22:46:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/28/2022 22:46:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
05/28/2022 22:46:15 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6541208260953484 on epoch=178
05/28/2022 22:46:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/28/2022 22:46:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
05/28/2022 22:46:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/28/2022 22:46:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/28/2022 22:46:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
05/28/2022 22:46:33 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.6600918238390043 on epoch=181
05/28/2022 22:46:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=181
05/28/2022 22:46:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/28/2022 22:46:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=183
05/28/2022 22:46:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
05/28/2022 22:46:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=184
05/28/2022 22:46:50 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.669937106918239 on epoch=184
05/28/2022 22:46:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/28/2022 22:46:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
05/28/2022 22:46:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
05/28/2022 22:47:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=186
05/28/2022 22:47:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/28/2022 22:47:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:47:04 - INFO - __main__ - Printing 3 examples
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:47:04 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:47:04 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:47:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:47:04 - INFO - __main__ - Printing 3 examples
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:47:04 - INFO - __main__ - ['false']
05/28/2022 22:47:04 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:47:04 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:47:05 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:47:07 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.6761133603238867 on epoch=187
05/28/2022 22:47:07 - INFO - __main__ - save last model!
05/28/2022 22:47:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 22:47:07 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 22:47:07 - INFO - __main__ - Printing 3 examples
05/28/2022 22:47:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 22:47:07 - INFO - __main__ - ['false']
05/28/2022 22:47:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 22:47:07 - INFO - __main__ - ['false']
05/28/2022 22:47:07 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 22:47:07 - INFO - __main__ - ['false']
05/28/2022 22:47:07 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:47:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:47:11 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 22:47:20 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:47:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:47:21 - INFO - __main__ - Starting training!
05/28/2022 22:47:58 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.3_8_predictions.txt
05/28/2022 22:47:58 - INFO - __main__ - Classification-F1 on test data: 0.4244
05/28/2022 22:47:58 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.3, bsz=8, dev_performance=0.7146303846564968, test_performance=0.42444033401079373
05/28/2022 22:47:58 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.2, bsz=8 ...
05/28/2022 22:47:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:47:59 - INFO - __main__ - Printing 3 examples
05/28/2022 22:47:59 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
05/28/2022 22:47:59 - INFO - __main__ - ['false']
05/28/2022 22:47:59 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
05/28/2022 22:47:59 - INFO - __main__ - ['false']
05/28/2022 22:47:59 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
05/28/2022 22:47:59 - INFO - __main__ - ['false']
05/28/2022 22:47:59 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:47:59 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:48:00 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 22:48:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 22:48:00 - INFO - __main__ - Printing 3 examples
05/28/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
05/28/2022 22:48:00 - INFO - __main__ - ['false']
05/28/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
05/28/2022 22:48:00 - INFO - __main__ - ['false']
05/28/2022 22:48:00 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
05/28/2022 22:48:00 - INFO - __main__ - ['false']
05/28/2022 22:48:00 - INFO - __main__ - Tokenizing Input ...
05/28/2022 22:48:00 - INFO - __main__ - Tokenizing Output ...
05/28/2022 22:48:00 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 22:48:16 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 22:48:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 22:48:16 - INFO - __main__ - Starting training!
05/28/2022 22:48:20 - INFO - __main__ - Step 10 Global step 10 Train loss 3.33 on epoch=0
05/28/2022 22:48:23 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=1
05/28/2022 22:48:25 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=1
05/28/2022 22:48:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=2
05/28/2022 22:48:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
05/28/2022 22:48:36 - INFO - __main__ - Global step 50 Train loss 1.24 Classification-F1 0.47626102292768957 on epoch=3
05/28/2022 22:48:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.47626102292768957 on epoch=3, global_step=50
05/28/2022 22:48:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
05/28/2022 22:48:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/28/2022 22:48:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=4
05/28/2022 22:48:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
05/28/2022 22:48:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=6
05/28/2022 22:48:55 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.41790848444987316 on epoch=6
05/28/2022 22:48:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=6
05/28/2022 22:49:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=7
05/28/2022 22:49:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=8
05/28/2022 22:49:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=8
05/28/2022 22:49:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
05/28/2022 22:49:14 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3980615931639617 on epoch=9
05/28/2022 22:49:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=9
05/28/2022 22:49:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
05/28/2022 22:49:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
05/28/2022 22:49:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=11
05/28/2022 22:49:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=12
05/28/2022 22:49:32 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=12
05/28/2022 22:49:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
05/28/2022 22:49:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=13
05/28/2022 22:49:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=14
05/28/2022 22:49:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=14
05/28/2022 22:49:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=15
05/28/2022 22:49:51 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5398297067171239 on epoch=15
05/28/2022 22:49:51 - INFO - __main__ - Saving model with best Classification-F1: 0.47626102292768957 -> 0.5398297067171239 on epoch=15, global_step=250
05/28/2022 22:49:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=16
05/28/2022 22:49:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/28/2022 22:49:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=17
05/28/2022 22:50:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=18
05/28/2022 22:50:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/28/2022 22:50:09 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.49189261925587024 on epoch=18
05/28/2022 22:50:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=19
05/28/2022 22:50:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/28/2022 22:50:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=20
05/28/2022 22:50:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=21
05/28/2022 22:50:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/28/2022 22:50:28 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.5397188623733247 on epoch=21
05/28/2022 22:50:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
05/28/2022 22:50:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/28/2022 22:50:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
05/28/2022 22:50:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=24
05/28/2022 22:50:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
05/28/2022 22:50:47 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.5503711558854718 on epoch=24
05/28/2022 22:50:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5398297067171239 -> 0.5503711558854718 on epoch=24, global_step=400
05/28/2022 22:50:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/28/2022 22:50:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
05/28/2022 22:50:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/28/2022 22:50:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
05/28/2022 22:50:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/28/2022 22:51:05 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.5963418479974771 on epoch=28
05/28/2022 22:51:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5503711558854718 -> 0.5963418479974771 on epoch=28, global_step=450
05/28/2022 22:51:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
05/28/2022 22:51:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/28/2022 22:51:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=29
05/28/2022 22:51:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/28/2022 22:51:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=31
05/28/2022 22:51:24 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5817196473347892 on epoch=31
05/28/2022 22:51:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/28/2022 22:51:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/28/2022 22:51:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=33
05/28/2022 22:51:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
05/28/2022 22:51:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/28/2022 22:51:42 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.5862523913186886 on epoch=34
05/28/2022 22:51:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/28/2022 22:51:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/28/2022 22:51:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=36
05/28/2022 22:51:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/28/2022 22:51:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/28/2022 22:52:01 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.5639429968800067 on epoch=37
05/28/2022 22:52:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/28/2022 22:52:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/28/2022 22:52:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/28/2022 22:52:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/28/2022 22:52:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=40
05/28/2022 22:52:19 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5862523913186886 on epoch=40
05/28/2022 22:52:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=41
05/28/2022 22:52:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
05/28/2022 22:52:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=42
05/28/2022 22:52:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
05/28/2022 22:52:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
05/28/2022 22:52:38 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6675578641814988 on epoch=43
05/28/2022 22:52:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5963418479974771 -> 0.6675578641814988 on epoch=43, global_step=700
05/28/2022 22:52:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/28/2022 22:52:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/28/2022 22:52:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=45
05/28/2022 22:52:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
05/28/2022 22:52:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=46
05/28/2022 22:52:56 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.67089072543618 on epoch=46
05/28/2022 22:52:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6675578641814988 -> 0.67089072543618 on epoch=46, global_step=750
05/28/2022 22:52:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=47
05/28/2022 22:53:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=48
05/28/2022 22:53:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=48
05/28/2022 22:53:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/28/2022 22:53:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=49
05/28/2022 22:53:14 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.6493150684931506 on epoch=49
05/28/2022 22:53:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
05/28/2022 22:53:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
05/28/2022 22:53:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=51
05/28/2022 22:53:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=52
05/28/2022 22:53:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=53
05/28/2022 22:53:33 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.6717948717948719 on epoch=53
05/28/2022 22:53:33 - INFO - __main__ - Saving model with best Classification-F1: 0.67089072543618 -> 0.6717948717948719 on epoch=53, global_step=850
05/28/2022 22:53:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=53
05/28/2022 22:53:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=54
05/28/2022 22:53:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=54
05/28/2022 22:53:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
05/28/2022 22:53:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
05/28/2022 22:53:50 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.6825037130039351 on epoch=56
05/28/2022 22:53:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6717948717948719 -> 0.6825037130039351 on epoch=56, global_step=900
05/28/2022 22:53:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=56
05/28/2022 22:53:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=57
05/28/2022 22:53:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=58
05/28/2022 22:54:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=58
05/28/2022 22:54:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/28/2022 22:54:08 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.6818411967779057 on epoch=59
05/28/2022 22:54:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=59
05/28/2022 22:54:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
05/28/2022 22:54:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
05/28/2022 22:54:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=61
05/28/2022 22:54:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/28/2022 22:54:26 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.6884349339865354 on epoch=62
05/28/2022 22:54:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6825037130039351 -> 0.6884349339865354 on epoch=62, global_step=1000
05/28/2022 22:54:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=63
05/28/2022 22:54:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=63
05/28/2022 22:54:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
05/28/2022 22:54:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=64
05/28/2022 22:54:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=65
05/28/2022 22:54:44 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.7070267795834286 on epoch=65
05/28/2022 22:54:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6884349339865354 -> 0.7070267795834286 on epoch=65, global_step=1050
05/28/2022 22:54:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=66
05/28/2022 22:54:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=66
05/28/2022 22:54:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
05/28/2022 22:54:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=68
05/28/2022 22:54:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.28 on epoch=68
05/28/2022 22:55:01 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.7223130394927814 on epoch=68
05/28/2022 22:55:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7070267795834286 -> 0.7223130394927814 on epoch=68, global_step=1100
05/28/2022 22:55:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=69
05/28/2022 22:55:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=69
05/28/2022 22:55:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=70
05/28/2022 22:55:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=71
05/28/2022 22:55:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
05/28/2022 22:55:19 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.7221432393181992 on epoch=71
05/28/2022 22:55:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=72
05/28/2022 22:55:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=73
05/28/2022 22:55:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=73
05/28/2022 22:55:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=74
05/28/2022 22:55:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
05/28/2022 22:55:36 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.7107786259541985 on epoch=74
05/28/2022 22:55:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=75
05/28/2022 22:55:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
05/28/2022 22:55:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=76
05/28/2022 22:55:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=77
05/28/2022 22:55:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=78
05/28/2022 22:55:54 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.7087868167004857 on epoch=78
05/28/2022 22:55:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=78
05/28/2022 22:55:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=79
05/28/2022 22:56:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=79
05/28/2022 22:56:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=80
05/28/2022 22:56:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=81
05/28/2022 22:56:12 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.7013018914271678 on epoch=81
05/28/2022 22:56:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=81
05/28/2022 22:56:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=82
05/28/2022 22:56:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=83
05/28/2022 22:56:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
05/28/2022 22:56:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=84
05/28/2022 22:56:30 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.7103009542451675 on epoch=84
05/28/2022 22:56:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=84
05/28/2022 22:56:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=85
05/28/2022 22:56:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=86
05/28/2022 22:56:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=86
05/28/2022 22:56:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=87
05/28/2022 22:56:47 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.703052503052503 on epoch=87
05/28/2022 22:56:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=88
05/28/2022 22:56:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
05/28/2022 22:56:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=89
05/28/2022 22:56:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
05/28/2022 22:57:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/28/2022 22:57:05 - INFO - __main__ - Global step 1450 Train loss 0.24 Classification-F1 0.710919855948239 on epoch=90
05/28/2022 22:57:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=91
05/28/2022 22:57:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=91
05/28/2022 22:57:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=92
05/28/2022 22:57:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=93
05/28/2022 22:57:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
05/28/2022 22:57:23 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.714833838727644 on epoch=93
05/28/2022 22:57:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
05/28/2022 22:57:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=94
05/28/2022 22:57:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=95
05/28/2022 22:57:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=96
05/28/2022 22:57:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=96
05/28/2022 22:57:40 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.6967711169566354 on epoch=96
05/28/2022 22:57:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=97
05/28/2022 22:57:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=98
05/28/2022 22:57:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=98
05/28/2022 22:57:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=99
05/28/2022 22:57:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/28/2022 22:57:58 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.7068120390306474 on epoch=99
05/28/2022 22:58:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
05/28/2022 22:58:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=101
05/28/2022 22:58:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/28/2022 22:58:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=102
05/28/2022 22:58:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=103
05/28/2022 22:58:16 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.7079247610237434 on epoch=103
05/28/2022 22:58:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=103
05/28/2022 22:58:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
05/28/2022 22:58:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=104
05/28/2022 22:58:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=105
05/28/2022 22:58:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
05/28/2022 22:58:33 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.7057471264367816 on epoch=106
05/28/2022 22:58:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/28/2022 22:58:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=107
05/28/2022 22:58:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/28/2022 22:58:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=108
05/28/2022 22:58:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
05/28/2022 22:58:51 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.714833838727644 on epoch=109
05/28/2022 22:58:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
05/28/2022 22:58:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
05/28/2022 22:58:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
05/28/2022 22:59:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
05/28/2022 22:59:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=112
05/28/2022 22:59:09 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.7135807010283999 on epoch=112
05/28/2022 22:59:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=113
05/28/2022 22:59:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
05/28/2022 22:59:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
05/28/2022 22:59:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
05/28/2022 22:59:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=115
05/28/2022 22:59:27 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.6984126984126984 on epoch=115
05/28/2022 22:59:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=116
05/28/2022 22:59:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=116
05/28/2022 22:59:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=117
05/28/2022 22:59:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=118
05/28/2022 22:59:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/28/2022 22:59:45 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.6868845261243179 on epoch=118
05/28/2022 22:59:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=119
05/28/2022 22:59:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/28/2022 22:59:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=120
05/28/2022 22:59:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
05/28/2022 22:59:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
05/28/2022 23:00:03 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7132642884541619 on epoch=121
05/28/2022 23:00:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
05/28/2022 23:00:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
05/28/2022 23:00:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=123
05/28/2022 23:00:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/28/2022 23:00:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/28/2022 23:00:21 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.7304317304317305 on epoch=124
05/28/2022 23:00:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7223130394927814 -> 0.7304317304317305 on epoch=124, global_step=2000
05/28/2022 23:00:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
05/28/2022 23:00:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/28/2022 23:00:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
05/28/2022 23:00:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
05/28/2022 23:00:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/28/2022 23:00:39 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.6796796796796798 on epoch=128
05/28/2022 23:00:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=128
05/28/2022 23:00:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=129
05/28/2022 23:00:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=129
05/28/2022 23:00:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/28/2022 23:00:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
05/28/2022 23:00:57 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.6984126984126984 on epoch=131
05/28/2022 23:00:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=131
05/28/2022 23:01:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=132
05/28/2022 23:01:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=133
05/28/2022 23:01:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=133
05/28/2022 23:01:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=134
05/28/2022 23:01:14 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.7079247610237434 on epoch=134
05/28/2022 23:01:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=134
05/28/2022 23:01:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=135
05/28/2022 23:01:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/28/2022 23:01:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/28/2022 23:01:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=137
05/28/2022 23:01:32 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.669937106918239 on epoch=137
05/28/2022 23:01:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/28/2022 23:01:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
05/28/2022 23:01:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=139
05/28/2022 23:01:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=139
05/28/2022 23:01:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=140
05/28/2022 23:01:49 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.7241379310344828 on epoch=140
05/28/2022 23:01:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=141
05/28/2022 23:01:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=141
05/28/2022 23:01:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=142
05/28/2022 23:01:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=143
05/28/2022 23:02:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
05/28/2022 23:02:07 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.6496052966641201 on epoch=143
05/28/2022 23:02:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
05/28/2022 23:02:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/28/2022 23:02:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=145
05/28/2022 23:02:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=146
05/28/2022 23:02:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=146
05/28/2022 23:02:25 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.6437107069155829 on epoch=146
05/28/2022 23:02:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/28/2022 23:02:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=148
05/28/2022 23:02:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
05/28/2022 23:02:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=149
05/28/2022 23:02:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/28/2022 23:02:42 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.6588616028935846 on epoch=149
05/28/2022 23:02:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/28/2022 23:02:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/28/2022 23:02:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=151
05/28/2022 23:02:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
05/28/2022 23:02:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=153
05/28/2022 23:03:00 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.6294151708164448 on epoch=153
05/28/2022 23:03:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/28/2022 23:03:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
05/28/2022 23:03:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=154
05/28/2022 23:03:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=155
05/28/2022 23:03:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/28/2022 23:03:18 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.7007807751648044 on epoch=156
05/28/2022 23:03:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/28/2022 23:03:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=157
05/28/2022 23:03:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/28/2022 23:03:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=158
05/28/2022 23:03:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/28/2022 23:03:35 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.7000702960243692 on epoch=159
05/28/2022 23:03:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=159
05/28/2022 23:03:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/28/2022 23:03:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=161
05/28/2022 23:03:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
05/28/2022 23:03:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
05/28/2022 23:03:53 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.7116360886941226 on epoch=162
05/28/2022 23:03:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
05/28/2022 23:03:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/28/2022 23:04:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
05/28/2022 23:04:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
05/28/2022 23:04:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=165
05/28/2022 23:04:10 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.6530999410142363 on epoch=165
05/28/2022 23:04:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=166
05/28/2022 23:04:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=166
05/28/2022 23:04:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
05/28/2022 23:04:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
05/28/2022 23:04:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/28/2022 23:04:27 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.6424293993931318 on epoch=168
05/28/2022 23:04:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/28/2022 23:04:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
05/28/2022 23:04:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/28/2022 23:04:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=171
05/28/2022 23:04:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
05/28/2022 23:04:45 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.681566972650407 on epoch=171
05/28/2022 23:04:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=172
05/28/2022 23:04:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=173
05/28/2022 23:04:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=173
05/28/2022 23:04:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=174
05/28/2022 23:04:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
05/28/2022 23:05:02 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.6779874213836479 on epoch=174
05/28/2022 23:05:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/28/2022 23:05:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
05/28/2022 23:05:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/28/2022 23:05:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=177
05/28/2022 23:05:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/28/2022 23:05:20 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.6779874213836479 on epoch=178
05/28/2022 23:05:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=178
05/28/2022 23:05:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=179
05/28/2022 23:05:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=179
05/28/2022 23:05:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=180
05/28/2022 23:05:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=181
05/28/2022 23:05:37 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.6964363727142233 on epoch=181
05/28/2022 23:05:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.14 on epoch=181
05/28/2022 23:05:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=182
05/28/2022 23:05:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=183
05/28/2022 23:05:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=183
05/28/2022 23:05:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/28/2022 23:05:55 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.6735053770213034 on epoch=184
05/28/2022 23:05:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
05/28/2022 23:06:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
05/28/2022 23:06:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
05/28/2022 23:06:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=186
05/28/2022 23:06:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
05/28/2022 23:06:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:06:09 - INFO - __main__ - Printing 3 examples
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:06:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:06:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:06:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:06:09 - INFO - __main__ - Printing 3 examples
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:06:09 - INFO - __main__ - ['false']
05/28/2022 23:06:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:06:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:06:09 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:06:12 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.6832824367708088 on epoch=187
05/28/2022 23:06:12 - INFO - __main__ - save last model!
05/28/2022 23:06:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 23:06:12 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 23:06:12 - INFO - __main__ - Printing 3 examples
05/28/2022 23:06:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 23:06:12 - INFO - __main__ - ['false']
05/28/2022 23:06:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 23:06:12 - INFO - __main__ - ['false']
05/28/2022 23:06:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 23:06:12 - INFO - __main__ - ['false']
05/28/2022 23:06:12 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:06:14 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:06:16 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 23:06:28 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:06:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:06:29 - INFO - __main__ - Starting training!
05/28/2022 23:07:07 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.2_8_predictions.txt
05/28/2022 23:07:07 - INFO - __main__ - Classification-F1 on test data: 0.4365
05/28/2022 23:07:08 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.2, bsz=8, dev_performance=0.7304317304317305, test_performance=0.4364695102950137
05/28/2022 23:07:08 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.5, bsz=8 ...
05/28/2022 23:07:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:07:09 - INFO - __main__ - Printing 3 examples
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:07:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:07:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:07:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:07:09 - INFO - __main__ - Printing 3 examples
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:07:09 - INFO - __main__ - ['false']
05/28/2022 23:07:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:07:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:07:10 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:07:28 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:07:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:07:29 - INFO - __main__ - Starting training!
05/28/2022 23:07:33 - INFO - __main__ - Step 10 Global step 10 Train loss 2.01 on epoch=0
05/28/2022 23:07:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=1
05/28/2022 23:07:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=1
05/28/2022 23:07:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=2
05/28/2022 23:07:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=3
05/28/2022 23:07:49 - INFO - __main__ - Global step 50 Train loss 0.75 Classification-F1 0.39405460814462767 on epoch=3
05/28/2022 23:07:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.39405460814462767 on epoch=3, global_step=50
05/28/2022 23:07:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
05/28/2022 23:07:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
05/28/2022 23:07:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=4
05/28/2022 23:07:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
05/28/2022 23:08:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=6
05/28/2022 23:08:07 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.40828713708540826 on epoch=6
05/28/2022 23:08:08 - INFO - __main__ - Saving model with best Classification-F1: 0.39405460814462767 -> 0.40828713708540826 on epoch=6, global_step=100
05/28/2022 23:08:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/28/2022 23:08:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
05/28/2022 23:08:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/28/2022 23:08:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
05/28/2022 23:08:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=9
05/28/2022 23:08:25 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.42329972108989483 on epoch=9
05/28/2022 23:08:25 - INFO - __main__ - Saving model with best Classification-F1: 0.40828713708540826 -> 0.42329972108989483 on epoch=9, global_step=150
05/28/2022 23:08:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/28/2022 23:08:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=10
05/28/2022 23:08:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
05/28/2022 23:08:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/28/2022 23:08:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=12
05/28/2022 23:08:43 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.4420755396365153 on epoch=12
05/28/2022 23:08:43 - INFO - __main__ - Saving model with best Classification-F1: 0.42329972108989483 -> 0.4420755396365153 on epoch=12, global_step=200
05/28/2022 23:08:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=13
05/28/2022 23:08:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
05/28/2022 23:08:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=14
05/28/2022 23:08:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=14
05/28/2022 23:08:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/28/2022 23:09:00 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.669937106918239 on epoch=15
05/28/2022 23:09:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4420755396365153 -> 0.669937106918239 on epoch=15, global_step=250
05/28/2022 23:09:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
05/28/2022 23:09:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/28/2022 23:09:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
05/28/2022 23:09:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=18
05/28/2022 23:09:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
05/28/2022 23:09:18 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6089311339824401 on epoch=18
05/28/2022 23:09:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
05/28/2022 23:09:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=19
05/28/2022 23:09:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=20
05/28/2022 23:09:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=21
05/28/2022 23:09:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
05/28/2022 23:09:36 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6482442748091604 on epoch=21
05/28/2022 23:09:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=22
05/28/2022 23:09:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=23
05/28/2022 23:09:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=23
05/28/2022 23:09:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=24
05/28/2022 23:09:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=24
05/28/2022 23:09:53 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6582738780207135 on epoch=24
05/28/2022 23:09:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=25
05/28/2022 23:09:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=26
05/28/2022 23:10:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=26
05/28/2022 23:10:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.30 on epoch=27
05/28/2022 23:10:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=28
05/28/2022 23:10:11 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.6716946564885495 on epoch=28
05/28/2022 23:10:11 - INFO - __main__ - Saving model with best Classification-F1: 0.669937106918239 -> 0.6716946564885495 on epoch=28, global_step=450
05/28/2022 23:10:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=28
05/28/2022 23:10:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=29
05/28/2022 23:10:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=29
05/28/2022 23:10:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=30
05/28/2022 23:10:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=31
05/28/2022 23:10:29 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.6842429848905335 on epoch=31
05/28/2022 23:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6716946564885495 -> 0.6842429848905335 on epoch=31, global_step=500
05/28/2022 23:10:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/28/2022 23:10:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=32
05/28/2022 23:10:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.28 on epoch=33
05/28/2022 23:10:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=33
05/28/2022 23:10:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=34
05/28/2022 23:10:46 - INFO - __main__ - Global step 550 Train loss 0.28 Classification-F1 0.6934414148857776 on epoch=34
05/28/2022 23:10:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6842429848905335 -> 0.6934414148857776 on epoch=34, global_step=550
05/28/2022 23:10:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=34
05/28/2022 23:10:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=35
05/28/2022 23:10:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=36
05/28/2022 23:10:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=36
05/28/2022 23:10:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=37
05/28/2022 23:11:04 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.6865625956535047 on epoch=37
05/28/2022 23:11:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=38
05/28/2022 23:11:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=38
05/28/2022 23:11:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=39
05/28/2022 23:11:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=39
05/28/2022 23:11:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/28/2022 23:11:22 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.6644823066841417 on epoch=40
05/28/2022 23:11:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=41
05/28/2022 23:11:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=41
05/28/2022 23:11:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=42
05/28/2022 23:11:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=43
05/28/2022 23:11:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=43
05/28/2022 23:11:40 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.686787955827441 on epoch=43
05/28/2022 23:11:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=44
05/28/2022 23:11:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=44
05/28/2022 23:11:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=45
05/28/2022 23:11:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=46
05/28/2022 23:11:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=46
05/28/2022 23:11:58 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.6583333333333333 on epoch=46
05/28/2022 23:12:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=47
05/28/2022 23:12:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/28/2022 23:12:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/28/2022 23:12:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=49
05/28/2022 23:12:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=49
05/28/2022 23:12:16 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.6858692844226298 on epoch=49
05/28/2022 23:12:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=50
05/28/2022 23:12:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=51
05/28/2022 23:12:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/28/2022 23:12:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/28/2022 23:12:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/28/2022 23:12:34 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.7185288424312815 on epoch=53
05/28/2022 23:12:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6934414148857776 -> 0.7185288424312815 on epoch=53, global_step=850
05/28/2022 23:12:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=53
05/28/2022 23:12:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=54
05/28/2022 23:12:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=54
05/28/2022 23:12:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=55
05/28/2022 23:12:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=56
05/28/2022 23:12:52 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.738085421534045 on epoch=56
05/28/2022 23:12:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7185288424312815 -> 0.738085421534045 on epoch=56, global_step=900
05/28/2022 23:12:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=56
05/28/2022 23:12:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/28/2022 23:12:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/28/2022 23:13:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=58
05/28/2022 23:13:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=59
05/28/2022 23:13:09 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.7446873801304181 on epoch=59
05/28/2022 23:13:10 - INFO - __main__ - Saving model with best Classification-F1: 0.738085421534045 -> 0.7446873801304181 on epoch=59, global_step=950
05/28/2022 23:13:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/28/2022 23:13:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=60
05/28/2022 23:13:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=61
05/28/2022 23:13:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=61
05/28/2022 23:13:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=62
05/28/2022 23:13:27 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.6658448150833938 on epoch=62
05/28/2022 23:13:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
05/28/2022 23:13:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=63
05/28/2022 23:13:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=64
05/28/2022 23:13:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
05/28/2022 23:13:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=65
05/28/2022 23:13:46 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7012929675181331 on epoch=65
05/28/2022 23:13:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=66
05/28/2022 23:13:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/28/2022 23:13:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=67
05/28/2022 23:13:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=68
05/28/2022 23:13:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=68
05/28/2022 23:14:04 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.6243865418669365 on epoch=68
05/28/2022 23:14:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=69
05/28/2022 23:14:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=69
05/28/2022 23:14:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=70
05/28/2022 23:14:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=71
05/28/2022 23:14:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/28/2022 23:14:22 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.6334332334332334 on epoch=71
05/28/2022 23:14:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=72
05/28/2022 23:14:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=73
05/28/2022 23:14:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/28/2022 23:14:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=74
05/28/2022 23:14:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=74
05/28/2022 23:14:40 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.655371683740478 on epoch=74
05/28/2022 23:14:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=75
05/28/2022 23:14:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=76
05/28/2022 23:14:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=76
05/28/2022 23:14:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=77
05/28/2022 23:14:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=78
05/28/2022 23:14:58 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.6177630144110591 on epoch=78
05/28/2022 23:15:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=78
05/28/2022 23:15:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=79
05/28/2022 23:15:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=79
05/28/2022 23:15:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=80
05/28/2022 23:15:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=81
05/28/2022 23:15:16 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.7277277277277278 on epoch=81
05/28/2022 23:15:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/28/2022 23:15:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=82
05/28/2022 23:15:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/28/2022 23:15:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=83
05/28/2022 23:15:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=84
05/28/2022 23:15:33 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.718998933701311 on epoch=84
05/28/2022 23:15:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=84
05/28/2022 23:15:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=85
05/28/2022 23:15:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=86
05/28/2022 23:15:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=86
05/28/2022 23:15:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=87
05/28/2022 23:15:51 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.721066843526458 on epoch=87
05/28/2022 23:15:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=88
05/28/2022 23:15:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=88
05/28/2022 23:15:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=89
05/28/2022 23:16:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=89
05/28/2022 23:16:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=90
05/28/2022 23:16:09 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.6939117126596203 on epoch=90
05/28/2022 23:16:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=91
05/28/2022 23:16:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=91
05/28/2022 23:16:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=92
05/28/2022 23:16:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/28/2022 23:16:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/28/2022 23:16:27 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.5709986320109439 on epoch=93
05/28/2022 23:16:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
05/28/2022 23:16:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=94
05/28/2022 23:16:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/28/2022 23:16:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
05/28/2022 23:16:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=96
05/28/2022 23:16:45 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.557530864197531 on epoch=96
05/28/2022 23:16:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=97
05/28/2022 23:16:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=98
05/28/2022 23:16:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=98
05/28/2022 23:16:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=99
05/28/2022 23:16:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
05/28/2022 23:17:03 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.6728302192024589 on epoch=99
05/28/2022 23:17:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=100
05/28/2022 23:17:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=101
05/28/2022 23:17:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/28/2022 23:17:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=102
05/28/2022 23:17:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=103
05/28/2022 23:17:21 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.6215722424302822 on epoch=103
05/28/2022 23:17:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=103
05/28/2022 23:17:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=104
05/28/2022 23:17:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/28/2022 23:17:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=105
05/28/2022 23:17:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=106
05/28/2022 23:17:38 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5952364933055239 on epoch=106
05/28/2022 23:17:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=106
05/28/2022 23:17:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=107
05/28/2022 23:17:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=108
05/28/2022 23:17:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=108
05/28/2022 23:17:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=109
05/28/2022 23:17:56 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.65 on epoch=109
05/28/2022 23:17:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=109
05/28/2022 23:18:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=110
05/28/2022 23:18:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=111
05/28/2022 23:18:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
05/28/2022 23:18:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=112
05/28/2022 23:18:13 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.6583333333333333 on epoch=112
05/28/2022 23:18:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
05/28/2022 23:18:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=113
05/28/2022 23:18:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=114
05/28/2022 23:18:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=114
05/28/2022 23:18:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=115
05/28/2022 23:18:30 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.6435533277638541 on epoch=115
05/28/2022 23:18:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=116
05/28/2022 23:18:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=116
05/28/2022 23:18:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=117
05/28/2022 23:18:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=118
05/28/2022 23:18:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=118
05/28/2022 23:18:48 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.5709986320109439 on epoch=118
05/28/2022 23:18:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=119
05/28/2022 23:18:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=119
05/28/2022 23:18:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=120
05/28/2022 23:18:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=121
05/28/2022 23:19:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/28/2022 23:19:06 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.6583333333333333 on epoch=121
05/28/2022 23:19:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=122
05/28/2022 23:19:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
05/28/2022 23:19:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=123
05/28/2022 23:19:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/28/2022 23:19:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=124
05/28/2022 23:19:24 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.577296181630547 on epoch=124
05/28/2022 23:19:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=125
05/28/2022 23:19:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=126
05/28/2022 23:19:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=126
05/28/2022 23:19:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=127
05/28/2022 23:19:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=128
05/28/2022 23:19:42 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.6514296829410621 on epoch=128
05/28/2022 23:19:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=128
05/28/2022 23:19:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=129
05/28/2022 23:19:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=129
05/28/2022 23:19:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=130
05/28/2022 23:19:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/28/2022 23:19:59 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7121089728359864 on epoch=131
05/28/2022 23:20:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=131
05/28/2022 23:20:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=132
05/28/2022 23:20:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/28/2022 23:20:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=133
05/28/2022 23:20:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=134
05/28/2022 23:20:16 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6658448150833938 on epoch=134
05/28/2022 23:20:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/28/2022 23:20:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=135
05/28/2022 23:20:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/28/2022 23:20:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/28/2022 23:20:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=137
05/28/2022 23:20:34 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.7138133551668214 on epoch=137
05/28/2022 23:20:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=138
05/28/2022 23:20:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=138
05/28/2022 23:20:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=139
05/28/2022 23:20:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=139
05/28/2022 23:20:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=140
05/28/2022 23:20:51 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.6753094546213495 on epoch=140
05/28/2022 23:20:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=141
05/28/2022 23:20:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=141
05/28/2022 23:20:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=142
05/28/2022 23:21:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=143
05/28/2022 23:21:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=143
05/28/2022 23:21:09 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.6645474399417617 on epoch=143
05/28/2022 23:21:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=144
05/28/2022 23:21:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=144
05/28/2022 23:21:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=145
05/28/2022 23:21:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=146
05/28/2022 23:21:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=146
05/28/2022 23:21:26 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.618239660657476 on epoch=146
05/28/2022 23:21:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
05/28/2022 23:21:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/28/2022 23:21:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/28/2022 23:21:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=149
05/28/2022 23:21:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=149
05/28/2022 23:21:44 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.5930735930735931 on epoch=149
05/28/2022 23:21:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
05/28/2022 23:21:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/28/2022 23:21:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/28/2022 23:21:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/28/2022 23:21:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/28/2022 23:22:01 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.631837341929085 on epoch=153
05/28/2022 23:22:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
05/28/2022 23:22:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/28/2022 23:22:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=154
05/28/2022 23:22:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=155
05/28/2022 23:22:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/28/2022 23:22:18 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.6562646606810645 on epoch=156
05/28/2022 23:22:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=156
05/28/2022 23:22:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/28/2022 23:22:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/28/2022 23:22:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=158
05/28/2022 23:22:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=159
05/28/2022 23:22:35 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7146042363433668 on epoch=159
05/28/2022 23:22:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=159
05/28/2022 23:22:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
05/28/2022 23:22:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/28/2022 23:22:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/28/2022 23:22:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
05/28/2022 23:22:52 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.6949131280185662 on epoch=162
05/28/2022 23:22:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/28/2022 23:22:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=163
05/28/2022 23:23:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/28/2022 23:23:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=164
05/28/2022 23:23:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/28/2022 23:23:10 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.618239660657476 on epoch=165
05/28/2022 23:23:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=166
05/28/2022 23:23:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
05/28/2022 23:23:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
05/28/2022 23:23:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
05/28/2022 23:23:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=168
05/28/2022 23:23:27 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6597289762043701 on epoch=168
05/28/2022 23:23:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/28/2022 23:23:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=169
05/28/2022 23:23:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/28/2022 23:23:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/28/2022 23:23:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/28/2022 23:23:45 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6369153638102778 on epoch=171
05/28/2022 23:23:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/28/2022 23:23:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=173
05/28/2022 23:23:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
05/28/2022 23:23:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
05/28/2022 23:23:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/28/2022 23:24:02 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.5627857229113117 on epoch=174
05/28/2022 23:24:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=175
05/28/2022 23:24:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/28/2022 23:24:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/28/2022 23:24:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/28/2022 23:24:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/28/2022 23:24:20 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6811242414564036 on epoch=178
05/28/2022 23:24:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/28/2022 23:24:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/28/2022 23:24:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/28/2022 23:24:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
05/28/2022 23:24:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
05/28/2022 23:24:37 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.614910323167204 on epoch=181
05/28/2022 23:24:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/28/2022 23:24:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/28/2022 23:24:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/28/2022 23:24:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=183
05/28/2022 23:24:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/28/2022 23:24:55 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6994733168348246 on epoch=184
05/28/2022 23:24:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
05/28/2022 23:25:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=185
05/28/2022 23:25:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=186
05/28/2022 23:25:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/28/2022 23:25:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/28/2022 23:25:08 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:25:08 - INFO - __main__ - Printing 3 examples
05/28/2022 23:25:08 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:25:08 - INFO - __main__ - ['false']
05/28/2022 23:25:08 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:25:08 - INFO - __main__ - ['false']
05/28/2022 23:25:08 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:25:08 - INFO - __main__ - ['false']
05/28/2022 23:25:08 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:25:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:25:09 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:25:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:25:09 - INFO - __main__ - Printing 3 examples
05/28/2022 23:25:09 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:25:09 - INFO - __main__ - ['false']
05/28/2022 23:25:09 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:25:09 - INFO - __main__ - ['false']
05/28/2022 23:25:09 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:25:09 - INFO - __main__ - ['false']
05/28/2022 23:25:09 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:25:09 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:25:09 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:25:12 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.6717948717948719 on epoch=187
05/28/2022 23:25:12 - INFO - __main__ - save last model!
05/28/2022 23:25:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 23:25:12 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 23:25:12 - INFO - __main__ - Printing 3 examples
05/28/2022 23:25:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 23:25:12 - INFO - __main__ - ['false']
05/28/2022 23:25:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 23:25:12 - INFO - __main__ - ['false']
05/28/2022 23:25:12 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 23:25:12 - INFO - __main__ - ['false']
05/28/2022 23:25:12 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:25:13 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:25:16 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 23:25:28 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:25:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:25:29 - INFO - __main__ - Starting training!
05/28/2022 23:26:01 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.5_8_predictions.txt
05/28/2022 23:26:01 - INFO - __main__ - Classification-F1 on test data: 0.4411
05/28/2022 23:26:01 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.5, bsz=8, dev_performance=0.7446873801304181, test_performance=0.44109729682354853
05/28/2022 23:26:01 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.4, bsz=8 ...
05/28/2022 23:26:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:26:02 - INFO - __main__ - Printing 3 examples
05/28/2022 23:26:02 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:26:02 - INFO - __main__ - ['false']
05/28/2022 23:26:02 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:26:02 - INFO - __main__ - ['false']
05/28/2022 23:26:02 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:26:02 - INFO - __main__ - ['false']
05/28/2022 23:26:02 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:26:02 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:26:03 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:26:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:26:03 - INFO - __main__ - Printing 3 examples
05/28/2022 23:26:03 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:26:03 - INFO - __main__ - ['false']
05/28/2022 23:26:03 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:26:03 - INFO - __main__ - ['false']
05/28/2022 23:26:03 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:26:03 - INFO - __main__ - ['false']
05/28/2022 23:26:03 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:26:03 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:26:03 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:26:22 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:26:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:26:23 - INFO - __main__ - Starting training!
05/28/2022 23:26:26 - INFO - __main__ - Step 10 Global step 10 Train loss 2.30 on epoch=0
05/28/2022 23:26:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=1
05/28/2022 23:26:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=1
05/28/2022 23:26:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=2
05/28/2022 23:26:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=3
05/28/2022 23:26:42 - INFO - __main__ - Global step 50 Train loss 0.81 Classification-F1 0.5076923076923077 on epoch=3
05/28/2022 23:26:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5076923076923077 on epoch=3, global_step=50
05/28/2022 23:26:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=3
05/28/2022 23:26:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=4
05/28/2022 23:26:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=4
05/28/2022 23:26:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/28/2022 23:26:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
05/28/2022 23:27:01 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.5093688128613288 on epoch=6
05/28/2022 23:27:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5093688128613288 on epoch=6, global_step=100
05/28/2022 23:27:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=6
05/28/2022 23:27:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/28/2022 23:27:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=8
05/28/2022 23:27:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/28/2022 23:27:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
05/28/2022 23:27:20 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3732922688146569 on epoch=9
05/28/2022 23:27:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
05/28/2022 23:27:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=10
05/28/2022 23:27:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/28/2022 23:27:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
05/28/2022 23:27:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=12
05/28/2022 23:27:38 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.42482504604051563 on epoch=12
05/28/2022 23:27:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=13
05/28/2022 23:27:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=13
05/28/2022 23:27:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=14
05/28/2022 23:27:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=14
05/28/2022 23:27:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=15
05/28/2022 23:27:57 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.5311355311355311 on epoch=15
05/28/2022 23:27:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5093688128613288 -> 0.5311355311355311 on epoch=15, global_step=250
05/28/2022 23:28:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
05/28/2022 23:28:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
05/28/2022 23:28:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
05/28/2022 23:28:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/28/2022 23:28:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/28/2022 23:28:15 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6101342932299598 on epoch=18
05/28/2022 23:28:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5311355311355311 -> 0.6101342932299598 on epoch=18, global_step=300
05/28/2022 23:28:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
05/28/2022 23:28:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
05/28/2022 23:28:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/28/2022 23:28:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
05/28/2022 23:28:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/28/2022 23:28:34 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6405372405372406 on epoch=21
05/28/2022 23:28:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6101342932299598 -> 0.6405372405372406 on epoch=21, global_step=350
05/28/2022 23:28:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=22
05/28/2022 23:28:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=23
05/28/2022 23:28:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/28/2022 23:28:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/28/2022 23:28:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
05/28/2022 23:28:52 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.6388394773967982 on epoch=24
05/28/2022 23:28:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/28/2022 23:28:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=26
05/28/2022 23:29:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
05/28/2022 23:29:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/28/2022 23:29:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=28
05/28/2022 23:29:10 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.6637341153470186 on epoch=28
05/28/2022 23:29:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6405372405372406 -> 0.6637341153470186 on epoch=28, global_step=450
05/28/2022 23:29:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=28
05/28/2022 23:29:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=29
05/28/2022 23:29:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=29
05/28/2022 23:29:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=30
05/28/2022 23:29:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=31
05/28/2022 23:29:28 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.6678420417945078 on epoch=31
05/28/2022 23:29:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6637341153470186 -> 0.6678420417945078 on epoch=31, global_step=500
05/28/2022 23:29:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/28/2022 23:29:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=32
05/28/2022 23:29:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/28/2022 23:29:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=33
05/28/2022 23:29:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=34
05/28/2022 23:29:46 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.6874236874236874 on epoch=34
05/28/2022 23:29:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6678420417945078 -> 0.6874236874236874 on epoch=34, global_step=550
05/28/2022 23:29:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/28/2022 23:29:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=35
05/28/2022 23:29:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=36
05/28/2022 23:29:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=36
05/28/2022 23:29:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=37
05/28/2022 23:30:03 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.67089072543618 on epoch=37
05/28/2022 23:30:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=38
05/28/2022 23:30:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=38
05/28/2022 23:30:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=39
05/28/2022 23:30:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=39
05/28/2022 23:30:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=40
05/28/2022 23:30:21 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.6953125 on epoch=40
05/28/2022 23:30:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6874236874236874 -> 0.6953125 on epoch=40, global_step=650
05/28/2022 23:30:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=41
05/28/2022 23:30:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=41
05/28/2022 23:30:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=42
05/28/2022 23:30:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=43
05/28/2022 23:30:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=43
05/28/2022 23:30:39 - INFO - __main__ - Global step 700 Train loss 0.27 Classification-F1 0.567855133254898 on epoch=43
05/28/2022 23:30:41 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=44
05/28/2022 23:30:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/28/2022 23:30:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=45
05/28/2022 23:30:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=46
05/28/2022 23:30:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=46
05/28/2022 23:30:56 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.6301549776727082 on epoch=46
05/28/2022 23:30:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=47
05/28/2022 23:31:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/28/2022 23:31:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=48
05/28/2022 23:31:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/28/2022 23:31:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/28/2022 23:31:14 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.7087599544937428 on epoch=49
05/28/2022 23:31:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6953125 -> 0.7087599544937428 on epoch=49, global_step=800
05/28/2022 23:31:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/28/2022 23:31:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=51
05/28/2022 23:31:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
05/28/2022 23:31:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=52
05/28/2022 23:31:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=53
05/28/2022 23:31:31 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.6335335252982313 on epoch=53
05/28/2022 23:31:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=53
05/28/2022 23:31:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/28/2022 23:31:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/28/2022 23:31:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=55
05/28/2022 23:31:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/28/2022 23:31:49 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.7187328328145028 on epoch=56
05/28/2022 23:31:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7087599544937428 -> 0.7187328328145028 on epoch=56, global_step=900
05/28/2022 23:31:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=56
05/28/2022 23:31:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=57
05/28/2022 23:31:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/28/2022 23:31:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/28/2022 23:32:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/28/2022 23:32:06 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.7368316072113541 on epoch=59
05/28/2022 23:32:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7187328328145028 -> 0.7368316072113541 on epoch=59, global_step=950
05/28/2022 23:32:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/28/2022 23:32:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/28/2022 23:32:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=61
05/28/2022 23:32:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=61
05/28/2022 23:32:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/28/2022 23:32:24 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.7278735499376068 on epoch=62
05/28/2022 23:32:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=63
05/28/2022 23:32:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
05/28/2022 23:32:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=64
05/28/2022 23:32:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=64
05/28/2022 23:32:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=65
05/28/2022 23:32:42 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.6857493583305434 on epoch=65
05/28/2022 23:32:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=66
05/28/2022 23:32:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
05/28/2022 23:32:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/28/2022 23:32:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/28/2022 23:32:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/28/2022 23:32:59 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.6617898908841806 on epoch=68
05/28/2022 23:33:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=69
05/28/2022 23:33:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=69
05/28/2022 23:33:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/28/2022 23:33:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
05/28/2022 23:33:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
05/28/2022 23:33:17 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.5829260345389378 on epoch=71
05/28/2022 23:33:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=72
05/28/2022 23:33:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
05/28/2022 23:33:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=73
05/28/2022 23:33:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=74
05/28/2022 23:33:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/28/2022 23:33:35 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.7174392935982341 on epoch=74
05/28/2022 23:33:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=75
05/28/2022 23:33:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=76
05/28/2022 23:33:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=76
05/28/2022 23:33:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
05/28/2022 23:33:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=78
05/28/2022 23:33:52 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.655371683740478 on epoch=78
05/28/2022 23:33:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/28/2022 23:33:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=79
05/28/2022 23:34:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/28/2022 23:34:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/28/2022 23:34:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=81
05/28/2022 23:34:10 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.7332960146786709 on epoch=81
05/28/2022 23:34:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/28/2022 23:34:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=82
05/28/2022 23:34:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/28/2022 23:34:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=83
05/28/2022 23:34:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/28/2022 23:34:28 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.7277277277277278 on epoch=84
05/28/2022 23:34:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=84
05/28/2022 23:34:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=85
05/28/2022 23:34:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=86
05/28/2022 23:34:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=86
05/28/2022 23:34:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
05/28/2022 23:34:45 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.7427931061133008 on epoch=87
05/28/2022 23:34:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7368316072113541 -> 0.7427931061133008 on epoch=87, global_step=1400
05/28/2022 23:34:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=88
05/28/2022 23:34:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=88
05/28/2022 23:34:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=89
05/28/2022 23:34:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
05/28/2022 23:34:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=90
05/28/2022 23:35:03 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.6976785569845506 on epoch=90
05/28/2022 23:35:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=91
05/28/2022 23:35:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=91
05/28/2022 23:35:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=92
05/28/2022 23:35:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
05/28/2022 23:35:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/28/2022 23:35:20 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.6436903499469778 on epoch=93
05/28/2022 23:35:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=94
05/28/2022 23:35:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=94
05/28/2022 23:35:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/28/2022 23:35:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=96
05/28/2022 23:35:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=96
05/28/2022 23:35:38 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.5810293195905007 on epoch=96
05/28/2022 23:35:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=97
05/28/2022 23:35:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=98
05/28/2022 23:35:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/28/2022 23:35:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=99
05/28/2022 23:35:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
05/28/2022 23:35:56 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.6124604723137994 on epoch=99
05/28/2022 23:35:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/28/2022 23:36:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=101
05/28/2022 23:36:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
05/28/2022 23:36:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/28/2022 23:36:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/28/2022 23:36:14 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.6243865418669365 on epoch=103
05/28/2022 23:36:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
05/28/2022 23:36:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=104
05/28/2022 23:36:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=104
05/28/2022 23:36:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=105
05/28/2022 23:36:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/28/2022 23:36:31 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.7350561374898075 on epoch=106
05/28/2022 23:36:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/28/2022 23:36:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=107
05/28/2022 23:36:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=108
05/28/2022 23:36:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/28/2022 23:36:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
05/28/2022 23:36:49 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.7710761640456367 on epoch=109
05/28/2022 23:36:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7427931061133008 -> 0.7710761640456367 on epoch=109, global_step=1750
05/28/2022 23:36:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=109
05/28/2022 23:36:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=110
05/28/2022 23:36:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/28/2022 23:36:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/28/2022 23:37:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=112
05/28/2022 23:37:06 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.675 on epoch=112
05/28/2022 23:37:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=113
05/28/2022 23:37:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=113
05/28/2022 23:37:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=114
05/28/2022 23:37:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=114
05/28/2022 23:37:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/28/2022 23:37:24 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.698497580850522 on epoch=115
05/28/2022 23:37:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
05/28/2022 23:37:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=116
05/28/2022 23:37:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/28/2022 23:37:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=118
05/28/2022 23:37:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=118
05/28/2022 23:37:41 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.6223484527342094 on epoch=118
05/28/2022 23:37:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=119
05/28/2022 23:37:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/28/2022 23:37:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/28/2022 23:37:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=121
05/28/2022 23:37:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/28/2022 23:37:59 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.5752336040520478 on epoch=121
05/28/2022 23:38:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=122
05/28/2022 23:38:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/28/2022 23:38:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=123
05/28/2022 23:38:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=124
05/28/2022 23:38:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=124
05/28/2022 23:38:16 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.5778877028551592 on epoch=124
05/28/2022 23:38:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/28/2022 23:38:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=126
05/28/2022 23:38:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=126
05/28/2022 23:38:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=127
05/28/2022 23:38:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/28/2022 23:38:34 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.6687142693334645 on epoch=128
05/28/2022 23:38:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=128
05/28/2022 23:38:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=129
05/28/2022 23:38:42 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=129
05/28/2022 23:38:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=130
05/28/2022 23:38:47 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=131
05/28/2022 23:38:52 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.6893957777238535 on epoch=131
05/28/2022 23:38:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=131
05/28/2022 23:38:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
05/28/2022 23:38:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/28/2022 23:39:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/28/2022 23:39:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/28/2022 23:39:09 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.6701407577759293 on epoch=134
05/28/2022 23:39:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=134
05/28/2022 23:39:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=135
05/28/2022 23:39:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
05/28/2022 23:39:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=136
05/28/2022 23:39:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=137
05/28/2022 23:39:27 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.7343396226415094 on epoch=137
05/28/2022 23:39:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/28/2022 23:39:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
05/28/2022 23:39:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=139
05/28/2022 23:39:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=139
05/28/2022 23:39:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=140
05/28/2022 23:39:44 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.6988477963097253 on epoch=140
05/28/2022 23:39:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/28/2022 23:39:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/28/2022 23:39:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/28/2022 23:39:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
05/28/2022 23:39:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
05/28/2022 23:40:02 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.6833333333333333 on epoch=143
05/28/2022 23:40:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=144
05/28/2022 23:40:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/28/2022 23:40:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=145
05/28/2022 23:40:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/28/2022 23:40:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=146
05/28/2022 23:40:20 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.5393304470219323 on epoch=146
05/28/2022 23:40:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/28/2022 23:40:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/28/2022 23:40:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/28/2022 23:40:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=149
05/28/2022 23:40:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=149
05/28/2022 23:40:37 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.583610188261351 on epoch=149
05/28/2022 23:40:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/28/2022 23:40:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=151
05/28/2022 23:40:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=151
05/28/2022 23:40:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=152
05/28/2022 23:40:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
05/28/2022 23:40:55 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.655371683740478 on epoch=153
05/28/2022 23:40:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=153
05/28/2022 23:41:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/28/2022 23:41:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=154
05/28/2022 23:41:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/28/2022 23:41:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/28/2022 23:41:12 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6352067868504772 on epoch=156
05/28/2022 23:41:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/28/2022 23:41:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/28/2022 23:41:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/28/2022 23:41:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/28/2022 23:41:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/28/2022 23:41:30 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7306828709155714 on epoch=159
05/28/2022 23:41:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/28/2022 23:41:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=160
05/28/2022 23:41:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
05/28/2022 23:41:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/28/2022 23:41:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
05/28/2022 23:41:47 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.7174392935982341 on epoch=162
05/28/2022 23:41:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=163
05/28/2022 23:41:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/28/2022 23:41:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=164
05/28/2022 23:41:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
05/28/2022 23:42:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/28/2022 23:42:05 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.6882051282051281 on epoch=165
05/28/2022 23:42:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
05/28/2022 23:42:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/28/2022 23:42:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=167
05/28/2022 23:42:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/28/2022 23:42:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/28/2022 23:42:22 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.6049851632047478 on epoch=168
05/28/2022 23:42:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
05/28/2022 23:42:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/28/2022 23:42:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/28/2022 23:42:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/28/2022 23:42:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=171
05/28/2022 23:42:40 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.5604149144939725 on epoch=171
05/28/2022 23:42:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
05/28/2022 23:42:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
05/28/2022 23:42:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=173
05/28/2022 23:42:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/28/2022 23:42:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=174
05/28/2022 23:42:57 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.6315930388219544 on epoch=174
05/28/2022 23:43:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/28/2022 23:43:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/28/2022 23:43:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=176
05/28/2022 23:43:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/28/2022 23:43:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/28/2022 23:43:15 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.6243865418669365 on epoch=178
05/28/2022 23:43:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=178
05/28/2022 23:43:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/28/2022 23:43:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/28/2022 23:43:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/28/2022 23:43:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/28/2022 23:43:32 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6470842260866676 on epoch=181
05/28/2022 23:43:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/28/2022 23:43:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
05/28/2022 23:43:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=183
05/28/2022 23:43:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
05/28/2022 23:43:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
05/28/2022 23:43:49 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7313914337170151 on epoch=184
05/28/2022 23:43:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
05/28/2022 23:43:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/28/2022 23:43:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/28/2022 23:43:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/28/2022 23:44:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/28/2022 23:44:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:44:03 - INFO - __main__ - Printing 3 examples
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:44:03 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:44:03 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:44:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:44:03 - INFO - __main__ - Printing 3 examples
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:44:03 - INFO - __main__ - ['false']
05/28/2022 23:44:03 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:44:04 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:44:04 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:44:06 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6715123361541085 on epoch=187
05/28/2022 23:44:06 - INFO - __main__ - save last model!
05/28/2022 23:44:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/28/2022 23:44:06 - INFO - __main__ - Start tokenizing ... 2733 instances
05/28/2022 23:44:06 - INFO - __main__ - Printing 3 examples
05/28/2022 23:44:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/28/2022 23:44:06 - INFO - __main__ - ['false']
05/28/2022 23:44:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/28/2022 23:44:06 - INFO - __main__ - ['false']
05/28/2022 23:44:06 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/28/2022 23:44:06 - INFO - __main__ - ['false']
05/28/2022 23:44:06 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:44:07 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:44:10 - INFO - __main__ - Loaded 2733 examples from test data
05/28/2022 23:44:19 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:44:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:44:20 - INFO - __main__ - Starting training!
05/28/2022 23:44:53 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.4_8_predictions.txt
05/28/2022 23:44:53 - INFO - __main__ - Classification-F1 on test data: 0.4098
05/28/2022 23:44:53 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.4, bsz=8, dev_performance=0.7710761640456367, test_performance=0.40976222815480934
05/28/2022 23:44:53 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.3, bsz=8 ...
05/28/2022 23:44:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:44:54 - INFO - __main__ - Printing 3 examples
05/28/2022 23:44:54 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/28/2022 23:44:54 - INFO - __main__ - ['false']
05/28/2022 23:44:54 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/28/2022 23:44:54 - INFO - __main__ - ['false']
05/28/2022 23:44:54 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/28/2022 23:44:54 - INFO - __main__ - ['false']
05/28/2022 23:44:54 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:44:54 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:44:55 - INFO - __main__ - Loaded 256 examples from train data
05/28/2022 23:44:55 - INFO - __main__ - Start tokenizing ... 256 instances
05/28/2022 23:44:55 - INFO - __main__ - Printing 3 examples
05/28/2022 23:44:55 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/28/2022 23:44:55 - INFO - __main__ - ['false']
05/28/2022 23:44:55 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/28/2022 23:44:55 - INFO - __main__ - ['false']
05/28/2022 23:44:55 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/28/2022 23:44:55 - INFO - __main__ - ['false']
05/28/2022 23:44:55 - INFO - __main__ - Tokenizing Input ...
05/28/2022 23:44:55 - INFO - __main__ - Tokenizing Output ...
05/28/2022 23:44:55 - INFO - __main__ - Loaded 256 examples from dev data
05/28/2022 23:45:11 - INFO - __main__ - load prompt embedding from ckpt
05/28/2022 23:45:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/28/2022 23:45:12 - INFO - __main__ - Starting training!
05/28/2022 23:45:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.01 on epoch=0
05/28/2022 23:45:17 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=1
05/28/2022 23:45:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
05/28/2022 23:45:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=2
05/28/2022 23:45:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=3
05/28/2022 23:45:30 - INFO - __main__ - Global step 50 Train loss 1.01 Classification-F1 0.5599535423925668 on epoch=3
05/28/2022 23:45:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5599535423925668 on epoch=3, global_step=50
05/28/2022 23:45:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
05/28/2022 23:45:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=4
05/28/2022 23:45:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/28/2022 23:45:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/28/2022 23:45:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
05/28/2022 23:45:49 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.36231884057971014 on epoch=6
05/28/2022 23:45:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=6
05/28/2022 23:45:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=7
05/28/2022 23:45:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=8
05/28/2022 23:45:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=8
05/28/2022 23:46:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
05/28/2022 23:46:07 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=9
05/28/2022 23:46:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=9
05/28/2022 23:46:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=10
05/28/2022 23:46:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/28/2022 23:46:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/28/2022 23:46:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
05/28/2022 23:46:25 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.42235512098475536 on epoch=12
05/28/2022 23:46:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=13
05/28/2022 23:46:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
05/28/2022 23:46:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=14
05/28/2022 23:46:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=14
05/28/2022 23:46:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=15
05/28/2022 23:46:43 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=15
05/28/2022 23:46:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=16
05/28/2022 23:46:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/28/2022 23:46:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/28/2022 23:46:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
05/28/2022 23:46:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/28/2022 23:47:02 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.5411320754716982 on epoch=18
05/28/2022 23:47:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
05/28/2022 23:47:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/28/2022 23:47:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
05/28/2022 23:47:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=21
05/28/2022 23:47:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
05/28/2022 23:47:20 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.6249084249084249 on epoch=21
05/28/2022 23:47:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5599535423925668 -> 0.6249084249084249 on epoch=21, global_step=350
05/28/2022 23:47:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=22
05/28/2022 23:47:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=23
05/28/2022 23:47:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/28/2022 23:47:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=24
05/28/2022 23:47:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/28/2022 23:47:37 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.6405372405372406 on epoch=24
05/28/2022 23:47:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6249084249084249 -> 0.6405372405372406 on epoch=24, global_step=400
05/28/2022 23:47:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
05/28/2022 23:47:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=26
05/28/2022 23:47:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
05/28/2022 23:47:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=27
05/28/2022 23:47:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
05/28/2022 23:47:54 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6191371075092005 on epoch=28
05/28/2022 23:47:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/28/2022 23:47:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
05/28/2022 23:48:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
05/28/2022 23:48:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
05/28/2022 23:48:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=31
05/28/2022 23:48:12 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.651146055029015 on epoch=31
05/28/2022 23:48:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6405372405372406 -> 0.651146055029015 on epoch=31, global_step=500
05/28/2022 23:48:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/28/2022 23:48:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
05/28/2022 23:48:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/28/2022 23:48:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/28/2022 23:48:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=34
05/28/2022 23:48:29 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.655493026669929 on epoch=34
05/28/2022 23:48:29 - INFO - __main__ - Saving model with best Classification-F1: 0.651146055029015 -> 0.655493026669929 on epoch=34, global_step=550
05/28/2022 23:48:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=34
05/28/2022 23:48:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=35
05/28/2022 23:48:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/28/2022 23:48:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=36
05/28/2022 23:48:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/28/2022 23:48:47 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.6562290178843924 on epoch=37
05/28/2022 23:48:47 - INFO - __main__ - Saving model with best Classification-F1: 0.655493026669929 -> 0.6562290178843924 on epoch=37, global_step=600
05/28/2022 23:48:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/28/2022 23:48:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.30 on epoch=38
05/28/2022 23:48:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=39
05/28/2022 23:48:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=39
05/28/2022 23:48:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/28/2022 23:49:04 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.65625 on epoch=40
05/28/2022 23:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6562290178843924 -> 0.65625 on epoch=40, global_step=650
05/28/2022 23:49:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=41
05/28/2022 23:49:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=41
05/28/2022 23:49:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=42
05/28/2022 23:49:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=43
05/28/2022 23:49:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=43
05/28/2022 23:49:21 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.6319962902851843 on epoch=43
05/28/2022 23:49:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.35 on epoch=44
05/28/2022 23:49:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
05/28/2022 23:49:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=45
05/28/2022 23:49:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/28/2022 23:49:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/28/2022 23:49:38 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.6226557109703318 on epoch=46
05/28/2022 23:49:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=47
05/28/2022 23:49:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/28/2022 23:49:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/28/2022 23:49:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=49
05/28/2022 23:49:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=49
05/28/2022 23:49:55 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.6605612087573234 on epoch=49
05/28/2022 23:49:55 - INFO - __main__ - Saving model with best Classification-F1: 0.65625 -> 0.6605612087573234 on epoch=49, global_step=800
05/28/2022 23:49:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=50
05/28/2022 23:50:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=51
05/28/2022 23:50:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/28/2022 23:50:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=52
05/28/2022 23:50:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=53
05/28/2022 23:50:13 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.6556556556556556 on epoch=53
05/28/2022 23:50:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=53
05/28/2022 23:50:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/28/2022 23:50:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=54
05/28/2022 23:50:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
05/28/2022 23:50:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=56
05/28/2022 23:50:30 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.6791979949874687 on epoch=56
05/28/2022 23:50:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6605612087573234 -> 0.6791979949874687 on epoch=56, global_step=900
05/28/2022 23:50:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=56
05/28/2022 23:50:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=57
05/28/2022 23:50:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
05/28/2022 23:50:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=58
05/28/2022 23:50:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=59
05/28/2022 23:50:47 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.6952380952380953 on epoch=59
05/28/2022 23:50:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6791979949874687 -> 0.6952380952380953 on epoch=59, global_step=950
05/28/2022 23:50:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=59
05/28/2022 23:50:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=60
05/28/2022 23:50:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=61
05/28/2022 23:50:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=61
05/28/2022 23:50:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=62
05/28/2022 23:51:04 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.6963226571767498 on epoch=62
05/28/2022 23:51:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6952380952380953 -> 0.6963226571767498 on epoch=62, global_step=1000
05/28/2022 23:51:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/28/2022 23:51:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=63
05/28/2022 23:51:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/28/2022 23:51:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=64
05/28/2022 23:51:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=65
05/28/2022 23:51:22 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.6645474399417617 on epoch=65
05/28/2022 23:51:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/28/2022 23:51:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=66
05/28/2022 23:51:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=67
05/28/2022 23:51:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/28/2022 23:51:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=68
05/28/2022 23:51:39 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.6197025587955468 on epoch=68
05/28/2022 23:51:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=69
05/28/2022 23:51:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=69
05/28/2022 23:51:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=70
05/28/2022 23:51:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
05/28/2022 23:51:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
05/28/2022 23:51:57 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.653439023992432 on epoch=71
05/28/2022 23:51:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=72
05/28/2022 23:52:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
05/28/2022 23:52:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/28/2022 23:52:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/28/2022 23:52:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/28/2022 23:52:14 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.6606574761399787 on epoch=74
05/28/2022 23:52:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=75
05/28/2022 23:52:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/28/2022 23:52:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=76
05/28/2022 23:52:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=77
05/28/2022 23:52:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=78
05/28/2022 23:52:32 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.640160642570281 on epoch=78
05/28/2022 23:52:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
05/28/2022 23:52:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/28/2022 23:52:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=79
05/28/2022 23:52:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=80
05/28/2022 23:52:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=81
05/28/2022 23:52:49 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.7167391338226814 on epoch=81
05/28/2022 23:52:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6963226571767498 -> 0.7167391338226814 on epoch=81, global_step=1300
05/28/2022 23:52:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=81
05/28/2022 23:52:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/28/2022 23:52:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=83
05/28/2022 23:52:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/28/2022 23:53:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=84
05/28/2022 23:53:06 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.7073369352439121 on epoch=84
05/28/2022 23:53:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/28/2022 23:53:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=85
05/28/2022 23:53:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=86
05/28/2022 23:53:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=86
05/28/2022 23:53:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/28/2022 23:53:24 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7048968054700206 on epoch=87
05/28/2022 23:53:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/28/2022 23:53:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=88
05/28/2022 23:53:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/28/2022 23:53:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=89
05/28/2022 23:53:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/28/2022 23:53:41 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7092351746092688 on epoch=90
05/28/2022 23:53:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=91
05/28/2022 23:53:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
05/28/2022 23:53:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/28/2022 23:53:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=93
05/28/2022 23:53:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/28/2022 23:53:59 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.5949088407292742 on epoch=93
05/28/2022 23:54:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=94
05/28/2022 23:54:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
05/28/2022 23:54:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/28/2022 23:54:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
05/28/2022 23:54:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
05/28/2022 23:54:16 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.5693860386879731 on epoch=96
05/28/2022 23:54:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/28/2022 23:54:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=98
05/28/2022 23:54:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
05/28/2022 23:54:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=99
05/28/2022 23:54:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=99
05/28/2022 23:54:34 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.6964607663196193 on epoch=99
05/28/2022 23:54:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
05/28/2022 23:54:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/28/2022 23:54:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
05/28/2022 23:54:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
05/28/2022 23:54:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=103
05/28/2022 23:54:51 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.6706906788046705 on epoch=103
05/28/2022 23:54:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=103
05/28/2022 23:54:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/28/2022 23:54:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=104
05/28/2022 23:55:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=105
05/28/2022 23:55:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/28/2022 23:55:09 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.6882051282051281 on epoch=106
05/28/2022 23:55:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
05/28/2022 23:55:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/28/2022 23:55:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=108
05/28/2022 23:55:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
05/28/2022 23:55:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
05/28/2022 23:55:26 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.7357357357357357 on epoch=109
05/28/2022 23:55:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7167391338226814 -> 0.7357357357357357 on epoch=109, global_step=1750
05/28/2022 23:55:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/28/2022 23:55:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
05/28/2022 23:55:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/28/2022 23:55:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/28/2022 23:55:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/28/2022 23:55:44 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.6771012498566678 on epoch=112
05/28/2022 23:55:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=113
05/28/2022 23:55:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/28/2022 23:55:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=114
05/28/2022 23:55:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/28/2022 23:55:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/28/2022 23:56:01 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.7106690777576854 on epoch=115
05/28/2022 23:56:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
05/28/2022 23:56:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/28/2022 23:56:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=117
05/28/2022 23:56:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=118
05/28/2022 23:56:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=118
05/28/2022 23:56:19 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.6169489219887097 on epoch=118
05/28/2022 23:56:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/28/2022 23:56:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/28/2022 23:56:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=120
05/28/2022 23:56:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=121
05/28/2022 23:56:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=121
05/28/2022 23:56:36 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.5693860386879731 on epoch=121
05/28/2022 23:56:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/28/2022 23:56:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/28/2022 23:56:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=123
05/28/2022 23:56:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=124
05/28/2022 23:56:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=124
05/28/2022 23:56:53 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.629684451322075 on epoch=124
05/28/2022 23:56:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/28/2022 23:56:58 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/28/2022 23:57:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
05/28/2022 23:57:03 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=127
05/28/2022 23:57:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=128
05/28/2022 23:57:11 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.5892835578863836 on epoch=128
05/28/2022 23:57:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/28/2022 23:57:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=129
05/28/2022 23:57:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=129
05/28/2022 23:57:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=130
05/28/2022 23:57:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=131
05/28/2022 23:57:28 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7255124566382845 on epoch=131
05/28/2022 23:57:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
05/28/2022 23:57:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/28/2022 23:57:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
05/28/2022 23:57:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=133
05/28/2022 23:57:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
05/28/2022 23:57:46 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.7412573276733373 on epoch=134
05/28/2022 23:57:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7357357357357357 -> 0.7412573276733373 on epoch=134, global_step=2150
05/28/2022 23:57:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=134
05/28/2022 23:57:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/28/2022 23:57:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=136
05/28/2022 23:57:56 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/28/2022 23:57:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
05/28/2022 23:58:04 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.6436903499469778 on epoch=137
05/28/2022 23:58:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=138
05/28/2022 23:58:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
05/28/2022 23:58:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
05/28/2022 23:58:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/28/2022 23:58:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=140
05/28/2022 23:58:21 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.5810293195905007 on epoch=140
05/28/2022 23:58:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/28/2022 23:58:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/28/2022 23:58:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=142
05/28/2022 23:58:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=143
05/28/2022 23:58:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
05/28/2022 23:58:38 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.7030587113688727 on epoch=143
05/28/2022 23:58:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=144
05/28/2022 23:58:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=144
05/28/2022 23:58:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/28/2022 23:58:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/28/2022 23:58:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/28/2022 23:58:56 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.5393304470219323 on epoch=146
05/28/2022 23:58:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
05/28/2022 23:59:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=148
05/28/2022 23:59:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/28/2022 23:59:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
05/28/2022 23:59:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
05/28/2022 23:59:13 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.6367945786249605 on epoch=149
05/28/2022 23:59:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/28/2022 23:59:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/28/2022 23:59:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=151
05/28/2022 23:59:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=152
05/28/2022 23:59:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=153
05/28/2022 23:59:31 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.5693860386879731 on epoch=153
05/28/2022 23:59:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=153
05/28/2022 23:59:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/28/2022 23:59:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/28/2022 23:59:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
05/28/2022 23:59:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
05/28/2022 23:59:48 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.698497580850522 on epoch=156
05/28/2022 23:59:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/28/2022 23:59:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
05/28/2022 23:59:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=158
05/28/2022 23:59:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=158
05/29/2022 00:00:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=159
05/29/2022 00:00:06 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.6903488668194551 on epoch=159
05/29/2022 00:00:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=159
05/29/2022 00:00:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
05/29/2022 00:00:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/29/2022 00:00:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/29/2022 00:00:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=162
05/29/2022 00:00:23 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.640160642570281 on epoch=162
05/29/2022 00:00:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=163
05/29/2022 00:00:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
05/29/2022 00:00:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/29/2022 00:00:33 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
05/29/2022 00:00:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
05/29/2022 00:00:41 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.6115061298958173 on epoch=165
05/29/2022 00:00:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
05/29/2022 00:00:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=166
05/29/2022 00:00:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/29/2022 00:00:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/29/2022 00:00:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/29/2022 00:00:58 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.6223484527342094 on epoch=168
05/29/2022 00:01:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/29/2022 00:01:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/29/2022 00:01:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
05/29/2022 00:01:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/29/2022 00:01:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
05/29/2022 00:01:16 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.5634855063547032 on epoch=171
05/29/2022 00:01:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/29/2022 00:01:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
05/29/2022 00:01:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/29/2022 00:01:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
05/29/2022 00:01:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=174
05/29/2022 00:01:33 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.5693860386879731 on epoch=174
05/29/2022 00:01:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=175
05/29/2022 00:01:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/29/2022 00:01:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/29/2022 00:01:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=177
05/29/2022 00:01:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/29/2022 00:01:51 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6169489219887097 on epoch=178
05/29/2022 00:01:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
05/29/2022 00:01:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
05/29/2022 00:01:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
05/29/2022 00:02:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/29/2022 00:02:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/29/2022 00:02:08 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.6631961927656049 on epoch=181
05/29/2022 00:02:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
05/29/2022 00:02:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
05/29/2022 00:02:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=183
05/29/2022 00:02:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=183
05/29/2022 00:02:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=184
05/29/2022 00:02:26 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.6728302192024589 on epoch=184
05/29/2022 00:02:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
05/29/2022 00:02:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/29/2022 00:02:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=186
05/29/2022 00:02:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=186
05/29/2022 00:02:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/29/2022 00:02:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:02:39 - INFO - __main__ - Printing 3 examples
05/29/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/29/2022 00:02:39 - INFO - __main__ - ['false']
05/29/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/29/2022 00:02:39 - INFO - __main__ - ['false']
05/29/2022 00:02:39 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/29/2022 00:02:39 - INFO - __main__ - ['false']
05/29/2022 00:02:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:02:40 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:02:40 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:02:40 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:02:40 - INFO - __main__ - Printing 3 examples
05/29/2022 00:02:40 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/29/2022 00:02:40 - INFO - __main__ - ['false']
05/29/2022 00:02:40 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/29/2022 00:02:40 - INFO - __main__ - ['false']
05/29/2022 00:02:40 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/29/2022 00:02:40 - INFO - __main__ - ['false']
05/29/2022 00:02:40 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:02:40 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:02:40 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:02:43 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.6230254350736278 on epoch=187
05/29/2022 00:02:43 - INFO - __main__ - save last model!
05/29/2022 00:02:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 00:02:43 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 00:02:43 - INFO - __main__ - Printing 3 examples
05/29/2022 00:02:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 00:02:43 - INFO - __main__ - ['false']
05/29/2022 00:02:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 00:02:43 - INFO - __main__ - ['false']
05/29/2022 00:02:43 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 00:02:43 - INFO - __main__ - ['false']
05/29/2022 00:02:43 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:02:44 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:02:47 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 00:02:56 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:02:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:02:56 - INFO - __main__ - Starting training!
05/29/2022 00:03:37 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.3_8_predictions.txt
05/29/2022 00:03:37 - INFO - __main__ - Classification-F1 on test data: 0.3651
05/29/2022 00:03:38 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.3, bsz=8, dev_performance=0.7412573276733373, test_performance=0.3651162843696943
05/29/2022 00:03:38 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.2, bsz=8 ...
05/29/2022 00:03:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:03:39 - INFO - __main__ - Printing 3 examples
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:03:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:03:39 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:03:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:03:39 - INFO - __main__ - Printing 3 examples
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
05/29/2022 00:03:39 - INFO - __main__ - ['false']
05/29/2022 00:03:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:03:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:03:40 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:03:55 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:03:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:03:56 - INFO - __main__ - Starting training!
05/29/2022 00:03:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.03 on epoch=0
05/29/2022 00:04:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.03 on epoch=1
05/29/2022 00:04:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=1
05/29/2022 00:04:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
05/29/2022 00:04:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=3
05/29/2022 00:04:15 - INFO - __main__ - Global step 50 Train loss 1.11 Classification-F1 0.36304897101085887 on epoch=3
05/29/2022 00:04:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36304897101085887 on epoch=3, global_step=50
05/29/2022 00:04:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=3
05/29/2022 00:04:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=4
05/29/2022 00:04:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/29/2022 00:04:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=5
05/29/2022 00:04:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
05/29/2022 00:04:34 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.34296770117665637 on epoch=6
05/29/2022 00:04:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
05/29/2022 00:04:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=7
05/29/2022 00:04:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
05/29/2022 00:04:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
05/29/2022 00:04:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=9
05/29/2022 00:04:52 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3671630170316302 on epoch=9
05/29/2022 00:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.36304897101085887 -> 0.3671630170316302 on epoch=9, global_step=150
05/29/2022 00:04:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.36 on epoch=9
05/29/2022 00:04:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
05/29/2022 00:04:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/29/2022 00:05:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=11
05/29/2022 00:05:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
05/29/2022 00:05:10 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=12
05/29/2022 00:05:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
05/29/2022 00:05:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=13
05/29/2022 00:05:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=14
05/29/2022 00:05:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=14
05/29/2022 00:05:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
05/29/2022 00:05:29 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3671451355661882 on epoch=15
05/29/2022 00:05:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=16
05/29/2022 00:05:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
05/29/2022 00:05:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/29/2022 00:05:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/29/2022 00:05:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=18
05/29/2022 00:05:47 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.5405128205128205 on epoch=18
05/29/2022 00:05:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3671630170316302 -> 0.5405128205128205 on epoch=18, global_step=300
05/29/2022 00:05:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=19
05/29/2022 00:05:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
05/29/2022 00:05:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/29/2022 00:05:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
05/29/2022 00:06:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
05/29/2022 00:06:06 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.45469441258397514 on epoch=21
05/29/2022 00:06:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
05/29/2022 00:06:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=23
05/29/2022 00:06:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=23
05/29/2022 00:06:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/29/2022 00:06:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/29/2022 00:06:24 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.4971598449859319 on epoch=24
05/29/2022 00:06:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
05/29/2022 00:06:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
05/29/2022 00:06:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
05/29/2022 00:06:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=27
05/29/2022 00:06:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
05/29/2022 00:06:43 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6075375279463431 on epoch=28
05/29/2022 00:06:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5405128205128205 -> 0.6075375279463431 on epoch=28, global_step=450
05/29/2022 00:06:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
05/29/2022 00:06:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/29/2022 00:06:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
05/29/2022 00:06:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=30
05/29/2022 00:06:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
05/29/2022 00:07:01 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.6351734398246026 on epoch=31
05/29/2022 00:07:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6075375279463431 -> 0.6351734398246026 on epoch=31, global_step=500
05/29/2022 00:07:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/29/2022 00:07:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/29/2022 00:07:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/29/2022 00:07:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=33
05/29/2022 00:07:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/29/2022 00:07:20 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.6182325098878299 on epoch=34
05/29/2022 00:07:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/29/2022 00:07:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=35
05/29/2022 00:07:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=36
05/29/2022 00:07:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/29/2022 00:07:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
05/29/2022 00:07:38 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6307692307692307 on epoch=37
05/29/2022 00:07:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/29/2022 00:07:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/29/2022 00:07:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/29/2022 00:07:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
05/29/2022 00:07:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
05/29/2022 00:07:56 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.6342110634823432 on epoch=40
05/29/2022 00:07:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
05/29/2022 00:08:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
05/29/2022 00:08:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/29/2022 00:08:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/29/2022 00:08:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/29/2022 00:08:14 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.6664980765399175 on epoch=43
05/29/2022 00:08:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6351734398246026 -> 0.6664980765399175 on epoch=43, global_step=700
05/29/2022 00:08:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/29/2022 00:08:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/29/2022 00:08:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=45
05/29/2022 00:08:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/29/2022 00:08:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/29/2022 00:08:32 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.6313725490196078 on epoch=46
05/29/2022 00:08:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=47
05/29/2022 00:08:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
05/29/2022 00:08:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=48
05/29/2022 00:08:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
05/29/2022 00:08:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/29/2022 00:08:50 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.6522110790554259 on epoch=49
05/29/2022 00:08:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=50
05/29/2022 00:08:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=51
05/29/2022 00:08:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/29/2022 00:09:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=52
05/29/2022 00:09:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=53
05/29/2022 00:09:08 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.6440913604766634 on epoch=53
05/29/2022 00:09:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=53
05/29/2022 00:09:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
05/29/2022 00:09:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
05/29/2022 00:09:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=55
05/29/2022 00:09:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
05/29/2022 00:09:25 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.6601510643167773 on epoch=56
05/29/2022 00:09:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=56
05/29/2022 00:09:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
05/29/2022 00:09:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
05/29/2022 00:09:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
05/29/2022 00:09:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=59
05/29/2022 00:09:43 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.6731428923039057 on epoch=59
05/29/2022 00:09:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6664980765399175 -> 0.6731428923039057 on epoch=59, global_step=950
05/29/2022 00:09:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/29/2022 00:09:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/29/2022 00:09:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=61
05/29/2022 00:09:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/29/2022 00:09:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=62
05/29/2022 00:10:00 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.6713735558408216 on epoch=62
05/29/2022 00:10:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=63
05/29/2022 00:10:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/29/2022 00:10:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/29/2022 00:10:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
05/29/2022 00:10:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=65
05/29/2022 00:10:17 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6520836196497015 on epoch=65
05/29/2022 00:10:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=66
05/29/2022 00:10:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=66
05/29/2022 00:10:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
05/29/2022 00:10:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=68
05/29/2022 00:10:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=68
05/29/2022 00:10:35 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.6673061451822514 on epoch=68
05/29/2022 00:10:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=69
05/29/2022 00:10:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=69
05/29/2022 00:10:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=70
05/29/2022 00:10:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
05/29/2022 00:10:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
05/29/2022 00:10:52 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.6608848785201116 on epoch=71
05/29/2022 00:10:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
05/29/2022 00:10:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=73
05/29/2022 00:11:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=73
05/29/2022 00:11:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=74
05/29/2022 00:11:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
05/29/2022 00:11:09 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.6739854238588416 on epoch=74
05/29/2022 00:11:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6731428923039057 -> 0.6739854238588416 on epoch=74, global_step=1200
05/29/2022 00:11:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=75
05/29/2022 00:11:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
05/29/2022 00:11:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
05/29/2022 00:11:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=77
05/29/2022 00:11:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
05/29/2022 00:11:27 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.6814501881864967 on epoch=78
05/29/2022 00:11:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6739854238588416 -> 0.6814501881864967 on epoch=78, global_step=1250
05/29/2022 00:11:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
05/29/2022 00:11:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=79
05/29/2022 00:11:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=79
05/29/2022 00:11:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
05/29/2022 00:11:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/29/2022 00:11:44 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.6941176470588235 on epoch=81
05/29/2022 00:11:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6814501881864967 -> 0.6941176470588235 on epoch=81, global_step=1300
05/29/2022 00:11:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=81
05/29/2022 00:11:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=82
05/29/2022 00:11:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=83
05/29/2022 00:11:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=83
05/29/2022 00:11:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=84
05/29/2022 00:12:02 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.6859473716493897 on epoch=84
05/29/2022 00:12:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=84
05/29/2022 00:12:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
05/29/2022 00:12:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=86
05/29/2022 00:12:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=86
05/29/2022 00:12:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=87
05/29/2022 00:12:19 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.6984411094283047 on epoch=87
05/29/2022 00:12:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6941176470588235 -> 0.6984411094283047 on epoch=87, global_step=1400
05/29/2022 00:12:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=88
05/29/2022 00:12:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=88
05/29/2022 00:12:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
05/29/2022 00:12:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=89
05/29/2022 00:12:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/29/2022 00:12:37 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.6873946981992426 on epoch=90
05/29/2022 00:12:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=91
05/29/2022 00:12:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
05/29/2022 00:12:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=92
05/29/2022 00:12:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=93
05/29/2022 00:12:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=93
05/29/2022 00:12:54 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.6470842260866676 on epoch=93
05/29/2022 00:12:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=94
05/29/2022 00:12:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
05/29/2022 00:13:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=95
05/29/2022 00:13:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/29/2022 00:13:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=96
05/29/2022 00:13:12 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.5917065390749602 on epoch=96
05/29/2022 00:13:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=97
05/29/2022 00:13:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=98
05/29/2022 00:13:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=98
05/29/2022 00:13:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=99
05/29/2022 00:13:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=99
05/29/2022 00:13:29 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.6658448150833938 on epoch=99
05/29/2022 00:13:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/29/2022 00:13:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/29/2022 00:13:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=101
05/29/2022 00:13:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=102
05/29/2022 00:13:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=103
05/29/2022 00:13:46 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.6436903499469778 on epoch=103
05/29/2022 00:13:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/29/2022 00:13:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/29/2022 00:13:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=104
05/29/2022 00:13:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/29/2022 00:13:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=106
05/29/2022 00:14:04 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.7074376081047691 on epoch=106
05/29/2022 00:14:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6984411094283047 -> 0.7074376081047691 on epoch=106, global_step=1700
05/29/2022 00:14:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=106
05/29/2022 00:14:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
05/29/2022 00:14:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=108
05/29/2022 00:14:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=108
05/29/2022 00:14:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
05/29/2022 00:14:22 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7051058530510586 on epoch=109
05/29/2022 00:14:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=109
05/29/2022 00:14:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=110
05/29/2022 00:14:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
05/29/2022 00:14:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=111
05/29/2022 00:14:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=112
05/29/2022 00:14:39 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.6993187690862108 on epoch=112
05/29/2022 00:14:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=113
05/29/2022 00:14:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
05/29/2022 00:14:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
05/29/2022 00:14:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=114
05/29/2022 00:14:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=115
05/29/2022 00:14:57 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.734879047839864 on epoch=115
05/29/2022 00:14:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7074376081047691 -> 0.734879047839864 on epoch=115, global_step=1850
05/29/2022 00:14:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=116
05/29/2022 00:15:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=116
05/29/2022 00:15:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/29/2022 00:15:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=118
05/29/2022 00:15:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
05/29/2022 00:15:14 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.678826481374252 on epoch=118
05/29/2022 00:15:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/29/2022 00:15:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
05/29/2022 00:15:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=120
05/29/2022 00:15:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=121
05/29/2022 00:15:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/29/2022 00:15:32 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.5995375093518329 on epoch=121
05/29/2022 00:15:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/29/2022 00:15:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=123
05/29/2022 00:15:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=123
05/29/2022 00:15:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=124
05/29/2022 00:15:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/29/2022 00:15:50 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.6503825586178527 on epoch=124
05/29/2022 00:15:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
05/29/2022 00:15:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/29/2022 00:15:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=126
05/29/2022 00:16:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
05/29/2022 00:16:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
05/29/2022 00:16:07 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.6652500817260543 on epoch=128
05/29/2022 00:16:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/29/2022 00:16:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=129
05/29/2022 00:16:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=129
05/29/2022 00:16:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=130
05/29/2022 00:16:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=131
05/29/2022 00:16:24 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.677602686644278 on epoch=131
05/29/2022 00:16:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
05/29/2022 00:16:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=132
05/29/2022 00:16:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/29/2022 00:16:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/29/2022 00:16:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
05/29/2022 00:16:42 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.729605467536502 on epoch=134
05/29/2022 00:16:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=134
05/29/2022 00:16:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=135
05/29/2022 00:16:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
05/29/2022 00:16:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/29/2022 00:16:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
05/29/2022 00:16:59 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.683529468428404 on epoch=137
05/29/2022 00:17:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
05/29/2022 00:17:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
05/29/2022 00:17:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/29/2022 00:17:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=139
05/29/2022 00:17:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=140
05/29/2022 00:17:16 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.6631961927656049 on epoch=140
05/29/2022 00:17:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
05/29/2022 00:17:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/29/2022 00:17:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=142
05/29/2022 00:17:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/29/2022 00:17:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
05/29/2022 00:17:34 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.668028269467678 on epoch=143
05/29/2022 00:17:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/29/2022 00:17:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=144
05/29/2022 00:17:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=145
05/29/2022 00:17:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=146
05/29/2022 00:17:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=146
05/29/2022 00:17:51 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.6282485687102536 on epoch=146
05/29/2022 00:17:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=147
05/29/2022 00:17:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=148
05/29/2022 00:17:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
05/29/2022 00:18:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/29/2022 00:18:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=149
05/29/2022 00:18:08 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.5963035184400169 on epoch=149
05/29/2022 00:18:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/29/2022 00:18:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/29/2022 00:18:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=151
05/29/2022 00:18:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/29/2022 00:18:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/29/2022 00:18:25 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.6715123361541085 on epoch=153
05/29/2022 00:18:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/29/2022 00:18:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
05/29/2022 00:18:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/29/2022 00:18:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=155
05/29/2022 00:18:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/29/2022 00:18:43 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.655371683740478 on epoch=156
05/29/2022 00:18:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/29/2022 00:18:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/29/2022 00:18:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/29/2022 00:18:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=158
05/29/2022 00:18:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
05/29/2022 00:19:00 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.7129818244982393 on epoch=159
05/29/2022 00:19:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=159
05/29/2022 00:19:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=160
05/29/2022 00:19:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/29/2022 00:19:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
05/29/2022 00:19:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=162
05/29/2022 00:19:17 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.6705912629479509 on epoch=162
05/29/2022 00:19:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=163
05/29/2022 00:19:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=163
05/29/2022 00:19:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=164
05/29/2022 00:19:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
05/29/2022 00:19:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/29/2022 00:19:34 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.668028269467678 on epoch=165
05/29/2022 00:19:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
05/29/2022 00:19:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=166
05/29/2022 00:19:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/29/2022 00:19:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=168
05/29/2022 00:19:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=168
05/29/2022 00:19:52 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.645359192558876 on epoch=168
05/29/2022 00:19:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/29/2022 00:19:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/29/2022 00:19:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=170
05/29/2022 00:20:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/29/2022 00:20:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
05/29/2022 00:20:09 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.5829260345389378 on epoch=171
05/29/2022 00:20:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
05/29/2022 00:20:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.26 on epoch=173
05/29/2022 00:20:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/29/2022 00:20:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=174
05/29/2022 00:20:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
05/29/2022 00:20:27 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.6017316017316017 on epoch=174
05/29/2022 00:20:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
05/29/2022 00:20:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
05/29/2022 00:20:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
05/29/2022 00:20:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=177
05/29/2022 00:20:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=178
05/29/2022 00:20:44 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.631837341929085 on epoch=178
05/29/2022 00:20:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=178
05/29/2022 00:20:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/29/2022 00:20:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=179
05/29/2022 00:20:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=180
05/29/2022 00:20:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/29/2022 00:21:01 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.6740955603899766 on epoch=181
05/29/2022 00:21:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.14 on epoch=181
05/29/2022 00:21:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=182
05/29/2022 00:21:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/29/2022 00:21:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/29/2022 00:21:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=184
05/29/2022 00:21:19 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.6958897600380138 on epoch=184
05/29/2022 00:21:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
05/29/2022 00:21:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=185
05/29/2022 00:21:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
05/29/2022 00:21:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/29/2022 00:21:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
05/29/2022 00:21:32 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:21:32 - INFO - __main__ - Printing 3 examples
05/29/2022 00:21:32 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 00:21:32 - INFO - __main__ - ['false']
05/29/2022 00:21:32 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 00:21:32 - INFO - __main__ - ['false']
05/29/2022 00:21:32 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 00:21:32 - INFO - __main__ - ['false']
05/29/2022 00:21:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:21:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:21:33 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:21:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:21:33 - INFO - __main__ - Printing 3 examples
05/29/2022 00:21:33 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 00:21:33 - INFO - __main__ - ['false']
05/29/2022 00:21:33 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 00:21:33 - INFO - __main__ - ['false']
05/29/2022 00:21:33 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 00:21:33 - INFO - __main__ - ['false']
05/29/2022 00:21:33 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:21:33 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:21:33 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:21:36 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.6728302192024589 on epoch=187
05/29/2022 00:21:36 - INFO - __main__ - save last model!
05/29/2022 00:21:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 00:21:36 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 00:21:36 - INFO - __main__ - Printing 3 examples
05/29/2022 00:21:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 00:21:36 - INFO - __main__ - ['false']
05/29/2022 00:21:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 00:21:36 - INFO - __main__ - ['false']
05/29/2022 00:21:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 00:21:36 - INFO - __main__ - ['false']
05/29/2022 00:21:36 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:21:37 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:21:40 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 00:21:48 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:21:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:21:49 - INFO - __main__ - Starting training!
05/29/2022 00:22:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.2_8_predictions.txt
05/29/2022 00:22:29 - INFO - __main__ - Classification-F1 on test data: 0.4310
05/29/2022 00:22:30 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.2, bsz=8, dev_performance=0.734879047839864, test_performance=0.4310092856676319
05/29/2022 00:22:30 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.5, bsz=8 ...
05/29/2022 00:22:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:22:31 - INFO - __main__ - Printing 3 examples
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:22:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:22:31 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:22:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:22:31 - INFO - __main__ - Printing 3 examples
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 00:22:31 - INFO - __main__ - ['false']
05/29/2022 00:22:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:22:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:22:31 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:22:50 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:22:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:22:51 - INFO - __main__ - Starting training!
05/29/2022 00:22:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.80 on epoch=0
05/29/2022 00:22:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=1
05/29/2022 00:23:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=1
05/29/2022 00:23:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=2
05/29/2022 00:23:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=3
05/29/2022 00:23:11 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.42025542025542023 on epoch=3
05/29/2022 00:23:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.42025542025542023 on epoch=3, global_step=50
05/29/2022 00:23:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=3
05/29/2022 00:23:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
05/29/2022 00:23:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
05/29/2022 00:23:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
05/29/2022 00:23:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
05/29/2022 00:23:30 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.475 on epoch=6
05/29/2022 00:23:31 - INFO - __main__ - Saving model with best Classification-F1: 0.42025542025542023 -> 0.475 on epoch=6, global_step=100
05/29/2022 00:23:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
05/29/2022 00:23:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
05/29/2022 00:23:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=8
05/29/2022 00:23:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=8
05/29/2022 00:23:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
05/29/2022 00:23:50 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.39267460026616785 on epoch=9
05/29/2022 00:23:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/29/2022 00:23:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
05/29/2022 00:23:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
05/29/2022 00:24:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
05/29/2022 00:24:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=12
05/29/2022 00:24:09 - INFO - __main__ - Global step 200 Train loss 0.38 Classification-F1 0.5068455452356381 on epoch=12
05/29/2022 00:24:09 - INFO - __main__ - Saving model with best Classification-F1: 0.475 -> 0.5068455452356381 on epoch=12, global_step=200
05/29/2022 00:24:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=13
05/29/2022 00:24:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=13
05/29/2022 00:24:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=14
05/29/2022 00:24:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=14
05/29/2022 00:24:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=15
05/29/2022 00:24:29 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.615961596159616 on epoch=15
05/29/2022 00:24:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5068455452356381 -> 0.615961596159616 on epoch=15, global_step=250
05/29/2022 00:24:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
05/29/2022 00:24:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=16
05/29/2022 00:24:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/29/2022 00:24:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
05/29/2022 00:24:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
05/29/2022 00:24:48 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6258864429984463 on epoch=18
05/29/2022 00:24:48 - INFO - __main__ - Saving model with best Classification-F1: 0.615961596159616 -> 0.6258864429984463 on epoch=18, global_step=300
05/29/2022 00:24:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=19
05/29/2022 00:24:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=19
05/29/2022 00:24:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=20
05/29/2022 00:24:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
05/29/2022 00:25:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
05/29/2022 00:25:06 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6484160410181287 on epoch=21
05/29/2022 00:25:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6258864429984463 -> 0.6484160410181287 on epoch=21, global_step=350
05/29/2022 00:25:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=22
05/29/2022 00:25:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/29/2022 00:25:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/29/2022 00:25:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=24
05/29/2022 00:25:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/29/2022 00:25:25 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.6652668174196625 on epoch=24
05/29/2022 00:25:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6484160410181287 -> 0.6652668174196625 on epoch=24, global_step=400
05/29/2022 00:25:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=25
05/29/2022 00:25:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=26
05/29/2022 00:25:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=26
05/29/2022 00:25:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=27
05/29/2022 00:25:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=28
05/29/2022 00:25:44 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.6615630572465104 on epoch=28
05/29/2022 00:25:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
05/29/2022 00:25:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=29
05/29/2022 00:25:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=29
05/29/2022 00:25:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
05/29/2022 00:25:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=31
05/29/2022 00:26:01 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.67089072543618 on epoch=31
05/29/2022 00:26:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6652668174196625 -> 0.67089072543618 on epoch=31, global_step=500
05/29/2022 00:26:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=31
05/29/2022 00:26:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=32
05/29/2022 00:26:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=33
05/29/2022 00:26:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=33
05/29/2022 00:26:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=34
05/29/2022 00:26:19 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.6984126984126984 on epoch=34
05/29/2022 00:26:19 - INFO - __main__ - Saving model with best Classification-F1: 0.67089072543618 -> 0.6984126984126984 on epoch=34, global_step=550
05/29/2022 00:26:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=34
05/29/2022 00:26:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=35
05/29/2022 00:26:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=36
05/29/2022 00:26:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/29/2022 00:26:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=37
05/29/2022 00:26:37 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.6884349339865354 on epoch=37
05/29/2022 00:26:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=38
05/29/2022 00:26:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=38
05/29/2022 00:26:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
05/29/2022 00:26:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=39
05/29/2022 00:26:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=40
05/29/2022 00:26:55 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.67185497161692 on epoch=40
05/29/2022 00:26:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=41
05/29/2022 00:27:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=41
05/29/2022 00:27:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=42
05/29/2022 00:27:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=43
05/29/2022 00:27:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=43
05/29/2022 00:27:13 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.6464691917389482 on epoch=43
05/29/2022 00:27:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=44
05/29/2022 00:27:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=44
05/29/2022 00:27:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/29/2022 00:27:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=46
05/29/2022 00:27:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=46
05/29/2022 00:27:31 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.6868118424272083 on epoch=46
05/29/2022 00:27:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=47
05/29/2022 00:27:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=48
05/29/2022 00:27:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
05/29/2022 00:27:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/29/2022 00:27:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=49
05/29/2022 00:27:48 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.6950146627565982 on epoch=49
05/29/2022 00:27:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=50
05/29/2022 00:27:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=51
05/29/2022 00:27:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
05/29/2022 00:27:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/29/2022 00:28:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/29/2022 00:28:06 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.6377358490566036 on epoch=53
05/29/2022 00:28:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=53
05/29/2022 00:28:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=54
05/29/2022 00:28:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=54
05/29/2022 00:28:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=55
05/29/2022 00:28:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
05/29/2022 00:28:24 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.683588921950103 on epoch=56
05/29/2022 00:28:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=56
05/29/2022 00:28:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
05/29/2022 00:28:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
05/29/2022 00:28:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=58
05/29/2022 00:28:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=59
05/29/2022 00:28:41 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.6952380952380953 on epoch=59
05/29/2022 00:28:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=59
05/29/2022 00:28:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=60
05/29/2022 00:28:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=61
05/29/2022 00:28:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=61
05/29/2022 00:28:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
05/29/2022 00:28:59 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.6832021999847222 on epoch=62
05/29/2022 00:29:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/29/2022 00:29:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=63
05/29/2022 00:29:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=64
05/29/2022 00:29:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
05/29/2022 00:29:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/29/2022 00:29:16 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.6911753477789484 on epoch=65
05/29/2022 00:29:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=66
05/29/2022 00:29:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=66
05/29/2022 00:29:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/29/2022 00:29:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/29/2022 00:29:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/29/2022 00:29:33 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.6280030999741669 on epoch=68
05/29/2022 00:29:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/29/2022 00:29:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/29/2022 00:29:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/29/2022 00:29:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=71
05/29/2022 00:29:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=71
05/29/2022 00:29:51 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.6818411967779057 on epoch=71
05/29/2022 00:29:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=72
05/29/2022 00:29:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=73
05/29/2022 00:29:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=73
05/29/2022 00:30:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=74
05/29/2022 00:30:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=74
05/29/2022 00:30:08 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.6834730045335897 on epoch=74
05/29/2022 00:30:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/29/2022 00:30:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/29/2022 00:30:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/29/2022 00:30:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
05/29/2022 00:30:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=78
05/29/2022 00:30:26 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.6344945745288406 on epoch=78
05/29/2022 00:30:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/29/2022 00:30:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
05/29/2022 00:30:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=79
05/29/2022 00:30:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=80
05/29/2022 00:30:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=81
05/29/2022 00:30:43 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.6859473716493897 on epoch=81
05/29/2022 00:30:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/29/2022 00:30:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/29/2022 00:30:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=83
05/29/2022 00:30:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=83
05/29/2022 00:30:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=84
05/29/2022 00:31:01 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.6657193332821261 on epoch=84
05/29/2022 00:31:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=84
05/29/2022 00:31:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=85
05/29/2022 00:31:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/29/2022 00:31:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/29/2022 00:31:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/29/2022 00:31:18 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.6835502922459444 on epoch=87
05/29/2022 00:31:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=88
05/29/2022 00:31:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/29/2022 00:31:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=89
05/29/2022 00:31:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=89
05/29/2022 00:31:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=90
05/29/2022 00:31:35 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.6912884859031307 on epoch=90
05/29/2022 00:31:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/29/2022 00:31:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/29/2022 00:31:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=92
05/29/2022 00:31:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=93
05/29/2022 00:31:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/29/2022 00:31:52 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.6634513344420685 on epoch=93
05/29/2022 00:31:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
05/29/2022 00:31:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/29/2022 00:32:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=95
05/29/2022 00:32:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/29/2022 00:32:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=96
05/29/2022 00:32:09 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.6947425474254743 on epoch=96
05/29/2022 00:32:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=97
05/29/2022 00:32:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/29/2022 00:32:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=98
05/29/2022 00:32:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
05/29/2022 00:32:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=99
05/29/2022 00:32:26 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.6788850174216028 on epoch=99
05/29/2022 00:32:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/29/2022 00:32:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/29/2022 00:32:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
05/29/2022 00:32:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=102
05/29/2022 00:32:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=103
05/29/2022 00:32:43 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.6183923861942437 on epoch=103
05/29/2022 00:32:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
05/29/2022 00:32:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=104
05/29/2022 00:32:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=104
05/29/2022 00:32:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=105
05/29/2022 00:32:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/29/2022 00:33:00 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.656493790176621 on epoch=106
05/29/2022 00:33:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=106
05/29/2022 00:33:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=107
05/29/2022 00:33:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=108
05/29/2022 00:33:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/29/2022 00:33:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=109
05/29/2022 00:33:17 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.6780960559406245 on epoch=109
05/29/2022 00:33:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=109
05/29/2022 00:33:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=110
05/29/2022 00:33:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=111
05/29/2022 00:33:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=111
05/29/2022 00:33:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=112
05/29/2022 00:33:34 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.6833570021530991 on epoch=112
05/29/2022 00:33:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
05/29/2022 00:33:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=113
05/29/2022 00:33:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/29/2022 00:33:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=114
05/29/2022 00:33:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=115
05/29/2022 00:33:51 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.7050464705430525 on epoch=115
05/29/2022 00:33:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6984126984126984 -> 0.7050464705430525 on epoch=115, global_step=1850
05/29/2022 00:33:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=116
05/29/2022 00:33:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=116
05/29/2022 00:33:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/29/2022 00:34:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=118
05/29/2022 00:34:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
05/29/2022 00:34:08 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.6458856076690471 on epoch=118
05/29/2022 00:34:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=119
05/29/2022 00:34:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/29/2022 00:34:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
05/29/2022 00:34:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=121
05/29/2022 00:34:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=121
05/29/2022 00:34:26 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.7032228147461164 on epoch=121
05/29/2022 00:34:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/29/2022 00:34:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/29/2022 00:34:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
05/29/2022 00:34:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=124
05/29/2022 00:34:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/29/2022 00:34:43 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.6842429848905335 on epoch=124
05/29/2022 00:34:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/29/2022 00:34:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=126
05/29/2022 00:34:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=126
05/29/2022 00:34:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=127
05/29/2022 00:34:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=128
05/29/2022 00:35:01 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.6694246496723898 on epoch=128
05/29/2022 00:35:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=128
05/29/2022 00:35:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=129
05/29/2022 00:35:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=129
05/29/2022 00:35:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=130
05/29/2022 00:35:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=131
05/29/2022 00:35:18 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.6916234247590808 on epoch=131
05/29/2022 00:35:21 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=131
05/29/2022 00:35:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/29/2022 00:35:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=133
05/29/2022 00:35:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=133
05/29/2022 00:35:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/29/2022 00:35:36 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.6796092796092795 on epoch=134
05/29/2022 00:35:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=134
05/29/2022 00:35:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/29/2022 00:35:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=136
05/29/2022 00:35:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=136
05/29/2022 00:35:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=137
05/29/2022 00:35:54 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.6466907931055633 on epoch=137
05/29/2022 00:35:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=138
05/29/2022 00:35:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=138
05/29/2022 00:36:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/29/2022 00:36:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/29/2022 00:36:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/29/2022 00:36:11 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.6746642985102049 on epoch=140
05/29/2022 00:36:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
05/29/2022 00:36:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/29/2022 00:36:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=142
05/29/2022 00:36:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=143
05/29/2022 00:36:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=143
05/29/2022 00:36:28 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.678247241919717 on epoch=143
05/29/2022 00:36:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=144
05/29/2022 00:36:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=144
05/29/2022 00:36:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
05/29/2022 00:36:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=146
05/29/2022 00:36:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=146
05/29/2022 00:36:45 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.6842429848905335 on epoch=146
05/29/2022 00:36:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=147
05/29/2022 00:36:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=148
05/29/2022 00:36:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=148
05/29/2022 00:36:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/29/2022 00:36:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=149
05/29/2022 00:37:03 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.6827757125154895 on epoch=149
05/29/2022 00:37:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/29/2022 00:37:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=151
05/29/2022 00:37:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=151
05/29/2022 00:37:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/29/2022 00:37:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
05/29/2022 00:37:20 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.6528539892778304 on epoch=153
05/29/2022 00:37:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/29/2022 00:37:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
05/29/2022 00:37:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=154
05/29/2022 00:37:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/29/2022 00:37:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
05/29/2022 00:37:37 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.6659832246039142 on epoch=156
05/29/2022 00:37:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/29/2022 00:37:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
05/29/2022 00:37:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/29/2022 00:37:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=158
05/29/2022 00:37:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
05/29/2022 00:37:55 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.6835502922459444 on epoch=159
05/29/2022 00:37:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/29/2022 00:38:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=160
05/29/2022 00:38:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
05/29/2022 00:38:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/29/2022 00:38:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/29/2022 00:38:12 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.6705882352941177 on epoch=162
05/29/2022 00:38:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
05/29/2022 00:38:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/29/2022 00:38:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=164
05/29/2022 00:38:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/29/2022 00:38:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/29/2022 00:38:30 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.6855809383443872 on epoch=165
05/29/2022 00:38:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/29/2022 00:38:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
05/29/2022 00:38:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/29/2022 00:38:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/29/2022 00:38:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/29/2022 00:38:47 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.6583290153406504 on epoch=168
05/29/2022 00:38:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/29/2022 00:38:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=169
05/29/2022 00:38:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/29/2022 00:38:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=171
05/29/2022 00:39:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
05/29/2022 00:39:04 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.6457861635220126 on epoch=171
05/29/2022 00:39:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/29/2022 00:39:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/29/2022 00:39:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=173
05/29/2022 00:39:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=174
05/29/2022 00:39:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
05/29/2022 00:39:22 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.6441808747220164 on epoch=174
05/29/2022 00:39:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/29/2022 00:39:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/29/2022 00:39:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=176
05/29/2022 00:39:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/29/2022 00:39:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/29/2022 00:39:39 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.6240466151878346 on epoch=178
05/29/2022 00:39:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/29/2022 00:39:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/29/2022 00:39:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/29/2022 00:39:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/29/2022 00:39:53 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=181
05/29/2022 00:39:57 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.6391225152451097 on epoch=181
05/29/2022 00:40:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=181
05/29/2022 00:40:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/29/2022 00:40:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/29/2022 00:40:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/29/2022 00:40:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/29/2022 00:40:16 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.6517006802721088 on epoch=184
05/29/2022 00:40:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
05/29/2022 00:40:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
05/29/2022 00:40:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=186
05/29/2022 00:40:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/29/2022 00:40:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/29/2022 00:40:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:40:31 - INFO - __main__ - Printing 3 examples
05/29/2022 00:40:31 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 00:40:31 - INFO - __main__ - ['false']
05/29/2022 00:40:31 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 00:40:31 - INFO - __main__ - ['false']
05/29/2022 00:40:31 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 00:40:31 - INFO - __main__ - ['false']
05/29/2022 00:40:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:40:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:40:32 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:40:32 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:40:32 - INFO - __main__ - Printing 3 examples
05/29/2022 00:40:32 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 00:40:32 - INFO - __main__ - ['false']
05/29/2022 00:40:32 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 00:40:32 - INFO - __main__ - ['false']
05/29/2022 00:40:32 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 00:40:32 - INFO - __main__ - ['false']
05/29/2022 00:40:32 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:40:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:40:32 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:40:34 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.6593848629246859 on epoch=187
05/29/2022 00:40:34 - INFO - __main__ - save last model!
05/29/2022 00:40:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 00:40:35 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 00:40:35 - INFO - __main__ - Printing 3 examples
05/29/2022 00:40:35 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 00:40:35 - INFO - __main__ - ['false']
05/29/2022 00:40:35 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 00:40:35 - INFO - __main__ - ['false']
05/29/2022 00:40:35 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 00:40:35 - INFO - __main__ - ['false']
05/29/2022 00:40:35 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:40:36 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:40:38 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 00:40:51 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:40:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:40:52 - INFO - __main__ - Starting training!
05/29/2022 00:41:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.5_8_predictions.txt
05/29/2022 00:41:29 - INFO - __main__ - Classification-F1 on test data: 0.4370
05/29/2022 00:41:29 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.5, bsz=8, dev_performance=0.7050464705430525, test_performance=0.4370089451658085
05/29/2022 00:41:29 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.4, bsz=8 ...
05/29/2022 00:41:30 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:41:30 - INFO - __main__ - Printing 3 examples
05/29/2022 00:41:30 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 00:41:30 - INFO - __main__ - ['false']
05/29/2022 00:41:30 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 00:41:30 - INFO - __main__ - ['false']
05/29/2022 00:41:30 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 00:41:30 - INFO - __main__ - ['false']
05/29/2022 00:41:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:41:30 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:41:31 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:41:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:41:31 - INFO - __main__ - Printing 3 examples
05/29/2022 00:41:31 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 00:41:31 - INFO - __main__ - ['false']
05/29/2022 00:41:31 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 00:41:31 - INFO - __main__ - ['false']
05/29/2022 00:41:31 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 00:41:31 - INFO - __main__ - ['false']
05/29/2022 00:41:31 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:41:31 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:41:31 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:41:50 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:41:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:41:51 - INFO - __main__ - Starting training!
05/29/2022 00:41:54 - INFO - __main__ - Step 10 Global step 10 Train loss 2.26 on epoch=0
05/29/2022 00:41:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=1
05/29/2022 00:41:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=1
05/29/2022 00:42:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=2
05/29/2022 00:42:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
05/29/2022 00:42:11 - INFO - __main__ - Global step 50 Train loss 0.82 Classification-F1 0.4035699216671864 on epoch=3
05/29/2022 00:42:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4035699216671864 on epoch=3, global_step=50
05/29/2022 00:42:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=3
05/29/2022 00:42:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
05/29/2022 00:42:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
05/29/2022 00:42:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
05/29/2022 00:42:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
05/29/2022 00:42:30 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.36318407960199 on epoch=6
05/29/2022 00:42:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=6
05/29/2022 00:42:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=7
05/29/2022 00:42:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=8
05/29/2022 00:42:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=8
05/29/2022 00:42:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/29/2022 00:42:49 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=9
05/29/2022 00:42:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
05/29/2022 00:42:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.35 on epoch=10
05/29/2022 00:42:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=11
05/29/2022 00:42:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/29/2022 00:43:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
05/29/2022 00:43:08 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.5460992907801419 on epoch=12
05/29/2022 00:43:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4035699216671864 -> 0.5460992907801419 on epoch=12, global_step=200
05/29/2022 00:43:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=13
05/29/2022 00:43:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
05/29/2022 00:43:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=14
05/29/2022 00:43:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/29/2022 00:43:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
05/29/2022 00:43:26 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.5952380952380953 on epoch=15
05/29/2022 00:43:26 - INFO - __main__ - Saving model with best Classification-F1: 0.5460992907801419 -> 0.5952380952380953 on epoch=15, global_step=250
05/29/2022 00:43:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
05/29/2022 00:43:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=16
05/29/2022 00:43:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/29/2022 00:43:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/29/2022 00:43:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
05/29/2022 00:43:45 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.6131976564909035 on epoch=18
05/29/2022 00:43:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5952380952380953 -> 0.6131976564909035 on epoch=18, global_step=300
05/29/2022 00:43:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=19
05/29/2022 00:43:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/29/2022 00:43:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
05/29/2022 00:43:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
05/29/2022 00:43:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=21
05/29/2022 00:44:04 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.652296000122087 on epoch=21
05/29/2022 00:44:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6131976564909035 -> 0.652296000122087 on epoch=21, global_step=350
05/29/2022 00:44:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
05/29/2022 00:44:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=23
05/29/2022 00:44:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
05/29/2022 00:44:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/29/2022 00:44:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/29/2022 00:44:21 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.6197025587955468 on epoch=24
05/29/2022 00:44:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/29/2022 00:44:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=26
05/29/2022 00:44:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
05/29/2022 00:44:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=27
05/29/2022 00:44:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=28
05/29/2022 00:44:39 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.6874809253494476 on epoch=28
05/29/2022 00:44:39 - INFO - __main__ - Saving model with best Classification-F1: 0.652296000122087 -> 0.6874809253494476 on epoch=28, global_step=450
05/29/2022 00:44:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=28
05/29/2022 00:44:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=29
05/29/2022 00:44:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
05/29/2022 00:44:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=30
05/29/2022 00:44:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=31
05/29/2022 00:44:57 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.6760759197063189 on epoch=31
05/29/2022 00:44:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
05/29/2022 00:45:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=32
05/29/2022 00:45:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/29/2022 00:45:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/29/2022 00:45:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/29/2022 00:45:14 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.6862745098039216 on epoch=34
05/29/2022 00:45:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=34
05/29/2022 00:45:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
05/29/2022 00:45:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=36
05/29/2022 00:45:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/29/2022 00:45:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=37
05/29/2022 00:45:32 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.670956252419667 on epoch=37
05/29/2022 00:45:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=38
05/29/2022 00:45:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/29/2022 00:45:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=39
05/29/2022 00:45:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=39
05/29/2022 00:45:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/29/2022 00:45:50 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.6793743890518085 on epoch=40
05/29/2022 00:45:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/29/2022 00:45:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=41
05/29/2022 00:45:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=42
05/29/2022 00:46:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=43
05/29/2022 00:46:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=43
05/29/2022 00:46:07 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6536828847320055 on epoch=43
05/29/2022 00:46:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=44
05/29/2022 00:46:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=44
05/29/2022 00:46:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=45
05/29/2022 00:46:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=46
05/29/2022 00:46:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=46
05/29/2022 00:46:25 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.6787266605448423 on epoch=46
05/29/2022 00:46:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=47
05/29/2022 00:46:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=48
05/29/2022 00:46:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=48
05/29/2022 00:46:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/29/2022 00:46:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/29/2022 00:46:42 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.6796679484831838 on epoch=49
05/29/2022 00:46:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=50
05/29/2022 00:46:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=51
05/29/2022 00:46:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=51
05/29/2022 00:46:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=52
05/29/2022 00:46:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=53
05/29/2022 00:46:59 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.6447808262377799 on epoch=53
05/29/2022 00:47:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
05/29/2022 00:47:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=54
05/29/2022 00:47:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=54
05/29/2022 00:47:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=55
05/29/2022 00:47:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
05/29/2022 00:47:16 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.6711524345485687 on epoch=56
05/29/2022 00:47:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=56
05/29/2022 00:47:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=57
05/29/2022 00:47:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
05/29/2022 00:47:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=58
05/29/2022 00:47:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=59
05/29/2022 00:47:33 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.6698599852616065 on epoch=59
05/29/2022 00:47:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/29/2022 00:47:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/29/2022 00:47:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=61
05/29/2022 00:47:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=61
05/29/2022 00:47:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=62
05/29/2022 00:47:51 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.7016500030669202 on epoch=62
05/29/2022 00:47:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6874809253494476 -> 0.7016500030669202 on epoch=62, global_step=1000
05/29/2022 00:47:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=63
05/29/2022 00:47:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=63
05/29/2022 00:47:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
05/29/2022 00:48:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=64
05/29/2022 00:48:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
05/29/2022 00:48:08 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.6794806399258057 on epoch=65
05/29/2022 00:48:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=66
05/29/2022 00:48:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=66
05/29/2022 00:48:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=67
05/29/2022 00:48:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/29/2022 00:48:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/29/2022 00:48:25 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.6556556556556556 on epoch=68
05/29/2022 00:48:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=69
05/29/2022 00:48:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
05/29/2022 00:48:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/29/2022 00:48:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=71
05/29/2022 00:48:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
05/29/2022 00:48:42 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.6580304442373408 on epoch=71
05/29/2022 00:48:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=72
05/29/2022 00:48:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=73
05/29/2022 00:48:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=73
05/29/2022 00:48:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/29/2022 00:48:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=74
05/29/2022 00:49:00 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.6773043103978356 on epoch=74
05/29/2022 00:49:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
05/29/2022 00:49:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/29/2022 00:49:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=76
05/29/2022 00:49:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
05/29/2022 00:49:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=78
05/29/2022 00:49:17 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.6231335436382754 on epoch=78
05/29/2022 00:49:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
05/29/2022 00:49:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=79
05/29/2022 00:49:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=79
05/29/2022 00:49:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
05/29/2022 00:49:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
05/29/2022 00:49:34 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.6679021497405486 on epoch=81
05/29/2022 00:49:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=81
05/29/2022 00:49:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/29/2022 00:49:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/29/2022 00:49:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/29/2022 00:49:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=84
05/29/2022 00:49:51 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.6756575231640487 on epoch=84
05/29/2022 00:49:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=84
05/29/2022 00:49:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/29/2022 00:49:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=86
05/29/2022 00:50:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=86
05/29/2022 00:50:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/29/2022 00:50:09 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.6739854238588416 on epoch=87
05/29/2022 00:50:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=88
05/29/2022 00:50:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=88
05/29/2022 00:50:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=89
05/29/2022 00:50:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=89
05/29/2022 00:50:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=90
05/29/2022 00:50:26 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.6367076631977294 on epoch=90
05/29/2022 00:50:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=91
05/29/2022 00:50:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=91
05/29/2022 00:50:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
05/29/2022 00:50:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=93
05/29/2022 00:50:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/29/2022 00:50:43 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.6182325098878299 on epoch=93
05/29/2022 00:50:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/29/2022 00:50:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=94
05/29/2022 00:50:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=95
05/29/2022 00:50:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=96
05/29/2022 00:50:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/29/2022 00:51:01 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.6536828847320055 on epoch=96
05/29/2022 00:51:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/29/2022 00:51:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/29/2022 00:51:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/29/2022 00:51:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/29/2022 00:51:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/29/2022 00:51:18 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.6331973988757853 on epoch=99
05/29/2022 00:51:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=100
05/29/2022 00:51:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=101
05/29/2022 00:51:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=101
05/29/2022 00:51:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=102
05/29/2022 00:51:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/29/2022 00:51:36 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.6148507643775782 on epoch=103
05/29/2022 00:51:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/29/2022 00:51:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/29/2022 00:51:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/29/2022 00:51:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=105
05/29/2022 00:51:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
05/29/2022 00:51:54 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.6379756892031461 on epoch=106
05/29/2022 00:51:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/29/2022 00:51:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/29/2022 00:52:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=108
05/29/2022 00:52:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
05/29/2022 00:52:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/29/2022 00:52:11 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.6478244068320582 on epoch=109
05/29/2022 00:52:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=109
05/29/2022 00:52:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/29/2022 00:52:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/29/2022 00:52:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/29/2022 00:52:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=112
05/29/2022 00:52:29 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.6637341153470186 on epoch=112
05/29/2022 00:52:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/29/2022 00:52:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
05/29/2022 00:52:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=114
05/29/2022 00:52:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=114
05/29/2022 00:52:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=115
05/29/2022 00:52:46 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.6479002384008803 on epoch=115
05/29/2022 00:52:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/29/2022 00:52:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
05/29/2022 00:52:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=117
05/29/2022 00:52:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/29/2022 00:52:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
05/29/2022 00:53:03 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.6530999410142363 on epoch=118
05/29/2022 00:53:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=119
05/29/2022 00:53:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=119
05/29/2022 00:53:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=120
05/29/2022 00:53:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/29/2022 00:53:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=121
05/29/2022 00:53:21 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.6541208260953484 on epoch=121
05/29/2022 00:53:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/29/2022 00:53:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
05/29/2022 00:53:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/29/2022 00:53:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=124
05/29/2022 00:53:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
05/29/2022 00:53:38 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.641258994504889 on epoch=124
05/29/2022 00:53:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=125
05/29/2022 00:53:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
05/29/2022 00:53:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=126
05/29/2022 00:53:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=127
05/29/2022 00:53:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/29/2022 00:53:56 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.6367752323560827 on epoch=128
05/29/2022 00:53:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
05/29/2022 00:54:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=129
05/29/2022 00:54:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=129
05/29/2022 00:54:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=130
05/29/2022 00:54:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/29/2022 00:54:13 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.6512097721400048 on epoch=131
05/29/2022 00:54:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=131
05/29/2022 00:54:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=132
05/29/2022 00:54:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=133
05/29/2022 00:54:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
05/29/2022 00:54:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
05/29/2022 00:54:31 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.6780960559406245 on epoch=134
05/29/2022 00:54:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/29/2022 00:54:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
05/29/2022 00:54:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/29/2022 00:54:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/29/2022 00:54:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/29/2022 00:54:48 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.6398336187912894 on epoch=137
05/29/2022 00:54:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
05/29/2022 00:54:53 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
05/29/2022 00:54:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
05/29/2022 00:54:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
05/29/2022 00:55:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=140
05/29/2022 00:55:06 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.6482442748091604 on epoch=140
05/29/2022 00:55:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=141
05/29/2022 00:55:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
05/29/2022 00:55:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
05/29/2022 00:55:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
05/29/2022 00:55:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/29/2022 00:55:23 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.6342093855860773 on epoch=143
05/29/2022 00:55:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=144
05/29/2022 00:55:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=144
05/29/2022 00:55:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/29/2022 00:55:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/29/2022 00:55:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
05/29/2022 00:55:40 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.6431916059823037 on epoch=146
05/29/2022 00:55:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=147
05/29/2022 00:55:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
05/29/2022 00:55:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=148
05/29/2022 00:55:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
05/29/2022 00:55:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=149
05/29/2022 00:55:58 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.63068431286458 on epoch=149
05/29/2022 00:56:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/29/2022 00:56:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
05/29/2022 00:56:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/29/2022 00:56:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
05/29/2022 00:56:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/29/2022 00:56:15 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.6129447339638423 on epoch=153
05/29/2022 00:56:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=153
05/29/2022 00:56:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=154
05/29/2022 00:56:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=154
05/29/2022 00:56:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=155
05/29/2022 00:56:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
05/29/2022 00:56:32 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.6262193227710469 on epoch=156
05/29/2022 00:56:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/29/2022 00:56:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/29/2022 00:56:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
05/29/2022 00:56:43 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=158
05/29/2022 00:56:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=159
05/29/2022 00:56:50 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.6337625178826896 on epoch=159
05/29/2022 00:56:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
05/29/2022 00:56:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/29/2022 00:56:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/29/2022 00:57:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/29/2022 00:57:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
05/29/2022 00:57:07 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.632251360107586 on epoch=162
05/29/2022 00:57:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
05/29/2022 00:57:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/29/2022 00:57:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/29/2022 00:57:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/29/2022 00:57:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/29/2022 00:57:24 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.6258864429984463 on epoch=165
05/29/2022 00:57:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/29/2022 00:57:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=166
05/29/2022 00:57:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/29/2022 00:57:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
05/29/2022 00:57:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=168
05/29/2022 00:57:41 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.6377602587426723 on epoch=168
05/29/2022 00:57:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/29/2022 00:57:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
05/29/2022 00:57:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/29/2022 00:57:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/29/2022 00:57:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
05/29/2022 00:57:59 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.6355201484538722 on epoch=171
05/29/2022 00:58:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/29/2022 00:58:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=173
05/29/2022 00:58:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
05/29/2022 00:58:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
05/29/2022 00:58:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=174
05/29/2022 00:58:16 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.6194154520514353 on epoch=174
05/29/2022 00:58:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=175
05/29/2022 00:58:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=176
05/29/2022 00:58:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/29/2022 00:58:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/29/2022 00:58:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
05/29/2022 00:58:34 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6690003153579313 on epoch=178
05/29/2022 00:58:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/29/2022 00:58:39 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=179
05/29/2022 00:58:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/29/2022 00:58:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
05/29/2022 00:58:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
05/29/2022 00:58:51 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.6431916059823037 on epoch=181
05/29/2022 00:58:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=181
05/29/2022 00:58:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
05/29/2022 00:58:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=183
05/29/2022 00:59:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/29/2022 00:59:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=184
05/29/2022 00:59:08 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.6087780426676447 on epoch=184
05/29/2022 00:59:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/29/2022 00:59:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=185
05/29/2022 00:59:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=186
05/29/2022 00:59:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/29/2022 00:59:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=187
05/29/2022 00:59:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:59:23 - INFO - __main__ - Printing 3 examples
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:59:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:59:23 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 00:59:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 00:59:23 - INFO - __main__ - Printing 3 examples
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 00:59:23 - INFO - __main__ - ['false']
05/29/2022 00:59:23 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:59:23 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:59:23 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 00:59:25 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.6279468233206358 on epoch=187
05/29/2022 00:59:25 - INFO - __main__ - save last model!
05/29/2022 00:59:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 00:59:25 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 00:59:25 - INFO - __main__ - Printing 3 examples
05/29/2022 00:59:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 00:59:25 - INFO - __main__ - ['false']
05/29/2022 00:59:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 00:59:25 - INFO - __main__ - ['false']
05/29/2022 00:59:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 00:59:25 - INFO - __main__ - ['false']
05/29/2022 00:59:25 - INFO - __main__ - Tokenizing Input ...
05/29/2022 00:59:26 - INFO - __main__ - Tokenizing Output ...
05/29/2022 00:59:29 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 00:59:39 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 00:59:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 00:59:40 - INFO - __main__ - Starting training!
05/29/2022 01:00:12 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.4_8_predictions.txt
05/29/2022 01:00:12 - INFO - __main__ - Classification-F1 on test data: 0.5047
05/29/2022 01:00:12 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.4, bsz=8, dev_performance=0.7016500030669202, test_performance=0.5046715490457169
05/29/2022 01:00:12 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.3, bsz=8 ...
05/29/2022 01:00:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:00:13 - INFO - __main__ - Printing 3 examples
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:00:13 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:00:13 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 01:00:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:00:13 - INFO - __main__ - Printing 3 examples
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 01:00:13 - INFO - __main__ - ['false']
05/29/2022 01:00:13 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:00:14 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:00:14 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 01:00:30 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:00:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:00:30 - INFO - __main__ - Starting training!
05/29/2022 01:00:34 - INFO - __main__ - Step 10 Global step 10 Train loss 2.57 on epoch=0
05/29/2022 01:00:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=1
05/29/2022 01:00:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
05/29/2022 01:00:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=2
05/29/2022 01:00:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
05/29/2022 01:00:50 - INFO - __main__ - Global step 50 Train loss 0.91 Classification-F1 0.3401530406766009 on epoch=3
05/29/2022 01:00:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3401530406766009 on epoch=3, global_step=50
05/29/2022 01:00:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=3
05/29/2022 01:00:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=4
05/29/2022 01:00:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=4
05/29/2022 01:01:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=5
05/29/2022 01:01:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
05/29/2022 01:01:09 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.3838573350768473 on epoch=6
05/29/2022 01:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3401530406766009 -> 0.3838573350768473 on epoch=6, global_step=100
05/29/2022 01:01:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
05/29/2022 01:01:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=7
05/29/2022 01:01:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
05/29/2022 01:01:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
05/29/2022 01:01:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
05/29/2022 01:01:28 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
05/29/2022 01:01:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
05/29/2022 01:01:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=10
05/29/2022 01:01:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
05/29/2022 01:01:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
05/29/2022 01:01:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
05/29/2022 01:01:46 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=12
05/29/2022 01:01:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=13
05/29/2022 01:01:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
05/29/2022 01:01:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/29/2022 01:01:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
05/29/2022 01:01:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
05/29/2022 01:02:05 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.35693779904306216 on epoch=15
05/29/2022 01:02:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
05/29/2022 01:02:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
05/29/2022 01:02:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
05/29/2022 01:02:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
05/29/2022 01:02:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
05/29/2022 01:02:24 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.570119086133321 on epoch=18
05/29/2022 01:02:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3838573350768473 -> 0.570119086133321 on epoch=18, global_step=300
05/29/2022 01:02:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=19
05/29/2022 01:02:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
05/29/2022 01:02:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
05/29/2022 01:02:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
05/29/2022 01:02:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/29/2022 01:02:43 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.4880996620127055 on epoch=21
05/29/2022 01:02:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=22
05/29/2022 01:02:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/29/2022 01:02:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
05/29/2022 01:02:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
05/29/2022 01:02:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=24
05/29/2022 01:03:01 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.5899313501144166 on epoch=24
05/29/2022 01:03:01 - INFO - __main__ - Saving model with best Classification-F1: 0.570119086133321 -> 0.5899313501144166 on epoch=24, global_step=400
05/29/2022 01:03:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
05/29/2022 01:03:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=26
05/29/2022 01:03:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=26
05/29/2022 01:03:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=27
05/29/2022 01:03:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=28
05/29/2022 01:03:20 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.6593848629246859 on epoch=28
05/29/2022 01:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5899313501144166 -> 0.6593848629246859 on epoch=28, global_step=450
05/29/2022 01:03:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/29/2022 01:03:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=29
05/29/2022 01:03:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=29
05/29/2022 01:03:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
05/29/2022 01:03:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
05/29/2022 01:03:39 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.6299316192115907 on epoch=31
05/29/2022 01:03:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
05/29/2022 01:03:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
05/29/2022 01:03:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=33
05/29/2022 01:03:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
05/29/2022 01:03:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=34
05/29/2022 01:03:58 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.6401438378555083 on epoch=34
05/29/2022 01:04:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/29/2022 01:04:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
05/29/2022 01:04:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/29/2022 01:04:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/29/2022 01:04:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=37
05/29/2022 01:04:16 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6873946981992426 on epoch=37
05/29/2022 01:04:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6593848629246859 -> 0.6873946981992426 on epoch=37, global_step=600
05/29/2022 01:04:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/29/2022 01:04:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/29/2022 01:04:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
05/29/2022 01:04:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=39
05/29/2022 01:04:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/29/2022 01:04:34 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.7095013187756855 on epoch=40
05/29/2022 01:04:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6873946981992426 -> 0.7095013187756855 on epoch=40, global_step=650
05/29/2022 01:04:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=41
05/29/2022 01:04:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
05/29/2022 01:04:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=42
05/29/2022 01:04:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=43
05/29/2022 01:04:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
05/29/2022 01:04:52 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6825396825396826 on epoch=43
05/29/2022 01:04:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
05/29/2022 01:04:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
05/29/2022 01:05:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=45
05/29/2022 01:05:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=46
05/29/2022 01:05:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/29/2022 01:05:10 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.6992141603723201 on epoch=46
05/29/2022 01:05:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=47
05/29/2022 01:05:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
05/29/2022 01:05:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
05/29/2022 01:05:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=49
05/29/2022 01:05:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=49
05/29/2022 01:05:27 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.703052503052503 on epoch=49
05/29/2022 01:05:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/29/2022 01:05:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=51
05/29/2022 01:05:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=51
05/29/2022 01:05:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=52
05/29/2022 01:05:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/29/2022 01:05:45 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.6565953369530502 on epoch=53
05/29/2022 01:05:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=53
05/29/2022 01:05:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=54
05/29/2022 01:05:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=54
05/29/2022 01:05:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
05/29/2022 01:05:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
05/29/2022 01:06:03 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.6953125 on epoch=56
05/29/2022 01:06:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=56
05/29/2022 01:06:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=57
05/29/2022 01:06:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
05/29/2022 01:06:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=58
05/29/2022 01:06:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
05/29/2022 01:06:21 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.6953125 on epoch=59
05/29/2022 01:06:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
05/29/2022 01:06:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
05/29/2022 01:06:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=61
05/29/2022 01:06:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=61
05/29/2022 01:06:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=62
05/29/2022 01:06:39 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.6988465357879459 on epoch=62
05/29/2022 01:06:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=63
05/29/2022 01:06:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
05/29/2022 01:06:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=64
05/29/2022 01:06:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=64
05/29/2022 01:06:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.28 on epoch=65
05/29/2022 01:06:56 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.7069910113388373 on epoch=65
05/29/2022 01:06:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=66
05/29/2022 01:07:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
05/29/2022 01:07:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=67
05/29/2022 01:07:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.26 on epoch=68
05/29/2022 01:07:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/29/2022 01:07:14 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.6437246963562753 on epoch=68
05/29/2022 01:07:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=69
05/29/2022 01:07:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
05/29/2022 01:07:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=70
05/29/2022 01:07:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=71
05/29/2022 01:07:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=71
05/29/2022 01:07:32 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.6157138294474608 on epoch=71
05/29/2022 01:07:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=72
05/29/2022 01:07:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=73
05/29/2022 01:07:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
05/29/2022 01:07:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=74
05/29/2022 01:07:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=74
05/29/2022 01:07:50 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.6644823066841417 on epoch=74
05/29/2022 01:07:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=75
05/29/2022 01:07:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=76
05/29/2022 01:07:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=76
05/29/2022 01:08:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=77
05/29/2022 01:08:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=78
05/29/2022 01:08:08 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.606567985116881 on epoch=78
05/29/2022 01:08:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
05/29/2022 01:08:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=79
05/29/2022 01:08:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=79
05/29/2022 01:08:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=80
05/29/2022 01:08:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=81
05/29/2022 01:08:26 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.6746031746031746 on epoch=81
05/29/2022 01:08:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=81
05/29/2022 01:08:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
05/29/2022 01:08:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=83
05/29/2022 01:08:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
05/29/2022 01:08:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=84
05/29/2022 01:08:44 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.6788850174216028 on epoch=84
05/29/2022 01:08:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/29/2022 01:08:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=85
05/29/2022 01:08:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
05/29/2022 01:08:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/29/2022 01:08:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=87
05/29/2022 01:09:02 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.7060219564851251 on epoch=87
05/29/2022 01:09:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=88
05/29/2022 01:09:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=88
05/29/2022 01:09:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=89
05/29/2022 01:09:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=89
05/29/2022 01:09:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=90
05/29/2022 01:09:20 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.7028347996089932 on epoch=90
05/29/2022 01:09:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=91
05/29/2022 01:09:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/29/2022 01:09:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=92
05/29/2022 01:09:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=93
05/29/2022 01:09:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=93
05/29/2022 01:09:38 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.6114699044174632 on epoch=93
05/29/2022 01:09:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/29/2022 01:09:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
05/29/2022 01:09:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=95
05/29/2022 01:09:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=96
05/29/2022 01:09:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
05/29/2022 01:09:56 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.6314163228989726 on epoch=96
05/29/2022 01:09:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=97
05/29/2022 01:10:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=98
05/29/2022 01:10:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=98
05/29/2022 01:10:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=99
05/29/2022 01:10:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
05/29/2022 01:10:14 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.6760759197063189 on epoch=99
05/29/2022 01:10:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
05/29/2022 01:10:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/29/2022 01:10:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/29/2022 01:10:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
05/29/2022 01:10:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
05/29/2022 01:10:32 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.6410925791636451 on epoch=103
05/29/2022 01:10:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=103
05/29/2022 01:10:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=104
05/29/2022 01:10:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=104
05/29/2022 01:10:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/29/2022 01:10:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=106
05/29/2022 01:10:50 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.6751062884700807 on epoch=106
05/29/2022 01:10:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=106
05/29/2022 01:10:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.20 on epoch=107
05/29/2022 01:10:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=108
05/29/2022 01:11:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
05/29/2022 01:11:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
05/29/2022 01:11:09 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.6796796796796798 on epoch=109
05/29/2022 01:11:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=109
05/29/2022 01:11:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/29/2022 01:11:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=111
05/29/2022 01:11:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/29/2022 01:11:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
05/29/2022 01:11:27 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.6981825419913951 on epoch=112
05/29/2022 01:11:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/29/2022 01:11:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/29/2022 01:11:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/29/2022 01:11:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/29/2022 01:11:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=115
05/29/2022 01:11:45 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.6993187690862108 on epoch=115
05/29/2022 01:11:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=116
05/29/2022 01:11:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/29/2022 01:11:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/29/2022 01:11:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/29/2022 01:11:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
05/29/2022 01:12:04 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.6514296829410621 on epoch=118
05/29/2022 01:12:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=119
05/29/2022 01:12:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
05/29/2022 01:12:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=120
05/29/2022 01:12:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=121
05/29/2022 01:12:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=121
05/29/2022 01:12:22 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.671560940841055 on epoch=121
05/29/2022 01:12:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=122
05/29/2022 01:12:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=123
05/29/2022 01:12:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
05/29/2022 01:12:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/29/2022 01:12:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/29/2022 01:12:40 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.681566972650407 on epoch=124
05/29/2022 01:12:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
05/29/2022 01:12:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=126
05/29/2022 01:12:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
05/29/2022 01:12:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
05/29/2022 01:12:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/29/2022 01:12:58 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.6111565488808399 on epoch=128
05/29/2022 01:13:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
05/29/2022 01:13:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
05/29/2022 01:13:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=129
05/29/2022 01:13:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
05/29/2022 01:13:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/29/2022 01:13:17 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.6600918238390043 on epoch=131
05/29/2022 01:13:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/29/2022 01:13:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=132
05/29/2022 01:13:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=133
05/29/2022 01:13:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
05/29/2022 01:13:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
05/29/2022 01:13:35 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.6934894497053381 on epoch=134
05/29/2022 01:13:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
05/29/2022 01:13:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/29/2022 01:13:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/29/2022 01:13:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=136
05/29/2022 01:13:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/29/2022 01:13:53 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.6984126984126984 on epoch=137
05/29/2022 01:13:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
05/29/2022 01:13:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=138
05/29/2022 01:14:01 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/29/2022 01:14:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
05/29/2022 01:14:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=140
05/29/2022 01:14:10 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.6855281367106716 on epoch=140
05/29/2022 01:14:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
05/29/2022 01:14:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=141
05/29/2022 01:14:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
05/29/2022 01:14:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=143
05/29/2022 01:14:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=143
05/29/2022 01:14:28 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.6575940697768109 on epoch=143
05/29/2022 01:14:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=144
05/29/2022 01:14:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=144
05/29/2022 01:14:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=145
05/29/2022 01:14:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/29/2022 01:14:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/29/2022 01:14:46 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.6796796796796798 on epoch=146
05/29/2022 01:14:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
05/29/2022 01:14:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
05/29/2022 01:14:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/29/2022 01:14:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
05/29/2022 01:14:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
05/29/2022 01:15:04 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.6949131280185662 on epoch=149
05/29/2022 01:15:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/29/2022 01:15:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
05/29/2022 01:15:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/29/2022 01:15:15 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
05/29/2022 01:15:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/29/2022 01:15:22 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.6575940697768109 on epoch=153
05/29/2022 01:15:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/29/2022 01:15:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=154
05/29/2022 01:15:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=154
05/29/2022 01:15:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=155
05/29/2022 01:15:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=156
05/29/2022 01:15:41 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.6855281367106716 on epoch=156
05/29/2022 01:15:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
05/29/2022 01:15:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/29/2022 01:15:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
05/29/2022 01:15:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/29/2022 01:15:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/29/2022 01:15:59 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.6840740451456689 on epoch=159
05/29/2022 01:16:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=159
05/29/2022 01:16:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/29/2022 01:16:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=161
05/29/2022 01:16:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
05/29/2022 01:16:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=162
05/29/2022 01:16:16 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.7000308356460068 on epoch=162
05/29/2022 01:16:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/29/2022 01:16:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/29/2022 01:16:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/29/2022 01:16:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
05/29/2022 01:16:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
05/29/2022 01:16:35 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.6644717103289277 on epoch=165
05/29/2022 01:16:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
05/29/2022 01:16:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
05/29/2022 01:16:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/29/2022 01:16:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/29/2022 01:16:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/29/2022 01:16:53 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.681566972650407 on epoch=168
05/29/2022 01:16:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=169
05/29/2022 01:16:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=169
05/29/2022 01:17:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=170
05/29/2022 01:17:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
05/29/2022 01:17:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
05/29/2022 01:17:11 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.6654437813921998 on epoch=171
05/29/2022 01:17:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=172
05/29/2022 01:17:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/29/2022 01:17:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/29/2022 01:17:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=174
05/29/2022 01:17:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/29/2022 01:17:29 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.6884478562067029 on epoch=174
05/29/2022 01:17:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/29/2022 01:17:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
05/29/2022 01:17:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/29/2022 01:17:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=177
05/29/2022 01:17:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=178
05/29/2022 01:17:47 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.6458856076690471 on epoch=178
05/29/2022 01:17:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=178
05/29/2022 01:17:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=179
05/29/2022 01:17:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/29/2022 01:17:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=180
05/29/2022 01:18:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=181
05/29/2022 01:18:06 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.6680161943319838 on epoch=181
05/29/2022 01:18:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=181
05/29/2022 01:18:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/29/2022 01:18:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/29/2022 01:18:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/29/2022 01:18:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
05/29/2022 01:18:24 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.6634513344420685 on epoch=184
05/29/2022 01:18:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
05/29/2022 01:18:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
05/29/2022 01:18:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=186
05/29/2022 01:18:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/29/2022 01:18:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=187
05/29/2022 01:18:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:18:38 - INFO - __main__ - Printing 3 examples
05/29/2022 01:18:38 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 01:18:38 - INFO - __main__ - ['false']
05/29/2022 01:18:38 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 01:18:38 - INFO - __main__ - ['false']
05/29/2022 01:18:38 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 01:18:38 - INFO - __main__ - ['false']
05/29/2022 01:18:38 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:18:38 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:18:39 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 01:18:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:18:39 - INFO - __main__ - Printing 3 examples
05/29/2022 01:18:39 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 01:18:39 - INFO - __main__ - ['false']
05/29/2022 01:18:39 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 01:18:39 - INFO - __main__ - ['false']
05/29/2022 01:18:39 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 01:18:39 - INFO - __main__ - ['false']
05/29/2022 01:18:39 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:18:39 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:18:39 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 01:18:42 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.7000308356460068 on epoch=187
05/29/2022 01:18:42 - INFO - __main__ - save last model!
05/29/2022 01:18:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 01:18:42 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 01:18:42 - INFO - __main__ - Printing 3 examples
05/29/2022 01:18:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 01:18:42 - INFO - __main__ - ['false']
05/29/2022 01:18:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 01:18:42 - INFO - __main__ - ['false']
05/29/2022 01:18:42 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 01:18:42 - INFO - __main__ - ['false']
05/29/2022 01:18:42 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:18:43 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:18:46 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 01:18:54 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:18:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:18:55 - INFO - __main__ - Starting training!
05/29/2022 01:19:39 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.3_8_predictions.txt
05/29/2022 01:19:39 - INFO - __main__ - Classification-F1 on test data: 0.4711
05/29/2022 01:19:40 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.3, bsz=8, dev_performance=0.7095013187756855, test_performance=0.47111012263875046
05/29/2022 01:19:40 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.2, bsz=8 ...
05/29/2022 01:19:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:19:41 - INFO - __main__ - Printing 3 examples
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:19:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:19:41 - INFO - __main__ - Loaded 256 examples from train data
05/29/2022 01:19:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/29/2022 01:19:41 - INFO - __main__ - Printing 3 examples
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
05/29/2022 01:19:41 - INFO - __main__ - ['false']
05/29/2022 01:19:41 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:19:41 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:19:41 - INFO - __main__ - Loaded 256 examples from dev data
05/29/2022 01:20:00 - INFO - __main__ - load prompt embedding from ckpt
05/29/2022 01:20:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/29/2022 01:20:01 - INFO - __main__ - Starting training!
05/29/2022 01:20:04 - INFO - __main__ - Step 10 Global step 10 Train loss 2.88 on epoch=0
05/29/2022 01:20:07 - INFO - __main__ - Step 20 Global step 20 Train loss 0.83 on epoch=1
05/29/2022 01:20:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=1
05/29/2022 01:20:13 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=2
05/29/2022 01:20:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
05/29/2022 01:20:21 - INFO - __main__ - Global step 50 Train loss 1.06 Classification-F1 0.38064516129032255 on epoch=3
05/29/2022 01:20:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.38064516129032255 on epoch=3, global_step=50
05/29/2022 01:20:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=3
05/29/2022 01:20:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=4
05/29/2022 01:20:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=4
05/29/2022 01:20:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
05/29/2022 01:20:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
05/29/2022 01:20:40 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.38245614035087716 on epoch=6
05/29/2022 01:20:40 - INFO - __main__ - Saving model with best Classification-F1: 0.38064516129032255 -> 0.38245614035087716 on epoch=6, global_step=100
05/29/2022 01:20:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
05/29/2022 01:20:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=7
05/29/2022 01:20:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=8
05/29/2022 01:20:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
05/29/2022 01:20:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
05/29/2022 01:20:59 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3822393822393822 on epoch=9
05/29/2022 01:21:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
05/29/2022 01:21:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=10
05/29/2022 01:21:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=11
05/29/2022 01:21:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
05/29/2022 01:21:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
05/29/2022 01:21:19 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.38064516129032255 on epoch=12
05/29/2022 01:21:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=13
05/29/2022 01:21:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=13
05/29/2022 01:21:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
05/29/2022 01:21:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
05/29/2022 01:21:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
05/29/2022 01:21:38 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.4968733249955333 on epoch=15
05/29/2022 01:21:38 - INFO - __main__ - Saving model with best Classification-F1: 0.38245614035087716 -> 0.4968733249955333 on epoch=15, global_step=250
05/29/2022 01:21:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=16
05/29/2022 01:21:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
05/29/2022 01:21:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
05/29/2022 01:21:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
05/29/2022 01:21:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
05/29/2022 01:21:57 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.5020424042015172 on epoch=18
05/29/2022 01:21:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4968733249955333 -> 0.5020424042015172 on epoch=18, global_step=300
05/29/2022 01:22:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
05/29/2022 01:22:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
05/29/2022 01:22:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/29/2022 01:22:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
05/29/2022 01:22:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/29/2022 01:22:16 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.5208825847123719 on epoch=21
05/29/2022 01:22:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5020424042015172 -> 0.5208825847123719 on epoch=21, global_step=350
05/29/2022 01:22:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
05/29/2022 01:22:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=23
05/29/2022 01:22:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=23
05/29/2022 01:22:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
05/29/2022 01:22:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=24
05/29/2022 01:22:35 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.39866578972094335 on epoch=24
05/29/2022 01:22:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
05/29/2022 01:22:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
05/29/2022 01:22:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
05/29/2022 01:22:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=27
05/29/2022 01:22:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=28
05/29/2022 01:22:54 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.5611876339149067 on epoch=28
05/29/2022 01:22:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5208825847123719 -> 0.5611876339149067 on epoch=28, global_step=450
05/29/2022 01:22:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/29/2022 01:23:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/29/2022 01:23:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/29/2022 01:23:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
05/29/2022 01:23:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
05/29/2022 01:23:13 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5882390786849385 on epoch=31
05/29/2022 01:23:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5611876339149067 -> 0.5882390786849385 on epoch=31, global_step=500
05/29/2022 01:23:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/29/2022 01:23:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/29/2022 01:23:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=33
05/29/2022 01:23:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/29/2022 01:23:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
05/29/2022 01:23:33 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.46997929606625255 on epoch=34
05/29/2022 01:23:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/29/2022 01:23:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
05/29/2022 01:23:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/29/2022 01:23:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/29/2022 01:23:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/29/2022 01:23:52 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.43468822516655437 on epoch=37
05/29/2022 01:23:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=38
05/29/2022 01:23:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/29/2022 01:24:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/29/2022 01:24:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/29/2022 01:24:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/29/2022 01:24:11 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5810562856434416 on epoch=40
05/29/2022 01:24:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/29/2022 01:24:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
05/29/2022 01:24:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
05/29/2022 01:24:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=43
05/29/2022 01:24:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/29/2022 01:24:30 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.6549019607843137 on epoch=43
05/29/2022 01:24:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5882390786849385 -> 0.6549019607843137 on epoch=43, global_step=700
05/29/2022 01:24:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/29/2022 01:24:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=44
05/29/2022 01:24:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=45
05/29/2022 01:24:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
05/29/2022 01:24:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
05/29/2022 01:24:49 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.6875 on epoch=46
05/29/2022 01:24:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6549019607843137 -> 0.6875 on epoch=46, global_step=750
05/29/2022 01:24:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=47
05/29/2022 01:24:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/29/2022 01:24:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=48
05/29/2022 01:25:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
05/29/2022 01:25:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/29/2022 01:25:08 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.6663683326267527 on epoch=49
05/29/2022 01:25:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=50
05/29/2022 01:25:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
05/29/2022 01:25:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
05/29/2022 01:25:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=52
05/29/2022 01:25:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=53
05/29/2022 01:25:27 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.6833570021530991 on epoch=53
05/29/2022 01:25:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
05/29/2022 01:25:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
05/29/2022 01:25:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
05/29/2022 01:25:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
05/29/2022 01:25:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=56
05/29/2022 01:25:46 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.6914015411612116 on epoch=56
05/29/2022 01:25:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6875 -> 0.6914015411612116 on epoch=56, global_step=900
05/29/2022 01:25:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/29/2022 01:25:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=57
05/29/2022 01:25:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=58
05/29/2022 01:25:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=58
05/29/2022 01:25:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/29/2022 01:26:05 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.6832021999847222 on epoch=59
05/29/2022 01:26:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/29/2022 01:26:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/29/2022 01:26:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/29/2022 01:26:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/29/2022 01:26:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/29/2022 01:26:24 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.680034564167451 on epoch=62
05/29/2022 01:26:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=63
05/29/2022 01:26:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/29/2022 01:26:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/29/2022 01:26:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=64
05/29/2022 01:26:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
05/29/2022 01:26:42 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.6818411967779057 on epoch=65
05/29/2022 01:26:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
05/29/2022 01:26:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
05/29/2022 01:26:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
05/29/2022 01:26:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=68
05/29/2022 01:26:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
05/29/2022 01:27:01 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.6743451806213312 on epoch=68
05/29/2022 01:27:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/29/2022 01:27:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=69
05/29/2022 01:27:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
05/29/2022 01:27:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=71
05/29/2022 01:27:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
05/29/2022 01:27:19 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.666824884016475 on epoch=71
05/29/2022 01:27:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=72
05/29/2022 01:27:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
05/29/2022 01:27:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=73
05/29/2022 01:27:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=74
05/29/2022 01:27:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=74
05/29/2022 01:27:37 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.6830084842925934 on epoch=74
05/29/2022 01:27:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=75
05/29/2022 01:27:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=76
05/29/2022 01:27:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=76
05/29/2022 01:27:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=77
05/29/2022 01:27:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=78
05/29/2022 01:27:55 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.67089072543618 on epoch=78
05/29/2022 01:27:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=78
05/29/2022 01:28:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
05/29/2022 01:28:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=79
05/29/2022 01:28:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=80
05/29/2022 01:28:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=81
05/29/2022 01:28:13 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.683588921950103 on epoch=81
05/29/2022 01:28:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=81
05/29/2022 01:28:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
05/29/2022 01:28:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=83
05/29/2022 01:28:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=83
05/29/2022 01:28:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
05/29/2022 01:28:31 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.6717948717948719 on epoch=84
05/29/2022 01:28:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=84
05/29/2022 01:28:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=85
05/29/2022 01:28:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=86
05/29/2022 01:28:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
05/29/2022 01:28:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/29/2022 01:28:49 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.6896969696969697 on epoch=87
05/29/2022 01:28:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=88
05/29/2022 01:28:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=88
05/29/2022 01:28:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=89
05/29/2022 01:29:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=89
05/29/2022 01:29:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
05/29/2022 01:29:07 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.6913638652769087 on epoch=90
05/29/2022 01:29:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=91
05/29/2022 01:29:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=91
05/29/2022 01:29:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
05/29/2022 01:29:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=93
05/29/2022 01:29:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/29/2022 01:29:25 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.6726594876061068 on epoch=93
05/29/2022 01:29:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
05/29/2022 01:29:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=94
05/29/2022 01:29:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=95
05/29/2022 01:29:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=96
05/29/2022 01:29:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=96
05/29/2022 01:29:43 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.6827757125154895 on epoch=96
05/29/2022 01:29:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=97
05/29/2022 01:29:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=98
05/29/2022 01:29:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=98
05/29/2022 01:29:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=99
05/29/2022 01:29:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=99
05/29/2022 01:30:01 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.6893156156386819 on epoch=99
05/29/2022 01:30:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=100
05/29/2022 01:30:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=101
05/29/2022 01:30:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=101
05/29/2022 01:30:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=102
05/29/2022 01:30:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=103
05/29/2022 01:30:20 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.6447808262377799 on epoch=103
05/29/2022 01:30:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=103
05/29/2022 01:30:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=104
05/29/2022 01:30:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=104
05/29/2022 01:30:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=105
05/29/2022 01:30:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=106
05/29/2022 01:30:38 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.6758092414133927 on epoch=106
05/29/2022 01:30:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=106
05/29/2022 01:30:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=107
05/29/2022 01:30:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=108
05/29/2022 01:30:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=108
05/29/2022 01:30:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=109
05/29/2022 01:30:56 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.7066687036895577 on epoch=109
05/29/2022 01:30:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6914015411612116 -> 0.7066687036895577 on epoch=109, global_step=1750
05/29/2022 01:30:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
05/29/2022 01:31:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=110
05/29/2022 01:31:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=111
05/29/2022 01:31:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
05/29/2022 01:31:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=112
05/29/2022 01:31:14 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.6984411094283047 on epoch=112
05/29/2022 01:31:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
05/29/2022 01:31:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=113
05/29/2022 01:31:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=114
05/29/2022 01:31:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=114
05/29/2022 01:31:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=115
05/29/2022 01:31:32 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7066687036895577 on epoch=115
05/29/2022 01:31:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=116
05/29/2022 01:31:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=116
05/29/2022 01:31:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/29/2022 01:31:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=118
05/29/2022 01:31:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=118
05/29/2022 01:31:50 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.6716716716716716 on epoch=118
05/29/2022 01:31:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=119
05/29/2022 01:31:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
05/29/2022 01:31:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=120
05/29/2022 01:32:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=121
05/29/2022 01:32:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=121
05/29/2022 01:32:08 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.6746031746031746 on epoch=121
05/29/2022 01:32:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=122
05/29/2022 01:32:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=123
05/29/2022 01:32:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=123
05/29/2022 01:32:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=124
05/29/2022 01:32:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=124
05/29/2022 01:32:26 - INFO - __main__ - Global step 2000 Train loss 0.28 Classification-F1 0.6652552926525529 on epoch=124
05/29/2022 01:32:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
05/29/2022 01:32:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=126
05/29/2022 01:32:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/29/2022 01:32:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=127
05/29/2022 01:32:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.28 on epoch=128
05/29/2022 01:32:44 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.6744076499128551 on epoch=128
05/29/2022 01:32:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=128
05/29/2022 01:32:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.28 on epoch=129
05/29/2022 01:32:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=129
05/29/2022 01:32:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=130
05/29/2022 01:32:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/29/2022 01:33:02 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.6840740451456689 on epoch=131
05/29/2022 01:33:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=131
05/29/2022 01:33:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
05/29/2022 01:33:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=133
05/29/2022 01:33:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=133
05/29/2022 01:33:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=134
05/29/2022 01:33:20 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.6910699919549477 on epoch=134
05/29/2022 01:33:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
05/29/2022 01:33:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=135
05/29/2022 01:33:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=136
05/29/2022 01:33:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=136
05/29/2022 01:33:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=137
05/29/2022 01:33:38 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.6837163330862368 on epoch=137
05/29/2022 01:33:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/29/2022 01:33:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
05/29/2022 01:33:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=139
05/29/2022 01:33:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=139
05/29/2022 01:33:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=140
05/29/2022 01:33:56 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7004926108374384 on epoch=140
05/29/2022 01:33:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
05/29/2022 01:34:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
05/29/2022 01:34:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
05/29/2022 01:34:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=143
05/29/2022 01:34:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=143
05/29/2022 01:34:14 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.6627987204415731 on epoch=143
05/29/2022 01:34:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=144
05/29/2022 01:34:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=144
05/29/2022 01:34:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=145
05/29/2022 01:34:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=146
05/29/2022 01:34:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=146
05/29/2022 01:34:33 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.6760759197063189 on epoch=146
05/29/2022 01:34:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=147
05/29/2022 01:34:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.24 on epoch=148
05/29/2022 01:34:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=148
05/29/2022 01:34:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=149
05/29/2022 01:34:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=149
05/29/2022 01:34:51 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.6832824367708088 on epoch=149
05/29/2022 01:34:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.31 on epoch=150
05/29/2022 01:34:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=151
05/29/2022 01:34:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=151
05/29/2022 01:35:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=152
05/29/2022 01:35:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/29/2022 01:35:09 - INFO - __main__ - Global step 2450 Train loss 0.24 Classification-F1 0.6424293993931318 on epoch=153
05/29/2022 01:35:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/29/2022 01:35:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=154
05/29/2022 01:35:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=154
05/29/2022 01:35:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=155
05/29/2022 01:35:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=156
05/29/2022 01:35:27 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.669937106918239 on epoch=156
05/29/2022 01:35:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
05/29/2022 01:35:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
05/29/2022 01:35:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=158
05/29/2022 01:35:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=158
05/29/2022 01:35:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=159
05/29/2022 01:35:46 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.6788559242300697 on epoch=159
05/29/2022 01:35:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
05/29/2022 01:35:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=160
05/29/2022 01:35:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=161
05/29/2022 01:35:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
05/29/2022 01:36:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
05/29/2022 01:36:05 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.6842429848905335 on epoch=162
05/29/2022 01:36:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/29/2022 01:36:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.22 on epoch=163
05/29/2022 01:36:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.23 on epoch=164
05/29/2022 01:36:15 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=164
05/29/2022 01:36:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/29/2022 01:36:23 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.7087868167004857 on epoch=165
05/29/2022 01:36:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7066687036895577 -> 0.7087868167004857 on epoch=165, global_step=2650
05/29/2022 01:36:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=166
05/29/2022 01:36:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
05/29/2022 01:36:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/29/2022 01:36:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=168
05/29/2022 01:36:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=168
05/29/2022 01:36:41 - INFO - __main__ - Global step 2700 Train loss 0.22 Classification-F1 0.6740514387573211 on epoch=168
05/29/2022 01:36:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=169
05/29/2022 01:36:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=169
05/29/2022 01:36:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=170
05/29/2022 01:36:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=171
05/29/2022 01:36:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.23 on epoch=171
05/29/2022 01:37:00 - INFO - __main__ - Global step 2750 Train loss 0.20 Classification-F1 0.6832824367708088 on epoch=171
05/29/2022 01:37:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/29/2022 01:37:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=173
05/29/2022 01:37:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=173
05/29/2022 01:37:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=174
05/29/2022 01:37:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
05/29/2022 01:37:18 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.6910699919549477 on epoch=174
05/29/2022 01:37:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=175
05/29/2022 01:37:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=176
05/29/2022 01:37:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/29/2022 01:37:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=177
05/29/2022 01:37:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=178
05/29/2022 01:37:36 - INFO - __main__ - Global step 2850 Train loss 0.21 Classification-F1 0.6553846153846153 on epoch=178
05/29/2022 01:37:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
05/29/2022 01:37:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=179
05/29/2022 01:37:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=179
05/29/2022 01:37:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=180
05/29/2022 01:37:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=181
05/29/2022 01:37:54 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.6761133603238867 on epoch=181
05/29/2022 01:37:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=181
05/29/2022 01:38:00 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=182
05/29/2022 01:38:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=183
05/29/2022 01:38:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.22 on epoch=183
05/29/2022 01:38:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=184
05/29/2022 01:38:12 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.6967711169566354 on epoch=184
05/29/2022 01:38:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=184
05/29/2022 01:38:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=185
05/29/2022 01:38:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=186
05/29/2022 01:38:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
05/29/2022 01:38:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=187
05/29/2022 01:38:30 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.678247241919717 on epoch=187
05/29/2022 01:38:30 - INFO - __main__ - save last model!
05/29/2022 01:38:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/29/2022 01:38:30 - INFO - __main__ - Start tokenizing ... 2733 instances
05/29/2022 01:38:30 - INFO - __main__ - Printing 3 examples
05/29/2022 01:38:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
05/29/2022 01:38:30 - INFO - __main__ - ['false']
05/29/2022 01:38:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
05/29/2022 01:38:30 - INFO - __main__ - ['false']
05/29/2022 01:38:30 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
05/29/2022 01:38:30 - INFO - __main__ - ['false']
05/29/2022 01:38:30 - INFO - __main__ - Tokenizing Input ...
05/29/2022 01:38:32 - INFO - __main__ - Tokenizing Output ...
05/29/2022 01:38:34 - INFO - __main__ - Loaded 2733 examples from test data
05/29/2022 01:39:20 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.2_8_predictions.txt
05/29/2022 01:39:20 - INFO - __main__ - Classification-F1 on test data: 0.4578
05/29/2022 01:39:21 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.2, bsz=8, dev_performance=0.7087868167004857, test_performance=0.45776199260167305
