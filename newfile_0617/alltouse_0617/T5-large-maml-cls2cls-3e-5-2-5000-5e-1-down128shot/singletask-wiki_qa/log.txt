05/21/2022 21:27:58 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:27:58 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa
05/21/2022 21:27:58 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:27:58 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa
05/21/2022 21:27:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:27:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:27:58 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:27:58 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:27:58 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:58 - INFO - __main__ - Using 2 gpus
05/21/2022 21:27:58 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/21/2022 21:27:58 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
05/21/2022 21:28:04 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.5, bsz=8 ...
06/17/2022 06:59:08 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/17/2022 06:59:08 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa
06/17/2022 06:59:08 - INFO - __main__ - Namespace(task_dir='data_128/wiki_qa/', task_name='wiki_qa', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/17/2022 06:59:08 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa
06/17/2022 06:59:10 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/17/2022 06:59:10 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/17/2022 06:59:10 - INFO - __main__ - args.device: cuda:1
06/17/2022 06:59:10 - INFO - __main__ - args.device: cuda:0
06/17/2022 06:59:10 - INFO - __main__ - Using 2 gpus
06/17/2022 06:59:10 - INFO - __main__ - Using 2 gpus
06/17/2022 06:59:10 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
06/17/2022 06:59:10 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_128_100', 'wiki_qa_128_13', 'wiki_qa_128_21', 'wiki_qa_128_42', 'wiki_qa_128_87']
06/17/2022 06:59:14 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.5, bsz=8 ...
06/17/2022 06:59:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 06:59:15 - INFO - __main__ - Printing 3 examples
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 06:59:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 06:59:15 - INFO - __main__ - Printing 3 examples
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 06:59:15 - INFO - __main__ - ['false']
06/17/2022 06:59:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 06:59:15 - INFO - __main__ - Tokenizing Output ...
06/17/2022 06:59:15 - INFO - __main__ - Tokenizing Output ...
06/17/2022 06:59:16 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 06:59:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 06:59:16 - INFO - __main__ - Printing 3 examples
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ - Tokenizing Input ...
06/17/2022 06:59:16 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 06:59:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 06:59:16 - INFO - __main__ - Printing 3 examples
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 06:59:16 - INFO - __main__ - ['false']
06/17/2022 06:59:16 - INFO - __main__ - Tokenizing Input ...
06/17/2022 06:59:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 06:59:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 06:59:16 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 06:59:16 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 06:59:34 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 06:59:34 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 06:59:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 06:59:35 - INFO - __main__ - Starting training!
06/17/2022 06:59:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 06:59:40 - INFO - __main__ - Starting training!
06/17/2022 06:59:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.70 on epoch=0
06/17/2022 06:59:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
06/17/2022 06:59:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=1
06/17/2022 06:59:51 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=2
06/17/2022 06:59:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=3
06/17/2022 06:59:57 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 06:59:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 06:59:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=3
06/17/2022 07:00:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
06/17/2022 07:00:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=4
06/17/2022 07:00:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=5
06/17/2022 07:00:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=6
06/17/2022 07:00:13 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 07:00:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=6
06/17/2022 07:00:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=7
06/17/2022 07:00:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=8
06/17/2022 07:00:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
06/17/2022 07:00:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
06/17/2022 07:00:29 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 07:00:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=9
06/17/2022 07:00:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
06/17/2022 07:00:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
06/17/2022 07:00:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
06/17/2022 07:00:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
06/17/2022 07:00:44 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3486005089058525 on epoch=12
06/17/2022 07:00:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3486005089058525 on epoch=12, global_step=200
06/17/2022 07:00:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
06/17/2022 07:00:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=13
06/17/2022 07:00:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=14
06/17/2022 07:00:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=14
06/17/2022 07:00:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
06/17/2022 07:01:00 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.5801720403490316 on epoch=15
06/17/2022 07:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3486005089058525 -> 0.5801720403490316 on epoch=15, global_step=250
06/17/2022 07:01:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=16
06/17/2022 07:01:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
06/17/2022 07:01:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
06/17/2022 07:01:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=18
06/17/2022 07:01:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
06/17/2022 07:01:16 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 07:01:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=19
06/17/2022 07:01:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=19
06/17/2022 07:01:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
06/17/2022 07:01:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
06/17/2022 07:01:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=21
06/17/2022 07:01:32 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.35693779904306216 on epoch=21
06/17/2022 07:01:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
06/17/2022 07:01:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
06/17/2022 07:01:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
06/17/2022 07:01:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/17/2022 07:01:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 07:01:48 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 07:01:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=25
06/17/2022 07:01:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
06/17/2022 07:01:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
06/17/2022 07:01:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=27
06/17/2022 07:02:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
06/17/2022 07:02:04 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 07:02:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
06/17/2022 07:02:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=29
06/17/2022 07:02:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
06/17/2022 07:02:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
06/17/2022 07:02:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 07:02:20 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3401530406766009 on epoch=31
06/17/2022 07:02:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
06/17/2022 07:02:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
06/17/2022 07:02:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
06/17/2022 07:02:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
06/17/2022 07:02:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 07:02:36 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 07:02:38 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
06/17/2022 07:02:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
06/17/2022 07:02:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=36
06/17/2022 07:02:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
06/17/2022 07:02:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=37
06/17/2022 07:02:52 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=37
06/17/2022 07:02:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
06/17/2022 07:02:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
06/17/2022 07:02:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
06/17/2022 07:03:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=39
06/17/2022 07:03:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
06/17/2022 07:03:08 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.41539594843462246 on epoch=40
06/17/2022 07:03:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
06/17/2022 07:03:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/17/2022 07:03:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
06/17/2022 07:03:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/17/2022 07:03:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=43
06/17/2022 07:03:24 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 07:03:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
06/17/2022 07:03:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
06/17/2022 07:03:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
06/17/2022 07:03:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
06/17/2022 07:03:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
06/17/2022 07:03:40 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=46
06/17/2022 07:03:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
06/17/2022 07:03:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
06/17/2022 07:03:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
06/17/2022 07:03:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=49
06/17/2022 07:03:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
06/17/2022 07:03:56 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 07:03:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
06/17/2022 07:04:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/17/2022 07:04:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 07:04:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=52
06/17/2022 07:04:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=53
06/17/2022 07:04:12 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 07:04:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
06/17/2022 07:04:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
06/17/2022 07:04:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
06/17/2022 07:04:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
06/17/2022 07:04:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
06/17/2022 07:04:27 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=56
06/17/2022 07:04:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=56
06/17/2022 07:04:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
06/17/2022 07:04:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=58
06/17/2022 07:04:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/17/2022 07:04:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
06/17/2022 07:04:43 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 07:04:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
06/17/2022 07:04:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/17/2022 07:04:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
06/17/2022 07:04:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
06/17/2022 07:04:56 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/17/2022 07:04:59 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=62
06/17/2022 07:05:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 07:05:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/17/2022 07:05:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
06/17/2022 07:05:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
06/17/2022 07:05:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
06/17/2022 07:05:16 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.4950166112956811 on epoch=65
06/17/2022 07:05:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
06/17/2022 07:05:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
06/17/2022 07:05:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
06/17/2022 07:05:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 07:05:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=68
06/17/2022 07:05:32 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 07:05:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=69
06/17/2022 07:05:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=69
06/17/2022 07:05:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
06/17/2022 07:05:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=71
06/17/2022 07:05:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=71
06/17/2022 07:05:48 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.33159268929503916 on epoch=71
06/17/2022 07:05:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
06/17/2022 07:05:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=73
06/17/2022 07:05:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=73
06/17/2022 07:05:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
06/17/2022 07:06:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=74
06/17/2022 07:06:05 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 07:06:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
06/17/2022 07:06:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
06/17/2022 07:06:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/17/2022 07:06:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
06/17/2022 07:06:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=78
06/17/2022 07:06:21 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.6374384236453202 on epoch=78
06/17/2022 07:06:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5801720403490316 -> 0.6374384236453202 on epoch=78, global_step=1250
06/17/2022 07:06:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=78
06/17/2022 07:06:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
06/17/2022 07:06:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=79
06/17/2022 07:06:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
06/17/2022 07:06:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.38 on epoch=81
06/17/2022 07:06:37 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=81
06/17/2022 07:06:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
06/17/2022 07:06:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=82
06/17/2022 07:06:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
06/17/2022 07:06:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
06/17/2022 07:06:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
06/17/2022 07:06:53 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.33159268929503916 on epoch=84
06/17/2022 07:06:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/17/2022 07:06:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
06/17/2022 07:07:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=86
06/17/2022 07:07:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
06/17/2022 07:07:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
06/17/2022 07:07:09 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.40784920953527937 on epoch=87
06/17/2022 07:07:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
06/17/2022 07:07:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=88
06/17/2022 07:07:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
06/17/2022 07:07:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=89
06/17/2022 07:07:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
06/17/2022 07:07:26 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=90
06/17/2022 07:07:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=91
06/17/2022 07:07:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=91
06/17/2022 07:07:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
06/17/2022 07:07:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=93
06/17/2022 07:07:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=93
06/17/2022 07:07:42 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=93
06/17/2022 07:07:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=94
06/17/2022 07:07:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
06/17/2022 07:07:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/17/2022 07:07:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
06/17/2022 07:07:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
06/17/2022 07:07:58 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=96
06/17/2022 07:08:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=97
06/17/2022 07:08:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
06/17/2022 07:08:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
06/17/2022 07:08:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
06/17/2022 07:08:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
06/17/2022 07:08:14 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=99
06/17/2022 07:08:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=100
06/17/2022 07:08:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=101
06/17/2022 07:08:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
06/17/2022 07:08:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=102
06/17/2022 07:08:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=103
06/17/2022 07:08:30 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=103
06/17/2022 07:08:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=103
06/17/2022 07:08:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=104
06/17/2022 07:08:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=104
06/17/2022 07:08:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=105
06/17/2022 07:08:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=106
06/17/2022 07:08:47 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.6130098025464803 on epoch=106
06/17/2022 07:08:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=106
06/17/2022 07:08:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=107
06/17/2022 07:08:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=108
06/17/2022 07:08:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=108
06/17/2022 07:08:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
06/17/2022 07:09:03 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=109
06/17/2022 07:09:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=109
06/17/2022 07:09:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=110
06/17/2022 07:09:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=111
06/17/2022 07:09:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
06/17/2022 07:09:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=112
06/17/2022 07:09:20 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.37496971391296247 on epoch=112
06/17/2022 07:09:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=113
06/17/2022 07:09:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=113
06/17/2022 07:09:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=114
06/17/2022 07:09:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/17/2022 07:09:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=115
06/17/2022 07:09:37 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.350463149416029 on epoch=115
06/17/2022 07:09:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=116
06/17/2022 07:09:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=116
06/17/2022 07:09:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=117
06/17/2022 07:09:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=118
06/17/2022 07:09:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=118
06/17/2022 07:09:55 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=118
06/17/2022 07:09:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
06/17/2022 07:10:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=119
06/17/2022 07:10:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=120
06/17/2022 07:10:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=121
06/17/2022 07:10:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=121
06/17/2022 07:10:13 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=121
06/17/2022 07:10:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=122
06/17/2022 07:10:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=123
06/17/2022 07:10:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
06/17/2022 07:10:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=124
06/17/2022 07:10:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=124
06/17/2022 07:10:30 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.33159268929503916 on epoch=124
06/17/2022 07:10:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=125
06/17/2022 07:10:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=126
06/17/2022 07:10:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=126
06/17/2022 07:10:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
06/17/2022 07:10:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=128
06/17/2022 07:10:47 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=128
06/17/2022 07:10:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=128
06/17/2022 07:10:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=129
06/17/2022 07:10:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.38 on epoch=129
06/17/2022 07:10:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.37 on epoch=130
06/17/2022 07:10:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=131
06/17/2022 07:11:03 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=131
06/17/2022 07:11:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.38 on epoch=131
06/17/2022 07:11:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=132
06/17/2022 07:11:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=133
06/17/2022 07:11:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=133
06/17/2022 07:11:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=134
06/17/2022 07:11:19 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=134
06/17/2022 07:11:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=134
06/17/2022 07:11:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=135
06/17/2022 07:11:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
06/17/2022 07:11:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=136
06/17/2022 07:11:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.39 on epoch=137
06/17/2022 07:11:35 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=137
06/17/2022 07:11:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=138
06/17/2022 07:11:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=138
06/17/2022 07:11:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=139
06/17/2022 07:11:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.33 on epoch=139
06/17/2022 07:11:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.36 on epoch=140
06/17/2022 07:11:51 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.5484865337252721 on epoch=140
06/17/2022 07:11:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.37 on epoch=141
06/17/2022 07:11:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=141
06/17/2022 07:11:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=142
06/17/2022 07:12:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.35 on epoch=143
06/17/2022 07:12:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.34 on epoch=143
06/17/2022 07:12:07 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=143
06/17/2022 07:12:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=144
06/17/2022 07:12:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=144
06/17/2022 07:12:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=145
06/17/2022 07:12:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
06/17/2022 07:12:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=146
06/17/2022 07:12:24 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.4059435586495995 on epoch=146
06/17/2022 07:12:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.34 on epoch=147
06/17/2022 07:12:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.39 on epoch=148
06/17/2022 07:12:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=148
06/17/2022 07:12:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=149
06/17/2022 07:12:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=149
06/17/2022 07:12:40 - INFO - __main__ - Global step 2400 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=149
06/17/2022 07:12:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=150
06/17/2022 07:12:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=151
06/17/2022 07:12:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=151
06/17/2022 07:12:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
06/17/2022 07:12:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=153
06/17/2022 07:12:56 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.42752983181433807 on epoch=153
06/17/2022 07:12:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
06/17/2022 07:13:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=154
06/17/2022 07:13:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=154
06/17/2022 07:13:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=155
06/17/2022 07:13:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=156
06/17/2022 07:13:13 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.33652312599681017 on epoch=156
06/17/2022 07:13:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
06/17/2022 07:13:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.38 on epoch=157
06/17/2022 07:13:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.39 on epoch=158
06/17/2022 07:13:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=158
06/17/2022 07:13:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=159
06/17/2022 07:13:29 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=159
06/17/2022 07:13:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=159
06/17/2022 07:13:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=160
06/17/2022 07:13:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=161
06/17/2022 07:13:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=161
06/17/2022 07:13:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=162
06/17/2022 07:13:45 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=162
06/17/2022 07:13:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=163
06/17/2022 07:13:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.35 on epoch=163
06/17/2022 07:13:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=164
06/17/2022 07:13:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=164
06/17/2022 07:13:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=165
06/17/2022 07:14:02 - INFO - __main__ - Global step 2650 Train loss 0.35 Classification-F1 0.40096618357487923 on epoch=165
06/17/2022 07:14:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=166
06/17/2022 07:14:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=166
06/17/2022 07:14:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.36 on epoch=167
06/17/2022 07:14:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=168
06/17/2022 07:14:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.37 on epoch=168
06/17/2022 07:14:18 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=168
06/17/2022 07:14:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=169
06/17/2022 07:14:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.38 on epoch=169
06/17/2022 07:14:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.35 on epoch=170
06/17/2022 07:14:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.35 on epoch=171
06/17/2022 07:14:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=171
06/17/2022 07:14:34 - INFO - __main__ - Global step 2750 Train loss 0.36 Classification-F1 0.4567943830110235 on epoch=171
06/17/2022 07:14:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=172
06/17/2022 07:14:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=173
06/17/2022 07:14:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.36 on epoch=173
06/17/2022 07:14:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.38 on epoch=174
06/17/2022 07:14:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=174
06/17/2022 07:14:51 - INFO - __main__ - Global step 2800 Train loss 0.36 Classification-F1 0.350463149416029 on epoch=174
06/17/2022 07:14:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=175
06/17/2022 07:14:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.37 on epoch=176
06/17/2022 07:14:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=176
06/17/2022 07:15:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=177
06/17/2022 07:15:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=178
06/17/2022 07:15:08 - INFO - __main__ - Global step 2850 Train loss 0.35 Classification-F1 0.5936507936507938 on epoch=178
06/17/2022 07:15:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.34 on epoch=178
06/17/2022 07:15:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.39 on epoch=179
06/17/2022 07:15:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=179
06/17/2022 07:15:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=180
06/17/2022 07:15:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=181
06/17/2022 07:15:25 - INFO - __main__ - Global step 2900 Train loss 0.36 Classification-F1 0.5760289517266761 on epoch=181
06/17/2022 07:15:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.37 on epoch=181
06/17/2022 07:15:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.36 on epoch=182
06/17/2022 07:15:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=183
06/17/2022 07:15:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=183
06/17/2022 07:15:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=184
06/17/2022 07:15:41 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.5823552704522056 on epoch=184
06/17/2022 07:15:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=184
06/17/2022 07:15:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=185
06/17/2022 07:15:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.34 on epoch=186
06/17/2022 07:15:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=186
06/17/2022 07:15:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=187
06/17/2022 07:15:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:15:55 - INFO - __main__ - Printing 3 examples
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:15:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:15:55 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:15:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:15:55 - INFO - __main__ - Printing 3 examples
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:15:55 - INFO - __main__ - ['false']
06/17/2022 07:15:55 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:15:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:15:56 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:15:58 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.47162022703818374 on epoch=187
06/17/2022 07:15:58 - INFO - __main__ - save last model!
06/17/2022 07:15:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 07:15:58 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 07:15:58 - INFO - __main__ - Printing 3 examples
06/17/2022 07:15:58 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 07:15:58 - INFO - __main__ - ['false']
06/17/2022 07:15:58 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 07:15:58 - INFO - __main__ - ['false']
06/17/2022 07:15:58 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 07:15:58 - INFO - __main__ - ['false']
06/17/2022 07:15:58 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:15:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:16:02 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 07:16:14 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:16:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:16:15 - INFO - __main__ - Starting training!
06/17/2022 07:16:42 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.5_8_predictions.txt
06/17/2022 07:16:42 - INFO - __main__ - Classification-F1 on test data: 0.5413
06/17/2022 07:16:42 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.5, bsz=8, dev_performance=0.6374384236453202, test_performance=0.5413175952744298
06/17/2022 07:16:42 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.4, bsz=8 ...
06/17/2022 07:16:43 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:16:43 - INFO - __main__ - Printing 3 examples
06/17/2022 07:16:43 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:16:43 - INFO - __main__ - ['false']
06/17/2022 07:16:43 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:16:43 - INFO - __main__ - ['false']
06/17/2022 07:16:43 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:16:43 - INFO - __main__ - ['false']
06/17/2022 07:16:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:16:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:16:44 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:16:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:16:44 - INFO - __main__ - Printing 3 examples
06/17/2022 07:16:44 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:16:44 - INFO - __main__ - ['false']
06/17/2022 07:16:44 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:16:44 - INFO - __main__ - ['false']
06/17/2022 07:16:44 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:16:44 - INFO - __main__ - ['false']
06/17/2022 07:16:44 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:16:44 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:16:44 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:16:59 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:17:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:17:00 - INFO - __main__ - Starting training!
06/17/2022 07:17:03 - INFO - __main__ - Step 10 Global step 10 Train loss 0.74 on epoch=0
06/17/2022 07:17:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=1
06/17/2022 07:17:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
06/17/2022 07:17:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=2
06/17/2022 07:17:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=3
06/17/2022 07:17:17 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 07:17:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 07:17:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=3
06/17/2022 07:17:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
06/17/2022 07:17:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
06/17/2022 07:17:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
06/17/2022 07:17:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
06/17/2022 07:17:33 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 07:17:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
06/17/2022 07:17:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
06/17/2022 07:17:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
06/17/2022 07:17:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
06/17/2022 07:17:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
06/17/2022 07:17:51 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 07:17:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
06/17/2022 07:17:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
06/17/2022 07:17:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
06/17/2022 07:18:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=11
06/17/2022 07:18:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
06/17/2022 07:18:07 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.42025542025542023 on epoch=12
06/17/2022 07:18:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.42025542025542023 on epoch=12, global_step=200
06/17/2022 07:18:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
06/17/2022 07:18:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 07:18:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 07:18:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
06/17/2022 07:18:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=15
06/17/2022 07:18:23 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.5858945325137497 on epoch=15
06/17/2022 07:18:23 - INFO - __main__ - Saving model with best Classification-F1: 0.42025542025542023 -> 0.5858945325137497 on epoch=15, global_step=250
06/17/2022 07:18:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
06/17/2022 07:18:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
06/17/2022 07:18:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
06/17/2022 07:18:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
06/17/2022 07:18:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=18
06/17/2022 07:18:40 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 07:18:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
06/17/2022 07:18:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=19
06/17/2022 07:18:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
06/17/2022 07:18:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=21
06/17/2022 07:18:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
06/17/2022 07:18:56 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3486005089058525 on epoch=21
06/17/2022 07:18:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
06/17/2022 07:19:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
06/17/2022 07:19:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=23
06/17/2022 07:19:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/17/2022 07:19:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
06/17/2022 07:19:12 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 07:19:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
06/17/2022 07:19:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=26
06/17/2022 07:19:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
06/17/2022 07:19:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
06/17/2022 07:19:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
06/17/2022 07:19:29 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 07:19:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
06/17/2022 07:19:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/17/2022 07:19:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=29
06/17/2022 07:19:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=30
06/17/2022 07:19:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
06/17/2022 07:19:45 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=31
06/17/2022 07:19:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
06/17/2022 07:19:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
06/17/2022 07:19:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/17/2022 07:19:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
06/17/2022 07:19:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 07:20:01 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 07:20:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/17/2022 07:20:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
06/17/2022 07:20:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
06/17/2022 07:20:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
06/17/2022 07:20:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=37
06/17/2022 07:20:18 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.4153026265702322 on epoch=37
06/17/2022 07:20:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
06/17/2022 07:20:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 07:20:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
06/17/2022 07:20:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
06/17/2022 07:20:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/17/2022 07:20:34 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.41013824884792627 on epoch=40
06/17/2022 07:20:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
06/17/2022 07:20:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
06/17/2022 07:20:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
06/17/2022 07:20:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/17/2022 07:20:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
06/17/2022 07:20:50 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 07:20:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
06/17/2022 07:20:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
06/17/2022 07:20:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=45
06/17/2022 07:21:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=46
06/17/2022 07:21:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
06/17/2022 07:21:07 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.6171875 on epoch=46
06/17/2022 07:21:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5858945325137497 -> 0.6171875 on epoch=46, global_step=750
06/17/2022 07:21:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 07:21:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/17/2022 07:21:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
06/17/2022 07:21:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
06/17/2022 07:21:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=49
06/17/2022 07:21:23 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 07:21:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 07:21:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/17/2022 07:21:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=51
06/17/2022 07:21:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=52
06/17/2022 07:21:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
06/17/2022 07:21:39 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 07:21:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/17/2022 07:21:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
06/17/2022 07:21:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 07:21:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
06/17/2022 07:21:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 07:21:55 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=56
06/17/2022 07:21:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
06/17/2022 07:22:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
06/17/2022 07:22:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/17/2022 07:22:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/17/2022 07:22:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
06/17/2022 07:22:11 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.37922403003754696 on epoch=59
06/17/2022 07:22:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=59
06/17/2022 07:22:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
06/17/2022 07:22:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
06/17/2022 07:22:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
06/17/2022 07:22:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 07:22:27 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.6326322773774438 on epoch=62
06/17/2022 07:22:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6171875 -> 0.6326322773774438 on epoch=62, global_step=1000
06/17/2022 07:22:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
06/17/2022 07:22:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=63
06/17/2022 07:22:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=64
06/17/2022 07:22:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
06/17/2022 07:22:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=65
06/17/2022 07:22:44 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.6014652014652014 on epoch=65
06/17/2022 07:22:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
06/17/2022 07:22:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
06/17/2022 07:22:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
06/17/2022 07:22:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=68
06/17/2022 07:22:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
06/17/2022 07:23:00 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 07:23:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
06/17/2022 07:23:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/17/2022 07:23:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
06/17/2022 07:23:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=71
06/17/2022 07:23:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
06/17/2022 07:23:16 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.4531123388581953 on epoch=71
06/17/2022 07:23:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=72
06/17/2022 07:23:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 07:23:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=73
06/17/2022 07:23:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=74
06/17/2022 07:23:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
06/17/2022 07:23:33 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 07:23:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=75
06/17/2022 07:23:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
06/17/2022 07:23:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=76
06/17/2022 07:23:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
06/17/2022 07:23:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
06/17/2022 07:23:49 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=78
06/17/2022 07:23:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=78
06/17/2022 07:23:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=79
06/17/2022 07:23:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
06/17/2022 07:23:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/17/2022 07:24:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=81
06/17/2022 07:24:05 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.5124804461674488 on epoch=81
06/17/2022 07:24:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=81
06/17/2022 07:24:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/17/2022 07:24:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/17/2022 07:24:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
06/17/2022 07:24:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
06/17/2022 07:24:21 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.38709489051094886 on epoch=84
06/17/2022 07:24:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/17/2022 07:24:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
06/17/2022 07:24:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
06/17/2022 07:24:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
06/17/2022 07:24:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
06/17/2022 07:24:37 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.5068455452356381 on epoch=87
06/17/2022 07:24:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=88
06/17/2022 07:24:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=88
06/17/2022 07:24:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
06/17/2022 07:24:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.35 on epoch=89
06/17/2022 07:24:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
06/17/2022 07:24:54 - INFO - __main__ - Global step 1450 Train loss 0.35 Classification-F1 0.584305408271474 on epoch=90
06/17/2022 07:24:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
06/17/2022 07:24:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
06/17/2022 07:25:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=92
06/17/2022 07:25:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
06/17/2022 07:25:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
06/17/2022 07:25:10 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.3948694102146787 on epoch=93
06/17/2022 07:25:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 07:25:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
06/17/2022 07:25:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=95
06/17/2022 07:25:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
06/17/2022 07:25:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=96
06/17/2022 07:25:26 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.554442748091603 on epoch=96
06/17/2022 07:25:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
06/17/2022 07:25:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
06/17/2022 07:25:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
06/17/2022 07:25:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
06/17/2022 07:25:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=99
06/17/2022 07:25:42 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.44622552554218203 on epoch=99
06/17/2022 07:25:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
06/17/2022 07:25:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
06/17/2022 07:25:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=101
06/17/2022 07:25:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
06/17/2022 07:25:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=103
06/17/2022 07:25:59 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.5976501106279087 on epoch=103
06/17/2022 07:26:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=103
06/17/2022 07:26:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
06/17/2022 07:26:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.36 on epoch=104
06/17/2022 07:26:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=105
06/17/2022 07:26:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=106
06/17/2022 07:26:15 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.5512814439338671 on epoch=106
06/17/2022 07:26:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
06/17/2022 07:26:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=107
06/17/2022 07:26:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=108
06/17/2022 07:26:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
06/17/2022 07:26:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=109
06/17/2022 07:26:33 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.5762668703845175 on epoch=109
06/17/2022 07:26:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
06/17/2022 07:26:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=110
06/17/2022 07:26:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=111
06/17/2022 07:26:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
06/17/2022 07:26:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=112
06/17/2022 07:26:50 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.5437892337084784 on epoch=112
06/17/2022 07:26:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=113
06/17/2022 07:26:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=113
06/17/2022 07:26:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=114
06/17/2022 07:27:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=114
06/17/2022 07:27:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=115
06/17/2022 07:27:12 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.598574483250178 on epoch=115
06/17/2022 07:27:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.32 on epoch=116
06/17/2022 07:27:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=116
06/17/2022 07:27:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=117
06/17/2022 07:27:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=118
06/17/2022 07:27:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=118
06/17/2022 07:27:29 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.5478040163948482 on epoch=118
06/17/2022 07:27:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=119
06/17/2022 07:27:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=119
06/17/2022 07:27:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=120
06/17/2022 07:27:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=121
06/17/2022 07:27:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=121
06/17/2022 07:27:56 - INFO - __main__ - Global step 1950 Train loss 0.32 Classification-F1 0.5705498602050326 on epoch=121
06/17/2022 07:27:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=122
06/17/2022 07:28:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=123
06/17/2022 07:28:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=123
06/17/2022 07:28:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=124
06/17/2022 07:28:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
06/17/2022 07:28:20 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.562753036437247 on epoch=124
06/17/2022 07:28:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=125
06/17/2022 07:28:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=126
06/17/2022 07:28:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.33 on epoch=126
06/17/2022 07:28:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.34 on epoch=127
06/17/2022 07:28:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=128
06/17/2022 07:28:47 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.5950826090360974 on epoch=128
06/17/2022 07:28:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=128
06/17/2022 07:28:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=129
06/17/2022 07:28:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=129
06/17/2022 07:28:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=130
06/17/2022 07:29:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=131
06/17/2022 07:29:05 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.5647572892667038 on epoch=131
06/17/2022 07:29:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=131
06/17/2022 07:29:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.31 on epoch=132
06/17/2022 07:29:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=133
06/17/2022 07:29:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=133
06/17/2022 07:29:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=134
06/17/2022 07:29:29 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.6171641335530733 on epoch=134
06/17/2022 07:29:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=134
06/17/2022 07:29:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.28 on epoch=135
06/17/2022 07:29:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.23 on epoch=136
06/17/2022 07:29:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=136
06/17/2022 07:29:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=137
06/17/2022 07:29:49 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.6633227306092488 on epoch=137
06/17/2022 07:29:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6326322773774438 -> 0.6633227306092488 on epoch=137, global_step=2200
06/17/2022 07:29:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.34 on epoch=138
06/17/2022 07:29:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
06/17/2022 07:29:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=139
06/17/2022 07:29:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=139
06/17/2022 07:30:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
06/17/2022 07:30:11 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.6295566502463054 on epoch=140
06/17/2022 07:30:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
06/17/2022 07:30:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=141
06/17/2022 07:30:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=142
06/17/2022 07:30:21 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=143
06/17/2022 07:30:24 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
06/17/2022 07:30:38 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.6122881693688523 on epoch=143
06/17/2022 07:30:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=144
06/17/2022 07:30:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
06/17/2022 07:30:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=145
06/17/2022 07:30:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
06/17/2022 07:30:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=146
06/17/2022 07:30:57 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.666824884016475 on epoch=146
06/17/2022 07:30:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6633227306092488 -> 0.666824884016475 on epoch=146, global_step=2350
06/17/2022 07:31:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=147
06/17/2022 07:31:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=148
06/17/2022 07:31:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
06/17/2022 07:31:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=149
06/17/2022 07:31:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=149
06/17/2022 07:31:20 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.6208102371463038 on epoch=149
06/17/2022 07:31:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
06/17/2022 07:31:25 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=151
06/17/2022 07:31:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=151
06/17/2022 07:31:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=152
06/17/2022 07:31:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=153
06/17/2022 07:31:44 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.5693647338214922 on epoch=153
06/17/2022 07:31:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=153
06/17/2022 07:31:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
06/17/2022 07:31:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=154
06/17/2022 07:31:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=155
06/17/2022 07:31:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=156
06/17/2022 07:32:08 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.5797161488300728 on epoch=156
06/17/2022 07:32:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
06/17/2022 07:32:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
06/17/2022 07:32:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
06/17/2022 07:32:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=158
06/17/2022 07:32:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
06/17/2022 07:32:26 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.6263921960211999 on epoch=159
06/17/2022 07:32:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=159
06/17/2022 07:32:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=160
06/17/2022 07:32:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.21 on epoch=161
06/17/2022 07:32:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
06/17/2022 07:32:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
06/17/2022 07:32:47 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.652296000122087 on epoch=162
06/17/2022 07:32:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=163
06/17/2022 07:32:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
06/17/2022 07:32:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
06/17/2022 07:32:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
06/17/2022 07:33:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
06/17/2022 07:33:09 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.6367132066834515 on epoch=165
06/17/2022 07:33:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/17/2022 07:33:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=166
06/17/2022 07:33:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=167
06/17/2022 07:33:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
06/17/2022 07:33:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
06/17/2022 07:33:31 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.5993612264452252 on epoch=168
06/17/2022 07:33:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
06/17/2022 07:33:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=169
06/17/2022 07:33:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=170
06/17/2022 07:33:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
06/17/2022 07:33:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
06/17/2022 07:33:53 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.6135849056603773 on epoch=171
06/17/2022 07:33:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
06/17/2022 07:33:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=173
06/17/2022 07:34:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
06/17/2022 07:34:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=174
06/17/2022 07:34:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=174
06/17/2022 07:34:19 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.6392411924119241 on epoch=174
06/17/2022 07:34:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=175
06/17/2022 07:34:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=176
06/17/2022 07:34:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
06/17/2022 07:34:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=177
06/17/2022 07:34:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=178
06/17/2022 07:34:43 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.6007130124777185 on epoch=178
06/17/2022 07:34:45 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
06/17/2022 07:34:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
06/17/2022 07:34:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=179
06/17/2022 07:34:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/17/2022 07:34:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=181
06/17/2022 07:35:04 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.6131976564909035 on epoch=181
06/17/2022 07:35:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
06/17/2022 07:35:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
06/17/2022 07:35:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=183
06/17/2022 07:35:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=183
06/17/2022 07:35:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
06/17/2022 07:35:27 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.6388394773967982 on epoch=184
06/17/2022 07:35:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
06/17/2022 07:35:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
06/17/2022 07:35:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
06/17/2022 07:35:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
06/17/2022 07:35:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=187
06/17/2022 07:35:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:35:41 - INFO - __main__ - Printing 3 examples
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:35:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:35:41 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:35:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:35:41 - INFO - __main__ - Printing 3 examples
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:35:41 - INFO - __main__ - ['false']
06/17/2022 07:35:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:35:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:35:42 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:35:52 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.6209889818754188 on epoch=187
06/17/2022 07:35:52 - INFO - __main__ - save last model!
06/17/2022 07:35:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 07:35:52 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 07:35:52 - INFO - __main__ - Printing 3 examples
06/17/2022 07:35:52 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 07:35:52 - INFO - __main__ - ['false']
06/17/2022 07:35:52 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 07:35:52 - INFO - __main__ - ['false']
06/17/2022 07:35:52 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 07:35:52 - INFO - __main__ - ['false']
06/17/2022 07:35:52 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:35:53 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:35:56 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 07:35:58 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:35:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:35:59 - INFO - __main__ - Starting training!
06/17/2022 07:37:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.4_8_predictions.txt
06/17/2022 07:37:57 - INFO - __main__ - Classification-F1 on test data: 0.4083
06/17/2022 07:37:58 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.4, bsz=8, dev_performance=0.666824884016475, test_performance=0.40829688750945825
06/17/2022 07:37:58 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.3, bsz=8 ...
06/17/2022 07:37:59 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:37:59 - INFO - __main__ - Printing 3 examples
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:37:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:37:59 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:37:59 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:37:59 - INFO - __main__ - Printing 3 examples
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:37:59 - INFO - __main__ - ['false']
06/17/2022 07:37:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:37:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:37:59 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:38:18 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:38:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:38:19 - INFO - __main__ - Starting training!
06/17/2022 07:38:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=0
06/17/2022 07:38:24 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=1
06/17/2022 07:38:26 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=1
06/17/2022 07:38:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
06/17/2022 07:38:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=3
06/17/2022 07:38:38 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 07:38:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 07:38:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=3
06/17/2022 07:38:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
06/17/2022 07:38:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
06/17/2022 07:38:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
06/17/2022 07:38:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=6
06/17/2022 07:38:54 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.5372334469133433 on epoch=6
06/17/2022 07:38:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5372334469133433 on epoch=6, global_step=100
06/17/2022 07:38:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
06/17/2022 07:38:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=7
06/17/2022 07:39:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=8
06/17/2022 07:39:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.36 on epoch=8
06/17/2022 07:39:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=9
06/17/2022 07:39:10 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.33159268929503916 on epoch=9
06/17/2022 07:39:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
06/17/2022 07:39:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
06/17/2022 07:39:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
06/17/2022 07:39:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
06/17/2022 07:39:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=12
06/17/2022 07:39:26 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3401530406766009 on epoch=12
06/17/2022 07:39:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
06/17/2022 07:39:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 07:39:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=14
06/17/2022 07:39:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 07:39:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=15
06/17/2022 07:39:42 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 07:39:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
06/17/2022 07:39:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
06/17/2022 07:39:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
06/17/2022 07:39:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=18
06/17/2022 07:39:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
06/17/2022 07:39:58 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 07:40:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
06/17/2022 07:40:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
06/17/2022 07:40:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
06/17/2022 07:40:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=21
06/17/2022 07:40:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 07:40:14 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.5925642984466514 on epoch=21
06/17/2022 07:40:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5372334469133433 -> 0.5925642984466514 on epoch=21, global_step=350
06/17/2022 07:40:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=22
06/17/2022 07:40:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
06/17/2022 07:40:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
06/17/2022 07:40:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=24
06/17/2022 07:40:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 07:40:30 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 07:40:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
06/17/2022 07:40:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
06/17/2022 07:40:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
06/17/2022 07:40:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=27
06/17/2022 07:40:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
06/17/2022 07:40:46 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 07:40:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
06/17/2022 07:40:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
06/17/2022 07:40:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
06/17/2022 07:40:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=30
06/17/2022 07:40:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
06/17/2022 07:41:02 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.45469441258397514 on epoch=31
06/17/2022 07:41:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
06/17/2022 07:41:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
06/17/2022 07:41:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=33
06/17/2022 07:41:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
06/17/2022 07:41:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
06/17/2022 07:41:17 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=34
06/17/2022 07:41:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
06/17/2022 07:41:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
06/17/2022 07:41:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/17/2022 07:41:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
06/17/2022 07:41:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
06/17/2022 07:41:33 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.4494623655913979 on epoch=37
06/17/2022 07:41:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
06/17/2022 07:41:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 07:41:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
06/17/2022 07:41:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=39
06/17/2022 07:41:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
06/17/2022 07:41:49 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.5748323219418715 on epoch=40
06/17/2022 07:41:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=41
06/17/2022 07:41:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
06/17/2022 07:41:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=42
06/17/2022 07:41:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
06/17/2022 07:42:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
06/17/2022 07:42:05 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 07:42:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
06/17/2022 07:42:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
06/17/2022 07:42:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
06/17/2022 07:42:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
06/17/2022 07:42:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=46
06/17/2022 07:42:21 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.36318407960199 on epoch=46
06/17/2022 07:42:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
06/17/2022 07:42:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
06/17/2022 07:42:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
06/17/2022 07:42:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
06/17/2022 07:42:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/17/2022 07:42:37 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 07:42:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
06/17/2022 07:42:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
06/17/2022 07:42:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 07:42:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
06/17/2022 07:42:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/17/2022 07:42:53 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 07:42:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
06/17/2022 07:42:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=54
06/17/2022 07:43:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
06/17/2022 07:43:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
06/17/2022 07:43:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
06/17/2022 07:43:08 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.4950166112956811 on epoch=56
06/17/2022 07:43:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
06/17/2022 07:43:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 07:43:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=58
06/17/2022 07:43:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/17/2022 07:43:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
06/17/2022 07:43:24 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 07:43:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=59
06/17/2022 07:43:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=60
06/17/2022 07:43:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
06/17/2022 07:43:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/17/2022 07:43:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=62
06/17/2022 07:43:40 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=62
06/17/2022 07:43:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 07:43:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
06/17/2022 07:43:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=64
06/17/2022 07:43:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
06/17/2022 07:43:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
06/17/2022 07:43:56 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.6467415166530767 on epoch=65
06/17/2022 07:43:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5925642984466514 -> 0.6467415166530767 on epoch=65, global_step=1050
06/17/2022 07:43:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
06/17/2022 07:44:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
06/17/2022 07:44:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
06/17/2022 07:44:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
06/17/2022 07:44:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
06/17/2022 07:44:12 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 07:44:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 07:44:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/17/2022 07:44:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
06/17/2022 07:44:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=71
06/17/2022 07:44:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=71
06/17/2022 07:44:28 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3970608272506082 on epoch=71
06/17/2022 07:44:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
06/17/2022 07:44:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
06/17/2022 07:44:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/17/2022 07:44:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
06/17/2022 07:44:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
06/17/2022 07:44:44 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 07:44:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=75
06/17/2022 07:44:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
06/17/2022 07:44:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=76
06/17/2022 07:44:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
06/17/2022 07:44:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
06/17/2022 07:45:00 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.6163403432438965 on epoch=78
06/17/2022 07:45:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=78
06/17/2022 07:45:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
06/17/2022 07:45:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
06/17/2022 07:45:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
06/17/2022 07:45:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
06/17/2022 07:45:15 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=81
06/17/2022 07:45:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
06/17/2022 07:45:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
06/17/2022 07:45:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=83
06/17/2022 07:45:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
06/17/2022 07:45:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
06/17/2022 07:45:32 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=84
06/17/2022 07:45:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
06/17/2022 07:45:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
06/17/2022 07:45:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=86
06/17/2022 07:45:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=86
06/17/2022 07:45:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
06/17/2022 07:45:47 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.3401530406766009 on epoch=87
06/17/2022 07:45:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
06/17/2022 07:45:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=88
06/17/2022 07:45:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
06/17/2022 07:45:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
06/17/2022 07:46:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
06/17/2022 07:46:03 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.5380386204570369 on epoch=90
06/17/2022 07:46:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 07:46:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
06/17/2022 07:46:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
06/17/2022 07:46:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
06/17/2022 07:46:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
06/17/2022 07:46:19 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=93
06/17/2022 07:46:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
06/17/2022 07:46:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
06/17/2022 07:46:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=95
06/17/2022 07:46:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=96
06/17/2022 07:46:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=96
06/17/2022 07:46:35 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.475475261460616 on epoch=96
06/17/2022 07:46:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
06/17/2022 07:46:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
06/17/2022 07:46:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
06/17/2022 07:46:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=99
06/17/2022 07:46:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=99
06/17/2022 07:46:51 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=99
06/17/2022 07:46:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=100
06/17/2022 07:46:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=101
06/17/2022 07:46:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/17/2022 07:47:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
06/17/2022 07:47:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=103
06/17/2022 07:47:07 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.5998388526307308 on epoch=103
06/17/2022 07:47:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=103
06/17/2022 07:47:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=104
06/17/2022 07:47:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=104
06/17/2022 07:47:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=105
06/17/2022 07:47:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=106
06/17/2022 07:47:23 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.6514635806671205 on epoch=106
06/17/2022 07:47:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6467415166530767 -> 0.6514635806671205 on epoch=106, global_step=1700
06/17/2022 07:47:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
06/17/2022 07:47:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=107
06/17/2022 07:47:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=108
06/17/2022 07:47:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=108
06/17/2022 07:47:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=109
06/17/2022 07:47:39 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.3383422492035824 on epoch=109
06/17/2022 07:47:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=109
06/17/2022 07:47:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=110
06/17/2022 07:47:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=111
06/17/2022 07:47:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
06/17/2022 07:47:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
06/17/2022 07:47:55 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=112
06/17/2022 07:47:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
06/17/2022 07:48:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=113
06/17/2022 07:48:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=114
06/17/2022 07:48:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/17/2022 07:48:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=115
06/17/2022 07:48:11 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.651146055029015 on epoch=115
06/17/2022 07:48:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=116
06/17/2022 07:48:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=116
06/17/2022 07:48:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=117
06/17/2022 07:48:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=118
06/17/2022 07:48:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=118
06/17/2022 07:48:27 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=118
06/17/2022 07:48:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
06/17/2022 07:48:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=119
06/17/2022 07:48:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=120
06/17/2022 07:48:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
06/17/2022 07:48:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=121
06/17/2022 07:48:43 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.652296000122087 on epoch=121
06/17/2022 07:48:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6514635806671205 -> 0.652296000122087 on epoch=121, global_step=1950
06/17/2022 07:48:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=122
06/17/2022 07:48:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=123
06/17/2022 07:48:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
06/17/2022 07:48:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=124
06/17/2022 07:48:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=124
06/17/2022 07:48:59 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=124
06/17/2022 07:49:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=125
06/17/2022 07:49:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=126
06/17/2022 07:49:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=126
06/17/2022 07:49:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
06/17/2022 07:49:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=128
06/17/2022 07:49:15 - INFO - __main__ - Global step 2050 Train loss 0.36 Classification-F1 0.6512097721400048 on epoch=128
06/17/2022 07:49:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=128
06/17/2022 07:49:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=129
06/17/2022 07:49:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.35 on epoch=129
06/17/2022 07:49:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=130
06/17/2022 07:49:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=131
06/17/2022 07:49:31 - INFO - __main__ - Global step 2100 Train loss 0.36 Classification-F1 0.4560645347162201 on epoch=131
06/17/2022 07:49:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=131
06/17/2022 07:49:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.35 on epoch=132
06/17/2022 07:49:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=133
06/17/2022 07:49:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=133
06/17/2022 07:49:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=134
06/17/2022 07:49:47 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.39267460026616785 on epoch=134
06/17/2022 07:49:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=134
06/17/2022 07:49:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.35 on epoch=135
06/17/2022 07:49:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
06/17/2022 07:49:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
06/17/2022 07:49:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=137
06/17/2022 07:50:03 - INFO - __main__ - Global step 2200 Train loss 0.35 Classification-F1 0.4886459209419681 on epoch=137
06/17/2022 07:50:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.35 on epoch=138
06/17/2022 07:50:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.34 on epoch=138
06/17/2022 07:50:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=139
06/17/2022 07:50:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.36 on epoch=139
06/17/2022 07:50:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
06/17/2022 07:50:19 - INFO - __main__ - Global step 2250 Train loss 0.35 Classification-F1 0.5418875927889714 on epoch=140
06/17/2022 07:50:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=141
06/17/2022 07:50:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=141
06/17/2022 07:50:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.32 on epoch=142
06/17/2022 07:50:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=143
06/17/2022 07:50:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.31 on epoch=143
06/17/2022 07:50:35 - INFO - __main__ - Global step 2300 Train loss 0.35 Classification-F1 0.6297989457480058 on epoch=143
06/17/2022 07:50:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.33 on epoch=144
06/17/2022 07:50:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.34 on epoch=144
06/17/2022 07:50:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=145
06/17/2022 07:50:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=146
06/17/2022 07:50:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=146
06/17/2022 07:50:51 - INFO - __main__ - Global step 2350 Train loss 0.35 Classification-F1 0.6166024818142919 on epoch=146
06/17/2022 07:50:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=147
06/17/2022 07:50:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=148
06/17/2022 07:50:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.32 on epoch=148
06/17/2022 07:51:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=149
06/17/2022 07:51:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.33 on epoch=149
06/17/2022 07:51:08 - INFO - __main__ - Global step 2400 Train loss 0.33 Classification-F1 0.36516753625488524 on epoch=149
06/17/2022 07:51:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.34 on epoch=150
06/17/2022 07:51:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=151
06/17/2022 07:51:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=151
06/17/2022 07:51:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=152
06/17/2022 07:51:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.31 on epoch=153
06/17/2022 07:51:25 - INFO - __main__ - Global step 2450 Train loss 0.34 Classification-F1 0.636085292509568 on epoch=153
06/17/2022 07:51:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=153
06/17/2022 07:51:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=154
06/17/2022 07:51:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=154
06/17/2022 07:51:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.33 on epoch=155
06/17/2022 07:51:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.32 on epoch=156
06/17/2022 07:51:42 - INFO - __main__ - Global step 2500 Train loss 0.33 Classification-F1 0.6074342145617371 on epoch=156
06/17/2022 07:51:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=156
06/17/2022 07:51:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.36 on epoch=157
06/17/2022 07:51:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.31 on epoch=158
06/17/2022 07:51:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.30 on epoch=158
06/17/2022 07:51:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=159
06/17/2022 07:51:59 - INFO - __main__ - Global step 2550 Train loss 0.33 Classification-F1 0.5380386204570369 on epoch=159
06/17/2022 07:52:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=159
06/17/2022 07:52:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=160
06/17/2022 07:52:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.34 on epoch=161
06/17/2022 07:52:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=161
06/17/2022 07:52:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=162
06/17/2022 07:52:16 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.602053794337934 on epoch=162
06/17/2022 07:52:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=163
06/17/2022 07:52:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.31 on epoch=163
06/17/2022 07:52:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.32 on epoch=164
06/17/2022 07:52:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.30 on epoch=164
06/17/2022 07:52:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.31 on epoch=165
06/17/2022 07:52:33 - INFO - __main__ - Global step 2650 Train loss 0.31 Classification-F1 0.6366688540601584 on epoch=165
06/17/2022 07:52:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.30 on epoch=166
06/17/2022 07:52:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=166
06/17/2022 07:52:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=167
06/17/2022 07:52:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=168
06/17/2022 07:52:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=168
06/17/2022 07:52:51 - INFO - __main__ - Global step 2700 Train loss 0.31 Classification-F1 0.5119639389242254 on epoch=168
06/17/2022 07:52:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.34 on epoch=169
06/17/2022 07:52:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.30 on epoch=169
06/17/2022 07:52:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.33 on epoch=170
06/17/2022 07:53:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=171
06/17/2022 07:53:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.28 on epoch=171
06/17/2022 07:53:08 - INFO - __main__ - Global step 2750 Train loss 0.31 Classification-F1 0.6226971260132645 on epoch=171
06/17/2022 07:53:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.33 on epoch=172
06/17/2022 07:53:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=173
06/17/2022 07:53:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.29 on epoch=173
06/17/2022 07:53:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.30 on epoch=174
06/17/2022 07:53:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.31 on epoch=174
06/17/2022 07:53:26 - INFO - __main__ - Global step 2800 Train loss 0.30 Classification-F1 0.5420708634330925 on epoch=174
06/17/2022 07:53:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.31 on epoch=175
06/17/2022 07:53:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=176
06/17/2022 07:53:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.30 on epoch=176
06/17/2022 07:53:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=177
06/17/2022 07:53:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=178
06/17/2022 07:53:43 - INFO - __main__ - Global step 2850 Train loss 0.30 Classification-F1 0.6217494089834515 on epoch=178
06/17/2022 07:53:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.26 on epoch=178
06/17/2022 07:53:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.26 on epoch=179
06/17/2022 07:53:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=179
06/17/2022 07:53:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.30 on epoch=180
06/17/2022 07:53:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.23 on epoch=181
06/17/2022 07:54:01 - INFO - __main__ - Global step 2900 Train loss 0.28 Classification-F1 0.6582738780207135 on epoch=181
06/17/2022 07:54:01 - INFO - __main__ - Saving model with best Classification-F1: 0.652296000122087 -> 0.6582738780207135 on epoch=181, global_step=2900
06/17/2022 07:54:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.31 on epoch=181
06/17/2022 07:54:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.30 on epoch=182
06/17/2022 07:54:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.31 on epoch=183
06/17/2022 07:54:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=183
06/17/2022 07:54:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.27 on epoch=184
06/17/2022 07:54:18 - INFO - __main__ - Global step 2950 Train loss 0.29 Classification-F1 0.6462785556374355 on epoch=184
06/17/2022 07:54:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=184
06/17/2022 07:54:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.27 on epoch=185
06/17/2022 07:54:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=186
06/17/2022 07:54:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=186
06/17/2022 07:54:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=187
06/17/2022 07:54:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:54:32 - INFO - __main__ - Printing 3 examples
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:54:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:54:32 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:54:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:54:32 - INFO - __main__ - Printing 3 examples
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:54:32 - INFO - __main__ - ['false']
06/17/2022 07:54:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:54:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:54:33 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:54:36 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.608203244566881 on epoch=187
06/17/2022 07:54:36 - INFO - __main__ - save last model!
06/17/2022 07:54:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 07:54:36 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 07:54:36 - INFO - __main__ - Printing 3 examples
06/17/2022 07:54:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 07:54:36 - INFO - __main__ - ['false']
06/17/2022 07:54:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 07:54:36 - INFO - __main__ - ['false']
06/17/2022 07:54:36 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 07:54:36 - INFO - __main__ - ['false']
06/17/2022 07:54:36 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:54:37 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:54:40 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 07:54:52 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:54:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:54:52 - INFO - __main__ - Starting training!
06/17/2022 07:55:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.3_8_predictions.txt
06/17/2022 07:55:32 - INFO - __main__ - Classification-F1 on test data: 0.5375
06/17/2022 07:55:32 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.3, bsz=8, dev_performance=0.6582738780207135, test_performance=0.5375262716555215
06/17/2022 07:55:32 - INFO - __main__ - Running ... prefix=wiki_qa_128_100, lr=0.2, bsz=8 ...
06/17/2022 07:55:33 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:55:33 - INFO - __main__ - Printing 3 examples
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: what version minecraft free [SEP] answer: Minecraft is a sandbox indie game originally created by Swedish programmer Markus "Notch" Persson and later developed and published by Mojang .
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: what year did the beatles came out with the song i wanna hold your hand [SEP] answer: It was also the group's first American number one, entering the Billboard Hot 100 chart on 18 January 1964 at number forty-five and starting the British invasion of the American music industry.
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: what is Roxio DLA [SEP] answer: As a replacement for DLA, it remedies compatibility issues Internet Explorer 8 .
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:55:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:55:33 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 07:55:33 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 07:55:33 - INFO - __main__ - Printing 3 examples
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: who plays judas in lady gaga video judas? [SEP] answer: "Judas" is a song by American recording artist Lady Gaga , from her second studio album Born This Way (2011).
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: when was the state of utah established [SEP] answer: The world headquarters of The Church of Jesus Christ of Latter-day Saints (LDS Church) is located in Utah's state capital .
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ -  [wiki_qa] question: WHAT SINGER MARRIED HIS COUSIN [SEP] answer: In the meantime he was determined to gain back some of his popularity.
06/17/2022 07:55:33 - INFO - __main__ - ['false']
06/17/2022 07:55:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 07:55:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 07:55:34 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 07:55:49 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 07:55:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 07:55:50 - INFO - __main__ - Starting training!
06/17/2022 07:55:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=0
06/17/2022 07:55:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=1
06/17/2022 07:55:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=1
06/17/2022 07:56:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
06/17/2022 07:56:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=3
06/17/2022 07:56:07 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 07:56:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 07:56:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=3
06/17/2022 07:56:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=4
06/17/2022 07:56:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=4
06/17/2022 07:56:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
06/17/2022 07:56:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
06/17/2022 07:56:23 - INFO - __main__ - Global step 100 Train loss 0.39 Classification-F1 0.44622552554218203 on epoch=6
06/17/2022 07:56:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.44622552554218203 on epoch=6, global_step=100
06/17/2022 07:56:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=6
06/17/2022 07:56:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
06/17/2022 07:56:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=8
06/17/2022 07:56:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=8
06/17/2022 07:56:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=9
06/17/2022 07:56:39 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 07:56:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
06/17/2022 07:56:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
06/17/2022 07:56:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
06/17/2022 07:56:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
06/17/2022 07:56:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
06/17/2022 07:56:56 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 07:56:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
06/17/2022 07:57:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
06/17/2022 07:57:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
06/17/2022 07:57:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=14
06/17/2022 07:57:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
06/17/2022 07:57:12 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 07:57:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=16
06/17/2022 07:57:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
06/17/2022 07:57:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
06/17/2022 07:57:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=18
06/17/2022 07:57:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=18
06/17/2022 07:57:28 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 07:57:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=19
06/17/2022 07:57:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=19
06/17/2022 07:57:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=20
06/17/2022 07:57:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=21
06/17/2022 07:57:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 07:57:45 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.39267460026616785 on epoch=21
06/17/2022 07:57:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=22
06/17/2022 07:57:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
06/17/2022 07:57:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
06/17/2022 07:57:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=24
06/17/2022 07:57:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
06/17/2022 07:58:01 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 07:58:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
06/17/2022 07:58:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
06/17/2022 07:58:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=26
06/17/2022 07:58:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
06/17/2022 07:58:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
06/17/2022 07:58:17 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 07:58:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
06/17/2022 07:58:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
06/17/2022 07:58:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=29
06/17/2022 07:58:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
06/17/2022 07:58:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=31
06/17/2022 07:58:34 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=31
06/17/2022 07:58:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
06/17/2022 07:58:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
06/17/2022 07:58:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/17/2022 07:58:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
06/17/2022 07:58:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
06/17/2022 07:58:50 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.35693779904306216 on epoch=34
06/17/2022 07:58:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/17/2022 07:58:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
06/17/2022 07:58:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/17/2022 07:59:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/17/2022 07:59:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
06/17/2022 07:59:06 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=37
06/17/2022 07:59:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
06/17/2022 07:59:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=38
06/17/2022 07:59:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
06/17/2022 07:59:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
06/17/2022 07:59:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
06/17/2022 07:59:22 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.46906958449545877 on epoch=40
06/17/2022 07:59:22 - INFO - __main__ - Saving model with best Classification-F1: 0.44622552554218203 -> 0.46906958449545877 on epoch=40, global_step=650
06/17/2022 07:59:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
06/17/2022 07:59:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
06/17/2022 07:59:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/17/2022 07:59:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
06/17/2022 07:59:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
06/17/2022 07:59:38 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 07:59:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
06/17/2022 07:59:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 07:59:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
06/17/2022 07:59:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=46
06/17/2022 07:59:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=46
06/17/2022 07:59:54 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.44160766645212046 on epoch=46
06/17/2022 07:59:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 07:59:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/17/2022 08:00:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
06/17/2022 08:00:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
06/17/2022 08:00:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/17/2022 08:00:10 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 08:00:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 08:00:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
06/17/2022 08:00:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
06/17/2022 08:00:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
06/17/2022 08:00:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
06/17/2022 08:00:27 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3383422492035824 on epoch=53
06/17/2022 08:00:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=53
06/17/2022 08:00:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
06/17/2022 08:00:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
06/17/2022 08:00:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=55
06/17/2022 08:00:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
06/17/2022 08:00:43 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.5097900529611675 on epoch=56
06/17/2022 08:00:43 - INFO - __main__ - Saving model with best Classification-F1: 0.46906958449545877 -> 0.5097900529611675 on epoch=56, global_step=900
06/17/2022 08:00:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=56
06/17/2022 08:00:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
06/17/2022 08:00:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/17/2022 08:00:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/17/2022 08:00:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
06/17/2022 08:00:59 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.4473152726512988 on epoch=59
06/17/2022 08:01:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=59
06/17/2022 08:01:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/17/2022 08:01:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/17/2022 08:01:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/17/2022 08:01:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 08:01:16 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3401530406766009 on epoch=62
06/17/2022 08:01:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 08:01:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=63
06/17/2022 08:01:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/17/2022 08:01:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
06/17/2022 08:01:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
06/17/2022 08:01:32 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.49475155008296223 on epoch=65
06/17/2022 08:01:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
06/17/2022 08:01:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=66
06/17/2022 08:01:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
06/17/2022 08:01:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
06/17/2022 08:01:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 08:01:48 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 08:01:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=69
06/17/2022 08:01:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/17/2022 08:01:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
06/17/2022 08:01:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
06/17/2022 08:02:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=71
06/17/2022 08:02:04 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.3849492366116407 on epoch=71
06/17/2022 08:02:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
06/17/2022 08:02:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 08:02:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=73
06/17/2022 08:02:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
06/17/2022 08:02:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=74
06/17/2022 08:02:20 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 08:02:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=75
06/17/2022 08:02:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=76
06/17/2022 08:02:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=76
06/17/2022 08:02:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=77
06/17/2022 08:02:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=78
06/17/2022 08:02:35 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=78
06/17/2022 08:02:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
06/17/2022 08:02:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
06/17/2022 08:02:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
06/17/2022 08:02:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=80
06/17/2022 08:02:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=81
06/17/2022 08:02:51 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.46777546777546775 on epoch=81
06/17/2022 08:02:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
06/17/2022 08:02:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=82
06/17/2022 08:02:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/17/2022 08:03:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
06/17/2022 08:03:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
06/17/2022 08:03:07 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.3486005089058525 on epoch=84
06/17/2022 08:03:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=84
06/17/2022 08:03:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
06/17/2022 08:03:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=86
06/17/2022 08:03:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/17/2022 08:03:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=87
06/17/2022 08:03:23 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=87
06/17/2022 08:03:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
06/17/2022 08:03:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=88
06/17/2022 08:03:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=89
06/17/2022 08:03:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=89
06/17/2022 08:03:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=90
06/17/2022 08:03:39 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.6508038683770903 on epoch=90
06/17/2022 08:03:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5097900529611675 -> 0.6508038683770903 on epoch=90, global_step=1450
06/17/2022 08:03:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=91
06/17/2022 08:03:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
06/17/2022 08:03:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
06/17/2022 08:03:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=93
06/17/2022 08:03:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=93
06/17/2022 08:03:55 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=93
06/17/2022 08:03:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 08:04:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=94
06/17/2022 08:04:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=95
06/17/2022 08:04:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
06/17/2022 08:04:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=96
06/17/2022 08:04:11 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.538597565122764 on epoch=96
06/17/2022 08:04:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
06/17/2022 08:04:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=98
06/17/2022 08:04:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=98
06/17/2022 08:04:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=99
06/17/2022 08:04:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=99
06/17/2022 08:04:27 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.3486005089058525 on epoch=99
06/17/2022 08:04:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=100
06/17/2022 08:04:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
06/17/2022 08:04:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=101
06/17/2022 08:04:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=102
06/17/2022 08:04:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=103
06/17/2022 08:04:43 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.6444824270911227 on epoch=103
06/17/2022 08:04:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=103
06/17/2022 08:04:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=104
06/17/2022 08:04:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=104
06/17/2022 08:04:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 08:04:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=106
06/17/2022 08:04:59 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.413139502376293 on epoch=106
06/17/2022 08:05:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=106
06/17/2022 08:05:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=107
06/17/2022 08:05:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
06/17/2022 08:05:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=108
06/17/2022 08:05:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=109
06/17/2022 08:05:14 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.48459743082651846 on epoch=109
06/17/2022 08:05:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=109
06/17/2022 08:05:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=110
06/17/2022 08:05:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=111
06/17/2022 08:05:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=111
06/17/2022 08:05:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
06/17/2022 08:05:30 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.4837222702391241 on epoch=112
06/17/2022 08:05:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
06/17/2022 08:05:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=113
06/17/2022 08:05:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=114
06/17/2022 08:05:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=114
06/17/2022 08:05:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
06/17/2022 08:05:46 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.5781862392218969 on epoch=115
06/17/2022 08:05:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=116
06/17/2022 08:05:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=116
06/17/2022 08:05:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=117
06/17/2022 08:05:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=118
06/17/2022 08:05:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.29 on epoch=118
06/17/2022 08:06:03 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.5577128753599342 on epoch=118
06/17/2022 08:06:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=119
06/17/2022 08:06:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=119
06/17/2022 08:06:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=120
06/17/2022 08:06:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=121
06/17/2022 08:06:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=121
06/17/2022 08:06:20 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.6120909161915177 on epoch=121
06/17/2022 08:06:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=122
06/17/2022 08:06:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=123
06/17/2022 08:06:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
06/17/2022 08:06:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=124
06/17/2022 08:06:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
06/17/2022 08:06:37 - INFO - __main__ - Global step 2000 Train loss 0.32 Classification-F1 0.44810875336290906 on epoch=124
06/17/2022 08:06:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=125
06/17/2022 08:06:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=126
06/17/2022 08:06:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=126
06/17/2022 08:06:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=127
06/17/2022 08:06:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.28 on epoch=128
06/17/2022 08:06:53 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.6504181051016494 on epoch=128
06/17/2022 08:06:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.33 on epoch=128
06/17/2022 08:06:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=129
06/17/2022 08:07:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.32 on epoch=129
06/17/2022 08:07:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=130
06/17/2022 08:07:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=131
06/17/2022 08:07:10 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.6715542521994136 on epoch=131
06/17/2022 08:07:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6508038683770903 -> 0.6715542521994136 on epoch=131, global_step=2100
06/17/2022 08:07:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=131
06/17/2022 08:07:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=132
06/17/2022 08:07:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=133
06/17/2022 08:07:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=133
06/17/2022 08:07:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=134
06/17/2022 08:07:27 - INFO - __main__ - Global step 2150 Train loss 0.29 Classification-F1 0.544213649851632 on epoch=134
06/17/2022 08:07:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=134
06/17/2022 08:07:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=135
06/17/2022 08:07:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
06/17/2022 08:07:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=136
06/17/2022 08:07:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=137
06/17/2022 08:07:43 - INFO - __main__ - Global step 2200 Train loss 0.26 Classification-F1 0.6943985307621672 on epoch=137
06/17/2022 08:07:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6715542521994136 -> 0.6943985307621672 on epoch=137, global_step=2200
06/17/2022 08:07:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=138
06/17/2022 08:07:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=138
06/17/2022 08:07:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=139
06/17/2022 08:07:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.27 on epoch=139
06/17/2022 08:07:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=140
06/17/2022 08:08:00 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.6484375 on epoch=140
06/17/2022 08:08:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=141
06/17/2022 08:08:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=141
06/17/2022 08:08:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=142
06/17/2022 08:08:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=143
06/17/2022 08:08:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=143
06/17/2022 08:08:17 - INFO - __main__ - Global step 2300 Train loss 0.25 Classification-F1 0.6027579146070292 on epoch=143
06/17/2022 08:08:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=144
06/17/2022 08:08:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
06/17/2022 08:08:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.24 on epoch=145
06/17/2022 08:08:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.24 on epoch=146
06/17/2022 08:08:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=146
06/17/2022 08:08:34 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.6083333333333334 on epoch=146
06/17/2022 08:08:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=147
06/17/2022 08:08:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.25 on epoch=148
06/17/2022 08:08:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=148
06/17/2022 08:08:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=149
06/17/2022 08:08:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=149
06/17/2022 08:08:51 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.6310129088461234 on epoch=149
06/17/2022 08:08:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=150
06/17/2022 08:08:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.24 on epoch=151
06/17/2022 08:08:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.33 on epoch=151
06/17/2022 08:09:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.22 on epoch=152
06/17/2022 08:09:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
06/17/2022 08:09:07 - INFO - __main__ - Global step 2450 Train loss 0.25 Classification-F1 0.6755386565272496 on epoch=153
06/17/2022 08:09:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.28 on epoch=153
06/17/2022 08:09:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=154
06/17/2022 08:09:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=154
06/17/2022 08:09:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.31 on epoch=155
06/17/2022 08:09:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=156
06/17/2022 08:09:23 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.6411085948452497 on epoch=156
06/17/2022 08:09:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=156
06/17/2022 08:09:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=157
06/17/2022 08:09:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=158
06/17/2022 08:09:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=158
06/17/2022 08:09:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=159
06/17/2022 08:09:40 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.6263921960211999 on epoch=159
06/17/2022 08:09:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=159
06/17/2022 08:09:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=160
06/17/2022 08:09:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.24 on epoch=161
06/17/2022 08:09:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
06/17/2022 08:09:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=162
06/17/2022 08:09:57 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.6580304442373408 on epoch=162
06/17/2022 08:09:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=163
06/17/2022 08:10:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.24 on epoch=163
06/17/2022 08:10:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=164
06/17/2022 08:10:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
06/17/2022 08:10:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.27 on epoch=165
06/17/2022 08:10:13 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.6083333333333334 on epoch=165
06/17/2022 08:10:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=166
06/17/2022 08:10:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.23 on epoch=166
06/17/2022 08:10:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.23 on epoch=167
06/17/2022 08:10:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
06/17/2022 08:10:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=168
06/17/2022 08:10:30 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.5949829557501916 on epoch=168
06/17/2022 08:10:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
06/17/2022 08:10:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=169
06/17/2022 08:10:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
06/17/2022 08:10:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=171
06/17/2022 08:10:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
06/17/2022 08:10:47 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.6200355564254376 on epoch=171
06/17/2022 08:10:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
06/17/2022 08:10:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=173
06/17/2022 08:10:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=173
06/17/2022 08:10:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=174
06/17/2022 08:11:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
06/17/2022 08:11:04 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.6552188552188551 on epoch=174
06/17/2022 08:11:06 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=175
06/17/2022 08:11:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
06/17/2022 08:11:11 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
06/17/2022 08:11:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=177
06/17/2022 08:11:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=178
06/17/2022 08:11:20 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.6851749369734981 on epoch=178
06/17/2022 08:11:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=178
06/17/2022 08:11:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=179
06/17/2022 08:11:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
06/17/2022 08:11:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=180
06/17/2022 08:11:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
06/17/2022 08:11:36 - INFO - __main__ - Global step 2900 Train loss 0.16 Classification-F1 0.670956252419667 on epoch=181
06/17/2022 08:11:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
06/17/2022 08:11:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.16 on epoch=182
06/17/2022 08:11:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
06/17/2022 08:11:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=183
06/17/2022 08:11:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=184
06/17/2022 08:11:53 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.6425623321825853 on epoch=184
06/17/2022 08:11:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.17 on epoch=184
06/17/2022 08:11:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
06/17/2022 08:12:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
06/17/2022 08:12:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.20 on epoch=186
06/17/2022 08:12:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
06/17/2022 08:12:07 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:12:07 - INFO - __main__ - Printing 3 examples
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:12:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:12:07 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:12:07 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:12:07 - INFO - __main__ - Printing 3 examples
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:12:07 - INFO - __main__ - ['false']
06/17/2022 08:12:07 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:12:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:12:07 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:12:10 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.6514449186898588 on epoch=187
06/17/2022 08:12:10 - INFO - __main__ - save last model!
06/17/2022 08:12:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 08:12:10 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 08:12:10 - INFO - __main__ - Printing 3 examples
06/17/2022 08:12:10 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 08:12:10 - INFO - __main__ - ['false']
06/17/2022 08:12:10 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 08:12:10 - INFO - __main__ - ['false']
06/17/2022 08:12:10 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 08:12:10 - INFO - __main__ - ['false']
06/17/2022 08:12:10 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:12:11 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:12:14 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 08:12:23 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:12:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:12:24 - INFO - __main__ - Starting training!
06/17/2022 08:13:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_100_0.2_8_predictions.txt
06/17/2022 08:13:00 - INFO - __main__ - Classification-F1 on test data: 0.5240
06/17/2022 08:13:01 - INFO - __main__ - prefix=wiki_qa_128_100, lr=0.2, bsz=8, dev_performance=0.6943985307621672, test_performance=0.5239674267954685
06/17/2022 08:13:01 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.5, bsz=8 ...
06/17/2022 08:13:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:13:02 - INFO - __main__ - Printing 3 examples
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:13:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:13:02 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:13:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:13:02 - INFO - __main__ - Printing 3 examples
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:13:02 - INFO - __main__ - ['false']
06/17/2022 08:13:02 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:13:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:13:03 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:13:21 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:13:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:13:22 - INFO - __main__ - Starting training!
06/17/2022 08:13:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.68 on epoch=0
06/17/2022 08:13:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
06/17/2022 08:13:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=1
06/17/2022 08:13:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=2
06/17/2022 08:13:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 08:13:40 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 08:13:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 08:13:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=3
06/17/2022 08:13:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
06/17/2022 08:13:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=4
06/17/2022 08:13:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=5
06/17/2022 08:13:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=6
06/17/2022 08:13:57 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.34673046251993617 on epoch=6
06/17/2022 08:13:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34673046251993617 on epoch=6, global_step=100
06/17/2022 08:14:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=6
06/17/2022 08:14:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=7
06/17/2022 08:14:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=8
06/17/2022 08:14:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
06/17/2022 08:14:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
06/17/2022 08:14:15 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 08:14:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
06/17/2022 08:14:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
06/17/2022 08:14:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
06/17/2022 08:14:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
06/17/2022 08:14:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=12
06/17/2022 08:14:31 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3845121610287951 on epoch=12
06/17/2022 08:14:32 - INFO - __main__ - Saving model with best Classification-F1: 0.34673046251993617 -> 0.3845121610287951 on epoch=12, global_step=200
06/17/2022 08:14:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=13
06/17/2022 08:14:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 08:14:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
06/17/2022 08:14:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
06/17/2022 08:14:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
06/17/2022 08:14:48 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 08:14:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
06/17/2022 08:14:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
06/17/2022 08:14:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
06/17/2022 08:14:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
06/17/2022 08:15:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
06/17/2022 08:15:04 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 08:15:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=19
06/17/2022 08:15:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=19
06/17/2022 08:15:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=20
06/17/2022 08:15:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=21
06/17/2022 08:15:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 08:15:21 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.475475261460616 on epoch=21
06/17/2022 08:15:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3845121610287951 -> 0.475475261460616 on epoch=21, global_step=350
06/17/2022 08:15:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=22
06/17/2022 08:15:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
06/17/2022 08:15:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
06/17/2022 08:15:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=24
06/17/2022 08:15:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 08:15:38 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3771289537712896 on epoch=24
06/17/2022 08:15:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=25
06/17/2022 08:15:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=26
06/17/2022 08:15:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
06/17/2022 08:15:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 08:15:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
06/17/2022 08:15:54 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.5086044299043582 on epoch=28
06/17/2022 08:15:54 - INFO - __main__ - Saving model with best Classification-F1: 0.475475261460616 -> 0.5086044299043582 on epoch=28, global_step=450
06/17/2022 08:15:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
06/17/2022 08:15:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/17/2022 08:16:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
06/17/2022 08:16:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
06/17/2022 08:16:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
06/17/2022 08:16:11 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.47162022703818374 on epoch=31
06/17/2022 08:16:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
06/17/2022 08:16:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
06/17/2022 08:16:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
06/17/2022 08:16:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
06/17/2022 08:16:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=34
06/17/2022 08:16:27 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 08:16:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=34
06/17/2022 08:16:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
06/17/2022 08:16:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
06/17/2022 08:16:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=36
06/17/2022 08:16:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
06/17/2022 08:16:44 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.35501021683496337 on epoch=37
06/17/2022 08:16:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
06/17/2022 08:16:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/17/2022 08:16:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
06/17/2022 08:16:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=39
06/17/2022 08:16:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
06/17/2022 08:17:00 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.48328062975173247 on epoch=40
06/17/2022 08:17:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
06/17/2022 08:17:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
06/17/2022 08:17:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
06/17/2022 08:17:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
06/17/2022 08:17:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/17/2022 08:17:17 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 08:17:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
06/17/2022 08:17:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 08:17:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=45
06/17/2022 08:17:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
06/17/2022 08:17:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
06/17/2022 08:17:33 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.5320070665949765 on epoch=46
06/17/2022 08:17:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5086044299043582 -> 0.5320070665949765 on epoch=46, global_step=750
06/17/2022 08:17:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
06/17/2022 08:17:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
06/17/2022 08:17:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
06/17/2022 08:17:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
06/17/2022 08:17:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
06/17/2022 08:17:50 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 08:17:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/17/2022 08:17:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=51
06/17/2022 08:17:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/17/2022 08:18:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
06/17/2022 08:18:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
06/17/2022 08:18:07 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.45341347447743785 on epoch=53
06/17/2022 08:18:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=53
06/17/2022 08:18:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/17/2022 08:18:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
06/17/2022 08:18:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
06/17/2022 08:18:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=56
06/17/2022 08:18:24 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.5317565436770735 on epoch=56
06/17/2022 08:18:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
06/17/2022 08:18:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 08:18:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=58
06/17/2022 08:18:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
06/17/2022 08:18:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
06/17/2022 08:18:41 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3383422492035824 on epoch=59
06/17/2022 08:18:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
06/17/2022 08:18:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=60
06/17/2022 08:18:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=61
06/17/2022 08:18:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/17/2022 08:18:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/17/2022 08:18:58 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.4059668508287293 on epoch=62
06/17/2022 08:19:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
06/17/2022 08:19:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/17/2022 08:19:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/17/2022 08:19:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
06/17/2022 08:19:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
06/17/2022 08:19:15 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.519825153724454 on epoch=65
06/17/2022 08:19:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
06/17/2022 08:19:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=66
06/17/2022 08:19:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
06/17/2022 08:19:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.33 on epoch=68
06/17/2022 08:19:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=68
06/17/2022 08:19:32 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.42025542025542023 on epoch=68
06/17/2022 08:19:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 08:19:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=69
06/17/2022 08:19:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
06/17/2022 08:19:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
06/17/2022 08:19:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
06/17/2022 08:19:50 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.5146126185027787 on epoch=71
06/17/2022 08:19:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
06/17/2022 08:19:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
06/17/2022 08:19:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=73
06/17/2022 08:20:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
06/17/2022 08:20:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
06/17/2022 08:20:09 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.36318407960199 on epoch=74
06/17/2022 08:20:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
06/17/2022 08:20:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
06/17/2022 08:20:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
06/17/2022 08:20:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=77
06/17/2022 08:20:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=78
06/17/2022 08:20:26 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.4402096322399927 on epoch=78
06/17/2022 08:20:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=78
06/17/2022 08:20:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=79
06/17/2022 08:20:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=79
06/17/2022 08:20:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/17/2022 08:20:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=81
06/17/2022 08:20:44 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.3732922688146569 on epoch=81
06/17/2022 08:20:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
06/17/2022 08:20:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
06/17/2022 08:20:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=83
06/17/2022 08:20:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=83
06/17/2022 08:20:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
06/17/2022 08:21:01 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.45341347447743785 on epoch=84
06/17/2022 08:21:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=84
06/17/2022 08:21:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
06/17/2022 08:21:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=86
06/17/2022 08:21:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
06/17/2022 08:21:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
06/17/2022 08:21:18 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.628900587472343 on epoch=87
06/17/2022 08:21:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5320070665949765 -> 0.628900587472343 on epoch=87, global_step=1400
06/17/2022 08:21:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=88
06/17/2022 08:21:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=88
06/17/2022 08:21:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=89
06/17/2022 08:21:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=89
06/17/2022 08:21:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
06/17/2022 08:21:36 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.6287646349468028 on epoch=90
06/17/2022 08:21:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
06/17/2022 08:21:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=91
06/17/2022 08:21:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
06/17/2022 08:21:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=93
06/17/2022 08:21:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=93
06/17/2022 08:21:53 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.5263405772724215 on epoch=93
06/17/2022 08:21:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
06/17/2022 08:21:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=94
06/17/2022 08:22:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=95
06/17/2022 08:22:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=96
06/17/2022 08:22:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
06/17/2022 08:22:11 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.5513344794778923 on epoch=96
06/17/2022 08:22:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=97
06/17/2022 08:22:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=98
06/17/2022 08:22:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
06/17/2022 08:22:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=99
06/17/2022 08:22:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=99
06/17/2022 08:22:28 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.5713159968479118 on epoch=99
06/17/2022 08:22:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=100
06/17/2022 08:22:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=101
06/17/2022 08:22:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=101
06/17/2022 08:22:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=102
06/17/2022 08:22:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
06/17/2022 08:22:46 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.5833333333333334 on epoch=103
06/17/2022 08:22:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=103
06/17/2022 08:22:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=104
06/17/2022 08:22:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=104
06/17/2022 08:22:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.29 on epoch=105
06/17/2022 08:22:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
06/17/2022 08:23:03 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.6627450980392157 on epoch=106
06/17/2022 08:23:03 - INFO - __main__ - Saving model with best Classification-F1: 0.628900587472343 -> 0.6627450980392157 on epoch=106, global_step=1700
06/17/2022 08:23:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=106
06/17/2022 08:23:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=107
06/17/2022 08:23:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=108
06/17/2022 08:23:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=108
06/17/2022 08:23:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=109
06/17/2022 08:23:21 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.6847290640394088 on epoch=109
06/17/2022 08:23:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6627450980392157 -> 0.6847290640394088 on epoch=109, global_step=1750
06/17/2022 08:23:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
06/17/2022 08:23:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=110
06/17/2022 08:23:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
06/17/2022 08:23:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=111
06/17/2022 08:23:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=112
06/17/2022 08:23:38 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.6351734398246026 on epoch=112
06/17/2022 08:23:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=113
06/17/2022 08:23:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=113
06/17/2022 08:23:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=114
06/17/2022 08:23:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=114
06/17/2022 08:23:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=115
06/17/2022 08:23:56 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.5028417695583232 on epoch=115
06/17/2022 08:23:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=116
06/17/2022 08:24:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=116
06/17/2022 08:24:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=117
06/17/2022 08:24:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
06/17/2022 08:24:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=118
06/17/2022 08:24:14 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.5245304177179704 on epoch=118
06/17/2022 08:24:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
06/17/2022 08:24:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
06/17/2022 08:24:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=120
06/17/2022 08:24:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=121
06/17/2022 08:24:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=121
06/17/2022 08:24:32 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.703052503052503 on epoch=121
06/17/2022 08:24:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6847290640394088 -> 0.703052503052503 on epoch=121, global_step=1950
06/17/2022 08:24:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=122
06/17/2022 08:24:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
06/17/2022 08:24:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=123
06/17/2022 08:24:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=124
06/17/2022 08:24:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
06/17/2022 08:24:50 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7181306581844874 on epoch=124
06/17/2022 08:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.703052503052503 -> 0.7181306581844874 on epoch=124, global_step=2000
06/17/2022 08:24:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=125
06/17/2022 08:24:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
06/17/2022 08:24:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=126
06/17/2022 08:25:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
06/17/2022 08:25:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
06/17/2022 08:25:08 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.6658448150833938 on epoch=128
06/17/2022 08:25:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=128
06/17/2022 08:25:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=129
06/17/2022 08:25:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=129
06/17/2022 08:25:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=130
06/17/2022 08:25:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
06/17/2022 08:25:26 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.6868137824235385 on epoch=131
06/17/2022 08:25:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=131
06/17/2022 08:25:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=132
06/17/2022 08:25:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=133
06/17/2022 08:25:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=133
06/17/2022 08:25:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
06/17/2022 08:25:43 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.6855281367106716 on epoch=134
06/17/2022 08:25:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=134
06/17/2022 08:25:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
06/17/2022 08:25:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
06/17/2022 08:25:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
06/17/2022 08:25:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
06/17/2022 08:26:01 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.6827757125154895 on epoch=137
06/17/2022 08:26:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=138
06/17/2022 08:26:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
06/17/2022 08:26:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=139
06/17/2022 08:26:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=139
06/17/2022 08:26:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=140
06/17/2022 08:26:19 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.5565845209308706 on epoch=140
06/17/2022 08:26:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
06/17/2022 08:26:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=141
06/17/2022 08:26:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
06/17/2022 08:26:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=143
06/17/2022 08:26:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
06/17/2022 08:26:36 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.5269040997577824 on epoch=143
06/17/2022 08:26:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
06/17/2022 08:26:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=144
06/17/2022 08:26:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
06/17/2022 08:26:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
06/17/2022 08:26:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=146
06/17/2022 08:26:54 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.6761133603238867 on epoch=146
06/17/2022 08:26:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
06/17/2022 08:27:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
06/17/2022 08:27:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=148
06/17/2022 08:27:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
06/17/2022 08:27:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
06/17/2022 08:27:12 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.7050464705430525 on epoch=149
06/17/2022 08:27:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
06/17/2022 08:27:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
06/17/2022 08:27:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
06/17/2022 08:27:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=152
06/17/2022 08:27:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
06/17/2022 08:27:30 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.6948468732807629 on epoch=153
06/17/2022 08:27:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
06/17/2022 08:27:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
06/17/2022 08:27:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
06/17/2022 08:27:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=155
06/17/2022 08:27:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
06/17/2022 08:27:48 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.623406341941811 on epoch=156
06/17/2022 08:27:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
06/17/2022 08:27:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
06/17/2022 08:27:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
06/17/2022 08:27:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=158
06/17/2022 08:28:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
06/17/2022 08:28:05 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6577540106951872 on epoch=159
06/17/2022 08:28:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=159
06/17/2022 08:28:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
06/17/2022 08:28:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=161
06/17/2022 08:28:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=161
06/17/2022 08:28:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=162
06/17/2022 08:28:23 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.7108669108669109 on epoch=162
06/17/2022 08:28:26 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
06/17/2022 08:28:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
06/17/2022 08:28:31 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
06/17/2022 08:28:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
06/17/2022 08:28:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
06/17/2022 08:28:41 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.598300098071265 on epoch=165
06/17/2022 08:28:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
06/17/2022 08:28:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=166
06/17/2022 08:28:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
06/17/2022 08:28:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=168
06/17/2022 08:28:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
06/17/2022 08:28:58 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.5993339993339992 on epoch=168
06/17/2022 08:29:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
06/17/2022 08:29:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=169
06/17/2022 08:29:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
06/17/2022 08:29:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
06/17/2022 08:29:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
06/17/2022 08:29:16 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.6556556556556556 on epoch=171
06/17/2022 08:29:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
06/17/2022 08:29:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=173
06/17/2022 08:29:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
06/17/2022 08:29:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
06/17/2022 08:29:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
06/17/2022 08:29:34 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7031068790819752 on epoch=174
06/17/2022 08:29:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
06/17/2022 08:29:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
06/17/2022 08:29:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
06/17/2022 08:29:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
06/17/2022 08:29:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=178
06/17/2022 08:29:51 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6912884859031307 on epoch=178
06/17/2022 08:29:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
06/17/2022 08:29:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
06/17/2022 08:29:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=179
06/17/2022 08:30:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
06/17/2022 08:30:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
06/17/2022 08:30:09 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.6200355564254376 on epoch=181
06/17/2022 08:30:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/17/2022 08:30:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
06/17/2022 08:30:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/17/2022 08:30:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=183
06/17/2022 08:30:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=184
06/17/2022 08:30:27 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7146303846564968 on epoch=184
06/17/2022 08:30:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
06/17/2022 08:30:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
06/17/2022 08:30:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.17 on epoch=186
06/17/2022 08:30:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
06/17/2022 08:30:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
06/17/2022 08:30:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:30:41 - INFO - __main__ - Printing 3 examples
06/17/2022 08:30:41 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:30:41 - INFO - __main__ - ['false']
06/17/2022 08:30:41 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:30:41 - INFO - __main__ - ['false']
06/17/2022 08:30:41 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:30:41 - INFO - __main__ - ['false']
06/17/2022 08:30:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:30:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:30:42 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:30:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:30:42 - INFO - __main__ - Printing 3 examples
06/17/2022 08:30:42 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:30:42 - INFO - __main__ - ['false']
06/17/2022 08:30:42 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:30:42 - INFO - __main__ - ['false']
06/17/2022 08:30:42 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:30:42 - INFO - __main__ - ['false']
06/17/2022 08:30:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:30:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:30:42 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:30:45 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.7032228147461164 on epoch=187
06/17/2022 08:30:45 - INFO - __main__ - save last model!
06/17/2022 08:30:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 08:30:45 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 08:30:45 - INFO - __main__ - Printing 3 examples
06/17/2022 08:30:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 08:30:45 - INFO - __main__ - ['false']
06/17/2022 08:30:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 08:30:45 - INFO - __main__ - ['false']
06/17/2022 08:30:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 08:30:45 - INFO - __main__ - ['false']
06/17/2022 08:30:45 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:30:46 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:30:49 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 08:31:01 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:31:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:31:02 - INFO - __main__ - Starting training!
06/17/2022 08:31:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.5_8_predictions.txt
06/17/2022 08:31:39 - INFO - __main__ - Classification-F1 on test data: 0.4735
06/17/2022 08:31:40 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.5, bsz=8, dev_performance=0.7181306581844874, test_performance=0.47346500159530197
06/17/2022 08:31:40 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.4, bsz=8 ...
06/17/2022 08:31:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:31:41 - INFO - __main__ - Printing 3 examples
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:31:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:31:41 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:31:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:31:41 - INFO - __main__ - Printing 3 examples
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:31:41 - INFO - __main__ - ['false']
06/17/2022 08:31:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:31:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:31:42 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:32:00 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:32:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:32:01 - INFO - __main__ - Starting training!
06/17/2022 08:32:04 - INFO - __main__ - Step 10 Global step 10 Train loss 0.78 on epoch=0
06/17/2022 08:32:07 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=1
06/17/2022 08:32:09 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
06/17/2022 08:32:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=2
06/17/2022 08:32:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 08:32:19 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 08:32:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 08:32:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=3
06/17/2022 08:32:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=4
06/17/2022 08:32:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
06/17/2022 08:32:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
06/17/2022 08:32:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=6
06/17/2022 08:32:38 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 08:32:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=6
06/17/2022 08:32:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=7
06/17/2022 08:32:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
06/17/2022 08:32:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=8
06/17/2022 08:32:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
06/17/2022 08:32:55 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.4376260443676175 on epoch=9
06/17/2022 08:32:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4376260443676175 on epoch=9, global_step=150
06/17/2022 08:32:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
06/17/2022 08:33:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=10
06/17/2022 08:33:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
06/17/2022 08:33:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=11
06/17/2022 08:33:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=12
06/17/2022 08:33:12 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 08:33:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
06/17/2022 08:33:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 08:33:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=14
06/17/2022 08:33:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
06/17/2022 08:33:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
06/17/2022 08:33:30 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3486005089058525 on epoch=15
06/17/2022 08:33:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
06/17/2022 08:33:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=16
06/17/2022 08:33:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
06/17/2022 08:33:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=18
06/17/2022 08:33:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
06/17/2022 08:33:48 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 08:33:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
06/17/2022 08:33:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
06/17/2022 08:33:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
06/17/2022 08:33:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.94 on epoch=21
06/17/2022 08:34:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
06/17/2022 08:34:05 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.49703629703629704 on epoch=21
06/17/2022 08:34:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4376260443676175 -> 0.49703629703629704 on epoch=21, global_step=350
06/17/2022 08:34:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
06/17/2022 08:34:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=23
06/17/2022 08:34:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=23
06/17/2022 08:34:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=24
06/17/2022 08:34:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
06/17/2022 08:34:23 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.41013824884792627 on epoch=24
06/17/2022 08:34:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=25
06/17/2022 08:34:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
06/17/2022 08:34:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=26
06/17/2022 08:34:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
06/17/2022 08:34:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=28
06/17/2022 08:34:40 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.35885876860812244 on epoch=28
06/17/2022 08:34:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=28
06/17/2022 08:34:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
06/17/2022 08:34:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
06/17/2022 08:34:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
06/17/2022 08:34:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
06/17/2022 08:34:57 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=31
06/17/2022 08:35:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
06/17/2022 08:35:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=32
06/17/2022 08:35:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
06/17/2022 08:35:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
06/17/2022 08:35:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=34
06/17/2022 08:35:14 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.39581271412257324 on epoch=34
06/17/2022 08:35:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=34
06/17/2022 08:35:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
06/17/2022 08:35:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
06/17/2022 08:35:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
06/17/2022 08:35:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=37
06/17/2022 08:35:31 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.35693779904306216 on epoch=37
06/17/2022 08:35:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
06/17/2022 08:35:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 08:35:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=39
06/17/2022 08:35:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
06/17/2022 08:35:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/17/2022 08:35:48 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.35693779904306216 on epoch=40
06/17/2022 08:35:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=41
06/17/2022 08:35:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
06/17/2022 08:35:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
06/17/2022 08:35:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/17/2022 08:36:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=43
06/17/2022 08:36:04 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.5294038283160469 on epoch=43
06/17/2022 08:36:04 - INFO - __main__ - Saving model with best Classification-F1: 0.49703629703629704 -> 0.5294038283160469 on epoch=43, global_step=700
06/17/2022 08:36:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
06/17/2022 08:36:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
06/17/2022 08:36:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=45
06/17/2022 08:36:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=46
06/17/2022 08:36:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
06/17/2022 08:36:21 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.4573099415204678 on epoch=46
06/17/2022 08:36:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=47
06/17/2022 08:36:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
06/17/2022 08:36:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
06/17/2022 08:36:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
06/17/2022 08:36:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/17/2022 08:36:39 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.44622552554218203 on epoch=49
06/17/2022 08:36:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/17/2022 08:36:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
06/17/2022 08:36:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 08:36:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
06/17/2022 08:36:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=53
06/17/2022 08:36:56 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.48459743082651846 on epoch=53
06/17/2022 08:36:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=53
06/17/2022 08:37:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
06/17/2022 08:37:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 08:37:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=55
06/17/2022 08:37:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 08:37:13 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.5205373288555928 on epoch=56
06/17/2022 08:37:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
06/17/2022 08:37:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 08:37:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
06/17/2022 08:37:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/17/2022 08:37:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
06/17/2022 08:37:30 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.36516753625488524 on epoch=59
06/17/2022 08:37:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/17/2022 08:37:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/17/2022 08:37:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/17/2022 08:37:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/17/2022 08:37:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=62
06/17/2022 08:37:47 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.350463149416029 on epoch=62
06/17/2022 08:37:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 08:37:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/17/2022 08:37:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=64
06/17/2022 08:37:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/17/2022 08:38:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=65
06/17/2022 08:38:04 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.3813144709696433 on epoch=65
06/17/2022 08:38:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=66
06/17/2022 08:38:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
06/17/2022 08:38:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
06/17/2022 08:38:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 08:38:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 08:38:21 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.5744840996048421 on epoch=68
06/17/2022 08:38:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5294038283160469 -> 0.5744840996048421 on epoch=68, global_step=1100
06/17/2022 08:38:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=69
06/17/2022 08:38:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
06/17/2022 08:38:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
06/17/2022 08:38:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=71
06/17/2022 08:38:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
06/17/2022 08:38:39 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.5457785733352176 on epoch=71
06/17/2022 08:38:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/17/2022 08:38:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 08:38:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=73
06/17/2022 08:38:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
06/17/2022 08:38:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=74
06/17/2022 08:38:56 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3712545436683367 on epoch=74
06/17/2022 08:38:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=75
06/17/2022 08:39:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
06/17/2022 08:39:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/17/2022 08:39:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
06/17/2022 08:39:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
06/17/2022 08:39:12 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.569023569023569 on epoch=78
06/17/2022 08:39:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
06/17/2022 08:39:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=79
06/17/2022 08:39:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
06/17/2022 08:39:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/17/2022 08:39:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=81
06/17/2022 08:39:29 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.39267460026616785 on epoch=81
06/17/2022 08:39:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=81
06/17/2022 08:39:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=82
06/17/2022 08:39:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/17/2022 08:39:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
06/17/2022 08:39:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=84
06/17/2022 08:39:46 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.4155251141552511 on epoch=84
06/17/2022 08:39:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
06/17/2022 08:39:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=85
06/17/2022 08:39:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=86
06/17/2022 08:39:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
06/17/2022 08:39:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=87
06/17/2022 08:40:03 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.3732922688146569 on epoch=87
06/17/2022 08:40:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=88
06/17/2022 08:40:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
06/17/2022 08:40:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=89
06/17/2022 08:40:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=89
06/17/2022 08:40:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
06/17/2022 08:40:19 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.5186409907281333 on epoch=90
06/17/2022 08:40:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 08:40:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
06/17/2022 08:40:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=92
06/17/2022 08:40:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
06/17/2022 08:40:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=93
06/17/2022 08:40:36 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.43748132262204015 on epoch=93
06/17/2022 08:40:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=94
06/17/2022 08:40:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
06/17/2022 08:40:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=95
06/17/2022 08:40:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
06/17/2022 08:40:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=96
06/17/2022 08:40:53 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.552024777390631 on epoch=96
06/17/2022 08:40:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=97
06/17/2022 08:40:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=98
06/17/2022 08:41:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=98
06/17/2022 08:41:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
06/17/2022 08:41:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
06/17/2022 08:41:10 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.5740697547853401 on epoch=99
06/17/2022 08:41:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
06/17/2022 08:41:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
06/17/2022 08:41:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
06/17/2022 08:41:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=102
06/17/2022 08:41:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=103
06/17/2022 08:41:27 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.579199631308088 on epoch=103
06/17/2022 08:41:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5744840996048421 -> 0.579199631308088 on epoch=103, global_step=1650
06/17/2022 08:41:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=103
06/17/2022 08:41:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
06/17/2022 08:41:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=104
06/17/2022 08:41:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=105
06/17/2022 08:41:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=106
06/17/2022 08:41:44 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.545424005375054 on epoch=106
06/17/2022 08:41:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
06/17/2022 08:41:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=107
06/17/2022 08:41:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=108
06/17/2022 08:41:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=108
06/17/2022 08:41:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=109
06/17/2022 08:42:01 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.3401530406766009 on epoch=109
06/17/2022 08:42:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=109
06/17/2022 08:42:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=110
06/17/2022 08:42:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=111
06/17/2022 08:42:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=111
06/17/2022 08:42:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
06/17/2022 08:42:18 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=112
06/17/2022 08:42:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=113
06/17/2022 08:42:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=113
06/17/2022 08:42:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=114
06/17/2022 08:42:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=114
06/17/2022 08:42:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
06/17/2022 08:42:34 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.5141265604024257 on epoch=115
06/17/2022 08:42:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=116
06/17/2022 08:42:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=116
06/17/2022 08:42:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=117
06/17/2022 08:42:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=118
06/17/2022 08:42:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=118
06/17/2022 08:42:51 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.3383422492035824 on epoch=118
06/17/2022 08:42:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
06/17/2022 08:42:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=119
06/17/2022 08:42:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=120
06/17/2022 08:43:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
06/17/2022 08:43:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=121
06/17/2022 08:43:08 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.566081817765358 on epoch=121
06/17/2022 08:43:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=122
06/17/2022 08:43:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=123
06/17/2022 08:43:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=123
06/17/2022 08:43:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=124
06/17/2022 08:43:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=124
06/17/2022 08:43:25 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.3813144709696433 on epoch=124
06/17/2022 08:43:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=125
06/17/2022 08:43:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.40 on epoch=126
06/17/2022 08:43:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=126
06/17/2022 08:43:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=127
06/17/2022 08:43:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=128
06/17/2022 08:43:42 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.477297895902547 on epoch=128
06/17/2022 08:43:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=128
06/17/2022 08:43:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=129
06/17/2022 08:43:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=129
06/17/2022 08:43:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=130
06/17/2022 08:43:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.38 on epoch=131
06/17/2022 08:43:59 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.39047619047619053 on epoch=131
06/17/2022 08:44:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=131
06/17/2022 08:44:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=132
06/17/2022 08:44:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=133
06/17/2022 08:44:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=133
06/17/2022 08:44:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=134
06/17/2022 08:44:16 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=134
06/17/2022 08:44:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=134
06/17/2022 08:44:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=135
06/17/2022 08:44:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=136
06/17/2022 08:44:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=136
06/17/2022 08:44:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.35 on epoch=137
06/17/2022 08:44:32 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.3980615931639617 on epoch=137
06/17/2022 08:44:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=138
06/17/2022 08:44:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=138
06/17/2022 08:44:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=139
06/17/2022 08:44:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.38 on epoch=139
06/17/2022 08:44:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
06/17/2022 08:44:49 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.5378277037974293 on epoch=140
06/17/2022 08:44:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=141
06/17/2022 08:44:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=141
06/17/2022 08:44:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=142
06/17/2022 08:45:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=143
06/17/2022 08:45:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=143
06/17/2022 08:45:06 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=143
06/17/2022 08:45:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=144
06/17/2022 08:45:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=144
06/17/2022 08:45:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=145
06/17/2022 08:45:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
06/17/2022 08:45:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=146
06/17/2022 08:45:24 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.5870644428783963 on epoch=146
06/17/2022 08:45:24 - INFO - __main__ - Saving model with best Classification-F1: 0.579199631308088 -> 0.5870644428783963 on epoch=146, global_step=2350
06/17/2022 08:45:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=147
06/17/2022 08:45:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=148
06/17/2022 08:45:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
06/17/2022 08:45:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=149
06/17/2022 08:45:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=149
06/17/2022 08:45:41 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=149
06/17/2022 08:45:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=150
06/17/2022 08:45:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=151
06/17/2022 08:45:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=151
06/17/2022 08:45:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
06/17/2022 08:45:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=153
06/17/2022 08:45:58 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.39047619047619053 on epoch=153
06/17/2022 08:46:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=153
06/17/2022 08:46:03 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=154
06/17/2022 08:46:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=154
06/17/2022 08:46:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=155
06/17/2022 08:46:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.39 on epoch=156
06/17/2022 08:46:15 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.5943821434614734 on epoch=156
06/17/2022 08:46:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5870644428783963 -> 0.5943821434614734 on epoch=156, global_step=2500
06/17/2022 08:46:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
06/17/2022 08:46:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=157
06/17/2022 08:46:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=158
06/17/2022 08:46:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=158
06/17/2022 08:46:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.34 on epoch=159
06/17/2022 08:46:32 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.4025498100179374 on epoch=159
06/17/2022 08:46:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.34 on epoch=159
06/17/2022 08:46:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=160
06/17/2022 08:46:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.34 on epoch=161
06/17/2022 08:46:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=161
06/17/2022 08:46:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=162
06/17/2022 08:46:49 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.4417668437237909 on epoch=162
06/17/2022 08:46:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=163
06/17/2022 08:46:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=163
06/17/2022 08:46:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=164
06/17/2022 08:46:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=164
06/17/2022 08:47:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=165
06/17/2022 08:47:06 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.5673382820784729 on epoch=165
06/17/2022 08:47:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=166
06/17/2022 08:47:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=166
06/17/2022 08:47:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=167
06/17/2022 08:47:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=168
06/17/2022 08:47:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.33 on epoch=168
06/17/2022 08:47:23 - INFO - __main__ - Global step 2700 Train loss 0.35 Classification-F1 0.35693779904306216 on epoch=168
06/17/2022 08:47:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=169
06/17/2022 08:47:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.32 on epoch=169
06/17/2022 08:47:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=170
06/17/2022 08:47:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.35 on epoch=171
06/17/2022 08:47:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=171
06/17/2022 08:47:41 - INFO - __main__ - Global step 2750 Train loss 0.35 Classification-F1 0.587571921749137 on epoch=171
06/17/2022 08:47:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=172
06/17/2022 08:47:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=173
06/17/2022 08:47:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=173
06/17/2022 08:47:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.37 on epoch=174
06/17/2022 08:47:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=174
06/17/2022 08:47:58 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.35885876860812244 on epoch=174
06/17/2022 08:48:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=175
06/17/2022 08:48:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.32 on epoch=176
06/17/2022 08:48:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=176
06/17/2022 08:48:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.35 on epoch=177
06/17/2022 08:48:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.32 on epoch=178
06/17/2022 08:48:15 - INFO - __main__ - Global step 2850 Train loss 0.35 Classification-F1 0.4950166112956811 on epoch=178
06/17/2022 08:48:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.35 on epoch=178
06/17/2022 08:48:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.36 on epoch=179
06/17/2022 08:48:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.37 on epoch=179
06/17/2022 08:48:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=180
06/17/2022 08:48:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.36 on epoch=181
06/17/2022 08:48:32 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.5324675324675325 on epoch=181
06/17/2022 08:48:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.37 on epoch=181
06/17/2022 08:48:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=182
06/17/2022 08:48:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=183
06/17/2022 08:48:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=183
06/17/2022 08:48:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=184
06/17/2022 08:48:48 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.5140859140859141 on epoch=184
06/17/2022 08:48:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.37 on epoch=184
06/17/2022 08:48:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=185
06/17/2022 08:48:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.35 on epoch=186
06/17/2022 08:48:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=186
06/17/2022 08:49:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.35 on epoch=187
06/17/2022 08:49:03 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:49:03 - INFO - __main__ - Printing 3 examples
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:49:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:49:03 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:49:03 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:49:03 - INFO - __main__ - Printing 3 examples
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:49:03 - INFO - __main__ - ['false']
06/17/2022 08:49:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:49:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:49:04 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:49:05 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.5549917782475922 on epoch=187
06/17/2022 08:49:05 - INFO - __main__ - save last model!
06/17/2022 08:49:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 08:49:05 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 08:49:05 - INFO - __main__ - Printing 3 examples
06/17/2022 08:49:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 08:49:05 - INFO - __main__ - ['false']
06/17/2022 08:49:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 08:49:05 - INFO - __main__ - ['false']
06/17/2022 08:49:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 08:49:05 - INFO - __main__ - ['false']
06/17/2022 08:49:05 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:49:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:49:09 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 08:49:20 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:49:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:49:21 - INFO - __main__ - Starting training!
06/17/2022 08:49:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.4_8_predictions.txt
06/17/2022 08:49:48 - INFO - __main__ - Classification-F1 on test data: 0.3518
06/17/2022 08:49:49 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.4, bsz=8, dev_performance=0.5943821434614734, test_performance=0.35183163874561213
06/17/2022 08:49:49 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.3, bsz=8 ...
06/17/2022 08:49:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:49:50 - INFO - __main__ - Printing 3 examples
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:49:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:49:50 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 08:49:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 08:49:50 - INFO - __main__ - Printing 3 examples
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 08:49:50 - INFO - __main__ - ['false']
06/17/2022 08:49:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 08:49:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 08:49:50 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 08:50:05 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 08:50:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 08:50:06 - INFO - __main__ - Starting training!
06/17/2022 08:50:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.80 on epoch=0
06/17/2022 08:50:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=1
06/17/2022 08:50:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=1
06/17/2022 08:50:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
06/17/2022 08:50:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 08:50:24 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 08:50:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 08:50:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=3
06/17/2022 08:50:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=4
06/17/2022 08:50:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=4
06/17/2022 08:50:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
06/17/2022 08:50:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=6
06/17/2022 08:50:41 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 08:50:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=6
06/17/2022 08:50:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=7
06/17/2022 08:50:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=8
06/17/2022 08:50:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=8
06/17/2022 08:50:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=9
06/17/2022 08:50:58 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 08:51:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=9
06/17/2022 08:51:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
06/17/2022 08:51:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
06/17/2022 08:51:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
06/17/2022 08:51:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
06/17/2022 08:51:15 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.4292397660818714 on epoch=12
06/17/2022 08:51:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4292397660818714 on epoch=12, global_step=200
06/17/2022 08:51:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
06/17/2022 08:51:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
06/17/2022 08:51:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=14
06/17/2022 08:51:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=14
06/17/2022 08:51:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=15
06/17/2022 08:51:32 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3849492366116407 on epoch=15
06/17/2022 08:51:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=16
06/17/2022 08:51:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=16
06/17/2022 08:51:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
06/17/2022 08:51:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=18
06/17/2022 08:51:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
06/17/2022 08:51:49 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 08:51:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
06/17/2022 08:51:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
06/17/2022 08:51:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
06/17/2022 08:51:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
06/17/2022 08:52:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
06/17/2022 08:52:06 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=21
06/17/2022 08:52:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
06/17/2022 08:52:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=23
06/17/2022 08:52:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=23
06/17/2022 08:52:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
06/17/2022 08:52:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 08:52:23 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 08:52:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
06/17/2022 08:52:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
06/17/2022 08:52:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=26
06/17/2022 08:52:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
06/17/2022 08:52:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
06/17/2022 08:52:40 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.5471256941560139 on epoch=28
06/17/2022 08:52:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4292397660818714 -> 0.5471256941560139 on epoch=28, global_step=450
06/17/2022 08:52:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
06/17/2022 08:52:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
06/17/2022 08:52:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=29
06/17/2022 08:52:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
06/17/2022 08:52:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 08:52:57 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3732922688146569 on epoch=31
06/17/2022 08:52:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
06/17/2022 08:53:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=32
06/17/2022 08:53:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
06/17/2022 08:53:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
06/17/2022 08:53:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
06/17/2022 08:53:14 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.33159268929503916 on epoch=34
06/17/2022 08:53:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/17/2022 08:53:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=35
06/17/2022 08:53:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
06/17/2022 08:53:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
06/17/2022 08:53:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=37
06/17/2022 08:53:30 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.5244582043343653 on epoch=37
06/17/2022 08:53:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
06/17/2022 08:53:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
06/17/2022 08:53:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=39
06/17/2022 08:53:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
06/17/2022 08:53:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/17/2022 08:53:47 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.4342541436464089 on epoch=40
06/17/2022 08:53:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
06/17/2022 08:53:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/17/2022 08:53:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
06/17/2022 08:53:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=43
06/17/2022 08:54:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
06/17/2022 08:54:04 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 08:54:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
06/17/2022 08:54:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 08:54:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
06/17/2022 08:54:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
06/17/2022 08:54:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
06/17/2022 08:54:21 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=46
06/17/2022 08:54:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
06/17/2022 08:54:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/17/2022 08:54:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
06/17/2022 08:54:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/17/2022 08:54:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
06/17/2022 08:54:38 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 08:54:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=50
06/17/2022 08:54:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=51
06/17/2022 08:54:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
06/17/2022 08:54:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
06/17/2022 08:54:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=53
06/17/2022 08:54:55 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=53
06/17/2022 08:54:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/17/2022 08:55:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
06/17/2022 08:55:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
06/17/2022 08:55:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=55
06/17/2022 08:55:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
06/17/2022 08:55:12 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.48902195608782434 on epoch=56
06/17/2022 08:55:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=56
06/17/2022 08:55:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 08:55:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
06/17/2022 08:55:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=58
06/17/2022 08:55:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
06/17/2022 08:55:29 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.33159268929503916 on epoch=59
06/17/2022 08:55:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/17/2022 08:55:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
06/17/2022 08:55:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/17/2022 08:55:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
06/17/2022 08:55:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 08:55:46 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.4155251141552511 on epoch=62
06/17/2022 08:55:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
06/17/2022 08:55:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/17/2022 08:55:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=64
06/17/2022 08:55:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
06/17/2022 08:55:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=65
06/17/2022 08:56:03 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.5663466967814794 on epoch=65
06/17/2022 08:56:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5471256941560139 -> 0.5663466967814794 on epoch=65, global_step=1050
06/17/2022 08:56:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
06/17/2022 08:56:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
06/17/2022 08:56:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
06/17/2022 08:56:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=68
06/17/2022 08:56:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
06/17/2022 08:56:19 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.35693779904306216 on epoch=68
06/17/2022 08:56:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=69
06/17/2022 08:56:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/17/2022 08:56:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=70
06/17/2022 08:56:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
06/17/2022 08:56:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
06/17/2022 08:56:36 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.4272727272727273 on epoch=71
06/17/2022 08:56:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
06/17/2022 08:56:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
06/17/2022 08:56:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
06/17/2022 08:56:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
06/17/2022 08:56:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
06/17/2022 08:56:53 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 08:56:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=75
06/17/2022 08:56:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
06/17/2022 08:57:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
06/17/2022 08:57:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
06/17/2022 08:57:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
06/17/2022 08:57:10 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.4527614241659658 on epoch=78
06/17/2022 08:57:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
06/17/2022 08:57:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
06/17/2022 08:57:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
06/17/2022 08:57:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=80
06/17/2022 08:57:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
06/17/2022 08:57:27 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.5334040296924708 on epoch=81
06/17/2022 08:57:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
06/17/2022 08:57:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/17/2022 08:57:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=83
06/17/2022 08:57:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
06/17/2022 08:57:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
06/17/2022 08:57:44 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.40326340326340326 on epoch=84
06/17/2022 08:57:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
06/17/2022 08:57:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
06/17/2022 08:57:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
06/17/2022 08:57:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=86
06/17/2022 08:57:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
06/17/2022 08:58:01 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.5186226646195249 on epoch=87
06/17/2022 08:58:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=88
06/17/2022 08:58:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
06/17/2022 08:58:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
06/17/2022 08:58:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
06/17/2022 08:58:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
06/17/2022 08:58:18 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.5376937220908852 on epoch=90
06/17/2022 08:58:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=91
06/17/2022 08:58:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=91
06/17/2022 08:58:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
06/17/2022 08:58:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=93
06/17/2022 08:58:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=93
06/17/2022 08:58:35 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.4442271463173948 on epoch=93
06/17/2022 08:58:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
06/17/2022 08:58:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=94
06/17/2022 08:58:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=95
06/17/2022 08:58:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=96
06/17/2022 08:58:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=96
06/17/2022 08:58:51 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.5820310677800066 on epoch=96
06/17/2022 08:58:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5663466967814794 -> 0.5820310677800066 on epoch=96, global_step=1550
06/17/2022 08:58:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
06/17/2022 08:58:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
06/17/2022 08:58:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
06/17/2022 08:59:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=99
06/17/2022 08:59:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/17/2022 08:59:09 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.5640491958372753 on epoch=99
06/17/2022 08:59:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=100
06/17/2022 08:59:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.32 on epoch=101
06/17/2022 08:59:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/17/2022 08:59:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=102
06/17/2022 08:59:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
06/17/2022 08:59:27 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.5067951054412914 on epoch=103
06/17/2022 08:59:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=103
06/17/2022 08:59:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
06/17/2022 08:59:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=104
06/17/2022 08:59:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=105
06/17/2022 08:59:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=106
06/17/2022 08:59:44 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.6545421088143286 on epoch=106
06/17/2022 08:59:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5820310677800066 -> 0.6545421088143286 on epoch=106, global_step=1700
06/17/2022 08:59:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=106
06/17/2022 08:59:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=107
06/17/2022 08:59:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=108
06/17/2022 08:59:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=108
06/17/2022 08:59:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=109
06/17/2022 09:00:03 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.6610837438423646 on epoch=109
06/17/2022 09:00:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6545421088143286 -> 0.6610837438423646 on epoch=109, global_step=1750
06/17/2022 09:00:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=109
06/17/2022 09:00:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=110
06/17/2022 09:00:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=111
06/17/2022 09:00:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
06/17/2022 09:00:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=112
06/17/2022 09:00:21 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.6483016722724905 on epoch=112
06/17/2022 09:00:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
06/17/2022 09:00:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=113
06/17/2022 09:00:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
06/17/2022 09:00:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=114
06/17/2022 09:00:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=115
06/17/2022 09:00:40 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.6563334207724474 on epoch=115
06/17/2022 09:00:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=116
06/17/2022 09:00:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=116
06/17/2022 09:00:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=117
06/17/2022 09:00:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
06/17/2022 09:00:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.28 on epoch=118
06/17/2022 09:00:57 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.47871071893746214 on epoch=118
06/17/2022 09:01:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=119
06/17/2022 09:01:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
06/17/2022 09:01:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
06/17/2022 09:01:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=121
06/17/2022 09:01:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=121
06/17/2022 09:01:14 - INFO - __main__ - Global step 1950 Train loss 0.28 Classification-F1 0.6705882352941177 on epoch=121
06/17/2022 09:01:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6610837438423646 -> 0.6705882352941177 on epoch=121, global_step=1950
06/17/2022 09:01:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
06/17/2022 09:01:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
06/17/2022 09:01:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=123
06/17/2022 09:01:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
06/17/2022 09:01:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=124
06/17/2022 09:01:32 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.6984411094283047 on epoch=124
06/17/2022 09:01:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6705882352941177 -> 0.6984411094283047 on epoch=124, global_step=2000
06/17/2022 09:01:35 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
06/17/2022 09:01:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=126
06/17/2022 09:01:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=126
06/17/2022 09:01:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=127
06/17/2022 09:01:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
06/17/2022 09:01:50 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.6724708231916621 on epoch=128
06/17/2022 09:01:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=128
06/17/2022 09:01:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
06/17/2022 09:01:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=129
06/17/2022 09:02:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=130
06/17/2022 09:02:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
06/17/2022 09:02:08 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.6779874213836479 on epoch=131
06/17/2022 09:02:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=131
06/17/2022 09:02:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=132
06/17/2022 09:02:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=133
06/17/2022 09:02:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=133
06/17/2022 09:02:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
06/17/2022 09:02:26 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.669937106918239 on epoch=134
06/17/2022 09:02:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=134
06/17/2022 09:02:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
06/17/2022 09:02:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
06/17/2022 09:02:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
06/17/2022 09:02:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=137
06/17/2022 09:02:43 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.6424293993931318 on epoch=137
06/17/2022 09:02:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
06/17/2022 09:02:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
06/17/2022 09:02:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
06/17/2022 09:02:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=139
06/17/2022 09:02:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=140
06/17/2022 09:03:01 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.6450881571006835 on epoch=140
06/17/2022 09:03:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
06/17/2022 09:03:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=141
06/17/2022 09:03:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=142
06/17/2022 09:03:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
06/17/2022 09:03:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
06/17/2022 09:03:19 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.5627857229113117 on epoch=143
06/17/2022 09:03:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=144
06/17/2022 09:03:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=144
06/17/2022 09:03:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
06/17/2022 09:03:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
06/17/2022 09:03:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
06/17/2022 09:03:36 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.6550987224157956 on epoch=146
06/17/2022 09:03:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.22 on epoch=147
06/17/2022 09:03:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
06/17/2022 09:03:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
06/17/2022 09:03:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=149
06/17/2022 09:03:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
06/17/2022 09:03:54 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.6746642985102049 on epoch=149
06/17/2022 09:03:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
06/17/2022 09:04:00 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
06/17/2022 09:04:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=151
06/17/2022 09:04:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
06/17/2022 09:04:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
06/17/2022 09:04:12 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.7069910113388373 on epoch=153
06/17/2022 09:04:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6984411094283047 -> 0.7069910113388373 on epoch=153, global_step=2450
06/17/2022 09:04:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=153
06/17/2022 09:04:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
06/17/2022 09:04:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=154
06/17/2022 09:04:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=155
06/17/2022 09:04:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
06/17/2022 09:04:30 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.6472321216440959 on epoch=156
06/17/2022 09:04:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
06/17/2022 09:04:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
06/17/2022 09:04:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=158
06/17/2022 09:04:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=158
06/17/2022 09:04:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
06/17/2022 09:04:48 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7037357075624546 on epoch=159
06/17/2022 09:04:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=159
06/17/2022 09:04:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=160
06/17/2022 09:04:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
06/17/2022 09:04:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=161
06/17/2022 09:05:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
06/17/2022 09:05:05 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.693798687358155 on epoch=162
06/17/2022 09:05:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
06/17/2022 09:05:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
06/17/2022 09:05:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
06/17/2022 09:05:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
06/17/2022 09:05:19 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=165
06/17/2022 09:05:23 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.6553846153846153 on epoch=165
06/17/2022 09:05:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=166
06/17/2022 09:05:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
06/17/2022 09:05:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
06/17/2022 09:05:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
06/17/2022 09:05:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=168
06/17/2022 09:05:40 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.6314163228989726 on epoch=168
06/17/2022 09:05:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
06/17/2022 09:05:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.19 on epoch=169
06/17/2022 09:05:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
06/17/2022 09:05:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=171
06/17/2022 09:05:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=171
06/17/2022 09:05:58 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.656386691300709 on epoch=171
06/17/2022 09:06:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
06/17/2022 09:06:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
06/17/2022 09:06:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
06/17/2022 09:06:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
06/17/2022 09:06:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
06/17/2022 09:06:15 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.6636525233789319 on epoch=174
06/17/2022 09:06:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
06/17/2022 09:06:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
06/17/2022 09:06:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
06/17/2022 09:06:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=177
06/17/2022 09:06:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
06/17/2022 09:06:33 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.6507392601053367 on epoch=178
06/17/2022 09:06:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=178
06/17/2022 09:06:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
06/17/2022 09:06:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
06/17/2022 09:06:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/17/2022 09:06:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
06/17/2022 09:06:50 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.6301549776727082 on epoch=181
06/17/2022 09:06:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
06/17/2022 09:06:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=182
06/17/2022 09:06:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=183
06/17/2022 09:07:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=183
06/17/2022 09:07:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=184
06/17/2022 09:07:08 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.6698599852616065 on epoch=184
06/17/2022 09:07:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
06/17/2022 09:07:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=185
06/17/2022 09:07:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=186
06/17/2022 09:07:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
06/17/2022 09:07:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
06/17/2022 09:07:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:07:23 - INFO - __main__ - Printing 3 examples
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:07:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:07:23 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:07:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:07:23 - INFO - __main__ - Printing 3 examples
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 09:07:23 - INFO - __main__ - ['false']
06/17/2022 09:07:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:07:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:07:24 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:07:26 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.6789821384878885 on epoch=187
06/17/2022 09:07:26 - INFO - __main__ - save last model!
06/17/2022 09:07:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 09:07:26 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 09:07:26 - INFO - __main__ - Printing 3 examples
06/17/2022 09:07:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 09:07:26 - INFO - __main__ - ['false']
06/17/2022 09:07:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 09:07:26 - INFO - __main__ - ['false']
06/17/2022 09:07:26 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 09:07:26 - INFO - __main__ - ['false']
06/17/2022 09:07:26 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:07:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:07:30 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 09:07:39 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:07:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:07:40 - INFO - __main__ - Starting training!
06/17/2022 09:08:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.3_8_predictions.txt
06/17/2022 09:08:14 - INFO - __main__ - Classification-F1 on test data: 0.4699
06/17/2022 09:08:14 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.3, bsz=8, dev_performance=0.7069910113388373, test_performance=0.46985944149900266
06/17/2022 09:08:14 - INFO - __main__ - Running ... prefix=wiki_qa_128_13, lr=0.2, bsz=8 ...
06/17/2022 09:08:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:08:15 - INFO - __main__ - Printing 3 examples
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: what part of the government governs the US post office? [SEP] answer: It is one of the few government agencies explicitly authorized by the United States Constitution .
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: what record company was john lennon with [SEP] answer: Lennon revealed a rebellious nature and acerbic wit in his music, writing, drawings, on film and in interviews.
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: where do most political candidates get their money [SEP] answer: In democracies , political campaigns often refer to electoral campaigns, wherein representatives are chosen or referendums are decided.
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:08:15 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:08:15 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:08:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:08:15 - INFO - __main__ - Printing 3 examples
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: what is a registered agent for an llc [SEP] answer: The registered agent's address may also be where the state will send the paperwork for the yearly renewal of the business entity's charter.
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: what does low self esteem mean [SEP] answer: Self-esteem encompasses beliefs (for example, "I am competent," "I am worthy") and emotions such as triumph, despair , pride and shame .
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ -  [wiki_qa] question: how many british soldiers were missing [SEP] answer: Becoming MIA has been an occupational risk for service personnel for as long as there has been warfare.
06/17/2022 09:08:15 - INFO - __main__ - ['false']
06/17/2022 09:08:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:08:15 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:08:16 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:08:30 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:08:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:08:31 - INFO - __main__ - Starting training!
06/17/2022 09:08:34 - INFO - __main__ - Step 10 Global step 10 Train loss 0.90 on epoch=0
06/17/2022 09:08:37 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=1
06/17/2022 09:08:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=1
06/17/2022 09:08:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=2
06/17/2022 09:08:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 09:08:49 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 09:08:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 09:08:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=3
06/17/2022 09:08:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=4
06/17/2022 09:08:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
06/17/2022 09:08:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
06/17/2022 09:09:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=6
06/17/2022 09:09:05 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 09:09:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
06/17/2022 09:09:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=7
06/17/2022 09:09:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
06/17/2022 09:09:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
06/17/2022 09:09:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
06/17/2022 09:09:22 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 09:09:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
06/17/2022 09:09:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=10
06/17/2022 09:09:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=11
06/17/2022 09:09:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=11
06/17/2022 09:09:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
06/17/2022 09:09:39 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=12
06/17/2022 09:09:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=12, global_step=200
06/17/2022 09:09:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
06/17/2022 09:09:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=13
06/17/2022 09:09:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 09:09:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 09:09:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=15
06/17/2022 09:09:56 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.4527614241659658 on epoch=15
06/17/2022 09:09:56 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.4527614241659658 on epoch=15, global_step=250
06/17/2022 09:09:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
06/17/2022 09:10:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
06/17/2022 09:10:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=17
06/17/2022 09:10:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=18
06/17/2022 09:10:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
06/17/2022 09:10:12 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 09:10:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
06/17/2022 09:10:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
06/17/2022 09:10:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=20
06/17/2022 09:10:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=21
06/17/2022 09:10:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 09:10:29 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3771289537712896 on epoch=21
06/17/2022 09:10:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
06/17/2022 09:10:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
06/17/2022 09:10:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
06/17/2022 09:10:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
06/17/2022 09:10:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 09:10:47 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 09:10:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
06/17/2022 09:10:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
06/17/2022 09:10:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
06/17/2022 09:10:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=27
06/17/2022 09:11:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
06/17/2022 09:11:03 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 09:11:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=28
06/17/2022 09:11:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
06/17/2022 09:11:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
06/17/2022 09:11:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
06/17/2022 09:11:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
06/17/2022 09:11:21 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.4775599801066694 on epoch=31
06/17/2022 09:11:21 - INFO - __main__ - Saving model with best Classification-F1: 0.4527614241659658 -> 0.4775599801066694 on epoch=31, global_step=500
06/17/2022 09:11:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
06/17/2022 09:11:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
06/17/2022 09:11:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
06/17/2022 09:11:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
06/17/2022 09:11:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 09:11:38 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.39047619047619053 on epoch=34
06/17/2022 09:11:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
06/17/2022 09:11:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
06/17/2022 09:11:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
06/17/2022 09:11:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
06/17/2022 09:11:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=37
06/17/2022 09:11:55 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=37
06/17/2022 09:11:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
06/17/2022 09:12:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/17/2022 09:12:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=39
06/17/2022 09:12:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
06/17/2022 09:12:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/17/2022 09:12:13 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3383422492035824 on epoch=40
06/17/2022 09:12:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=41
06/17/2022 09:12:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
06/17/2022 09:12:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
06/17/2022 09:12:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=43
06/17/2022 09:12:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
06/17/2022 09:12:30 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 09:12:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
06/17/2022 09:12:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
06/17/2022 09:12:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=45
06/17/2022 09:12:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
06/17/2022 09:12:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
06/17/2022 09:12:46 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.4996742671009772 on epoch=46
06/17/2022 09:12:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4775599801066694 -> 0.4996742671009772 on epoch=46, global_step=750
06/17/2022 09:12:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=47
06/17/2022 09:12:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
06/17/2022 09:12:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=48
06/17/2022 09:12:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
06/17/2022 09:12:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=49
06/17/2022 09:13:03 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 09:13:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 09:13:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/17/2022 09:13:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=51
06/17/2022 09:13:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
06/17/2022 09:13:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
06/17/2022 09:13:20 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 09:13:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/17/2022 09:13:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
06/17/2022 09:13:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
06/17/2022 09:13:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=55
06/17/2022 09:13:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
06/17/2022 09:13:37 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.34673046251993617 on epoch=56
06/17/2022 09:13:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
06/17/2022 09:13:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
06/17/2022 09:13:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
06/17/2022 09:13:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=58
06/17/2022 09:13:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
06/17/2022 09:13:53 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3401530406766009 on epoch=59
06/17/2022 09:13:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
06/17/2022 09:13:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=60
06/17/2022 09:14:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
06/17/2022 09:14:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=61
06/17/2022 09:14:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 09:14:10 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.4980392156862745 on epoch=62
06/17/2022 09:14:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 09:14:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
06/17/2022 09:14:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
06/17/2022 09:14:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
06/17/2022 09:14:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
06/17/2022 09:14:26 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=65
06/17/2022 09:14:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=66
06/17/2022 09:14:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
06/17/2022 09:14:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=67
06/17/2022 09:14:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 09:14:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 09:14:43 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 09:14:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=69
06/17/2022 09:14:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=69
06/17/2022 09:14:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
06/17/2022 09:14:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
06/17/2022 09:14:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=71
06/17/2022 09:14:59 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.48329221798609556 on epoch=71
06/17/2022 09:15:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=72
06/17/2022 09:15:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
06/17/2022 09:15:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=73
06/17/2022 09:15:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
06/17/2022 09:15:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=74
06/17/2022 09:15:16 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 09:15:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=75
06/17/2022 09:15:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
06/17/2022 09:15:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/17/2022 09:15:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=77
06/17/2022 09:15:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=78
06/17/2022 09:15:32 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=78
06/17/2022 09:15:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=78
06/17/2022 09:15:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=79
06/17/2022 09:15:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
06/17/2022 09:15:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
06/17/2022 09:15:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=81
06/17/2022 09:15:49 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=81
06/17/2022 09:15:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
06/17/2022 09:15:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/17/2022 09:15:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=83
06/17/2022 09:15:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=83
06/17/2022 09:16:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
06/17/2022 09:16:05 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=84
06/17/2022 09:16:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/17/2022 09:16:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=85
06/17/2022 09:16:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=86
06/17/2022 09:16:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/17/2022 09:16:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
06/17/2022 09:16:22 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.5349525667765592 on epoch=87
06/17/2022 09:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4996742671009772 -> 0.5349525667765592 on epoch=87, global_step=1400
06/17/2022 09:16:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=88
06/17/2022 09:16:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=88
06/17/2022 09:16:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
06/17/2022 09:16:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=89
06/17/2022 09:16:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
06/17/2022 09:16:38 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.33159268929503916 on epoch=90
06/17/2022 09:16:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
06/17/2022 09:16:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=91
06/17/2022 09:16:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=92
06/17/2022 09:16:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=93
06/17/2022 09:16:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=93
06/17/2022 09:16:55 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=93
06/17/2022 09:16:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 09:17:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
06/17/2022 09:17:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=95
06/17/2022 09:17:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=96
06/17/2022 09:17:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=96
06/17/2022 09:17:11 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.521821755161044 on epoch=96
06/17/2022 09:17:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=97
06/17/2022 09:17:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
06/17/2022 09:17:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=98
06/17/2022 09:17:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
06/17/2022 09:17:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=99
06/17/2022 09:17:28 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=99
06/17/2022 09:17:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
06/17/2022 09:17:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=101
06/17/2022 09:17:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/17/2022 09:17:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=102
06/17/2022 09:17:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=103
06/17/2022 09:17:44 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=103
06/17/2022 09:17:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
06/17/2022 09:17:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=104
06/17/2022 09:17:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=104
06/17/2022 09:17:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 09:17:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.36 on epoch=106
06/17/2022 09:18:01 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.45400756793945646 on epoch=106
06/17/2022 09:18:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=106
06/17/2022 09:18:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=107
06/17/2022 09:18:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=108
06/17/2022 09:18:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=108
06/17/2022 09:18:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=109
06/17/2022 09:18:18 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=109
06/17/2022 09:18:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=109
06/17/2022 09:18:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=110
06/17/2022 09:18:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=111
06/17/2022 09:18:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=111
06/17/2022 09:18:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
06/17/2022 09:18:34 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.4342541436464089 on epoch=112
06/17/2022 09:18:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=113
06/17/2022 09:18:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=113
06/17/2022 09:18:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=114
06/17/2022 09:18:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/17/2022 09:18:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
06/17/2022 09:18:51 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.47124880673435743 on epoch=115
06/17/2022 09:18:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=116
06/17/2022 09:18:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=116
06/17/2022 09:18:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=117
06/17/2022 09:19:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=118
06/17/2022 09:19:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=118
06/17/2022 09:19:07 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=118
06/17/2022 09:19:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.36 on epoch=119
06/17/2022 09:19:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=119
06/17/2022 09:19:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=120
06/17/2022 09:19:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
06/17/2022 09:19:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=121
06/17/2022 09:19:24 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.5238095238095237 on epoch=121
06/17/2022 09:19:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=122
06/17/2022 09:19:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=123
06/17/2022 09:19:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
06/17/2022 09:19:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
06/17/2022 09:19:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=124
06/17/2022 09:19:41 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
06/17/2022 09:19:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=125
06/17/2022 09:19:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.38 on epoch=126
06/17/2022 09:19:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=126
06/17/2022 09:19:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
06/17/2022 09:19:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.34 on epoch=128
06/17/2022 09:19:58 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.515621394619041 on epoch=128
06/17/2022 09:20:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=128
06/17/2022 09:20:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=129
06/17/2022 09:20:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=129
06/17/2022 09:20:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=130
06/17/2022 09:20:12 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=131
06/17/2022 09:20:15 - INFO - __main__ - Global step 2100 Train loss 0.36 Classification-F1 0.5925313743495563 on epoch=131
06/17/2022 09:20:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5349525667765592 -> 0.5925313743495563 on epoch=131, global_step=2100
06/17/2022 09:20:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.38 on epoch=131
06/17/2022 09:20:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=132
06/17/2022 09:20:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.34 on epoch=133
06/17/2022 09:20:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=133
06/17/2022 09:20:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=134
06/17/2022 09:20:32 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.38709489051094886 on epoch=134
06/17/2022 09:20:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
06/17/2022 09:20:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.34 on epoch=135
06/17/2022 09:20:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=136
06/17/2022 09:20:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=136
06/17/2022 09:20:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=137
06/17/2022 09:20:49 - INFO - __main__ - Global step 2200 Train loss 0.36 Classification-F1 0.42482504604051563 on epoch=137
06/17/2022 09:20:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=138
06/17/2022 09:20:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=138
06/17/2022 09:20:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=139
06/17/2022 09:21:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=139
06/17/2022 09:21:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.36 on epoch=140
06/17/2022 09:21:06 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.49750868186622377 on epoch=140
06/17/2022 09:21:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=141
06/17/2022 09:21:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=141
06/17/2022 09:21:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
06/17/2022 09:21:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.39 on epoch=143
06/17/2022 09:21:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.38 on epoch=143
06/17/2022 09:21:23 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=143
06/17/2022 09:21:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.34 on epoch=144
06/17/2022 09:21:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.32 on epoch=144
06/17/2022 09:21:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=145
06/17/2022 09:21:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.35 on epoch=146
06/17/2022 09:21:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=146
06/17/2022 09:21:39 - INFO - __main__ - Global step 2350 Train loss 0.34 Classification-F1 0.5652852356693745 on epoch=146
06/17/2022 09:21:42 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=147
06/17/2022 09:21:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.31 on epoch=148
06/17/2022 09:21:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
06/17/2022 09:21:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=149
06/17/2022 09:21:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=149
06/17/2022 09:21:56 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.3732922688146569 on epoch=149
06/17/2022 09:21:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.34 on epoch=150
06/17/2022 09:22:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=151
06/17/2022 09:22:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.33 on epoch=151
06/17/2022 09:22:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
06/17/2022 09:22:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.32 on epoch=153
06/17/2022 09:22:13 - INFO - __main__ - Global step 2450 Train loss 0.34 Classification-F1 0.46906958449545877 on epoch=153
06/17/2022 09:22:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
06/17/2022 09:22:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.32 on epoch=154
06/17/2022 09:22:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=154
06/17/2022 09:22:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=155
06/17/2022 09:22:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.30 on epoch=156
06/17/2022 09:22:30 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.5397188623733247 on epoch=156
06/17/2022 09:22:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.30 on epoch=156
06/17/2022 09:22:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=157
06/17/2022 09:22:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.29 on epoch=158
06/17/2022 09:22:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.34 on epoch=158
06/17/2022 09:22:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.32 on epoch=159
06/17/2022 09:22:47 - INFO - __main__ - Global step 2550 Train loss 0.32 Classification-F1 0.6032834675872651 on epoch=159
06/17/2022 09:22:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5925313743495563 -> 0.6032834675872651 on epoch=159, global_step=2550
06/17/2022 09:22:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.33 on epoch=159
06/17/2022 09:22:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=160
06/17/2022 09:22:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=161
06/17/2022 09:22:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=161
06/17/2022 09:23:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=162
06/17/2022 09:23:04 - INFO - __main__ - Global step 2600 Train loss 0.33 Classification-F1 0.6137931034482759 on epoch=162
06/17/2022 09:23:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6032834675872651 -> 0.6137931034482759 on epoch=162, global_step=2600
06/17/2022 09:23:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.31 on epoch=163
06/17/2022 09:23:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=163
06/17/2022 09:23:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=164
06/17/2022 09:23:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.30 on epoch=164
06/17/2022 09:23:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=165
06/17/2022 09:23:20 - INFO - __main__ - Global step 2650 Train loss 0.32 Classification-F1 0.4652837798905215 on epoch=165
06/17/2022 09:23:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=166
06/17/2022 09:23:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.33 on epoch=166
06/17/2022 09:23:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=167
06/17/2022 09:23:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.30 on epoch=168
06/17/2022 09:23:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.32 on epoch=168
06/17/2022 09:23:37 - INFO - __main__ - Global step 2700 Train loss 0.32 Classification-F1 0.4153026265702322 on epoch=168
06/17/2022 09:23:40 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=169
06/17/2022 09:23:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.32 on epoch=169
06/17/2022 09:23:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.33 on epoch=170
06/17/2022 09:23:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.31 on epoch=171
06/17/2022 09:23:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=171
06/17/2022 09:23:54 - INFO - __main__ - Global step 2750 Train loss 0.32 Classification-F1 0.6275303643724697 on epoch=171
06/17/2022 09:23:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6137931034482759 -> 0.6275303643724697 on epoch=171, global_step=2750
06/17/2022 09:23:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.30 on epoch=172
06/17/2022 09:24:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=173
06/17/2022 09:24:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=173
06/17/2022 09:24:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=174
06/17/2022 09:24:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.28 on epoch=174
06/17/2022 09:24:11 - INFO - __main__ - Global step 2800 Train loss 0.30 Classification-F1 0.6205613371176285 on epoch=174
06/17/2022 09:24:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=175
06/17/2022 09:24:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.24 on epoch=176
06/17/2022 09:24:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=176
06/17/2022 09:24:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=177
06/17/2022 09:24:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=178
06/17/2022 09:24:28 - INFO - __main__ - Global step 2850 Train loss 0.28 Classification-F1 0.5021607605877269 on epoch=178
06/17/2022 09:24:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=178
06/17/2022 09:24:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.26 on epoch=179
06/17/2022 09:24:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=179
06/17/2022 09:24:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.31 on epoch=180
06/17/2022 09:24:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.25 on epoch=181
06/17/2022 09:24:45 - INFO - __main__ - Global step 2900 Train loss 0.26 Classification-F1 0.6483516483516483 on epoch=181
06/17/2022 09:24:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6275303643724697 -> 0.6483516483516483 on epoch=181, global_step=2900
06/17/2022 09:24:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=181
06/17/2022 09:24:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.30 on epoch=182
06/17/2022 09:24:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=183
06/17/2022 09:24:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=183
06/17/2022 09:24:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.30 on epoch=184
06/17/2022 09:25:02 - INFO - __main__ - Global step 2950 Train loss 0.29 Classification-F1 0.6129447339638423 on epoch=184
06/17/2022 09:25:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=184
06/17/2022 09:25:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.35 on epoch=185
06/17/2022 09:25:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.26 on epoch=186
06/17/2022 09:25:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=186
06/17/2022 09:25:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=187
06/17/2022 09:25:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:25:17 - INFO - __main__ - Printing 3 examples
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:25:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:25:17 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:25:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:25:17 - INFO - __main__ - Printing 3 examples
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 09:25:17 - INFO - __main__ - ['false']
06/17/2022 09:25:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:25:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:25:17 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:25:19 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.6411085948452497 on epoch=187
06/17/2022 09:25:19 - INFO - __main__ - save last model!
06/17/2022 09:25:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 09:25:19 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 09:25:19 - INFO - __main__ - Printing 3 examples
06/17/2022 09:25:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 09:25:19 - INFO - __main__ - ['false']
06/17/2022 09:25:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 09:25:19 - INFO - __main__ - ['false']
06/17/2022 09:25:19 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 09:25:19 - INFO - __main__ - ['false']
06/17/2022 09:25:19 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:25:20 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:25:23 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 09:25:36 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:25:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:25:37 - INFO - __main__ - Starting training!
06/17/2022 09:26:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_13_0.2_8_predictions.txt
06/17/2022 09:26:02 - INFO - __main__ - Classification-F1 on test data: 0.4296
06/17/2022 09:26:02 - INFO - __main__ - prefix=wiki_qa_128_13, lr=0.2, bsz=8, dev_performance=0.6483516483516483, test_performance=0.42955809898640585
06/17/2022 09:26:02 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.5, bsz=8 ...
06/17/2022 09:26:03 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:26:03 - INFO - __main__ - Printing 3 examples
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:26:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:26:03 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:26:03 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:26:03 - INFO - __main__ - Printing 3 examples
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 09:26:03 - INFO - __main__ - ['false']
06/17/2022 09:26:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:26:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:26:04 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:26:18 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:26:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:26:19 - INFO - __main__ - Starting training!
06/17/2022 09:26:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.73 on epoch=0
06/17/2022 09:26:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
06/17/2022 09:26:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
06/17/2022 09:26:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=2
06/17/2022 09:26:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
06/17/2022 09:26:36 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 09:26:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 09:26:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=3
06/17/2022 09:26:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
06/17/2022 09:26:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=4
06/17/2022 09:26:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
06/17/2022 09:26:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=6
06/17/2022 09:26:52 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 09:26:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
06/17/2022 09:26:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=7
06/17/2022 09:27:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=8
06/17/2022 09:27:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=8
06/17/2022 09:27:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=9
06/17/2022 09:27:08 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.43736263736263736 on epoch=9
06/17/2022 09:27:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43736263736263736 on epoch=9, global_step=150
06/17/2022 09:27:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
06/17/2022 09:27:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
06/17/2022 09:27:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=11
06/17/2022 09:27:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
06/17/2022 09:27:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
06/17/2022 09:27:24 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 09:27:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=13
06/17/2022 09:27:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=13
06/17/2022 09:27:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=14
06/17/2022 09:27:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
06/17/2022 09:27:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
06/17/2022 09:27:40 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3692115143929912 on epoch=15
06/17/2022 09:27:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
06/17/2022 09:27:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=16
06/17/2022 09:27:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
06/17/2022 09:27:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=18
06/17/2022 09:27:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
06/17/2022 09:27:56 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 09:27:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
06/17/2022 09:28:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
06/17/2022 09:28:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
06/17/2022 09:28:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=21
06/17/2022 09:28:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=21
06/17/2022 09:28:12 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.33159268929503916 on epoch=21
06/17/2022 09:28:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=22
06/17/2022 09:28:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=23
06/17/2022 09:28:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
06/17/2022 09:28:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=24
06/17/2022 09:28:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=24
06/17/2022 09:28:28 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 09:28:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
06/17/2022 09:28:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=26
06/17/2022 09:28:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
06/17/2022 09:28:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 09:28:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=28
06/17/2022 09:28:44 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 09:28:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
06/17/2022 09:28:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=29
06/17/2022 09:28:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=29
06/17/2022 09:28:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=30
06/17/2022 09:28:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
06/17/2022 09:29:00 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3799228491970934 on epoch=31
06/17/2022 09:29:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
06/17/2022 09:29:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=32
06/17/2022 09:29:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/17/2022 09:29:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=33
06/17/2022 09:29:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
06/17/2022 09:29:16 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.500504608760008 on epoch=34
06/17/2022 09:29:16 - INFO - __main__ - Saving model with best Classification-F1: 0.43736263736263736 -> 0.500504608760008 on epoch=34, global_step=550
06/17/2022 09:29:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/17/2022 09:29:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
06/17/2022 09:29:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/17/2022 09:29:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/17/2022 09:29:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
06/17/2022 09:29:32 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.39181286549707606 on epoch=37
06/17/2022 09:29:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
06/17/2022 09:29:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 09:29:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=39
06/17/2022 09:29:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
06/17/2022 09:29:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
06/17/2022 09:29:48 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3659442724458204 on epoch=40
06/17/2022 09:29:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
06/17/2022 09:29:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=41
06/17/2022 09:29:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/17/2022 09:29:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
06/17/2022 09:30:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
06/17/2022 09:30:04 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 09:30:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
06/17/2022 09:30:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 09:30:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
06/17/2022 09:30:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=46
06/17/2022 09:30:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
06/17/2022 09:30:20 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3771988921326446 on epoch=46
06/17/2022 09:30:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 09:30:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
06/17/2022 09:30:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
06/17/2022 09:30:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
06/17/2022 09:30:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/17/2022 09:30:36 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 09:30:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 09:30:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
06/17/2022 09:30:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/17/2022 09:30:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=52
06/17/2022 09:30:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=53
06/17/2022 09:30:51 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 09:30:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=53
06/17/2022 09:30:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
06/17/2022 09:30:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 09:31:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
06/17/2022 09:31:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 09:31:07 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.4287919204120322 on epoch=56
06/17/2022 09:31:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
06/17/2022 09:31:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=57
06/17/2022 09:31:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/17/2022 09:31:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/17/2022 09:31:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=59
06/17/2022 09:31:23 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3704956828812838 on epoch=59
06/17/2022 09:31:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=59
06/17/2022 09:31:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/17/2022 09:31:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/17/2022 09:31:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/17/2022 09:31:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=62
06/17/2022 09:31:40 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3818948513908726 on epoch=62
06/17/2022 09:31:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=63
06/17/2022 09:31:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
06/17/2022 09:31:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
06/17/2022 09:31:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
06/17/2022 09:31:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.18 on epoch=65
06/17/2022 09:31:56 - INFO - __main__ - Global step 1050 Train loss 0.52 Classification-F1 0.36098310291858676 on epoch=65
06/17/2022 09:31:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=66
06/17/2022 09:32:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=66
06/17/2022 09:32:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
06/17/2022 09:32:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
06/17/2022 09:32:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
06/17/2022 09:32:13 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.34723114355231144 on epoch=68
06/17/2022 09:32:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
06/17/2022 09:32:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
06/17/2022 09:32:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/17/2022 09:32:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=71
06/17/2022 09:32:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
06/17/2022 09:32:29 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.49478877582485353 on epoch=71
06/17/2022 09:32:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
06/17/2022 09:32:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
06/17/2022 09:32:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
06/17/2022 09:32:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
06/17/2022 09:32:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
06/17/2022 09:32:46 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.4199134199134199 on epoch=74
06/17/2022 09:32:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=75
06/17/2022 09:32:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
06/17/2022 09:32:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=76
06/17/2022 09:32:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=77
06/17/2022 09:32:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
06/17/2022 09:33:02 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.4391291596943089 on epoch=78
06/17/2022 09:33:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=78
06/17/2022 09:33:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=79
06/17/2022 09:33:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
06/17/2022 09:33:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
06/17/2022 09:33:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
06/17/2022 09:33:19 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.4882924043403769 on epoch=81
06/17/2022 09:33:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
06/17/2022 09:33:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
06/17/2022 09:33:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=83
06/17/2022 09:33:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
06/17/2022 09:33:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=84
06/17/2022 09:33:35 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.501245608431811 on epoch=84
06/17/2022 09:33:35 - INFO - __main__ - Saving model with best Classification-F1: 0.500504608760008 -> 0.501245608431811 on epoch=84, global_step=1350
06/17/2022 09:33:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
06/17/2022 09:33:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=85
06/17/2022 09:33:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=86
06/17/2022 09:33:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
06/17/2022 09:33:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
06/17/2022 09:33:52 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.4199134199134199 on epoch=87
06/17/2022 09:33:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
06/17/2022 09:33:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
06/17/2022 09:33:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=89
06/17/2022 09:34:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
06/17/2022 09:34:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
06/17/2022 09:34:08 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.519523918516823 on epoch=90
06/17/2022 09:34:08 - INFO - __main__ - Saving model with best Classification-F1: 0.501245608431811 -> 0.519523918516823 on epoch=90, global_step=1450
06/17/2022 09:34:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 09:34:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=91
06/17/2022 09:34:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
06/17/2022 09:34:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=93
06/17/2022 09:34:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=93
06/17/2022 09:34:25 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.43174772064353983 on epoch=93
06/17/2022 09:34:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=94
06/17/2022 09:34:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=94
06/17/2022 09:34:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=95
06/17/2022 09:34:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
06/17/2022 09:34:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
06/17/2022 09:34:41 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.46256715431027356 on epoch=96
06/17/2022 09:34:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
06/17/2022 09:34:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=98
06/17/2022 09:34:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=98
06/17/2022 09:34:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
06/17/2022 09:34:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/17/2022 09:34:58 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.4701122749796149 on epoch=99
06/17/2022 09:35:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
06/17/2022 09:35:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
06/17/2022 09:35:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=101
06/17/2022 09:35:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
06/17/2022 09:35:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
06/17/2022 09:35:14 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.5145583557621727 on epoch=103
06/17/2022 09:35:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
06/17/2022 09:35:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
06/17/2022 09:35:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=104
06/17/2022 09:35:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 09:35:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=106
06/17/2022 09:35:31 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.5220079583715946 on epoch=106
06/17/2022 09:35:31 - INFO - __main__ - Saving model with best Classification-F1: 0.519523918516823 -> 0.5220079583715946 on epoch=106, global_step=1700
06/17/2022 09:35:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
06/17/2022 09:35:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=107
06/17/2022 09:35:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=108
06/17/2022 09:35:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=108
06/17/2022 09:35:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=109
06/17/2022 09:35:48 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.5024179378333411 on epoch=109
06/17/2022 09:35:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
06/17/2022 09:35:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=110
06/17/2022 09:35:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=111
06/17/2022 09:35:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=111
06/17/2022 09:36:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
06/17/2022 09:36:04 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.51417004048583 on epoch=112
06/17/2022 09:36:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=113
06/17/2022 09:36:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=113
06/17/2022 09:36:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=114
06/17/2022 09:36:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=114
06/17/2022 09:36:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=115
06/17/2022 09:36:21 - INFO - __main__ - Global step 1850 Train loss 0.33 Classification-F1 0.5193569910158735 on epoch=115
06/17/2022 09:36:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=116
06/17/2022 09:36:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=116
06/17/2022 09:36:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=117
06/17/2022 09:36:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=118
06/17/2022 09:36:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.37 on epoch=118
06/17/2022 09:36:37 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3651088894055646 on epoch=118
06/17/2022 09:36:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=119
06/17/2022 09:36:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=119
06/17/2022 09:36:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=120
06/17/2022 09:36:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=121
06/17/2022 09:36:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=121
06/17/2022 09:36:54 - INFO - __main__ - Global step 1950 Train loss 0.36 Classification-F1 0.5220079583715946 on epoch=121
06/17/2022 09:36:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=122
06/17/2022 09:36:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=123
06/17/2022 09:37:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=123
06/17/2022 09:37:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=124
06/17/2022 09:37:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=124
06/17/2022 09:37:11 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.514900947459087 on epoch=124
06/17/2022 09:37:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=125
06/17/2022 09:37:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=126
06/17/2022 09:37:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=126
06/17/2022 09:37:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.34 on epoch=127
06/17/2022 09:37:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=128
06/17/2022 09:37:29 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.4882924043403769 on epoch=128
06/17/2022 09:37:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=128
06/17/2022 09:37:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=129
06/17/2022 09:37:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=129
06/17/2022 09:37:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=130
06/17/2022 09:37:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=131
06/17/2022 09:37:46 - INFO - __main__ - Global step 2100 Train loss 0.34 Classification-F1 0.5093688128613288 on epoch=131
06/17/2022 09:37:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=131
06/17/2022 09:37:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.32 on epoch=132
06/17/2022 09:37:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=133
06/17/2022 09:37:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=133
06/17/2022 09:37:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=134
06/17/2022 09:38:04 - INFO - __main__ - Global step 2150 Train loss 0.33 Classification-F1 0.533554837623065 on epoch=134
06/17/2022 09:38:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5220079583715946 -> 0.533554837623065 on epoch=134, global_step=2150
06/17/2022 09:38:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
06/17/2022 09:38:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=135
06/17/2022 09:38:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=136
06/17/2022 09:38:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.32 on epoch=136
06/17/2022 09:38:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=137
06/17/2022 09:38:21 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.5536737235367373 on epoch=137
06/17/2022 09:38:21 - INFO - __main__ - Saving model with best Classification-F1: 0.533554837623065 -> 0.5536737235367373 on epoch=137, global_step=2200
06/17/2022 09:38:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=138
06/17/2022 09:38:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=138
06/17/2022 09:38:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.31 on epoch=139
06/17/2022 09:38:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.33 on epoch=139
06/17/2022 09:38:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.33 on epoch=140
06/17/2022 09:38:39 - INFO - __main__ - Global step 2250 Train loss 0.33 Classification-F1 0.5546603186229628 on epoch=140
06/17/2022 09:38:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5536737235367373 -> 0.5546603186229628 on epoch=140, global_step=2250
06/17/2022 09:38:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=141
06/17/2022 09:38:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=141
06/17/2022 09:38:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=142
06/17/2022 09:38:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.33 on epoch=143
06/17/2022 09:38:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=143
06/17/2022 09:38:57 - INFO - __main__ - Global step 2300 Train loss 0.31 Classification-F1 0.47162022703818374 on epoch=143
06/17/2022 09:39:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.31 on epoch=144
06/17/2022 09:39:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.30 on epoch=144
06/17/2022 09:39:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.30 on epoch=145
06/17/2022 09:39:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.31 on epoch=146
06/17/2022 09:39:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=146
06/17/2022 09:39:15 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.5492957746478874 on epoch=146
06/17/2022 09:39:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.28 on epoch=147
06/17/2022 09:39:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.31 on epoch=148
06/17/2022 09:39:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=148
06/17/2022 09:39:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=149
06/17/2022 09:39:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.32 on epoch=149
06/17/2022 09:39:33 - INFO - __main__ - Global step 2400 Train loss 0.30 Classification-F1 0.5549917782475922 on epoch=149
06/17/2022 09:39:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5546603186229628 -> 0.5549917782475922 on epoch=149, global_step=2400
06/17/2022 09:39:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.31 on epoch=150
06/17/2022 09:39:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=151
06/17/2022 09:39:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=151
06/17/2022 09:39:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.30 on epoch=152
06/17/2022 09:39:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.25 on epoch=153
06/17/2022 09:39:50 - INFO - __main__ - Global step 2450 Train loss 0.28 Classification-F1 0.5305925497008299 on epoch=153
06/17/2022 09:39:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.33 on epoch=153
06/17/2022 09:39:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.32 on epoch=154
06/17/2022 09:39:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.28 on epoch=154
06/17/2022 09:40:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.28 on epoch=155
06/17/2022 09:40:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.28 on epoch=156
06/17/2022 09:40:08 - INFO - __main__ - Global step 2500 Train loss 0.30 Classification-F1 0.5345225809622094 on epoch=156
06/17/2022 09:40:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=156
06/17/2022 09:40:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.31 on epoch=157
06/17/2022 09:40:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.30 on epoch=158
06/17/2022 09:40:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.24 on epoch=158
06/17/2022 09:40:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.33 on epoch=159
06/17/2022 09:40:25 - INFO - __main__ - Global step 2550 Train loss 0.29 Classification-F1 0.5492336666105251 on epoch=159
06/17/2022 09:40:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=159
06/17/2022 09:40:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.29 on epoch=160
06/17/2022 09:40:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.26 on epoch=161
06/17/2022 09:40:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.28 on epoch=161
06/17/2022 09:40:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.29 on epoch=162
06/17/2022 09:40:43 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.5599535423925668 on epoch=162
06/17/2022 09:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5549917782475922 -> 0.5599535423925668 on epoch=162, global_step=2600
06/17/2022 09:40:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.27 on epoch=163
06/17/2022 09:40:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.25 on epoch=163
06/17/2022 09:40:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=164
06/17/2022 09:40:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=164
06/17/2022 09:40:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.28 on epoch=165
06/17/2022 09:41:01 - INFO - __main__ - Global step 2650 Train loss 0.26 Classification-F1 0.5479028697571744 on epoch=165
06/17/2022 09:41:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=166
06/17/2022 09:41:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=166
06/17/2022 09:41:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.25 on epoch=167
06/17/2022 09:41:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.21 on epoch=168
06/17/2022 09:41:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=168
06/17/2022 09:41:19 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.42504758279406174 on epoch=168
06/17/2022 09:41:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.31 on epoch=169
06/17/2022 09:41:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=169
06/17/2022 09:41:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.21 on epoch=170
06/17/2022 09:41:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=171
06/17/2022 09:41:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.22 on epoch=171
06/17/2022 09:41:36 - INFO - __main__ - Global step 2750 Train loss 0.24 Classification-F1 0.5912552198477032 on epoch=171
06/17/2022 09:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5599535423925668 -> 0.5912552198477032 on epoch=171, global_step=2750
06/17/2022 09:41:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.24 on epoch=172
06/17/2022 09:41:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.22 on epoch=173
06/17/2022 09:41:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
06/17/2022 09:41:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.24 on epoch=174
06/17/2022 09:41:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=174
06/17/2022 09:41:54 - INFO - __main__ - Global step 2800 Train loss 0.22 Classification-F1 0.5895368546429063 on epoch=174
06/17/2022 09:41:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.25 on epoch=175
06/17/2022 09:41:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=176
06/17/2022 09:42:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=176
06/17/2022 09:42:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
06/17/2022 09:42:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.17 on epoch=178
06/17/2022 09:42:12 - INFO - __main__ - Global step 2850 Train loss 0.22 Classification-F1 0.545917001338688 on epoch=178
06/17/2022 09:42:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.23 on epoch=178
06/17/2022 09:42:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=179
06/17/2022 09:42:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=179
06/17/2022 09:42:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.22 on epoch=180
06/17/2022 09:42:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=181
06/17/2022 09:42:29 - INFO - __main__ - Global step 2900 Train loss 0.22 Classification-F1 0.5895158803576935 on epoch=181
06/17/2022 09:42:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=181
06/17/2022 09:42:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=182
06/17/2022 09:42:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
06/17/2022 09:42:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=183
06/17/2022 09:42:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=184
06/17/2022 09:42:47 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.5686838240976891 on epoch=184
06/17/2022 09:42:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
06/17/2022 09:42:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
06/17/2022 09:42:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.25 on epoch=186
06/17/2022 09:42:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=186
06/17/2022 09:43:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.27 on epoch=187
06/17/2022 09:43:01 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:43:01 - INFO - __main__ - Printing 3 examples
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:43:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:43:02 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:43:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:43:02 - INFO - __main__ - Printing 3 examples
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 09:43:02 - INFO - __main__ - ['false']
06/17/2022 09:43:02 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:43:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:43:02 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:43:05 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.5586206896551724 on epoch=187
06/17/2022 09:43:05 - INFO - __main__ - save last model!
06/17/2022 09:43:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 09:43:05 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 09:43:05 - INFO - __main__ - Printing 3 examples
06/17/2022 09:43:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 09:43:05 - INFO - __main__ - ['false']
06/17/2022 09:43:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 09:43:05 - INFO - __main__ - ['false']
06/17/2022 09:43:05 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 09:43:05 - INFO - __main__ - ['false']
06/17/2022 09:43:05 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:43:07 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:43:09 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 09:43:21 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:43:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:43:22 - INFO - __main__ - Starting training!
06/17/2022 09:44:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.5_8_predictions.txt
06/17/2022 09:44:01 - INFO - __main__ - Classification-F1 on test data: 0.4144
06/17/2022 09:44:01 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.5, bsz=8, dev_performance=0.5912552198477032, test_performance=0.4144180306749299
06/17/2022 09:44:01 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.4, bsz=8 ...
06/17/2022 09:44:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:44:02 - INFO - __main__ - Printing 3 examples
06/17/2022 09:44:02 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 09:44:02 - INFO - __main__ - ['false']
06/17/2022 09:44:02 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 09:44:02 - INFO - __main__ - ['false']
06/17/2022 09:44:02 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 09:44:02 - INFO - __main__ - ['false']
06/17/2022 09:44:02 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:44:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:44:03 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 09:44:03 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 09:44:03 - INFO - __main__ - Printing 3 examples
06/17/2022 09:44:03 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 09:44:03 - INFO - __main__ - ['false']
06/17/2022 09:44:03 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 09:44:03 - INFO - __main__ - ['false']
06/17/2022 09:44:03 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 09:44:03 - INFO - __main__ - ['false']
06/17/2022 09:44:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 09:44:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 09:44:03 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 09:44:22 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 09:44:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 09:44:22 - INFO - __main__ - Starting training!
06/17/2022 09:44:26 - INFO - __main__ - Step 10 Global step 10 Train loss 0.76 on epoch=0
06/17/2022 09:44:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=1
06/17/2022 09:44:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=1
06/17/2022 09:44:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=2
06/17/2022 09:44:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 09:44:40 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 09:44:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 09:44:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=3
06/17/2022 09:44:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
06/17/2022 09:44:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
06/17/2022 09:44:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=5
06/17/2022 09:44:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
06/17/2022 09:44:58 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 09:45:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=6
06/17/2022 09:45:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=7
06/17/2022 09:45:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=8
06/17/2022 09:45:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=8
06/17/2022 09:45:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=9
06/17/2022 09:45:15 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 09:45:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
06/17/2022 09:45:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
06/17/2022 09:45:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
06/17/2022 09:45:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=11
06/17/2022 09:45:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=12
06/17/2022 09:45:32 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.35693779904306216 on epoch=12
06/17/2022 09:45:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35693779904306216 on epoch=12, global_step=200
06/17/2022 09:45:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
06/17/2022 09:45:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=13
06/17/2022 09:45:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=14
06/17/2022 09:45:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=14
06/17/2022 09:45:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
06/17/2022 09:45:50 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 09:45:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
06/17/2022 09:45:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
06/17/2022 09:45:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
06/17/2022 09:46:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
06/17/2022 09:46:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
06/17/2022 09:46:06 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 09:46:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
06/17/2022 09:46:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
06/17/2022 09:46:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
06/17/2022 09:46:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=21
06/17/2022 09:46:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=21
06/17/2022 09:46:23 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.40326340326340326 on epoch=21
06/17/2022 09:46:23 - INFO - __main__ - Saving model with best Classification-F1: 0.35693779904306216 -> 0.40326340326340326 on epoch=21, global_step=350
06/17/2022 09:46:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=22
06/17/2022 09:46:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
06/17/2022 09:46:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=23
06/17/2022 09:46:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
06/17/2022 09:46:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 09:46:40 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 09:46:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
06/17/2022 09:46:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
06/17/2022 09:46:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
06/17/2022 09:46:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 09:46:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
06/17/2022 09:46:57 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 09:47:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
06/17/2022 09:47:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
06/17/2022 09:47:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
06/17/2022 09:47:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
06/17/2022 09:47:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=31
06/17/2022 09:47:15 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=31
06/17/2022 09:47:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
06/17/2022 09:47:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
06/17/2022 09:47:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=33
06/17/2022 09:47:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=33
06/17/2022 09:47:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
06/17/2022 09:47:32 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 09:47:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
06/17/2022 09:47:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
06/17/2022 09:47:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
06/17/2022 09:47:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
06/17/2022 09:47:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
06/17/2022 09:47:48 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.5526430755788554 on epoch=37
06/17/2022 09:47:48 - INFO - __main__ - Saving model with best Classification-F1: 0.40326340326340326 -> 0.5526430755788554 on epoch=37, global_step=600
06/17/2022 09:47:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=38
06/17/2022 09:47:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=38
06/17/2022 09:47:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
06/17/2022 09:47:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
06/17/2022 09:48:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=40
06/17/2022 09:48:05 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.5426267808878098 on epoch=40
06/17/2022 09:48:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=41
06/17/2022 09:48:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
06/17/2022 09:48:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
06/17/2022 09:48:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/17/2022 09:48:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/17/2022 09:48:22 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 09:48:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
06/17/2022 09:48:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
06/17/2022 09:48:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
06/17/2022 09:48:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=46
06/17/2022 09:48:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
06/17/2022 09:48:39 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.4599156118143459 on epoch=46
06/17/2022 09:48:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 09:48:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
06/17/2022 09:48:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
06/17/2022 09:48:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=49
06/17/2022 09:48:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
06/17/2022 09:48:56 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 09:48:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/17/2022 09:49:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
06/17/2022 09:49:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
06/17/2022 09:49:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=52
06/17/2022 09:49:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/17/2022 09:49:13 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 09:49:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=53
06/17/2022 09:49:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
06/17/2022 09:49:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 09:49:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=55
06/17/2022 09:49:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=56
06/17/2022 09:49:29 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.39047619047619053 on epoch=56
06/17/2022 09:49:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
06/17/2022 09:49:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=57
06/17/2022 09:49:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
06/17/2022 09:49:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
06/17/2022 09:49:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=59
06/17/2022 09:49:46 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 09:49:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=59
06/17/2022 09:49:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
06/17/2022 09:49:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/17/2022 09:49:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
06/17/2022 09:49:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/17/2022 09:50:03 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=62
06/17/2022 09:50:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 09:50:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=63
06/17/2022 09:50:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
06/17/2022 09:50:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=64
06/17/2022 09:50:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
06/17/2022 09:50:20 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.502226682556608 on epoch=65
06/17/2022 09:50:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=66
06/17/2022 09:50:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=66
06/17/2022 09:50:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=67
06/17/2022 09:50:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 09:50:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
06/17/2022 09:50:37 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 09:50:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 09:50:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=69
06/17/2022 09:50:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
06/17/2022 09:50:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=71
06/17/2022 09:50:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=71
06/17/2022 09:50:54 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=71
06/17/2022 09:50:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/17/2022 09:50:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 09:51:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
06/17/2022 09:51:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
06/17/2022 09:51:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
06/17/2022 09:51:10 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 09:51:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
06/17/2022 09:51:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
06/17/2022 09:51:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
06/17/2022 09:51:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
06/17/2022 09:51:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
06/17/2022 09:51:27 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=78
06/17/2022 09:51:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
06/17/2022 09:51:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=79
06/17/2022 09:51:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
06/17/2022 09:51:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
06/17/2022 09:51:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=81
06/17/2022 09:51:44 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.454560705957354 on epoch=81
06/17/2022 09:51:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
06/17/2022 09:51:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/17/2022 09:51:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
06/17/2022 09:51:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
06/17/2022 09:51:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=84
06/17/2022 09:52:01 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.34673046251993617 on epoch=84
06/17/2022 09:52:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=84
06/17/2022 09:52:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
06/17/2022 09:52:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
06/17/2022 09:52:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
06/17/2022 09:52:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
06/17/2022 09:52:18 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.5320070665949765 on epoch=87
06/17/2022 09:52:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=88
06/17/2022 09:52:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
06/17/2022 09:52:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=89
06/17/2022 09:52:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
06/17/2022 09:52:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
06/17/2022 09:52:34 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.4545454545454546 on epoch=90
06/17/2022 09:52:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 09:52:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=91
06/17/2022 09:52:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
06/17/2022 09:52:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
06/17/2022 09:52:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.35 on epoch=93
06/17/2022 09:52:51 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.341074761764417 on epoch=93
06/17/2022 09:52:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=94
06/17/2022 09:52:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=94
06/17/2022 09:53:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=95
06/17/2022 09:53:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=96
06/17/2022 09:53:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
06/17/2022 09:53:09 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.523683380637023 on epoch=96
06/17/2022 09:53:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=97
06/17/2022 09:53:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=98
06/17/2022 09:53:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
06/17/2022 09:53:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
06/17/2022 09:53:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/17/2022 09:53:26 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.3999999999999999 on epoch=99
06/17/2022 09:53:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=100
06/17/2022 09:53:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.31 on epoch=101
06/17/2022 09:53:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
06/17/2022 09:53:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=102
06/17/2022 09:53:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.29 on epoch=103
06/17/2022 09:53:42 - INFO - __main__ - Global step 1650 Train loss 0.31 Classification-F1 0.4059435586495995 on epoch=103
06/17/2022 09:53:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=103
06/17/2022 09:53:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
06/17/2022 09:53:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=104
06/17/2022 09:53:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=105
06/17/2022 09:53:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=106
06/17/2022 09:53:59 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.48024103821852615 on epoch=106
06/17/2022 09:54:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=106
06/17/2022 09:54:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=107
06/17/2022 09:54:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=108
06/17/2022 09:54:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.28 on epoch=108
06/17/2022 09:54:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=109
06/17/2022 09:54:16 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.5273365377279318 on epoch=109
06/17/2022 09:54:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=109
06/17/2022 09:54:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=110
06/17/2022 09:54:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.32 on epoch=111
06/17/2022 09:54:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
06/17/2022 09:54:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=112
06/17/2022 09:54:32 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.534341090133601 on epoch=112
06/17/2022 09:54:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=113
06/17/2022 09:54:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=113
06/17/2022 09:54:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=114
06/17/2022 09:54:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=114
06/17/2022 09:54:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=115
06/17/2022 09:54:48 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.5208825847123719 on epoch=115
06/17/2022 09:54:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=116
06/17/2022 09:54:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=116
06/17/2022 09:54:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
06/17/2022 09:54:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=118
06/17/2022 09:55:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
06/17/2022 09:55:05 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.4226819494921256 on epoch=118
06/17/2022 09:55:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=119
06/17/2022 09:55:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=119
06/17/2022 09:55:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=120
06/17/2022 09:55:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=121
06/17/2022 09:55:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=121
06/17/2022 09:55:21 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.5264694641901704 on epoch=121
06/17/2022 09:55:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=122
06/17/2022 09:55:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=123
06/17/2022 09:55:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=123
06/17/2022 09:55:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=124
06/17/2022 09:55:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=124
06/17/2022 09:55:39 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=124
06/17/2022 09:55:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=125
06/17/2022 09:55:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=126
06/17/2022 09:55:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.25 on epoch=126
06/17/2022 09:55:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=127
06/17/2022 09:55:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=128
06/17/2022 09:55:55 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.5492957746478874 on epoch=128
06/17/2022 09:55:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.26 on epoch=128
06/17/2022 09:56:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
06/17/2022 09:56:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=129
06/17/2022 09:56:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.22 on epoch=130
06/17/2022 09:56:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
06/17/2022 09:56:12 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.5477379214993471 on epoch=131
06/17/2022 09:56:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=131
06/17/2022 09:56:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=132
06/17/2022 09:56:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.22 on epoch=133
06/17/2022 09:56:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
06/17/2022 09:56:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=134
06/17/2022 09:56:28 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.554442748091603 on epoch=134
06/17/2022 09:56:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5526430755788554 -> 0.554442748091603 on epoch=134, global_step=2150
06/17/2022 09:56:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
06/17/2022 09:56:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=135
06/17/2022 09:56:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=136
06/17/2022 09:56:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=136
06/17/2022 09:56:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=137
06/17/2022 09:56:46 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.5859122260880181 on epoch=137
06/17/2022 09:56:46 - INFO - __main__ - Saving model with best Classification-F1: 0.554442748091603 -> 0.5859122260880181 on epoch=137, global_step=2200
06/17/2022 09:56:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
06/17/2022 09:56:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
06/17/2022 09:56:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=139
06/17/2022 09:56:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=139
06/17/2022 09:56:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
06/17/2022 09:57:02 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.5893361851653808 on epoch=140
06/17/2022 09:57:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5859122260880181 -> 0.5893361851653808 on epoch=140, global_step=2250
06/17/2022 09:57:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
06/17/2022 09:57:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=141
06/17/2022 09:57:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=142
06/17/2022 09:57:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=143
06/17/2022 09:57:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
06/17/2022 09:57:18 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.5705498602050326 on epoch=143
06/17/2022 09:57:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
06/17/2022 09:57:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=144
06/17/2022 09:57:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=145
06/17/2022 09:57:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=146
06/17/2022 09:57:31 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
06/17/2022 09:57:35 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.5971583530669926 on epoch=146
06/17/2022 09:57:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5893361851653808 -> 0.5971583530669926 on epoch=146, global_step=2350
06/17/2022 09:57:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
06/17/2022 09:57:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
06/17/2022 09:57:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
06/17/2022 09:57:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
06/17/2022 09:57:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
06/17/2022 09:57:53 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.581973842843408 on epoch=149
06/17/2022 09:57:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=150
06/17/2022 09:57:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
06/17/2022 09:58:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=151
06/17/2022 09:58:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
06/17/2022 09:58:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
06/17/2022 09:58:10 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.5467643467643467 on epoch=153
06/17/2022 09:58:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
06/17/2022 09:58:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.24 on epoch=154
06/17/2022 09:58:18 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=154
06/17/2022 09:58:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
06/17/2022 09:58:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
06/17/2022 09:58:27 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.5305925497008299 on epoch=156
06/17/2022 09:58:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=156
06/17/2022 09:58:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
06/17/2022 09:58:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
06/17/2022 09:58:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
06/17/2022 09:58:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=159
06/17/2022 09:58:44 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.5428571428571429 on epoch=159
06/17/2022 09:58:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=159
06/17/2022 09:58:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=160
06/17/2022 09:58:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=161
06/17/2022 09:58:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=161
06/17/2022 09:58:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=162
06/17/2022 09:59:02 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.5585870145723659 on epoch=162
06/17/2022 09:59:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
06/17/2022 09:59:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=163
06/17/2022 09:59:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
06/17/2022 09:59:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=164
06/17/2022 09:59:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
06/17/2022 09:59:19 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.5511350411966423 on epoch=165
06/17/2022 09:59:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
06/17/2022 09:59:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=166
06/17/2022 09:59:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=167
06/17/2022 09:59:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
06/17/2022 09:59:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=168
06/17/2022 09:59:36 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.5484869113187698 on epoch=168
06/17/2022 09:59:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
06/17/2022 09:59:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
06/17/2022 09:59:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
06/17/2022 09:59:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
06/17/2022 09:59:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=171
06/17/2022 09:59:54 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.5603263203091455 on epoch=171
06/17/2022 09:59:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
06/17/2022 09:59:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.25 on epoch=173
06/17/2022 10:00:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
06/17/2022 10:00:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
06/17/2022 10:00:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
06/17/2022 10:00:11 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.5618314077877622 on epoch=174
06/17/2022 10:00:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
06/17/2022 10:00:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=176
06/17/2022 10:00:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=176
06/17/2022 10:00:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=177
06/17/2022 10:00:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=178
06/17/2022 10:00:28 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.5336976320582878 on epoch=178
06/17/2022 10:00:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=178
06/17/2022 10:00:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=179
06/17/2022 10:00:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
06/17/2022 10:00:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
06/17/2022 10:00:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
06/17/2022 10:00:46 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.5755342667649226 on epoch=181
06/17/2022 10:00:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
06/17/2022 10:00:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
06/17/2022 10:00:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=183
06/17/2022 10:00:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
06/17/2022 10:00:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=184
06/17/2022 10:01:03 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.5693662833374113 on epoch=184
06/17/2022 10:01:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
06/17/2022 10:01:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
06/17/2022 10:01:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
06/17/2022 10:01:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
06/17/2022 10:01:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
06/17/2022 10:01:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:01:17 - INFO - __main__ - Printing 3 examples
06/17/2022 10:01:17 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 10:01:17 - INFO - __main__ - ['false']
06/17/2022 10:01:17 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 10:01:17 - INFO - __main__ - ['false']
06/17/2022 10:01:17 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 10:01:17 - INFO - __main__ - ['false']
06/17/2022 10:01:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:01:17 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:01:18 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:01:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:01:18 - INFO - __main__ - Printing 3 examples
06/17/2022 10:01:18 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 10:01:18 - INFO - __main__ - ['false']
06/17/2022 10:01:18 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 10:01:18 - INFO - __main__ - ['false']
06/17/2022 10:01:18 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 10:01:18 - INFO - __main__ - ['false']
06/17/2022 10:01:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:01:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:01:18 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:01:21 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.5545787545787546 on epoch=187
06/17/2022 10:01:21 - INFO - __main__ - save last model!
06/17/2022 10:01:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 10:01:21 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 10:01:21 - INFO - __main__ - Printing 3 examples
06/17/2022 10:01:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 10:01:21 - INFO - __main__ - ['false']
06/17/2022 10:01:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 10:01:21 - INFO - __main__ - ['false']
06/17/2022 10:01:21 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 10:01:21 - INFO - __main__ - ['false']
06/17/2022 10:01:21 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:01:22 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:01:25 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 10:01:34 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:01:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:01:35 - INFO - __main__ - Starting training!
06/17/2022 10:02:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.4_8_predictions.txt
06/17/2022 10:02:14 - INFO - __main__ - Classification-F1 on test data: 0.4089
06/17/2022 10:02:14 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.4, bsz=8, dev_performance=0.5971583530669926, test_performance=0.4089071230268291
06/17/2022 10:02:14 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.3, bsz=8 ...
06/17/2022 10:02:15 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:02:15 - INFO - __main__ - Printing 3 examples
06/17/2022 10:02:15 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 10:02:15 - INFO - __main__ - ['false']
06/17/2022 10:02:15 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 10:02:15 - INFO - __main__ - ['false']
06/17/2022 10:02:15 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 10:02:15 - INFO - __main__ - ['false']
06/17/2022 10:02:15 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:02:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:02:16 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:02:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:02:16 - INFO - __main__ - Printing 3 examples
06/17/2022 10:02:16 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 10:02:16 - INFO - __main__ - ['false']
06/17/2022 10:02:16 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 10:02:16 - INFO - __main__ - ['false']
06/17/2022 10:02:16 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 10:02:16 - INFO - __main__ - ['false']
06/17/2022 10:02:16 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:02:16 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:02:16 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:02:31 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:02:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:02:32 - INFO - __main__ - Starting training!
06/17/2022 10:02:35 - INFO - __main__ - Step 10 Global step 10 Train loss 0.76 on epoch=0
06/17/2022 10:02:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
06/17/2022 10:02:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=1
06/17/2022 10:02:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=2
06/17/2022 10:02:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
06/17/2022 10:02:49 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 10:02:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 10:02:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=3
06/17/2022 10:02:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=4
06/17/2022 10:02:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
06/17/2022 10:03:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
06/17/2022 10:03:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=6
06/17/2022 10:03:06 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 10:03:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
06/17/2022 10:03:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
06/17/2022 10:03:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
06/17/2022 10:03:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
06/17/2022 10:03:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
06/17/2022 10:03:22 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 10:03:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=9
06/17/2022 10:03:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=10
06/17/2022 10:03:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
06/17/2022 10:03:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
06/17/2022 10:03:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=12
06/17/2022 10:03:39 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 10:03:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=13
06/17/2022 10:03:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 10:03:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=14
06/17/2022 10:03:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 10:03:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
06/17/2022 10:03:55 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.35693779904306216 on epoch=15
06/17/2022 10:03:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35693779904306216 on epoch=15, global_step=250
06/17/2022 10:03:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
06/17/2022 10:04:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=16
06/17/2022 10:04:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=17
06/17/2022 10:04:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=18
06/17/2022 10:04:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
06/17/2022 10:04:12 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 10:04:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=19
06/17/2022 10:04:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=19
06/17/2022 10:04:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
06/17/2022 10:04:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=21
06/17/2022 10:04:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 10:04:28 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.5168017822885079 on epoch=21
06/17/2022 10:04:28 - INFO - __main__ - Saving model with best Classification-F1: 0.35693779904306216 -> 0.5168017822885079 on epoch=21, global_step=350
06/17/2022 10:04:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=22
06/17/2022 10:04:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
06/17/2022 10:04:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=23
06/17/2022 10:04:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=24
06/17/2022 10:04:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=24
06/17/2022 10:04:44 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 10:04:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
06/17/2022 10:04:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
06/17/2022 10:04:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=26
06/17/2022 10:04:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
06/17/2022 10:04:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
06/17/2022 10:05:00 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.4602462082568183 on epoch=28
06/17/2022 10:05:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
06/17/2022 10:05:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
06/17/2022 10:05:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
06/17/2022 10:05:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
06/17/2022 10:05:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=31
06/17/2022 10:05:16 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5226107226107226 on epoch=31
06/17/2022 10:05:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5168017822885079 -> 0.5226107226107226 on epoch=31, global_step=500
06/17/2022 10:05:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=31
06/17/2022 10:05:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
06/17/2022 10:05:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=33
06/17/2022 10:05:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
06/17/2022 10:05:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
06/17/2022 10:05:33 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 10:05:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
06/17/2022 10:05:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
06/17/2022 10:05:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/17/2022 10:05:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/17/2022 10:05:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
06/17/2022 10:05:49 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.33159268929503916 on epoch=37
06/17/2022 10:05:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
06/17/2022 10:05:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/17/2022 10:05:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
06/17/2022 10:05:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
06/17/2022 10:06:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
06/17/2022 10:06:05 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.47192489483002253 on epoch=40
06/17/2022 10:06:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=41
06/17/2022 10:06:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
06/17/2022 10:06:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
06/17/2022 10:06:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
06/17/2022 10:06:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/17/2022 10:06:21 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 10:06:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=44
06/17/2022 10:06:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
06/17/2022 10:06:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
06/17/2022 10:06:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
06/17/2022 10:06:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=46
06/17/2022 10:06:36 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.44597111077558005 on epoch=46
06/17/2022 10:06:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
06/17/2022 10:06:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=48
06/17/2022 10:06:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
06/17/2022 10:06:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
06/17/2022 10:06:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=49
06/17/2022 10:06:52 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 10:06:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/17/2022 10:06:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
06/17/2022 10:07:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 10:07:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
06/17/2022 10:07:05 - INFO - __main__ - Step 850 Global step 850 Train loss 1.82 on epoch=53
06/17/2022 10:07:09 - INFO - __main__ - Global step 850 Train loss 0.68 Classification-F1 0.5373493975903615 on epoch=53
06/17/2022 10:07:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5226107226107226 -> 0.5373493975903615 on epoch=53, global_step=850
06/17/2022 10:07:11 - INFO - __main__ - Step 860 Global step 860 Train loss 2.81 on epoch=53
06/17/2022 10:07:14 - INFO - __main__ - Step 870 Global step 870 Train loss 3.35 on epoch=54
06/17/2022 10:07:16 - INFO - __main__ - Step 880 Global step 880 Train loss 2.20 on epoch=54
06/17/2022 10:07:19 - INFO - __main__ - Step 890 Global step 890 Train loss 1.35 on epoch=55
06/17/2022 10:07:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=56
06/17/2022 10:07:25 - INFO - __main__ - Global step 900 Train loss 2.04 Classification-F1 0.46895191457525676 on epoch=56
06/17/2022 10:07:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
06/17/2022 10:07:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=57
06/17/2022 10:07:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
06/17/2022 10:07:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=58
06/17/2022 10:07:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
06/17/2022 10:07:41 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.5062438705459301 on epoch=59
06/17/2022 10:07:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
06/17/2022 10:07:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
06/17/2022 10:07:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=61
06/17/2022 10:07:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=61
06/17/2022 10:07:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=62
06/17/2022 10:07:57 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.48189222542163723 on epoch=62
06/17/2022 10:07:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=63
06/17/2022 10:08:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=63
06/17/2022 10:08:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
06/17/2022 10:08:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
06/17/2022 10:08:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=65
06/17/2022 10:08:12 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.5296999387630129 on epoch=65
06/17/2022 10:08:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
06/17/2022 10:08:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
06/17/2022 10:08:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=67
06/17/2022 10:08:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=68
06/17/2022 10:08:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
06/17/2022 10:08:28 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.5467643467643467 on epoch=68
06/17/2022 10:08:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5373493975903615 -> 0.5467643467643467 on epoch=68, global_step=1100
06/17/2022 10:08:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
06/17/2022 10:08:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=69
06/17/2022 10:08:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.50 on epoch=70
06/17/2022 10:08:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=71
06/17/2022 10:08:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=71
06/17/2022 10:08:44 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.590857603922603 on epoch=71
06/17/2022 10:08:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5467643467643467 -> 0.590857603922603 on epoch=71, global_step=1150
06/17/2022 10:08:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=72
06/17/2022 10:08:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 10:08:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
06/17/2022 10:08:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
06/17/2022 10:08:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=74
06/17/2022 10:09:00 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.5931291643743506 on epoch=74
06/17/2022 10:09:00 - INFO - __main__ - Saving model with best Classification-F1: 0.590857603922603 -> 0.5931291643743506 on epoch=74, global_step=1200
06/17/2022 10:09:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=75
06/17/2022 10:09:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
06/17/2022 10:09:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=76
06/17/2022 10:09:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=77
06/17/2022 10:09:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=78
06/17/2022 10:09:16 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.5786299936929868 on epoch=78
06/17/2022 10:09:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=78
06/17/2022 10:09:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=79
06/17/2022 10:09:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=79
06/17/2022 10:09:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
06/17/2022 10:09:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=81
06/17/2022 10:09:32 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.5901477832512315 on epoch=81
06/17/2022 10:09:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
06/17/2022 10:09:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/17/2022 10:09:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/17/2022 10:09:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
06/17/2022 10:09:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
06/17/2022 10:09:48 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.587571921749137 on epoch=84
06/17/2022 10:09:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
06/17/2022 10:09:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=85
06/17/2022 10:09:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
06/17/2022 10:09:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=86
06/17/2022 10:10:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
06/17/2022 10:10:04 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.5110771581359816 on epoch=87
06/17/2022 10:10:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=88
06/17/2022 10:10:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=88
06/17/2022 10:10:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=89
06/17/2022 10:10:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=89
06/17/2022 10:10:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=90
06/17/2022 10:10:20 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.581514017263769 on epoch=90
06/17/2022 10:10:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=91
06/17/2022 10:10:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
06/17/2022 10:10:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
06/17/2022 10:10:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
06/17/2022 10:10:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=93
06/17/2022 10:10:36 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.5755755755755756 on epoch=93
06/17/2022 10:10:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=94
06/17/2022 10:10:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=94
06/17/2022 10:10:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=95
06/17/2022 10:10:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=96
06/17/2022 10:10:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=96
06/17/2022 10:10:52 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.5584797189396359 on epoch=96
06/17/2022 10:10:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=97
06/17/2022 10:10:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=98
06/17/2022 10:10:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=98
06/17/2022 10:11:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=99
06/17/2022 10:11:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=99
06/17/2022 10:11:08 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.5952380952380953 on epoch=99
06/17/2022 10:11:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5931291643743506 -> 0.5952380952380953 on epoch=99, global_step=1600
06/17/2022 10:11:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=100
06/17/2022 10:11:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=101
06/17/2022 10:11:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=101
06/17/2022 10:11:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=102
06/17/2022 10:11:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=103
06/17/2022 10:11:24 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.5813836477987422 on epoch=103
06/17/2022 10:11:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
06/17/2022 10:11:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
06/17/2022 10:11:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=104
06/17/2022 10:11:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 10:11:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=106
06/17/2022 10:11:40 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.5403057510313031 on epoch=106
06/17/2022 10:11:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
06/17/2022 10:11:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=107
06/17/2022 10:11:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=108
06/17/2022 10:11:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=108
06/17/2022 10:11:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=109
06/17/2022 10:11:56 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.6106613411168291 on epoch=109
06/17/2022 10:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5952380952380953 -> 0.6106613411168291 on epoch=109, global_step=1750
06/17/2022 10:11:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
06/17/2022 10:12:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=110
06/17/2022 10:12:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=111
06/17/2022 10:12:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=111
06/17/2022 10:12:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=112
06/17/2022 10:12:12 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.5976009889053366 on epoch=112
06/17/2022 10:12:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=113
06/17/2022 10:12:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=113
06/17/2022 10:12:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=114
06/17/2022 10:12:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/17/2022 10:12:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=115
06/17/2022 10:12:28 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.5721002889947668 on epoch=115
06/17/2022 10:12:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=116
06/17/2022 10:12:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=116
06/17/2022 10:12:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=117
06/17/2022 10:12:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=118
06/17/2022 10:12:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=118
06/17/2022 10:12:43 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.4796747967479674 on epoch=118
06/17/2022 10:12:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=119
06/17/2022 10:12:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=119
06/17/2022 10:12:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=120
06/17/2022 10:12:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
06/17/2022 10:12:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=121
06/17/2022 10:12:59 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.5585851260978165 on epoch=121
06/17/2022 10:13:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=122
06/17/2022 10:13:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=123
06/17/2022 10:13:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=123
06/17/2022 10:13:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=124
06/17/2022 10:13:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=124
06/17/2022 10:13:15 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.5232774674115457 on epoch=124
06/17/2022 10:13:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=125
06/17/2022 10:13:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=126
06/17/2022 10:13:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=126
06/17/2022 10:13:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.39 on epoch=127
06/17/2022 10:13:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=128
06/17/2022 10:13:31 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.5793650793650793 on epoch=128
06/17/2022 10:13:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=128
06/17/2022 10:13:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=129
06/17/2022 10:13:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=129
06/17/2022 10:13:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=130
06/17/2022 10:13:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=131
06/17/2022 10:13:47 - INFO - __main__ - Global step 2100 Train loss 0.39 Classification-F1 0.5333333333333333 on epoch=131
06/17/2022 10:13:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
06/17/2022 10:13:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=132
06/17/2022 10:13:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=133
06/17/2022 10:13:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=133
06/17/2022 10:13:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=134
06/17/2022 10:14:03 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.578502640571606 on epoch=134
06/17/2022 10:14:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.35 on epoch=134
06/17/2022 10:14:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=135
06/17/2022 10:14:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.37 on epoch=136
06/17/2022 10:14:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
06/17/2022 10:14:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.39 on epoch=137
06/17/2022 10:14:19 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.5928553951553707 on epoch=137
06/17/2022 10:14:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.37 on epoch=138
06/17/2022 10:14:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.33 on epoch=138
06/17/2022 10:14:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=139
06/17/2022 10:14:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.38 on epoch=139
06/17/2022 10:14:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=140
06/17/2022 10:14:35 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.5925313743495563 on epoch=140
06/17/2022 10:14:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.37 on epoch=141
06/17/2022 10:14:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.34 on epoch=141
06/17/2022 10:14:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
06/17/2022 10:14:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=143
06/17/2022 10:14:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.39 on epoch=143
06/17/2022 10:14:51 - INFO - __main__ - Global step 2300 Train loss 0.38 Classification-F1 0.5355320050869012 on epoch=143
06/17/2022 10:14:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=144
06/17/2022 10:14:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=144
06/17/2022 10:14:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=145
06/17/2022 10:15:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=146
06/17/2022 10:15:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=146
06/17/2022 10:15:07 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.44114032030232425 on epoch=146
06/17/2022 10:15:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=147
06/17/2022 10:15:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=148
06/17/2022 10:15:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
06/17/2022 10:15:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=149
06/17/2022 10:15:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=149
06/17/2022 10:15:23 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.534341090133601 on epoch=149
06/17/2022 10:15:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=150
06/17/2022 10:15:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=151
06/17/2022 10:15:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=151
06/17/2022 10:15:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
06/17/2022 10:15:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=153
06/17/2022 10:15:39 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.49512098917871417 on epoch=153
06/17/2022 10:15:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=153
06/17/2022 10:15:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.38 on epoch=154
06/17/2022 10:15:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=154
06/17/2022 10:15:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.35 on epoch=155
06/17/2022 10:15:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=156
06/17/2022 10:15:55 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.6171641335530733 on epoch=156
06/17/2022 10:15:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6106613411168291 -> 0.6171641335530733 on epoch=156, global_step=2500
06/17/2022 10:15:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
06/17/2022 10:16:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=157
06/17/2022 10:16:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=158
06/17/2022 10:16:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.39 on epoch=158
06/17/2022 10:16:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.37 on epoch=159
06/17/2022 10:16:11 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.4527614241659658 on epoch=159
06/17/2022 10:16:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=159
06/17/2022 10:16:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=160
06/17/2022 10:16:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.37 on epoch=161
06/17/2022 10:16:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=161
06/17/2022 10:16:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=162
06/17/2022 10:16:27 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.4080870974202123 on epoch=162
06/17/2022 10:16:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.36 on epoch=163
06/17/2022 10:16:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.39 on epoch=163
06/17/2022 10:16:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=164
06/17/2022 10:16:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.37 on epoch=164
06/17/2022 10:16:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=165
06/17/2022 10:16:42 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.45071982281284606 on epoch=165
06/17/2022 10:16:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=166
06/17/2022 10:16:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.37 on epoch=166
06/17/2022 10:16:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=167
06/17/2022 10:16:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=168
06/17/2022 10:16:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.38 on epoch=168
06/17/2022 10:16:58 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.5191291922873151 on epoch=168
06/17/2022 10:17:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=169
06/17/2022 10:17:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.34 on epoch=169
06/17/2022 10:17:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.36 on epoch=170
06/17/2022 10:17:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=171
06/17/2022 10:17:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=171
06/17/2022 10:17:14 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.5678823073945024 on epoch=171
06/17/2022 10:17:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=172
06/17/2022 10:17:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=173
06/17/2022 10:17:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=173
06/17/2022 10:17:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.37 on epoch=174
06/17/2022 10:17:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=174
06/17/2022 10:17:30 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.44545454545454544 on epoch=174
06/17/2022 10:17:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=175
06/17/2022 10:17:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.36 on epoch=176
06/17/2022 10:17:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=176
06/17/2022 10:17:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=177
06/17/2022 10:17:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.38 on epoch=178
06/17/2022 10:17:46 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.5921568627450979 on epoch=178
06/17/2022 10:17:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=178
06/17/2022 10:17:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=179
06/17/2022 10:17:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=179
06/17/2022 10:17:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=180
06/17/2022 10:17:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=181
06/17/2022 10:18:02 - INFO - __main__ - Global step 2900 Train loss 0.39 Classification-F1 0.5263644773358003 on epoch=181
06/17/2022 10:18:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=181
06/17/2022 10:18:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=182
06/17/2022 10:18:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=183
06/17/2022 10:18:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=183
06/17/2022 10:18:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=184
06/17/2022 10:18:18 - INFO - __main__ - Global step 2950 Train loss 0.38 Classification-F1 0.5700763358778627 on epoch=184
06/17/2022 10:18:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.37 on epoch=184
06/17/2022 10:18:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=185
06/17/2022 10:18:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.35 on epoch=186
06/17/2022 10:18:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.34 on epoch=186
06/17/2022 10:18:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=187
06/17/2022 10:18:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:18:32 - INFO - __main__ - Printing 3 examples
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:18:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:18:32 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:18:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:18:32 - INFO - __main__ - Printing 3 examples
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 10:18:32 - INFO - __main__ - ['false']
06/17/2022 10:18:32 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:18:32 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:18:33 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:18:34 - INFO - __main__ - Global step 3000 Train loss 0.36 Classification-F1 0.5581471287960069 on epoch=187
06/17/2022 10:18:34 - INFO - __main__ - save last model!
06/17/2022 10:18:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 10:18:34 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 10:18:34 - INFO - __main__ - Printing 3 examples
06/17/2022 10:18:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 10:18:34 - INFO - __main__ - ['false']
06/17/2022 10:18:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 10:18:34 - INFO - __main__ - ['false']
06/17/2022 10:18:34 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 10:18:34 - INFO - __main__ - ['false']
06/17/2022 10:18:34 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:18:35 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:18:38 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 10:18:51 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:18:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:18:52 - INFO - __main__ - Starting training!
06/17/2022 10:19:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.3_8_predictions.txt
06/17/2022 10:19:17 - INFO - __main__ - Classification-F1 on test data: 0.4755
06/17/2022 10:19:17 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.3, bsz=8, dev_performance=0.6171641335530733, test_performance=0.47552301335125724
06/17/2022 10:19:17 - INFO - __main__ - Running ... prefix=wiki_qa_128_21, lr=0.2, bsz=8 ...
06/17/2022 10:19:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:19:18 - INFO - __main__ - Printing 3 examples
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: how fire extinguisher works [SEP] answer: Typically, a fire extinguisher consists of a hand-held cylindrical pressure vessel containing an agent which can be discharged to extinguish a fire .
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: what is another name for cpu [SEP] answer: The term has been in use in the computer industry at least since the early 1960s.
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: what is vitamin a for [SEP] answer: In foods of animal origin, the major form of vitamin A is an ester , primarily retinyl palmitate , which is converted to retinol (chemically an alcohol ) in the small intestine.
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:19:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:19:18 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:19:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:19:18 - INFO - __main__ - Printing 3 examples
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: who started world war i [SEP] answer: These alliances were both reorganised and expanded as more nations entered the war: Italy, Japan and the United States joined the Allies, and the Ottoman Empire and Bulgaria the Central Powers.
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: when did wwi begin [SEP] answer: One of the long-term causes of the war was the resurgence of imperialism in the foreign policies of the great powers of Europe.
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ -  [wiki_qa] question: how old were the twin towers when destroyed [SEP] answer: The new World Trade Center complex will include One World Trade Center , three other high-rise office towers, and the National September 11 Memorial & Museum .
06/17/2022 10:19:18 - INFO - __main__ - ['false']
06/17/2022 10:19:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:19:19 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:19:19 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:19:37 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:19:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:19:38 - INFO - __main__ - Starting training!
06/17/2022 10:19:41 - INFO - __main__ - Step 10 Global step 10 Train loss 0.83 on epoch=0
06/17/2022 10:19:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=1
06/17/2022 10:19:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
06/17/2022 10:19:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=2
06/17/2022 10:19:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=3
06/17/2022 10:19:55 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 10:19:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 10:19:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=3
06/17/2022 10:20:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=4
06/17/2022 10:20:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
06/17/2022 10:20:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
06/17/2022 10:20:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=6
06/17/2022 10:20:11 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.42020232523025813 on epoch=6
06/17/2022 10:20:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.42020232523025813 on epoch=6, global_step=100
06/17/2022 10:20:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=6
06/17/2022 10:20:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
06/17/2022 10:20:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
06/17/2022 10:20:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
06/17/2022 10:20:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=9
06/17/2022 10:20:28 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 10:20:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=9
06/17/2022 10:20:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
06/17/2022 10:20:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=11
06/17/2022 10:20:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=11
06/17/2022 10:20:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
06/17/2022 10:20:44 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 10:20:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=13
06/17/2022 10:20:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
06/17/2022 10:20:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 10:20:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=14
06/17/2022 10:20:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
06/17/2022 10:21:00 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 10:21:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
06/17/2022 10:21:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
06/17/2022 10:21:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=17
06/17/2022 10:21:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
06/17/2022 10:21:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=18
06/17/2022 10:21:16 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 10:21:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=19
06/17/2022 10:21:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
06/17/2022 10:21:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
06/17/2022 10:21:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=21
06/17/2022 10:21:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
06/17/2022 10:21:32 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=21
06/17/2022 10:21:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
06/17/2022 10:21:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
06/17/2022 10:21:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
06/17/2022 10:21:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=24
06/17/2022 10:21:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 10:21:49 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 10:21:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
06/17/2022 10:21:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
06/17/2022 10:21:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=26
06/17/2022 10:21:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
06/17/2022 10:22:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
06/17/2022 10:22:05 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.49783549783549785 on epoch=28
06/17/2022 10:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.42020232523025813 -> 0.49783549783549785 on epoch=28, global_step=450
06/17/2022 10:22:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
06/17/2022 10:22:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/17/2022 10:22:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
06/17/2022 10:22:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
06/17/2022 10:22:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=31
06/17/2022 10:22:21 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.35307589038932324 on epoch=31
06/17/2022 10:22:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=31
06/17/2022 10:22:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
06/17/2022 10:22:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=33
06/17/2022 10:22:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=33
06/17/2022 10:22:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=34
06/17/2022 10:22:38 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 10:22:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/17/2022 10:22:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.35 on epoch=35
06/17/2022 10:22:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=36
06/17/2022 10:22:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/17/2022 10:22:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
06/17/2022 10:22:54 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.36516753625488524 on epoch=37
06/17/2022 10:22:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
06/17/2022 10:22:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
06/17/2022 10:23:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=39
06/17/2022 10:23:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=39
06/17/2022 10:23:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
06/17/2022 10:23:10 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.41918755401901464 on epoch=40
06/17/2022 10:23:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
06/17/2022 10:23:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=41
06/17/2022 10:23:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
06/17/2022 10:23:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
06/17/2022 10:23:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
06/17/2022 10:23:27 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 10:23:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
06/17/2022 10:23:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=44
06/17/2022 10:23:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
06/17/2022 10:23:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=46
06/17/2022 10:23:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=46
06/17/2022 10:23:43 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.4427987471465732 on epoch=46
06/17/2022 10:23:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
06/17/2022 10:23:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
06/17/2022 10:23:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
06/17/2022 10:23:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
06/17/2022 10:23:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=49
06/17/2022 10:23:59 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 10:24:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=50
06/17/2022 10:24:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
06/17/2022 10:24:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=51
06/17/2022 10:24:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
06/17/2022 10:24:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
06/17/2022 10:24:16 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 10:24:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=53
06/17/2022 10:24:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/17/2022 10:24:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=54
06/17/2022 10:24:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
06/17/2022 10:24:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
06/17/2022 10:24:32 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.3913043478260869 on epoch=56
06/17/2022 10:24:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
06/17/2022 10:24:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=57
06/17/2022 10:24:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
06/17/2022 10:24:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/17/2022 10:24:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
06/17/2022 10:24:48 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.33159268929503916 on epoch=59
06/17/2022 10:24:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
06/17/2022 10:24:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/17/2022 10:24:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
06/17/2022 10:24:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=61
06/17/2022 10:25:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=62
06/17/2022 10:25:05 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.45400756793945646 on epoch=62
06/17/2022 10:25:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
06/17/2022 10:25:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
06/17/2022 10:25:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/17/2022 10:25:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
06/17/2022 10:25:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=65
06/17/2022 10:25:21 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.49031674208144793 on epoch=65
06/17/2022 10:25:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
06/17/2022 10:25:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
06/17/2022 10:25:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
06/17/2022 10:25:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=68
06/17/2022 10:25:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
06/17/2022 10:25:37 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 10:25:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 10:25:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=69
06/17/2022 10:25:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
06/17/2022 10:25:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=71
06/17/2022 10:25:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
06/17/2022 10:25:54 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.46281389748882007 on epoch=71
06/17/2022 10:25:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
06/17/2022 10:25:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=73
06/17/2022 10:26:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=73
06/17/2022 10:26:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
06/17/2022 10:26:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
06/17/2022 10:26:10 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.37632280167491433 on epoch=74
06/17/2022 10:26:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
06/17/2022 10:26:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=76
06/17/2022 10:26:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
06/17/2022 10:26:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
06/17/2022 10:26:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
06/17/2022 10:26:27 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.5203097173828881 on epoch=78
06/17/2022 10:26:27 - INFO - __main__ - Saving model with best Classification-F1: 0.49783549783549785 -> 0.5203097173828881 on epoch=78, global_step=1250
06/17/2022 10:26:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=78
06/17/2022 10:26:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
06/17/2022 10:26:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
06/17/2022 10:26:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=80
06/17/2022 10:26:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=81
06/17/2022 10:26:44 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.4834421364985163 on epoch=81
06/17/2022 10:26:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
06/17/2022 10:26:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/17/2022 10:26:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=83
06/17/2022 10:26:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
06/17/2022 10:26:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=84
06/17/2022 10:27:00 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.5422245028066738 on epoch=84
06/17/2022 10:27:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5203097173828881 -> 0.5422245028066738 on epoch=84, global_step=1350
06/17/2022 10:27:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
06/17/2022 10:27:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=85
06/17/2022 10:27:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=86
06/17/2022 10:27:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=86
06/17/2022 10:27:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=87
06/17/2022 10:27:17 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.5778931297709924 on epoch=87
06/17/2022 10:27:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5422245028066738 -> 0.5778931297709924 on epoch=87, global_step=1400
06/17/2022 10:27:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=88
06/17/2022 10:27:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=88
06/17/2022 10:27:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=89
06/17/2022 10:27:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=89
06/17/2022 10:27:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=90
06/17/2022 10:27:35 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.5755342667649226 on epoch=90
06/17/2022 10:27:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=91
06/17/2022 10:27:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
06/17/2022 10:27:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
06/17/2022 10:27:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=93
06/17/2022 10:27:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
06/17/2022 10:27:51 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.4796747967479674 on epoch=93
06/17/2022 10:27:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
06/17/2022 10:27:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=94
06/17/2022 10:27:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=95
06/17/2022 10:28:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=96
06/17/2022 10:28:04 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=96
06/17/2022 10:28:08 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.6013435114503817 on epoch=96
06/17/2022 10:28:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5778931297709924 -> 0.6013435114503817 on epoch=96, global_step=1550
06/17/2022 10:28:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=97
06/17/2022 10:28:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=98
06/17/2022 10:28:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=98
06/17/2022 10:28:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
06/17/2022 10:28:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=99
06/17/2022 10:28:26 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.5481059126767164 on epoch=99
06/17/2022 10:28:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=100
06/17/2022 10:28:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=101
06/17/2022 10:28:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=101
06/17/2022 10:28:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=102
06/17/2022 10:28:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
06/17/2022 10:28:42 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.609375 on epoch=103
06/17/2022 10:28:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6013435114503817 -> 0.609375 on epoch=103, global_step=1650
06/17/2022 10:28:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=103
06/17/2022 10:28:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=104
06/17/2022 10:28:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
06/17/2022 10:28:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=105
06/17/2022 10:28:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=106
06/17/2022 10:28:59 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.583394743306313 on epoch=106
06/17/2022 10:29:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=106
06/17/2022 10:29:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=107
06/17/2022 10:29:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
06/17/2022 10:29:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
06/17/2022 10:29:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=109
06/17/2022 10:29:15 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.5743842364532019 on epoch=109
06/17/2022 10:29:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
06/17/2022 10:29:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=110
06/17/2022 10:29:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
06/17/2022 10:29:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
06/17/2022 10:29:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
06/17/2022 10:29:32 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.5640491958372753 on epoch=112
06/17/2022 10:29:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
06/17/2022 10:29:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
06/17/2022 10:29:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=114
06/17/2022 10:29:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
06/17/2022 10:29:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=115
06/17/2022 10:29:49 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.5742456640532193 on epoch=115
06/17/2022 10:29:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=116
06/17/2022 10:29:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=116
06/17/2022 10:29:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=117
06/17/2022 10:29:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
06/17/2022 10:30:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=118
06/17/2022 10:30:06 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.6132753490501259 on epoch=118
06/17/2022 10:30:06 - INFO - __main__ - Saving model with best Classification-F1: 0.609375 -> 0.6132753490501259 on epoch=118, global_step=1900
06/17/2022 10:30:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
06/17/2022 10:30:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
06/17/2022 10:30:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=120
06/17/2022 10:30:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
06/17/2022 10:30:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
06/17/2022 10:30:23 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.6284470246734397 on epoch=121
06/17/2022 10:30:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6132753490501259 -> 0.6284470246734397 on epoch=121, global_step=1950
06/17/2022 10:30:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=122
06/17/2022 10:30:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=123
06/17/2022 10:30:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
06/17/2022 10:30:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=124
06/17/2022 10:30:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=124
06/17/2022 10:30:40 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.608203244566881 on epoch=124
06/17/2022 10:30:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
06/17/2022 10:30:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=126
06/17/2022 10:30:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=126
06/17/2022 10:30:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=127
06/17/2022 10:30:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=128
06/17/2022 10:30:56 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.6180103681142031 on epoch=128
06/17/2022 10:30:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=128
06/17/2022 10:31:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
06/17/2022 10:31:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.32 on epoch=129
06/17/2022 10:31:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
06/17/2022 10:31:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=131
06/17/2022 10:31:12 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.6010307528507723 on epoch=131
06/17/2022 10:31:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
06/17/2022 10:31:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.24 on epoch=132
06/17/2022 10:31:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=133
06/17/2022 10:31:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
06/17/2022 10:31:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=134
06/17/2022 10:31:28 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.5897874158743723 on epoch=134
06/17/2022 10:31:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
06/17/2022 10:31:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
06/17/2022 10:31:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
06/17/2022 10:31:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
06/17/2022 10:31:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
06/17/2022 10:31:44 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.5995828988529719 on epoch=137
06/17/2022 10:31:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
06/17/2022 10:31:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
06/17/2022 10:31:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
06/17/2022 10:31:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=139
06/17/2022 10:31:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
06/17/2022 10:32:00 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.590857603922603 on epoch=140
06/17/2022 10:32:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
06/17/2022 10:32:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=141
06/17/2022 10:32:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.14 on epoch=142
06/17/2022 10:32:10 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
06/17/2022 10:32:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
06/17/2022 10:32:17 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.6160391796755433 on epoch=143
06/17/2022 10:32:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=144
06/17/2022 10:32:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=144
06/17/2022 10:32:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
06/17/2022 10:32:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=146
06/17/2022 10:32:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
06/17/2022 10:32:33 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.5967383246849518 on epoch=146
06/17/2022 10:32:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=147
06/17/2022 10:32:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
06/17/2022 10:32:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
06/17/2022 10:32:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
06/17/2022 10:32:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
06/17/2022 10:32:49 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.609464508094645 on epoch=149
06/17/2022 10:32:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
06/17/2022 10:32:55 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=151
06/17/2022 10:32:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
06/17/2022 10:33:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
06/17/2022 10:33:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
06/17/2022 10:33:06 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.5944082013047531 on epoch=153
06/17/2022 10:33:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
06/17/2022 10:33:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=154
06/17/2022 10:33:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
06/17/2022 10:33:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=155
06/17/2022 10:33:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=156
06/17/2022 10:33:22 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.5898374914168001 on epoch=156
06/17/2022 10:33:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
06/17/2022 10:33:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.21 on epoch=157
06/17/2022 10:33:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
06/17/2022 10:33:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=158
06/17/2022 10:33:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
06/17/2022 10:33:38 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.5974098057354302 on epoch=159
06/17/2022 10:33:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
06/17/2022 10:33:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
06/17/2022 10:33:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=161
06/17/2022 10:33:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=161
06/17/2022 10:33:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
06/17/2022 10:33:54 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.5749685788322187 on epoch=162
06/17/2022 10:33:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=163
06/17/2022 10:33:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=163
06/17/2022 10:34:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
06/17/2022 10:34:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
06/17/2022 10:34:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=165
06/17/2022 10:34:10 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.5652852356693745 on epoch=165
06/17/2022 10:34:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
06/17/2022 10:34:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
06/17/2022 10:34:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
06/17/2022 10:34:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
06/17/2022 10:34:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=168
06/17/2022 10:34:26 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.6011730205278591 on epoch=168
06/17/2022 10:34:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=169
06/17/2022 10:34:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
06/17/2022 10:34:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
06/17/2022 10:34:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
06/17/2022 10:34:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=171
06/17/2022 10:34:43 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.6054341781202829 on epoch=171
06/17/2022 10:34:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
06/17/2022 10:34:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=173
06/17/2022 10:34:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
06/17/2022 10:34:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=174
06/17/2022 10:34:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
06/17/2022 10:34:59 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.5962701535729049 on epoch=174
06/17/2022 10:35:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=175
06/17/2022 10:35:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=176
06/17/2022 10:35:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
06/17/2022 10:35:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
06/17/2022 10:35:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
06/17/2022 10:35:15 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.5781862392218969 on epoch=178
06/17/2022 10:35:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=178
06/17/2022 10:35:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
06/17/2022 10:35:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
06/17/2022 10:35:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
06/17/2022 10:35:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
06/17/2022 10:35:31 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.5976009889053366 on epoch=181
06/17/2022 10:35:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
06/17/2022 10:35:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
06/17/2022 10:35:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
06/17/2022 10:35:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
06/17/2022 10:35:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=184
06/17/2022 10:35:47 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.5766086251014167 on epoch=184
06/17/2022 10:35:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
06/17/2022 10:35:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=185
06/17/2022 10:35:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
06/17/2022 10:35:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
06/17/2022 10:35:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
06/17/2022 10:36:00 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:36:00 - INFO - __main__ - Printing 3 examples
06/17/2022 10:36:00 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 10:36:00 - INFO - __main__ - ['false']
06/17/2022 10:36:00 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 10:36:00 - INFO - __main__ - ['false']
06/17/2022 10:36:00 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 10:36:00 - INFO - __main__ - ['false']
06/17/2022 10:36:00 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:36:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:36:01 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:36:01 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:36:01 - INFO - __main__ - Printing 3 examples
06/17/2022 10:36:01 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 10:36:01 - INFO - __main__ - ['false']
06/17/2022 10:36:01 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 10:36:01 - INFO - __main__ - ['false']
06/17/2022 10:36:01 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 10:36:01 - INFO - __main__ - ['false']
06/17/2022 10:36:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:36:01 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:36:01 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:36:03 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.5859751597657293 on epoch=187
06/17/2022 10:36:03 - INFO - __main__ - save last model!
06/17/2022 10:36:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 10:36:03 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 10:36:03 - INFO - __main__ - Printing 3 examples
06/17/2022 10:36:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 10:36:03 - INFO - __main__ - ['false']
06/17/2022 10:36:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 10:36:03 - INFO - __main__ - ['false']
06/17/2022 10:36:03 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 10:36:03 - INFO - __main__ - ['false']
06/17/2022 10:36:03 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:36:04 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:36:07 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 10:36:17 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:36:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:36:18 - INFO - __main__ - Starting training!
06/17/2022 10:36:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_21_0.2_8_predictions.txt
06/17/2022 10:36:46 - INFO - __main__ - Classification-F1 on test data: 0.4978
06/17/2022 10:36:47 - INFO - __main__ - prefix=wiki_qa_128_21, lr=0.2, bsz=8, dev_performance=0.6284470246734397, test_performance=0.49784844687629765
06/17/2022 10:36:47 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.5, bsz=8 ...
06/17/2022 10:36:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:36:48 - INFO - __main__ - Printing 3 examples
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:36:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:36:48 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:36:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:36:48 - INFO - __main__ - Printing 3 examples
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 10:36:48 - INFO - __main__ - ['false']
06/17/2022 10:36:48 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:36:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:36:48 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:37:07 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:37:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:37:08 - INFO - __main__ - Starting training!
06/17/2022 10:37:11 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=0
06/17/2022 10:37:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
06/17/2022 10:37:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=1
06/17/2022 10:37:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=2
06/17/2022 10:37:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
06/17/2022 10:37:25 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 10:37:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 10:37:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=3
06/17/2022 10:37:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
06/17/2022 10:37:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=4
06/17/2022 10:37:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=5
06/17/2022 10:37:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=6
06/17/2022 10:37:41 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.41832245588590766 on epoch=6
06/17/2022 10:37:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.41832245588590766 on epoch=6, global_step=100
06/17/2022 10:37:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=6
06/17/2022 10:37:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=7
06/17/2022 10:37:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=8
06/17/2022 10:37:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=8
06/17/2022 10:37:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=9
06/17/2022 10:37:58 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 10:38:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
06/17/2022 10:38:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=10
06/17/2022 10:38:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=11
06/17/2022 10:38:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
06/17/2022 10:38:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=12
06/17/2022 10:38:15 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 10:38:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
06/17/2022 10:38:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=13
06/17/2022 10:38:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 10:38:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 10:38:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
06/17/2022 10:38:31 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 10:38:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
06/17/2022 10:38:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
06/17/2022 10:38:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
06/17/2022 10:38:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=18
06/17/2022 10:38:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
06/17/2022 10:38:48 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 10:38:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
06/17/2022 10:38:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=19
06/17/2022 10:38:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=20
06/17/2022 10:38:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
06/17/2022 10:39:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
06/17/2022 10:39:04 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3771988921326446 on epoch=21
06/17/2022 10:39:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
06/17/2022 10:39:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=23
06/17/2022 10:39:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=23
06/17/2022 10:39:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=24
06/17/2022 10:39:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 10:39:21 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 10:39:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
06/17/2022 10:39:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=26
06/17/2022 10:39:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
06/17/2022 10:39:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
06/17/2022 10:39:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
06/17/2022 10:39:38 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 10:39:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=28
06/17/2022 10:39:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
06/17/2022 10:39:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=29
06/17/2022 10:39:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=30
06/17/2022 10:39:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
06/17/2022 10:39:54 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=31
06/17/2022 10:39:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
06/17/2022 10:39:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=32
06/17/2022 10:40:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
06/17/2022 10:40:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
06/17/2022 10:40:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
06/17/2022 10:40:10 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.37942309499201354 on epoch=34
06/17/2022 10:40:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
06/17/2022 10:40:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=35
06/17/2022 10:40:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=36
06/17/2022 10:40:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
06/17/2022 10:40:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
06/17/2022 10:40:27 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.497651675995625 on epoch=37
06/17/2022 10:40:27 - INFO - __main__ - Saving model with best Classification-F1: 0.41832245588590766 -> 0.497651675995625 on epoch=37, global_step=600
06/17/2022 10:40:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
06/17/2022 10:40:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
06/17/2022 10:40:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
06/17/2022 10:40:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
06/17/2022 10:40:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
06/17/2022 10:40:43 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.3729241807626285 on epoch=40
06/17/2022 10:40:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=41
06/17/2022 10:40:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=41
06/17/2022 10:40:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
06/17/2022 10:40:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
06/17/2022 10:40:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
06/17/2022 10:41:00 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 10:41:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=44
06/17/2022 10:41:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
06/17/2022 10:41:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
06/17/2022 10:41:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
06/17/2022 10:41:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
06/17/2022 10:41:17 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.4418604651162791 on epoch=46
06/17/2022 10:41:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 10:41:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
06/17/2022 10:41:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
06/17/2022 10:41:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
06/17/2022 10:41:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
06/17/2022 10:41:33 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=49
06/17/2022 10:41:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
06/17/2022 10:41:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
06/17/2022 10:41:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/17/2022 10:41:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=52
06/17/2022 10:41:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
06/17/2022 10:41:50 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=53
06/17/2022 10:41:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
06/17/2022 10:41:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=54
06/17/2022 10:41:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=54
06/17/2022 10:42:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=55
06/17/2022 10:42:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
06/17/2022 10:42:06 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.3816425120772947 on epoch=56
06/17/2022 10:42:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
06/17/2022 10:42:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
06/17/2022 10:42:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/17/2022 10:42:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
06/17/2022 10:42:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
06/17/2022 10:42:24 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3838573350768473 on epoch=59
06/17/2022 10:42:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/17/2022 10:42:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/17/2022 10:42:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=61
06/17/2022 10:42:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/17/2022 10:42:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
06/17/2022 10:42:41 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.5411971885860333 on epoch=62
06/17/2022 10:42:41 - INFO - __main__ - Saving model with best Classification-F1: 0.497651675995625 -> 0.5411971885860333 on epoch=62, global_step=1000
06/17/2022 10:42:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
06/17/2022 10:42:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
06/17/2022 10:42:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=64
06/17/2022 10:42:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=64
06/17/2022 10:42:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
06/17/2022 10:42:58 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.5630099444052933 on epoch=65
06/17/2022 10:42:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5411971885860333 -> 0.5630099444052933 on epoch=65, global_step=1050
06/17/2022 10:43:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=66
06/17/2022 10:43:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=66
06/17/2022 10:43:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=67
06/17/2022 10:43:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 10:43:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=68
06/17/2022 10:43:15 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.40030721966205834 on epoch=68
06/17/2022 10:43:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 10:43:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
06/17/2022 10:43:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/17/2022 10:43:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
06/17/2022 10:43:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
06/17/2022 10:43:33 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.5723328275629531 on epoch=71
06/17/2022 10:43:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5630099444052933 -> 0.5723328275629531 on epoch=71, global_step=1150
06/17/2022 10:43:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
06/17/2022 10:43:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
06/17/2022 10:43:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/17/2022 10:43:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=74
06/17/2022 10:43:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
06/17/2022 10:43:51 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.5301378985495476 on epoch=74
06/17/2022 10:43:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=75
06/17/2022 10:43:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
06/17/2022 10:43:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=76
06/17/2022 10:44:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=77
06/17/2022 10:44:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=78
06/17/2022 10:44:09 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.5618051222984755 on epoch=78
06/17/2022 10:44:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=78
06/17/2022 10:44:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=79
06/17/2022 10:44:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=79
06/17/2022 10:44:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=80
06/17/2022 10:44:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=81
06/17/2022 10:44:27 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.6367132066834515 on epoch=81
06/17/2022 10:44:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5723328275629531 -> 0.6367132066834515 on epoch=81, global_step=1300
06/17/2022 10:44:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=81
06/17/2022 10:44:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=82
06/17/2022 10:44:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=83
06/17/2022 10:44:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=83
06/17/2022 10:44:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=84
06/17/2022 10:44:44 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.6689655172413793 on epoch=84
06/17/2022 10:44:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6367132066834515 -> 0.6689655172413793 on epoch=84, global_step=1350
06/17/2022 10:44:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=84
06/17/2022 10:44:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=85
06/17/2022 10:44:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=86
06/17/2022 10:44:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=86
06/17/2022 10:44:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=87
06/17/2022 10:45:07 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.6578539058299409 on epoch=87
06/17/2022 10:45:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=88
06/17/2022 10:45:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
06/17/2022 10:45:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=89
06/17/2022 10:45:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=89
06/17/2022 10:45:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
06/17/2022 10:45:28 - INFO - __main__ - Global step 1450 Train loss 0.24 Classification-F1 0.671560940841055 on epoch=90
06/17/2022 10:45:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6689655172413793 -> 0.671560940841055 on epoch=90, global_step=1450
06/17/2022 10:45:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=91
06/17/2022 10:45:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
06/17/2022 10:45:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=92
06/17/2022 10:45:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=93
06/17/2022 10:45:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
06/17/2022 10:45:48 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.5028417695583232 on epoch=93
06/17/2022 10:45:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
06/17/2022 10:45:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=94
06/17/2022 10:45:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
06/17/2022 10:45:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
06/17/2022 10:46:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=96
06/17/2022 10:46:05 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.47871071893746214 on epoch=96
06/17/2022 10:46:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
06/17/2022 10:46:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=98
06/17/2022 10:46:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
06/17/2022 10:46:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
06/17/2022 10:46:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
06/17/2022 10:46:27 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.656386691300709 on epoch=99
06/17/2022 10:46:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
06/17/2022 10:46:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
06/17/2022 10:46:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=101
06/17/2022 10:46:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=102
06/17/2022 10:46:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=103
06/17/2022 10:46:47 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.5812125811619867 on epoch=103
06/17/2022 10:46:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
06/17/2022 10:46:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=104
06/17/2022 10:46:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
06/17/2022 10:46:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=105
06/17/2022 10:47:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=106
06/17/2022 10:47:07 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.7031068790819752 on epoch=106
06/17/2022 10:47:07 - INFO - __main__ - Saving model with best Classification-F1: 0.671560940841055 -> 0.7031068790819752 on epoch=106, global_step=1700
06/17/2022 10:47:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
06/17/2022 10:47:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
06/17/2022 10:47:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
06/17/2022 10:47:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
06/17/2022 10:47:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=109
06/17/2022 10:47:37 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.687934945299119 on epoch=109
06/17/2022 10:47:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
06/17/2022 10:47:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=110
06/17/2022 10:47:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=111
06/17/2022 10:47:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
06/17/2022 10:47:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=112
06/17/2022 10:48:00 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.6780960559406245 on epoch=112
06/17/2022 10:48:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
06/17/2022 10:48:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
06/17/2022 10:48:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
06/17/2022 10:48:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
06/17/2022 10:48:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
06/17/2022 10:48:20 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.6263722317405926 on epoch=115
06/17/2022 10:48:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=116
06/17/2022 10:48:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
06/17/2022 10:48:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=117
06/17/2022 10:48:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=118
06/17/2022 10:48:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
06/17/2022 10:48:48 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.48810331859624667 on epoch=118
06/17/2022 10:48:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=119
06/17/2022 10:48:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=119
06/17/2022 10:48:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
06/17/2022 10:48:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=121
06/17/2022 10:49:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=121
06/17/2022 10:49:12 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.6326322773774438 on epoch=121
06/17/2022 10:49:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
06/17/2022 10:49:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
06/17/2022 10:49:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=123
06/17/2022 10:49:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
06/17/2022 10:49:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
06/17/2022 10:49:35 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.6313725490196078 on epoch=124
06/17/2022 10:49:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
06/17/2022 10:49:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=126
06/17/2022 10:49:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=126
06/17/2022 10:49:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=127
06/17/2022 10:49:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=128
06/17/2022 10:49:55 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6374384236453202 on epoch=128
06/17/2022 10:49:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
06/17/2022 10:50:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=129
06/17/2022 10:50:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=129
06/17/2022 10:50:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=130
06/17/2022 10:50:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=131
06/17/2022 10:50:15 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.6309881616880328 on epoch=131
06/17/2022 10:50:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=131
06/17/2022 10:50:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
06/17/2022 10:50:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
06/17/2022 10:50:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=133
06/17/2022 10:50:28 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=134
06/17/2022 10:50:39 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.5966670456575763 on epoch=134
06/17/2022 10:50:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=134
06/17/2022 10:50:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
06/17/2022 10:50:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=136
06/17/2022 10:50:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=136
06/17/2022 10:50:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=137
06/17/2022 10:51:01 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.6483516483516483 on epoch=137
06/17/2022 10:51:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
06/17/2022 10:51:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=138
06/17/2022 10:51:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
06/17/2022 10:51:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=139
06/17/2022 10:51:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
06/17/2022 10:51:24 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.6678420417945078 on epoch=140
06/17/2022 10:51:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=141
06/17/2022 10:51:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
06/17/2022 10:51:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=142
06/17/2022 10:51:34 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=143
06/17/2022 10:51:37 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=143
06/17/2022 10:51:45 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.6536924306708478 on epoch=143
06/17/2022 10:51:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
06/17/2022 10:51:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
06/17/2022 10:51:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
06/17/2022 10:51:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=146
06/17/2022 10:51:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=146
06/17/2022 10:52:05 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.597948717948718 on epoch=146
06/17/2022 10:52:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
06/17/2022 10:52:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
06/17/2022 10:52:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
06/17/2022 10:52:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
06/17/2022 10:52:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=149
06/17/2022 10:52:26 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.6422503807684457 on epoch=149
06/17/2022 10:52:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
06/17/2022 10:52:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=151
06/17/2022 10:52:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=151
06/17/2022 10:52:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=152
06/17/2022 10:52:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
06/17/2022 10:52:46 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6552188552188551 on epoch=153
06/17/2022 10:52:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=153
06/17/2022 10:52:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
06/17/2022 10:52:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=154
06/17/2022 10:52:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=155
06/17/2022 10:52:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
06/17/2022 10:53:04 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.65625 on epoch=156
06/17/2022 10:53:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
06/17/2022 10:53:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
06/17/2022 10:53:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=158
06/17/2022 10:53:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=158
06/17/2022 10:53:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=159
06/17/2022 10:53:23 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6757367192149801 on epoch=159
06/17/2022 10:53:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
06/17/2022 10:53:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=160
06/17/2022 10:53:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
06/17/2022 10:53:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
06/17/2022 10:53:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
06/17/2022 10:53:43 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.6517006802721088 on epoch=162
06/17/2022 10:53:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
06/17/2022 10:53:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
06/17/2022 10:53:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=164
06/17/2022 10:53:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
06/17/2022 10:53:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=165
06/17/2022 10:54:03 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.669433683822173 on epoch=165
06/17/2022 10:54:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
06/17/2022 10:54:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
06/17/2022 10:54:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=167
06/17/2022 10:54:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
06/17/2022 10:54:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
06/17/2022 10:54:23 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.5998388526307308 on epoch=168
06/17/2022 10:54:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=169
06/17/2022 10:54:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=169
06/17/2022 10:54:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
06/17/2022 10:54:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
06/17/2022 10:54:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=171
06/17/2022 10:54:40 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.590464876851192 on epoch=171
06/17/2022 10:54:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
06/17/2022 10:54:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
06/17/2022 10:54:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=173
06/17/2022 10:54:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
06/17/2022 10:54:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=174
06/17/2022 10:54:58 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.6536924306708478 on epoch=174
06/17/2022 10:55:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=175
06/17/2022 10:55:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
06/17/2022 10:55:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
06/17/2022 10:55:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
06/17/2022 10:55:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
06/17/2022 10:55:16 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6592776171462664 on epoch=178
06/17/2022 10:55:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
06/17/2022 10:55:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=179
06/17/2022 10:55:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
06/17/2022 10:55:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
06/17/2022 10:55:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
06/17/2022 10:55:37 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6347065592635213 on epoch=181
06/17/2022 10:55:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/17/2022 10:55:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
06/17/2022 10:55:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
06/17/2022 10:55:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
06/17/2022 10:55:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
06/17/2022 10:55:55 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6357795217770434 on epoch=184
06/17/2022 10:55:57 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
06/17/2022 10:56:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
06/17/2022 10:56:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
06/17/2022 10:56:05 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
06/17/2022 10:56:08 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
06/17/2022 10:56:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:56:09 - INFO - __main__ - Printing 3 examples
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:56:09 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:56:09 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:56:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:56:09 - INFO - __main__ - Printing 3 examples
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 10:56:09 - INFO - __main__ - ['false']
06/17/2022 10:56:09 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:56:10 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:56:10 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:56:13 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6675578641814988 on epoch=187
06/17/2022 10:56:13 - INFO - __main__ - save last model!
06/17/2022 10:56:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 10:56:13 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 10:56:13 - INFO - __main__ - Printing 3 examples
06/17/2022 10:56:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 10:56:13 - INFO - __main__ - ['false']
06/17/2022 10:56:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 10:56:13 - INFO - __main__ - ['false']
06/17/2022 10:56:13 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 10:56:13 - INFO - __main__ - ['false']
06/17/2022 10:56:13 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:56:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:56:17 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 10:56:28 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:56:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:56:29 - INFO - __main__ - Starting training!
06/17/2022 10:57:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.5_8_predictions.txt
06/17/2022 10:57:16 - INFO - __main__ - Classification-F1 on test data: 0.5206
06/17/2022 10:57:17 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.5, bsz=8, dev_performance=0.7031068790819752, test_performance=0.5205800712500459
06/17/2022 10:57:17 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.4, bsz=8 ...
06/17/2022 10:57:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:57:17 - INFO - __main__ - Printing 3 examples
06/17/2022 10:57:17 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 10:57:17 - INFO - __main__ - ['false']
06/17/2022 10:57:17 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 10:57:17 - INFO - __main__ - ['false']
06/17/2022 10:57:17 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 10:57:17 - INFO - __main__ - ['false']
06/17/2022 10:57:17 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:57:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:57:18 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 10:57:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 10:57:18 - INFO - __main__ - Printing 3 examples
06/17/2022 10:57:18 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 10:57:18 - INFO - __main__ - ['false']
06/17/2022 10:57:18 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 10:57:18 - INFO - __main__ - ['false']
06/17/2022 10:57:18 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 10:57:18 - INFO - __main__ - ['false']
06/17/2022 10:57:18 - INFO - __main__ - Tokenizing Input ...
06/17/2022 10:57:18 - INFO - __main__ - Tokenizing Output ...
06/17/2022 10:57:18 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 10:57:37 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 10:57:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 10:57:38 - INFO - __main__ - Starting training!
06/17/2022 10:57:41 - INFO - __main__ - Step 10 Global step 10 Train loss 0.77 on epoch=0
06/17/2022 10:57:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
06/17/2022 10:57:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=1
06/17/2022 10:57:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=2
06/17/2022 10:57:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=3
06/17/2022 10:57:55 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 10:57:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 10:57:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
06/17/2022 10:58:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=4
06/17/2022 10:58:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
06/17/2022 10:58:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=5
06/17/2022 10:58:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=6
06/17/2022 10:58:12 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.45261279554509515 on epoch=6
06/17/2022 10:58:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.45261279554509515 on epoch=6, global_step=100
06/17/2022 10:58:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=6
06/17/2022 10:58:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
06/17/2022 10:58:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=8
06/17/2022 10:58:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=8
06/17/2022 10:58:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=9
06/17/2022 10:58:29 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 10:58:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=9
06/17/2022 10:58:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
06/17/2022 10:58:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=11
06/17/2022 10:58:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
06/17/2022 10:58:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=12
06/17/2022 10:58:46 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3383422492035824 on epoch=12
06/17/2022 10:58:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
06/17/2022 10:58:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=13
06/17/2022 10:58:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
06/17/2022 10:58:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=14
06/17/2022 10:58:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=15
06/17/2022 10:59:03 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.5652852356693745 on epoch=15
06/17/2022 10:59:03 - INFO - __main__ - Saving model with best Classification-F1: 0.45261279554509515 -> 0.5652852356693745 on epoch=15, global_step=250
06/17/2022 10:59:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=16
06/17/2022 10:59:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
06/17/2022 10:59:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
06/17/2022 10:59:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=18
06/17/2022 10:59:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=18
06/17/2022 10:59:20 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 10:59:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
06/17/2022 10:59:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
06/17/2022 10:59:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
06/17/2022 10:59:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
06/17/2022 10:59:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=21
06/17/2022 10:59:36 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.5652852356693745 on epoch=21
06/17/2022 10:59:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
06/17/2022 10:59:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
06/17/2022 10:59:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=23
06/17/2022 10:59:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
06/17/2022 10:59:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=24
06/17/2022 10:59:53 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 10:59:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=25
06/17/2022 10:59:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=26
06/17/2022 11:00:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
06/17/2022 11:00:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
06/17/2022 11:00:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
06/17/2022 11:00:10 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 11:00:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
06/17/2022 11:00:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
06/17/2022 11:00:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=29
06/17/2022 11:00:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=30
06/17/2022 11:00:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 11:00:27 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.33159268929503916 on epoch=31
06/17/2022 11:00:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
06/17/2022 11:00:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
06/17/2022 11:00:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=33
06/17/2022 11:00:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
06/17/2022 11:00:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=34
06/17/2022 11:00:44 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.39653775322283613 on epoch=34
06/17/2022 11:00:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
06/17/2022 11:00:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
06/17/2022 11:00:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=36
06/17/2022 11:00:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
06/17/2022 11:00:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=37
06/17/2022 11:01:01 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=37
06/17/2022 11:01:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
06/17/2022 11:01:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 11:01:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
06/17/2022 11:01:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
06/17/2022 11:01:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
06/17/2022 11:01:18 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.40828713708540826 on epoch=40
06/17/2022 11:01:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=41
06/17/2022 11:01:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=41
06/17/2022 11:01:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/17/2022 11:01:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/17/2022 11:01:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
06/17/2022 11:01:34 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 11:01:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
06/17/2022 11:01:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=44
06/17/2022 11:01:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=45
06/17/2022 11:01:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
06/17/2022 11:01:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
06/17/2022 11:01:51 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.5436720142602496 on epoch=46
06/17/2022 11:01:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
06/17/2022 11:01:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/17/2022 11:01:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=48
06/17/2022 11:02:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=49
06/17/2022 11:02:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/17/2022 11:02:08 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 11:02:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=50
06/17/2022 11:02:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=51
06/17/2022 11:02:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 11:02:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
06/17/2022 11:02:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
06/17/2022 11:02:25 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 11:02:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
06/17/2022 11:02:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
06/17/2022 11:02:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
06/17/2022 11:02:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
06/17/2022 11:02:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
06/17/2022 11:02:42 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=56
06/17/2022 11:02:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
06/17/2022 11:02:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 11:02:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
06/17/2022 11:02:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=58
06/17/2022 11:02:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
06/17/2022 11:02:59 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=59
06/17/2022 11:03:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=59
06/17/2022 11:03:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/17/2022 11:03:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
06/17/2022 11:03:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
06/17/2022 11:03:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 11:03:16 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=62
06/17/2022 11:03:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=63
06/17/2022 11:03:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
06/17/2022 11:03:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=64
06/17/2022 11:03:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
06/17/2022 11:03:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
06/17/2022 11:03:33 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.43504081815735174 on epoch=65
06/17/2022 11:03:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
06/17/2022 11:03:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
06/17/2022 11:03:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=67
06/17/2022 11:03:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
06/17/2022 11:03:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 11:03:50 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 11:03:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
06/17/2022 11:03:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
06/17/2022 11:03:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
06/17/2022 11:04:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=71
06/17/2022 11:04:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=71
06/17/2022 11:04:07 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.4330011074197121 on epoch=71
06/17/2022 11:04:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
06/17/2022 11:04:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
06/17/2022 11:04:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
06/17/2022 11:04:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
06/17/2022 11:04:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
06/17/2022 11:04:24 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=74
06/17/2022 11:04:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
06/17/2022 11:04:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
06/17/2022 11:04:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/17/2022 11:04:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.37 on epoch=77
06/17/2022 11:04:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=78
06/17/2022 11:04:41 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.470572685854675 on epoch=78
06/17/2022 11:04:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=78
06/17/2022 11:04:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=79
06/17/2022 11:04:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=79
06/17/2022 11:04:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
06/17/2022 11:04:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=81
06/17/2022 11:04:57 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.42578632262416727 on epoch=81
06/17/2022 11:05:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
06/17/2022 11:05:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/17/2022 11:05:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
06/17/2022 11:05:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
06/17/2022 11:05:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
06/17/2022 11:05:14 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.34673046251993617 on epoch=84
06/17/2022 11:05:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
06/17/2022 11:05:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
06/17/2022 11:05:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
06/17/2022 11:05:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
06/17/2022 11:05:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
06/17/2022 11:05:31 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.41165819346751165 on epoch=87
06/17/2022 11:05:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=88
06/17/2022 11:05:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=88
06/17/2022 11:05:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=89
06/17/2022 11:05:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=89
06/17/2022 11:05:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
06/17/2022 11:05:49 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.4546262663417474 on epoch=90
06/17/2022 11:05:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
06/17/2022 11:05:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
06/17/2022 11:05:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
06/17/2022 11:05:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
06/17/2022 11:06:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
06/17/2022 11:06:06 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.4079058031959629 on epoch=93
06/17/2022 11:06:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=94
06/17/2022 11:06:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
06/17/2022 11:06:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=95
06/17/2022 11:06:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
06/17/2022 11:06:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=96
06/17/2022 11:06:24 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.4478563781091439 on epoch=96
06/17/2022 11:06:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=97
06/17/2022 11:06:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=98
06/17/2022 11:06:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
06/17/2022 11:06:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=99
06/17/2022 11:06:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=99
06/17/2022 11:06:42 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.40096618357487923 on epoch=99
06/17/2022 11:06:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=100
06/17/2022 11:06:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
06/17/2022 11:06:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/17/2022 11:06:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
06/17/2022 11:06:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=103
06/17/2022 11:07:00 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.4514923513877124 on epoch=103
06/17/2022 11:07:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=103
06/17/2022 11:07:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=104
06/17/2022 11:07:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=104
06/17/2022 11:07:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 11:07:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=106
06/17/2022 11:07:18 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.47200366664120386 on epoch=106
06/17/2022 11:07:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
06/17/2022 11:07:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=107
06/17/2022 11:07:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=108
06/17/2022 11:07:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=108
06/17/2022 11:07:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
06/17/2022 11:07:36 - INFO - __main__ - Global step 1750 Train loss 0.35 Classification-F1 0.5070173006116067 on epoch=109
06/17/2022 11:07:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=109
06/17/2022 11:07:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=110
06/17/2022 11:07:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=111
06/17/2022 11:07:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=111
06/17/2022 11:07:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
06/17/2022 11:07:53 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.5403511559545564 on epoch=112
06/17/2022 11:07:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=113
06/17/2022 11:07:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=113
06/17/2022 11:08:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=114
06/17/2022 11:08:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=114
06/17/2022 11:08:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=115
06/17/2022 11:08:11 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.5263644773358003 on epoch=115
06/17/2022 11:08:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=116
06/17/2022 11:08:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=116
06/17/2022 11:08:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=117
06/17/2022 11:08:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=118
06/17/2022 11:08:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=118
06/17/2022 11:08:28 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.5218117199768576 on epoch=118
06/17/2022 11:08:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
06/17/2022 11:08:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.34 on epoch=119
06/17/2022 11:08:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.28 on epoch=120
06/17/2022 11:08:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=121
06/17/2022 11:08:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=121
06/17/2022 11:08:45 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.5545787545787546 on epoch=121
06/17/2022 11:08:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=122
06/17/2022 11:08:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=123
06/17/2022 11:08:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=123
06/17/2022 11:08:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=124
06/17/2022 11:08:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
06/17/2022 11:09:03 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.495069033530572 on epoch=124
06/17/2022 11:09:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.32 on epoch=125
06/17/2022 11:09:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=126
06/17/2022 11:09:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=126
06/17/2022 11:09:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.33 on epoch=127
06/17/2022 11:09:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=128
06/17/2022 11:09:20 - INFO - __main__ - Global step 2050 Train loss 0.32 Classification-F1 0.42752983181433807 on epoch=128
06/17/2022 11:09:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=128
06/17/2022 11:09:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.33 on epoch=129
06/17/2022 11:09:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=129
06/17/2022 11:09:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=130
06/17/2022 11:09:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.29 on epoch=131
06/17/2022 11:09:37 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.5532711466463609 on epoch=131
06/17/2022 11:09:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.27 on epoch=131
06/17/2022 11:09:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.34 on epoch=132
06/17/2022 11:09:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=133
06/17/2022 11:09:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=133
06/17/2022 11:09:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.32 on epoch=134
06/17/2022 11:09:54 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.5303643724696356 on epoch=134
06/17/2022 11:09:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.29 on epoch=134
06/17/2022 11:10:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=135
06/17/2022 11:10:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.29 on epoch=136
06/17/2022 11:10:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=136
06/17/2022 11:10:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.35 on epoch=137
06/17/2022 11:10:12 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.5405656290612042 on epoch=137
06/17/2022 11:10:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=138
06/17/2022 11:10:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=138
06/17/2022 11:10:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=139
06/17/2022 11:10:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=139
06/17/2022 11:10:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=140
06/17/2022 11:10:29 - INFO - __main__ - Global step 2250 Train loss 0.27 Classification-F1 0.5513742851872347 on epoch=140
06/17/2022 11:10:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=141
06/17/2022 11:10:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=141
06/17/2022 11:10:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=142
06/17/2022 11:10:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.33 on epoch=143
06/17/2022 11:10:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.31 on epoch=143
06/17/2022 11:10:47 - INFO - __main__ - Global step 2300 Train loss 0.29 Classification-F1 0.4804265565875472 on epoch=143
06/17/2022 11:10:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=144
06/17/2022 11:10:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=144
06/17/2022 11:10:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=145
06/17/2022 11:10:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.28 on epoch=146
06/17/2022 11:11:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=146
06/17/2022 11:11:03 - INFO - __main__ - Global step 2350 Train loss 0.26 Classification-F1 0.5611876339149067 on epoch=146
06/17/2022 11:11:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=147
06/17/2022 11:11:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.32 on epoch=148
06/17/2022 11:11:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.24 on epoch=148
06/17/2022 11:11:14 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.26 on epoch=149
06/17/2022 11:11:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=149
06/17/2022 11:11:21 - INFO - __main__ - Global step 2400 Train loss 0.26 Classification-F1 0.5728926670374195 on epoch=149
06/17/2022 11:11:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5652852356693745 -> 0.5728926670374195 on epoch=149, global_step=2400
06/17/2022 11:11:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.33 on epoch=150
06/17/2022 11:11:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=151
06/17/2022 11:11:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.28 on epoch=151
06/17/2022 11:11:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.30 on epoch=152
06/17/2022 11:11:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=153
06/17/2022 11:11:38 - INFO - __main__ - Global step 2450 Train loss 0.29 Classification-F1 0.6009510163593708 on epoch=153
06/17/2022 11:11:38 - INFO - __main__ - Saving model with best Classification-F1: 0.5728926670374195 -> 0.6009510163593708 on epoch=153, global_step=2450
06/17/2022 11:11:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.28 on epoch=153
06/17/2022 11:11:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=154
06/17/2022 11:11:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=154
06/17/2022 11:11:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.26 on epoch=155
06/17/2022 11:11:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=156
06/17/2022 11:11:55 - INFO - __main__ - Global step 2500 Train loss 0.25 Classification-F1 0.5615365793980915 on epoch=156
06/17/2022 11:11:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
06/17/2022 11:12:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=157
06/17/2022 11:12:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.24 on epoch=158
06/17/2022 11:12:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.23 on epoch=158
06/17/2022 11:12:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.30 on epoch=159
06/17/2022 11:12:13 - INFO - __main__ - Global step 2550 Train loss 0.25 Classification-F1 0.5813836477987422 on epoch=159
06/17/2022 11:12:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=159
06/17/2022 11:12:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.24 on epoch=160
06/17/2022 11:12:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=161
06/17/2022 11:12:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=161
06/17/2022 11:12:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=162
06/17/2022 11:12:31 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.5733333333333333 on epoch=162
06/17/2022 11:12:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=163
06/17/2022 11:12:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
06/17/2022 11:12:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.24 on epoch=164
06/17/2022 11:12:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.20 on epoch=164
06/17/2022 11:12:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=165
06/17/2022 11:12:48 - INFO - __main__ - Global step 2650 Train loss 0.21 Classification-F1 0.5518207282913166 on epoch=165
06/17/2022 11:12:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=166
06/17/2022 11:12:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=166
06/17/2022 11:12:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.23 on epoch=167
06/17/2022 11:12:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=168
06/17/2022 11:13:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.23 on epoch=168
06/17/2022 11:13:06 - INFO - __main__ - Global step 2700 Train loss 0.23 Classification-F1 0.4796747967479674 on epoch=168
06/17/2022 11:13:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=169
06/17/2022 11:13:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=169
06/17/2022 11:13:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=170
06/17/2022 11:13:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=171
06/17/2022 11:13:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
06/17/2022 11:13:23 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.611948982560365 on epoch=171
06/17/2022 11:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6009510163593708 -> 0.611948982560365 on epoch=171, global_step=2750
06/17/2022 11:13:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=172
06/17/2022 11:13:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=173
06/17/2022 11:13:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
06/17/2022 11:13:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=174
06/17/2022 11:13:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.24 on epoch=174
06/17/2022 11:13:41 - INFO - __main__ - Global step 2800 Train loss 0.21 Classification-F1 0.6210879682612345 on epoch=174
06/17/2022 11:13:41 - INFO - __main__ - Saving model with best Classification-F1: 0.611948982560365 -> 0.6210879682612345 on epoch=174, global_step=2800
06/17/2022 11:13:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
06/17/2022 11:13:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
06/17/2022 11:13:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=176
06/17/2022 11:13:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
06/17/2022 11:13:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=178
06/17/2022 11:13:59 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.6443955976858848 on epoch=178
06/17/2022 11:13:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6210879682612345 -> 0.6443955976858848 on epoch=178, global_step=2850
06/17/2022 11:14:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
06/17/2022 11:14:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=179
06/17/2022 11:14:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=179
06/17/2022 11:14:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=180
06/17/2022 11:14:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
06/17/2022 11:14:17 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.5708502024291497 on epoch=181
06/17/2022 11:14:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
06/17/2022 11:14:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
06/17/2022 11:14:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=183
06/17/2022 11:14:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=183
06/17/2022 11:14:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=184
06/17/2022 11:14:35 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.5713342038559028 on epoch=184
06/17/2022 11:14:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=184
06/17/2022 11:14:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
06/17/2022 11:14:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
06/17/2022 11:14:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=186
06/17/2022 11:14:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=187
06/17/2022 11:14:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:14:50 - INFO - __main__ - Printing 3 examples
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:14:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:14:50 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:14:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:14:50 - INFO - __main__ - Printing 3 examples
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 11:14:50 - INFO - __main__ - ['false']
06/17/2022 11:14:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:14:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:14:50 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:14:53 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.6053037311131668 on epoch=187
06/17/2022 11:14:53 - INFO - __main__ - save last model!
06/17/2022 11:14:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 11:14:53 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 11:14:53 - INFO - __main__ - Printing 3 examples
06/17/2022 11:14:53 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 11:14:53 - INFO - __main__ - ['false']
06/17/2022 11:14:53 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 11:14:53 - INFO - __main__ - ['false']
06/17/2022 11:14:53 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 11:14:53 - INFO - __main__ - ['false']
06/17/2022 11:14:53 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:14:55 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:14:57 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 11:15:09 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:15:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:15:10 - INFO - __main__ - Starting training!
06/17/2022 11:15:49 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.4_8_predictions.txt
06/17/2022 11:15:49 - INFO - __main__ - Classification-F1 on test data: 0.4252
06/17/2022 11:15:49 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.4, bsz=8, dev_performance=0.6443955976858848, test_performance=0.4252110162312843
06/17/2022 11:15:49 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.3, bsz=8 ...
06/17/2022 11:15:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:15:50 - INFO - __main__ - Printing 3 examples
06/17/2022 11:15:50 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 11:15:50 - INFO - __main__ - ['false']
06/17/2022 11:15:50 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 11:15:50 - INFO - __main__ - ['false']
06/17/2022 11:15:50 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 11:15:50 - INFO - __main__ - ['false']
06/17/2022 11:15:50 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:15:50 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:15:51 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:15:51 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:15:51 - INFO - __main__ - Printing 3 examples
06/17/2022 11:15:51 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 11:15:51 - INFO - __main__ - ['false']
06/17/2022 11:15:51 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 11:15:51 - INFO - __main__ - ['false']
06/17/2022 11:15:51 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 11:15:51 - INFO - __main__ - ['false']
06/17/2022 11:15:51 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:15:51 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:15:51 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:16:09 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:16:10 - INFO - __main__ - Starting training!
06/17/2022 11:16:13 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=0
06/17/2022 11:16:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
06/17/2022 11:16:18 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=1
06/17/2022 11:16:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
06/17/2022 11:16:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=3
06/17/2022 11:16:27 - INFO - __main__ - Global step 50 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 11:16:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 11:16:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=3
06/17/2022 11:16:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
06/17/2022 11:16:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=4
06/17/2022 11:16:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=5
06/17/2022 11:16:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=6
06/17/2022 11:16:44 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3651088894055646 on epoch=6
06/17/2022 11:16:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3651088894055646 on epoch=6, global_step=100
06/17/2022 11:16:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=6
06/17/2022 11:16:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=7
06/17/2022 11:16:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
06/17/2022 11:16:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
06/17/2022 11:16:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
06/17/2022 11:17:00 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 11:17:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.35 on epoch=9
06/17/2022 11:17:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=10
06/17/2022 11:17:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=11
06/17/2022 11:17:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=11
06/17/2022 11:17:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=12
06/17/2022 11:17:17 - INFO - __main__ - Global step 200 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 11:17:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=13
06/17/2022 11:17:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 11:17:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=14
06/17/2022 11:17:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=14
06/17/2022 11:17:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=15
06/17/2022 11:17:33 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.34195559333697656 on epoch=15
06/17/2022 11:17:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=16
06/17/2022 11:17:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=16
06/17/2022 11:17:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=17
06/17/2022 11:17:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=18
06/17/2022 11:17:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
06/17/2022 11:17:49 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=18
06/17/2022 11:17:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
06/17/2022 11:17:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
06/17/2022 11:17:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=20
06/17/2022 11:18:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=21
06/17/2022 11:18:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
06/17/2022 11:18:06 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.4181818181818182 on epoch=21
06/17/2022 11:18:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3651088894055646 -> 0.4181818181818182 on epoch=21, global_step=350
06/17/2022 11:18:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=22
06/17/2022 11:18:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
06/17/2022 11:18:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
06/17/2022 11:18:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/17/2022 11:18:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 11:18:22 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=24
06/17/2022 11:18:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
06/17/2022 11:18:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
06/17/2022 11:18:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=26
06/17/2022 11:18:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=27
06/17/2022 11:18:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
06/17/2022 11:18:39 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.34195559333697656 on epoch=28
06/17/2022 11:18:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=28
06/17/2022 11:18:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
06/17/2022 11:18:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=29
06/17/2022 11:18:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=30
06/17/2022 11:18:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 11:18:55 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.4083606042117329 on epoch=31
06/17/2022 11:18:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
06/17/2022 11:19:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=32
06/17/2022 11:19:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/17/2022 11:19:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=33
06/17/2022 11:19:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
06/17/2022 11:19:12 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.34195559333697656 on epoch=34
06/17/2022 11:19:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
06/17/2022 11:19:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
06/17/2022 11:19:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
06/17/2022 11:19:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
06/17/2022 11:19:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=37
06/17/2022 11:19:28 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3651088894055646 on epoch=37
06/17/2022 11:19:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=38
06/17/2022 11:19:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
06/17/2022 11:19:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
06/17/2022 11:19:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=39
06/17/2022 11:19:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
06/17/2022 11:19:45 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.39653775322283613 on epoch=40
06/17/2022 11:19:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
06/17/2022 11:19:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=41
06/17/2022 11:19:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
06/17/2022 11:19:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=43
06/17/2022 11:19:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=43
06/17/2022 11:20:01 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 11:20:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
06/17/2022 11:20:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
06/17/2022 11:20:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
06/17/2022 11:20:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
06/17/2022 11:20:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
06/17/2022 11:20:18 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=46
06/17/2022 11:20:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
06/17/2022 11:20:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
06/17/2022 11:20:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=48
06/17/2022 11:20:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/17/2022 11:20:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/17/2022 11:20:34 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3486005089058525 on epoch=49
06/17/2022 11:20:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/17/2022 11:20:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=51
06/17/2022 11:20:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
06/17/2022 11:20:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
06/17/2022 11:20:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/17/2022 11:20:51 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 11:20:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
06/17/2022 11:20:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/17/2022 11:20:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 11:21:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=55
06/17/2022 11:21:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 11:21:07 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.44713730383452505 on epoch=56
06/17/2022 11:21:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.44713730383452505 on epoch=56, global_step=900
06/17/2022 11:21:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=56
06/17/2022 11:21:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
06/17/2022 11:21:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
06/17/2022 11:21:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
06/17/2022 11:21:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=59
06/17/2022 11:21:24 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.39047619047619053 on epoch=59
06/17/2022 11:21:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=59
06/17/2022 11:21:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/17/2022 11:21:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=61
06/17/2022 11:21:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/17/2022 11:21:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/17/2022 11:21:40 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.4035699216671864 on epoch=62
06/17/2022 11:21:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
06/17/2022 11:21:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/17/2022 11:21:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
06/17/2022 11:21:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
06/17/2022 11:21:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=65
06/17/2022 11:21:57 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.4171048536373614 on epoch=65
06/17/2022 11:21:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=66
06/17/2022 11:22:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
06/17/2022 11:22:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
06/17/2022 11:22:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
06/17/2022 11:22:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
06/17/2022 11:22:13 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 11:22:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
06/17/2022 11:22:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=69
06/17/2022 11:22:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
06/17/2022 11:22:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=71
06/17/2022 11:22:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
06/17/2022 11:22:29 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=71
06/17/2022 11:22:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=72
06/17/2022 11:22:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
06/17/2022 11:22:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
06/17/2022 11:22:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=74
06/17/2022 11:22:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=74
06/17/2022 11:22:46 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 11:22:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=75
06/17/2022 11:22:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=76
06/17/2022 11:22:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
06/17/2022 11:22:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
06/17/2022 11:22:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.37 on epoch=78
06/17/2022 11:23:02 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3409139669598799 on epoch=78
06/17/2022 11:23:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=78
06/17/2022 11:23:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
06/17/2022 11:23:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
06/17/2022 11:23:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
06/17/2022 11:23:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=81
06/17/2022 11:23:19 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.41296252132597094 on epoch=81
06/17/2022 11:23:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
06/17/2022 11:23:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=82
06/17/2022 11:23:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=83
06/17/2022 11:23:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=83
06/17/2022 11:23:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
06/17/2022 11:23:35 - INFO - __main__ - Global step 1350 Train loss 0.39 Classification-F1 0.35501021683496337 on epoch=84
06/17/2022 11:23:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
06/17/2022 11:23:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
06/17/2022 11:23:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=86
06/17/2022 11:23:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/17/2022 11:23:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
06/17/2022 11:23:52 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3822393822393822 on epoch=87
06/17/2022 11:23:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=88
06/17/2022 11:23:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
06/17/2022 11:23:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=89
06/17/2022 11:24:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=89
06/17/2022 11:24:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
06/17/2022 11:24:08 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.41296252132597094 on epoch=90
06/17/2022 11:24:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
06/17/2022 11:24:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=91
06/17/2022 11:24:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=92
06/17/2022 11:24:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=93
06/17/2022 11:24:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=93
06/17/2022 11:24:25 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.4536380351983189 on epoch=93
06/17/2022 11:24:25 - INFO - __main__ - Saving model with best Classification-F1: 0.44713730383452505 -> 0.4536380351983189 on epoch=93, global_step=1500
06/17/2022 11:24:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 11:24:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=94
06/17/2022 11:24:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/17/2022 11:24:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=96
06/17/2022 11:24:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=96
06/17/2022 11:24:42 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.5145583557621727 on epoch=96
06/17/2022 11:24:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4536380351983189 -> 0.5145583557621727 on epoch=96, global_step=1550
06/17/2022 11:24:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=97
06/17/2022 11:24:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=98
06/17/2022 11:24:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
06/17/2022 11:24:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=99
06/17/2022 11:24:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/17/2022 11:24:59 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.4168769273899635 on epoch=99
06/17/2022 11:25:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
06/17/2022 11:25:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
06/17/2022 11:25:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=101
06/17/2022 11:25:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=102
06/17/2022 11:25:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
06/17/2022 11:25:15 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.5174030989930571 on epoch=103
06/17/2022 11:25:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5145583557621727 -> 0.5174030989930571 on epoch=103, global_step=1650
06/17/2022 11:25:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=103
06/17/2022 11:25:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
06/17/2022 11:25:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=104
06/17/2022 11:25:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.36 on epoch=105
06/17/2022 11:25:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=106
06/17/2022 11:25:32 - INFO - __main__ - Global step 1700 Train loss 0.36 Classification-F1 0.49996948055911616 on epoch=106
06/17/2022 11:25:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=106
06/17/2022 11:25:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=107
06/17/2022 11:25:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=108
06/17/2022 11:25:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=108
06/17/2022 11:25:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=109
06/17/2022 11:25:48 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3963845952318575 on epoch=109
06/17/2022 11:25:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
06/17/2022 11:25:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=110
06/17/2022 11:25:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=111
06/17/2022 11:25:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=111
06/17/2022 11:26:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=112
06/17/2022 11:26:05 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.460545937884977 on epoch=112
06/17/2022 11:26:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=113
06/17/2022 11:26:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=113
06/17/2022 11:26:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=114
06/17/2022 11:26:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=114
06/17/2022 11:26:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=115
06/17/2022 11:26:22 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.5026827012025902 on epoch=115
06/17/2022 11:26:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=116
06/17/2022 11:26:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=116
06/17/2022 11:26:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.34 on epoch=117
06/17/2022 11:26:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=118
06/17/2022 11:26:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=118
06/17/2022 11:26:39 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.49215650369285235 on epoch=118
06/17/2022 11:26:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
06/17/2022 11:26:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=119
06/17/2022 11:26:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=120
06/17/2022 11:26:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=121
06/17/2022 11:26:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=121
06/17/2022 11:26:56 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.526990089636111 on epoch=121
06/17/2022 11:26:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5174030989930571 -> 0.526990089636111 on epoch=121, global_step=1950
06/17/2022 11:26:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=122
06/17/2022 11:27:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=123
06/17/2022 11:27:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=123
06/17/2022 11:27:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=124
06/17/2022 11:27:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=124
06/17/2022 11:27:12 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.4506527757301751 on epoch=124
06/17/2022 11:27:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=125
06/17/2022 11:27:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=126
06/17/2022 11:27:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.34 on epoch=126
06/17/2022 11:27:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.37 on epoch=127
06/17/2022 11:27:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=128
06/17/2022 11:27:29 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.5220079583715946 on epoch=128
06/17/2022 11:27:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=128
06/17/2022 11:27:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=129
06/17/2022 11:27:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=129
06/17/2022 11:27:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=130
06/17/2022 11:27:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=131
06/17/2022 11:27:46 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.4874874874874875 on epoch=131
06/17/2022 11:27:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=131
06/17/2022 11:27:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=132
06/17/2022 11:27:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=133
06/17/2022 11:27:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.34 on epoch=133
06/17/2022 11:27:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.35 on epoch=134
06/17/2022 11:28:03 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.39405460814462767 on epoch=134
06/17/2022 11:28:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=134
06/17/2022 11:28:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=135
06/17/2022 11:28:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
06/17/2022 11:28:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
06/17/2022 11:28:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=137
06/17/2022 11:28:20 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.40116959064327484 on epoch=137
06/17/2022 11:28:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=138
06/17/2022 11:28:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=138
06/17/2022 11:28:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=139
06/17/2022 11:28:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=139
06/17/2022 11:28:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.36 on epoch=140
06/17/2022 11:28:37 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.5153587786259541 on epoch=140
06/17/2022 11:28:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=141
06/17/2022 11:28:42 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=141
06/17/2022 11:28:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=142
06/17/2022 11:28:47 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=143
06/17/2022 11:28:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.35 on epoch=143
06/17/2022 11:28:53 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.45115113134111745 on epoch=143
06/17/2022 11:28:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=144
06/17/2022 11:28:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=144
06/17/2022 11:29:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.34 on epoch=145
06/17/2022 11:29:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
06/17/2022 11:29:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=146
06/17/2022 11:29:10 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.50778245742538 on epoch=146
06/17/2022 11:29:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=147
06/17/2022 11:29:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=148
06/17/2022 11:29:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=148
06/17/2022 11:29:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=149
06/17/2022 11:29:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.33 on epoch=149
06/17/2022 11:29:26 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.4284810237839928 on epoch=149
06/17/2022 11:29:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=150
06/17/2022 11:29:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=151
06/17/2022 11:29:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.33 on epoch=151
06/17/2022 11:29:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.35 on epoch=152
06/17/2022 11:29:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=153
06/17/2022 11:29:43 - INFO - __main__ - Global step 2450 Train loss 0.35 Classification-F1 0.5111145061492628 on epoch=153
06/17/2022 11:29:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
06/17/2022 11:29:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=154
06/17/2022 11:29:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=154
06/17/2022 11:29:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=155
06/17/2022 11:29:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=156
06/17/2022 11:29:59 - INFO - __main__ - Global step 2500 Train loss 0.35 Classification-F1 0.49912957742139236 on epoch=156
06/17/2022 11:30:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.33 on epoch=156
06/17/2022 11:30:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.36 on epoch=157
06/17/2022 11:30:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=158
06/17/2022 11:30:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=158
06/17/2022 11:30:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.37 on epoch=159
06/17/2022 11:30:16 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.4272727272727273 on epoch=159
06/17/2022 11:30:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=159
06/17/2022 11:30:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=160
06/17/2022 11:30:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=161
06/17/2022 11:30:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=161
06/17/2022 11:30:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=162
06/17/2022 11:30:33 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.4298644146311045 on epoch=162
06/17/2022 11:30:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.36 on epoch=163
06/17/2022 11:30:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=163
06/17/2022 11:30:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=164
06/17/2022 11:30:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=164
06/17/2022 11:30:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=165
06/17/2022 11:30:49 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.5247257383966245 on epoch=165
06/17/2022 11:30:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=166
06/17/2022 11:30:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=166
06/17/2022 11:30:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=167
06/17/2022 11:31:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=168
06/17/2022 11:31:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=168
06/17/2022 11:31:06 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.3651088894055646 on epoch=168
06/17/2022 11:31:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=169
06/17/2022 11:31:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=169
06/17/2022 11:31:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=170
06/17/2022 11:31:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=171
06/17/2022 11:31:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=171
06/17/2022 11:31:23 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.5206890095487736 on epoch=171
06/17/2022 11:31:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=172
06/17/2022 11:31:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=173
06/17/2022 11:31:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=173
06/17/2022 11:31:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=174
06/17/2022 11:31:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=174
06/17/2022 11:31:40 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.41539594843462246 on epoch=174
06/17/2022 11:31:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.35 on epoch=175
06/17/2022 11:31:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.34 on epoch=176
06/17/2022 11:31:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=176
06/17/2022 11:31:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.31 on epoch=177
06/17/2022 11:31:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.35 on epoch=178
06/17/2022 11:31:57 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.476482617586912 on epoch=178
06/17/2022 11:31:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.34 on epoch=178
06/17/2022 11:32:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=179
06/17/2022 11:32:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=179
06/17/2022 11:32:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=180
06/17/2022 11:32:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=181
06/17/2022 11:32:13 - INFO - __main__ - Global step 2900 Train loss 0.34 Classification-F1 0.5191717440102617 on epoch=181
06/17/2022 11:32:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.37 on epoch=181
06/17/2022 11:32:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=182
06/17/2022 11:32:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=183
06/17/2022 11:32:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=183
06/17/2022 11:32:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=184
06/17/2022 11:32:30 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.46281389748882007 on epoch=184
06/17/2022 11:32:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=184
06/17/2022 11:32:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=185
06/17/2022 11:32:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=186
06/17/2022 11:32:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.31 on epoch=186
06/17/2022 11:32:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.38 on epoch=187
06/17/2022 11:32:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:32:44 - INFO - __main__ - Printing 3 examples
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:32:44 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:32:44 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:32:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:32:44 - INFO - __main__ - Printing 3 examples
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 11:32:44 - INFO - __main__ - ['false']
06/17/2022 11:32:44 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:32:45 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:32:45 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:32:46 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.5586206896551724 on epoch=187
06/17/2022 11:32:46 - INFO - __main__ - Saving model with best Classification-F1: 0.526990089636111 -> 0.5586206896551724 on epoch=187, global_step=3000
06/17/2022 11:32:47 - INFO - __main__ - save last model!
06/17/2022 11:32:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 11:32:47 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 11:32:47 - INFO - __main__ - Printing 3 examples
06/17/2022 11:32:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 11:32:47 - INFO - __main__ - ['false']
06/17/2022 11:32:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 11:32:47 - INFO - __main__ - ['false']
06/17/2022 11:32:47 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 11:32:47 - INFO - __main__ - ['false']
06/17/2022 11:32:47 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:32:48 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:32:51 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 11:33:03 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:33:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:33:04 - INFO - __main__ - Starting training!
06/17/2022 11:33:32 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.3_8_predictions.txt
06/17/2022 11:33:32 - INFO - __main__ - Classification-F1 on test data: 0.4579
06/17/2022 11:33:32 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.3, bsz=8, dev_performance=0.5586206896551724, test_performance=0.4579081544544287
06/17/2022 11:33:32 - INFO - __main__ - Running ... prefix=wiki_qa_128_42, lr=0.2, bsz=8 ...
06/17/2022 11:33:33 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:33:33 - INFO - __main__ - Printing 3 examples
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: who is leaving criminal minds [SEP] answer: Criminal Minds is an American police procedural television program that premiered September 22, 2005, on CBS .
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: how many states have open carry gun laws [SEP] answer: This has been marked by a number of organized events intended to increase the visibility of open carry and public awareness about the practice.
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: how many countries have english as an official language [SEP] answer: Notable exceptions include Rwanda , which was a former Belgian colony and Eritrea , which was an Italian colony where the British Empire spanned its control only in World War II and shortly after( 1941-1952).
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:33:33 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:33:33 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:33:33 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:33:33 - INFO - __main__ - Printing 3 examples
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: what are the side effects for lyme disease [SEP] answer: Lyme disease is the most common tick-borne disease in the Northern Hemisphere .
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: how many gold medals usa won for basketball [SEP] answer: The USA won its first seven games at the 2006 FIBA World Championship in Japan before losing against Greece in the semi-finals, ending the competition with the bronze medal.
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ -  [wiki_qa] question: who played in the 2010 NBA Finals [SEP] answer: Repeated baskets from starters Kobe Bryant , Pau Gasol , and Ron Artest brought the Lakers close to victory in Game 1.
06/17/2022 11:33:33 - INFO - __main__ - ['false']
06/17/2022 11:33:33 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:33:34 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:33:34 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:33:52 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:33:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:33:53 - INFO - __main__ - Starting training!
06/17/2022 11:33:56 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=0
06/17/2022 11:33:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.45 on epoch=1
06/17/2022 11:34:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=1
06/17/2022 11:34:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=2
06/17/2022 11:34:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=3
06/17/2022 11:34:10 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 11:34:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 11:34:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
06/17/2022 11:34:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=4
06/17/2022 11:34:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
06/17/2022 11:34:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=5
06/17/2022 11:34:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
06/17/2022 11:34:27 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.35693779904306216 on epoch=6
06/17/2022 11:34:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.35693779904306216 on epoch=6, global_step=100
06/17/2022 11:34:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=6
06/17/2022 11:34:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=7
06/17/2022 11:34:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=8
06/17/2022 11:34:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=8
06/17/2022 11:34:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=9
06/17/2022 11:34:43 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 11:34:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=9
06/17/2022 11:34:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=10
06/17/2022 11:34:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=11
06/17/2022 11:34:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
06/17/2022 11:34:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=12
06/17/2022 11:35:00 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 11:35:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=13
06/17/2022 11:35:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=13
06/17/2022 11:35:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
06/17/2022 11:35:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=14
06/17/2022 11:35:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=15
06/17/2022 11:35:16 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=15
06/17/2022 11:35:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=16
06/17/2022 11:35:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=16
06/17/2022 11:35:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
06/17/2022 11:35:26 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=18
06/17/2022 11:35:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
06/17/2022 11:35:32 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 11:35:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=19
06/17/2022 11:35:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=19
06/17/2022 11:35:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=20
06/17/2022 11:35:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
06/17/2022 11:35:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=21
06/17/2022 11:35:49 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.48432841833857065 on epoch=21
06/17/2022 11:35:49 - INFO - __main__ - Saving model with best Classification-F1: 0.35693779904306216 -> 0.48432841833857065 on epoch=21, global_step=350
06/17/2022 11:35:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=22
06/17/2022 11:35:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
06/17/2022 11:35:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
06/17/2022 11:35:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/17/2022 11:36:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=24
06/17/2022 11:36:05 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 11:36:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
06/17/2022 11:36:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
06/17/2022 11:36:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
06/17/2022 11:36:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 11:36:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
06/17/2022 11:36:22 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.35885876860812244 on epoch=28
06/17/2022 11:36:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=28
06/17/2022 11:36:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/17/2022 11:36:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
06/17/2022 11:36:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
06/17/2022 11:36:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 11:36:39 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5261217433873361 on epoch=31
06/17/2022 11:36:39 - INFO - __main__ - Saving model with best Classification-F1: 0.48432841833857065 -> 0.5261217433873361 on epoch=31, global_step=500
06/17/2022 11:36:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
06/17/2022 11:36:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
06/17/2022 11:36:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
06/17/2022 11:36:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=33
06/17/2022 11:36:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 11:36:55 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.4572309227288115 on epoch=34
06/17/2022 11:36:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
06/17/2022 11:37:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
06/17/2022 11:37:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
06/17/2022 11:37:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/17/2022 11:37:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=37
06/17/2022 11:37:11 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.47807081926571093 on epoch=37
06/17/2022 11:37:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
06/17/2022 11:37:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=38
06/17/2022 11:37:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
06/17/2022 11:37:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
06/17/2022 11:37:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=40
06/17/2022 11:37:28 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5403511559545564 on epoch=40
06/17/2022 11:37:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5261217433873361 -> 0.5403511559545564 on epoch=40, global_step=650
06/17/2022 11:37:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=41
06/17/2022 11:37:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
06/17/2022 11:37:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/17/2022 11:37:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
06/17/2022 11:37:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
06/17/2022 11:37:44 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.36516753625488524 on epoch=43
06/17/2022 11:37:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
06/17/2022 11:37:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 11:37:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
06/17/2022 11:37:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=46
06/17/2022 11:37:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=46
06/17/2022 11:38:01 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.5857099236641221 on epoch=46
06/17/2022 11:38:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5403511559545564 -> 0.5857099236641221 on epoch=46, global_step=750
06/17/2022 11:38:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/17/2022 11:38:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
06/17/2022 11:38:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=48
06/17/2022 11:38:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
06/17/2022 11:38:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=49
06/17/2022 11:38:18 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.48726030995534536 on epoch=49
06/17/2022 11:38:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=50
06/17/2022 11:38:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/17/2022 11:38:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
06/17/2022 11:38:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
06/17/2022 11:38:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=53
06/17/2022 11:38:34 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.45207864760937383 on epoch=53
06/17/2022 11:38:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=53
06/17/2022 11:38:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
06/17/2022 11:38:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/17/2022 11:38:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
06/17/2022 11:38:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
06/17/2022 11:38:51 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.5446545590713349 on epoch=56
06/17/2022 11:38:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
06/17/2022 11:38:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
06/17/2022 11:38:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
06/17/2022 11:39:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/17/2022 11:39:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=59
06/17/2022 11:39:07 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.5545787545787546 on epoch=59
06/17/2022 11:39:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=59
06/17/2022 11:39:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/17/2022 11:39:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=61
06/17/2022 11:39:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/17/2022 11:39:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=62
06/17/2022 11:39:24 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.5778931297709924 on epoch=62
06/17/2022 11:39:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=63
06/17/2022 11:39:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
06/17/2022 11:39:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=64
06/17/2022 11:39:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=64
06/17/2022 11:39:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
06/17/2022 11:39:40 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.5651099579935754 on epoch=65
06/17/2022 11:39:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=66
06/17/2022 11:39:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=66
06/17/2022 11:39:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=67
06/17/2022 11:39:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=68
06/17/2022 11:39:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=68
06/17/2022 11:39:57 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.5440960123886953 on epoch=68
06/17/2022 11:39:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/17/2022 11:40:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=69
06/17/2022 11:40:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=70
06/17/2022 11:40:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=71
06/17/2022 11:40:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
06/17/2022 11:40:13 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.520769036175057 on epoch=71
06/17/2022 11:40:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=72
06/17/2022 11:40:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=73
06/17/2022 11:40:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
06/17/2022 11:40:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=74
06/17/2022 11:40:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
06/17/2022 11:40:29 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.5519528371407516 on epoch=74
06/17/2022 11:40:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=75
06/17/2022 11:40:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
06/17/2022 11:40:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.30 on epoch=76
06/17/2022 11:40:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=77
06/17/2022 11:40:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=78
06/17/2022 11:40:46 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.5755365010185228 on epoch=78
06/17/2022 11:40:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=78
06/17/2022 11:40:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=79
06/17/2022 11:40:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=79
06/17/2022 11:40:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
06/17/2022 11:40:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=81
06/17/2022 11:41:02 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.5440332475756976 on epoch=81
06/17/2022 11:41:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=81
06/17/2022 11:41:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=82
06/17/2022 11:41:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=83
06/17/2022 11:41:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=83
06/17/2022 11:41:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=84
06/17/2022 11:41:19 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.5897874158743723 on epoch=84
06/17/2022 11:41:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5857099236641221 -> 0.5897874158743723 on epoch=84, global_step=1350
06/17/2022 11:41:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=84
06/17/2022 11:41:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
06/17/2022 11:41:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
06/17/2022 11:41:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=86
06/17/2022 11:41:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
06/17/2022 11:41:35 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.5928553951553707 on epoch=87
06/17/2022 11:41:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5897874158743723 -> 0.5928553951553707 on epoch=87, global_step=1400
06/17/2022 11:41:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
06/17/2022 11:41:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
06/17/2022 11:41:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
06/17/2022 11:41:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=89
06/17/2022 11:41:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=90
06/17/2022 11:41:51 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.5755755755755756 on epoch=90
06/17/2022 11:41:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
06/17/2022 11:41:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=91
06/17/2022 11:41:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
06/17/2022 11:42:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=93
06/17/2022 11:42:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
06/17/2022 11:42:08 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.605173545894605 on epoch=93
06/17/2022 11:42:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5928553951553707 -> 0.605173545894605 on epoch=93, global_step=1500
06/17/2022 11:42:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=94
06/17/2022 11:42:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=94
06/17/2022 11:42:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=95
06/17/2022 11:42:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=96
06/17/2022 11:42:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
06/17/2022 11:42:25 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.609375 on epoch=96
06/17/2022 11:42:25 - INFO - __main__ - Saving model with best Classification-F1: 0.605173545894605 -> 0.609375 on epoch=96, global_step=1550
06/17/2022 11:42:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=97
06/17/2022 11:42:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=98
06/17/2022 11:42:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
06/17/2022 11:42:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=99
06/17/2022 11:42:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
06/17/2022 11:42:43 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.6053181908381798 on epoch=99
06/17/2022 11:42:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=100
06/17/2022 11:42:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
06/17/2022 11:42:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=101
06/17/2022 11:42:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=102
06/17/2022 11:42:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=103
06/17/2022 11:42:59 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.6049805209686044 on epoch=103
06/17/2022 11:43:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=103
06/17/2022 11:43:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=104
06/17/2022 11:43:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
06/17/2022 11:43:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
06/17/2022 11:43:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
06/17/2022 11:43:16 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.5974098057354302 on epoch=106
06/17/2022 11:43:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
06/17/2022 11:43:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=107
06/17/2022 11:43:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=108
06/17/2022 11:43:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
06/17/2022 11:43:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
06/17/2022 11:43:33 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.615686274509804 on epoch=109
06/17/2022 11:43:33 - INFO - __main__ - Saving model with best Classification-F1: 0.609375 -> 0.615686274509804 on epoch=109, global_step=1750
06/17/2022 11:43:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=109
06/17/2022 11:43:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
06/17/2022 11:43:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=111
06/17/2022 11:43:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=111
06/17/2022 11:43:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=112
06/17/2022 11:43:49 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.5945673561453184 on epoch=112
06/17/2022 11:43:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=113
06/17/2022 11:43:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
06/17/2022 11:43:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=114
06/17/2022 11:43:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
06/17/2022 11:44:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
06/17/2022 11:44:06 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.6326322773774438 on epoch=115
06/17/2022 11:44:06 - INFO - __main__ - Saving model with best Classification-F1: 0.615686274509804 -> 0.6326322773774438 on epoch=115, global_step=1850
06/17/2022 11:44:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=116
06/17/2022 11:44:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
06/17/2022 11:44:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
06/17/2022 11:44:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
06/17/2022 11:44:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
06/17/2022 11:44:24 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.5762668703845175 on epoch=118
06/17/2022 11:44:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.21 on epoch=119
06/17/2022 11:44:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=119
06/17/2022 11:44:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=120
06/17/2022 11:44:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
06/17/2022 11:44:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
06/17/2022 11:44:41 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.5666666666666667 on epoch=121
06/17/2022 11:44:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
06/17/2022 11:44:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=123
06/17/2022 11:44:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
06/17/2022 11:44:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
06/17/2022 11:44:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=124
06/17/2022 11:44:58 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.5588547189819725 on epoch=124
06/17/2022 11:45:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
06/17/2022 11:45:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
06/17/2022 11:45:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
06/17/2022 11:45:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
06/17/2022 11:45:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
06/17/2022 11:45:15 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.632453567937439 on epoch=128
06/17/2022 11:45:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=128
06/17/2022 11:45:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
06/17/2022 11:45:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=129
06/17/2022 11:45:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=130
06/17/2022 11:45:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=131
06/17/2022 11:45:34 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.5651282051282052 on epoch=131
06/17/2022 11:45:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
06/17/2022 11:45:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=132
06/17/2022 11:45:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=133
06/17/2022 11:45:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
06/17/2022 11:45:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=134
06/17/2022 11:45:52 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.6210417079982297 on epoch=134
06/17/2022 11:45:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=134
06/17/2022 11:45:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=135
06/17/2022 11:45:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=136
06/17/2022 11:46:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
06/17/2022 11:46:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
06/17/2022 11:46:10 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.5836215200467415 on epoch=137
06/17/2022 11:46:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
06/17/2022 11:46:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
06/17/2022 11:46:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=139
06/17/2022 11:46:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
06/17/2022 11:46:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
06/17/2022 11:46:27 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.6405326585091117 on epoch=140
06/17/2022 11:46:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6326322773774438 -> 0.6405326585091117 on epoch=140, global_step=2250
06/17/2022 11:46:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=141
06/17/2022 11:46:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
06/17/2022 11:46:34 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=142
06/17/2022 11:46:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
06/17/2022 11:46:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=143
06/17/2022 11:46:44 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.5935242839352428 on epoch=143
06/17/2022 11:46:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=144
06/17/2022 11:46:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=144
06/17/2022 11:46:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
06/17/2022 11:46:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
06/17/2022 11:46:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=146
06/17/2022 11:47:01 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.5762668703845175 on epoch=146
06/17/2022 11:47:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
06/17/2022 11:47:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
06/17/2022 11:47:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
06/17/2022 11:47:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
06/17/2022 11:47:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=149
06/17/2022 11:47:18 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.5974842767295597 on epoch=149
06/17/2022 11:47:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=150
06/17/2022 11:47:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
06/17/2022 11:47:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
06/17/2022 11:47:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
06/17/2022 11:47:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
06/17/2022 11:47:35 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.6365801163163438 on epoch=153
06/17/2022 11:47:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
06/17/2022 11:47:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=154
06/17/2022 11:47:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=154
06/17/2022 11:47:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=155
06/17/2022 11:47:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
06/17/2022 11:47:52 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6268507863444572 on epoch=156
06/17/2022 11:47:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
06/17/2022 11:47:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=157
06/17/2022 11:48:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
06/17/2022 11:48:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
06/17/2022 11:48:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
06/17/2022 11:48:10 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.5933528836754642 on epoch=159
06/17/2022 11:48:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
06/17/2022 11:48:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
06/17/2022 11:48:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
06/17/2022 11:48:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
06/17/2022 11:48:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=162
06/17/2022 11:48:27 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.5915915915915916 on epoch=162
06/17/2022 11:48:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=163
06/17/2022 11:48:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=163
06/17/2022 11:48:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
06/17/2022 11:48:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=164
06/17/2022 11:48:40 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=165
06/17/2022 11:48:44 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.541394252116795 on epoch=165
06/17/2022 11:48:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/17/2022 11:48:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/17/2022 11:48:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
06/17/2022 11:48:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
06/17/2022 11:48:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
06/17/2022 11:49:01 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.4921237812632472 on epoch=168
06/17/2022 11:49:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
06/17/2022 11:49:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
06/17/2022 11:49:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
06/17/2022 11:49:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
06/17/2022 11:49:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
06/17/2022 11:49:18 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.5666560204407538 on epoch=171
06/17/2022 11:49:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=172
06/17/2022 11:49:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
06/17/2022 11:49:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
06/17/2022 11:49:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
06/17/2022 11:49:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
06/17/2022 11:49:36 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.6111189413514995 on epoch=174
06/17/2022 11:49:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
06/17/2022 11:49:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
06/17/2022 11:49:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=176
06/17/2022 11:49:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=177
06/17/2022 11:49:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
06/17/2022 11:49:53 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.584313725490196 on epoch=178
06/17/2022 11:49:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
06/17/2022 11:49:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
06/17/2022 11:50:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
06/17/2022 11:50:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
06/17/2022 11:50:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
06/17/2022 11:50:10 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.6016699788941782 on epoch=181
06/17/2022 11:50:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/17/2022 11:50:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
06/17/2022 11:50:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/17/2022 11:50:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
06/17/2022 11:50:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
06/17/2022 11:50:28 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.5809241413392636 on epoch=184
06/17/2022 11:50:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=184
06/17/2022 11:50:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=185
06/17/2022 11:50:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=186
06/17/2022 11:50:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
06/17/2022 11:50:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
06/17/2022 11:50:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:50:42 - INFO - __main__ - Printing 3 examples
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:50:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:50:42 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:50:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:50:42 - INFO - __main__ - Printing 3 examples
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 11:50:42 - INFO - __main__ - ['false']
06/17/2022 11:50:42 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:50:42 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:50:43 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:50:45 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.6085148030340103 on epoch=187
06/17/2022 11:50:45 - INFO - __main__ - save last model!
06/17/2022 11:50:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 11:50:45 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 11:50:45 - INFO - __main__ - Printing 3 examples
06/17/2022 11:50:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 11:50:45 - INFO - __main__ - ['false']
06/17/2022 11:50:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 11:50:45 - INFO - __main__ - ['false']
06/17/2022 11:50:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 11:50:45 - INFO - __main__ - ['false']
06/17/2022 11:50:45 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:50:47 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:50:49 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 11:51:01 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:51:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:51:02 - INFO - __main__ - Starting training!
06/17/2022 11:51:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_42_0.2_8_predictions.txt
06/17/2022 11:51:39 - INFO - __main__ - Classification-F1 on test data: 0.4373
06/17/2022 11:51:39 - INFO - __main__ - prefix=wiki_qa_128_42, lr=0.2, bsz=8, dev_performance=0.6405326585091117, test_performance=0.43729074511459926
06/17/2022 11:51:39 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.5, bsz=8 ...
06/17/2022 11:51:40 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:51:40 - INFO - __main__ - Printing 3 examples
06/17/2022 11:51:40 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 11:51:40 - INFO - __main__ - ['false']
06/17/2022 11:51:40 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 11:51:40 - INFO - __main__ - ['false']
06/17/2022 11:51:40 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 11:51:40 - INFO - __main__ - ['false']
06/17/2022 11:51:40 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:51:40 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:51:41 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 11:51:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 11:51:41 - INFO - __main__ - Printing 3 examples
06/17/2022 11:51:41 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 11:51:41 - INFO - __main__ - ['false']
06/17/2022 11:51:41 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 11:51:41 - INFO - __main__ - ['false']
06/17/2022 11:51:41 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 11:51:41 - INFO - __main__ - ['false']
06/17/2022 11:51:41 - INFO - __main__ - Tokenizing Input ...
06/17/2022 11:51:41 - INFO - __main__ - Tokenizing Output ...
06/17/2022 11:51:41 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 11:51:56 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 11:51:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 11:51:57 - INFO - __main__ - Starting training!
06/17/2022 11:52:00 - INFO - __main__ - Step 10 Global step 10 Train loss 0.73 on epoch=0
06/17/2022 11:52:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=1
06/17/2022 11:52:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=1
06/17/2022 11:52:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=2
06/17/2022 11:52:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=3
06/17/2022 11:52:14 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 11:52:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 11:52:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
06/17/2022 11:52:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=4
06/17/2022 11:52:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=4
06/17/2022 11:52:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=5
06/17/2022 11:52:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=6
06/17/2022 11:52:31 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.5658696814605454 on epoch=6
06/17/2022 11:52:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5658696814605454 on epoch=6, global_step=100
06/17/2022 11:52:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=6
06/17/2022 11:52:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=7
06/17/2022 11:52:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
06/17/2022 11:52:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=8
06/17/2022 11:52:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=9
06/17/2022 11:52:48 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.4734677087618264 on epoch=9
06/17/2022 11:52:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=9
06/17/2022 11:52:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=10
06/17/2022 11:52:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=11
06/17/2022 11:52:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=11
06/17/2022 11:53:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=12
06/17/2022 11:53:05 - INFO - __main__ - Global step 200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 11:53:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=13
06/17/2022 11:53:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=13
06/17/2022 11:53:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 11:53:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=14
06/17/2022 11:53:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=15
06/17/2022 11:53:21 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.44795321637426905 on epoch=15
06/17/2022 11:53:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=16
06/17/2022 11:53:27 - INFO - __main__ - Step 270 Global step 270 Train loss 1.05 on epoch=16
06/17/2022 11:53:29 - INFO - __main__ - Step 280 Global step 280 Train loss 2.22 on epoch=17
06/17/2022 11:53:32 - INFO - __main__ - Step 290 Global step 290 Train loss 1.51 on epoch=18
06/17/2022 11:53:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.71 on epoch=18
06/17/2022 11:53:38 - INFO - __main__ - Global step 300 Train loss 1.38 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 11:53:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=19
06/17/2022 11:53:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=19
06/17/2022 11:53:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
06/17/2022 11:53:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
06/17/2022 11:53:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=21
06/17/2022 11:53:55 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.4660788469690547 on epoch=21
06/17/2022 11:53:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=22
06/17/2022 11:54:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=23
06/17/2022 11:54:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=23
06/17/2022 11:54:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=24
06/17/2022 11:54:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 11:54:12 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.5240148087206911 on epoch=24
06/17/2022 11:54:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
06/17/2022 11:54:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=26
06/17/2022 11:54:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
06/17/2022 11:54:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=27
06/17/2022 11:54:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=28
06/17/2022 11:54:28 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.5365268875079512 on epoch=28
06/17/2022 11:54:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
06/17/2022 11:54:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
06/17/2022 11:54:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=29
06/17/2022 11:54:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=30
06/17/2022 11:54:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=31
06/17/2022 11:54:45 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5110203132526118 on epoch=31
06/17/2022 11:54:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
06/17/2022 11:54:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
06/17/2022 11:54:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=33
06/17/2022 11:54:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
06/17/2022 11:54:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 11:55:02 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.525582144871485 on epoch=34
06/17/2022 11:55:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
06/17/2022 11:55:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
06/17/2022 11:55:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
06/17/2022 11:55:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
06/17/2022 11:55:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
06/17/2022 11:55:19 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.41296643735668126 on epoch=37
06/17/2022 11:55:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
06/17/2022 11:55:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=38
06/17/2022 11:55:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=39
06/17/2022 11:55:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=39
06/17/2022 11:55:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
06/17/2022 11:55:35 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.36516753625488524 on epoch=40
06/17/2022 11:55:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=41
06/17/2022 11:55:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=41
06/17/2022 11:55:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=42
06/17/2022 11:55:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
06/17/2022 11:55:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
06/17/2022 11:55:52 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.5476190476190477 on epoch=43
06/17/2022 11:55:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
06/17/2022 11:55:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=44
06/17/2022 11:56:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
06/17/2022 11:56:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
06/17/2022 11:56:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
06/17/2022 11:56:09 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.4950166112956811 on epoch=46
06/17/2022 11:56:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
06/17/2022 11:56:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
06/17/2022 11:56:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=48
06/17/2022 11:56:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
06/17/2022 11:56:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=49
06/17/2022 11:56:25 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.41013824884792627 on epoch=49
06/17/2022 11:56:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 11:56:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=51
06/17/2022 11:56:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
06/17/2022 11:56:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=52
06/17/2022 11:56:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
06/17/2022 11:56:42 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.5193569910158735 on epoch=53
06/17/2022 11:56:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=53
06/17/2022 11:56:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
06/17/2022 11:56:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
06/17/2022 11:56:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
06/17/2022 11:56:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 11:56:59 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.5195243078047403 on epoch=56
06/17/2022 11:57:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=56
06/17/2022 11:57:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
06/17/2022 11:57:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=58
06/17/2022 11:57:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=58
06/17/2022 11:57:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
06/17/2022 11:57:16 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.5060982770479977 on epoch=59
06/17/2022 11:57:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=59
06/17/2022 11:57:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=60
06/17/2022 11:57:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
06/17/2022 11:57:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/17/2022 11:57:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=62
06/17/2022 11:57:32 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.40030721966205834 on epoch=62
06/17/2022 11:57:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 11:57:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=63
06/17/2022 11:57:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
06/17/2022 11:57:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=64
06/17/2022 11:57:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
06/17/2022 11:57:49 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.46723104056437387 on epoch=65
06/17/2022 11:57:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=66
06/17/2022 11:57:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=66
06/17/2022 11:57:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
06/17/2022 11:58:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=68
06/17/2022 11:58:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
06/17/2022 11:58:06 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.5332180092570948 on epoch=68
06/17/2022 11:58:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
06/17/2022 11:58:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=69
06/17/2022 11:58:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=70
06/17/2022 11:58:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=71
06/17/2022 11:58:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=71
06/17/2022 11:58:23 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.44279184502648194 on epoch=71
06/17/2022 11:58:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
06/17/2022 11:58:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 11:58:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
06/17/2022 11:58:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
06/17/2022 11:58:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=74
06/17/2022 11:58:39 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 11:58:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=75
06/17/2022 11:58:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=76
06/17/2022 11:58:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=76
06/17/2022 11:58:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=77
06/17/2022 11:58:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=78
06/17/2022 11:58:56 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.5480876103301733 on epoch=78
06/17/2022 11:58:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=78
06/17/2022 11:59:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
06/17/2022 11:59:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
06/17/2022 11:59:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=80
06/17/2022 11:59:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=81
06/17/2022 11:59:13 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.500805426027978 on epoch=81
06/17/2022 11:59:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=81
06/17/2022 11:59:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=82
06/17/2022 11:59:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/17/2022 11:59:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=83
06/17/2022 11:59:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
06/17/2022 11:59:30 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=84
06/17/2022 11:59:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=84
06/17/2022 11:59:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=85
06/17/2022 11:59:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
06/17/2022 11:59:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/17/2022 11:59:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=87
06/17/2022 11:59:47 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.5333333333333333 on epoch=87
06/17/2022 11:59:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=88
06/17/2022 11:59:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=88
06/17/2022 11:59:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=89
06/17/2022 11:59:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=89
06/17/2022 12:00:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
06/17/2022 12:00:03 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.4640158756167276 on epoch=90
06/17/2022 12:00:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
06/17/2022 12:00:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=91
06/17/2022 12:00:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=92
06/17/2022 12:00:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
06/17/2022 12:00:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=93
06/17/2022 12:00:20 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.43468822516655437 on epoch=93
06/17/2022 12:00:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 12:00:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
06/17/2022 12:00:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/17/2022 12:00:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=96
06/17/2022 12:00:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=96
06/17/2022 12:00:37 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.5288039266339447 on epoch=96
06/17/2022 12:00:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
06/17/2022 12:00:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=98
06/17/2022 12:00:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=98
06/17/2022 12:00:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=99
06/17/2022 12:00:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
06/17/2022 12:00:54 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=99
06/17/2022 12:00:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=100
06/17/2022 12:00:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=101
06/17/2022 12:01:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/17/2022 12:01:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=102
06/17/2022 12:01:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=103
06/17/2022 12:01:10 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.35307589038932324 on epoch=103
06/17/2022 12:01:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=103
06/17/2022 12:01:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=104
06/17/2022 12:01:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=104
06/17/2022 12:01:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=105
06/17/2022 12:01:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=106
06/17/2022 12:01:27 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.5289041182816169 on epoch=106
06/17/2022 12:01:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=106
06/17/2022 12:01:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=107
06/17/2022 12:01:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=108
06/17/2022 12:01:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=108
06/17/2022 12:01:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
06/17/2022 12:01:44 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=109
06/17/2022 12:01:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=109
06/17/2022 12:01:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=110
06/17/2022 12:01:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=111
06/17/2022 12:01:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=111
06/17/2022 12:01:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
06/17/2022 12:02:01 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=112
06/17/2022 12:02:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=113
06/17/2022 12:02:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=113
06/17/2022 12:02:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=114
06/17/2022 12:02:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=114
06/17/2022 12:02:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=115
06/17/2022 12:02:18 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.5091165982641459 on epoch=115
06/17/2022 12:02:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=116
06/17/2022 12:02:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=116
06/17/2022 12:02:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=117
06/17/2022 12:02:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=118
06/17/2022 12:02:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=118
06/17/2022 12:02:34 - INFO - __main__ - Global step 1900 Train loss 0.38 Classification-F1 0.35501021683496337 on epoch=118
06/17/2022 12:02:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=119
06/17/2022 12:02:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=119
06/17/2022 12:02:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.33 on epoch=120
06/17/2022 12:02:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=121
06/17/2022 12:02:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=121
06/17/2022 12:02:51 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.5384540419380512 on epoch=121
06/17/2022 12:02:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=122
06/17/2022 12:02:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=123
06/17/2022 12:02:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=123
06/17/2022 12:03:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
06/17/2022 12:03:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=124
06/17/2022 12:03:08 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.5110771581359816 on epoch=124
06/17/2022 12:03:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.88 on epoch=125
06/17/2022 12:03:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=126
06/17/2022 12:03:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.39 on epoch=126
06/17/2022 12:03:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=127
06/17/2022 12:03:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.32 on epoch=128
06/17/2022 12:03:25 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.36318407960199 on epoch=128
06/17/2022 12:03:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=128
06/17/2022 12:03:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=129
06/17/2022 12:03:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=129
06/17/2022 12:03:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=130
06/17/2022 12:03:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=131
06/17/2022 12:03:42 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.47405808203913913 on epoch=131
06/17/2022 12:03:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
06/17/2022 12:03:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.38 on epoch=132
06/17/2022 12:03:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=133
06/17/2022 12:03:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=133
06/17/2022 12:03:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=134
06/17/2022 12:03:59 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.41790848444987316 on epoch=134
06/17/2022 12:04:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
06/17/2022 12:04:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=135
06/17/2022 12:04:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=136
06/17/2022 12:04:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=136
06/17/2022 12:04:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=137
06/17/2022 12:04:15 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=137
06/17/2022 12:04:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=138
06/17/2022 12:04:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.39 on epoch=138
06/17/2022 12:04:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.34 on epoch=139
06/17/2022 12:04:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=139
06/17/2022 12:04:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
06/17/2022 12:04:32 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=140
06/17/2022 12:04:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.38 on epoch=141
06/17/2022 12:04:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.35 on epoch=141
06/17/2022 12:04:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=142
06/17/2022 12:04:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=143
06/17/2022 12:04:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=143
06/17/2022 12:04:49 - INFO - __main__ - Global step 2300 Train loss 0.38 Classification-F1 0.45053291996110884 on epoch=143
06/17/2022 12:04:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=144
06/17/2022 12:04:54 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=144
06/17/2022 12:04:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=145
06/17/2022 12:04:59 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=146
06/17/2022 12:05:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=146
06/17/2022 12:05:06 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.5309372797744891 on epoch=146
06/17/2022 12:05:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=147
06/17/2022 12:05:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=148
06/17/2022 12:05:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=148
06/17/2022 12:05:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=149
06/17/2022 12:05:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=149
06/17/2022 12:05:22 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.35693779904306216 on epoch=149
06/17/2022 12:05:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=150
06/17/2022 12:05:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.41 on epoch=151
06/17/2022 12:05:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.37 on epoch=151
06/17/2022 12:05:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.35 on epoch=152
06/17/2022 12:05:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=153
06/17/2022 12:05:39 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.4916666666666667 on epoch=153
06/17/2022 12:05:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=153
06/17/2022 12:05:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=154
06/17/2022 12:05:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=154
06/17/2022 12:05:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=155
06/17/2022 12:05:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=156
06/17/2022 12:05:56 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.5445198469556455 on epoch=156
06/17/2022 12:05:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=156
06/17/2022 12:06:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=157
06/17/2022 12:06:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=158
06/17/2022 12:06:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=158
06/17/2022 12:06:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=159
06/17/2022 12:06:13 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.49948717948717947 on epoch=159
06/17/2022 12:06:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=159
06/17/2022 12:06:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=160
06/17/2022 12:06:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.37 on epoch=161
06/17/2022 12:06:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=161
06/17/2022 12:06:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=162
06/17/2022 12:06:30 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.3486005089058525 on epoch=162
06/17/2022 12:06:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.36 on epoch=163
06/17/2022 12:06:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.37 on epoch=163
06/17/2022 12:06:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=164
06/17/2022 12:06:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=164
06/17/2022 12:06:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=165
06/17/2022 12:06:47 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.5257745129351645 on epoch=165
06/17/2022 12:06:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=166
06/17/2022 12:06:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=166
06/17/2022 12:06:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=167
06/17/2022 12:06:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=168
06/17/2022 12:07:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.37 on epoch=168
06/17/2022 12:07:04 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.5741602698124437 on epoch=168
06/17/2022 12:07:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5658696814605454 -> 0.5741602698124437 on epoch=168, global_step=2700
06/17/2022 12:07:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.35 on epoch=169
06/17/2022 12:07:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.38 on epoch=169
06/17/2022 12:07:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=170
06/17/2022 12:07:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=171
06/17/2022 12:07:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.38 on epoch=171
06/17/2022 12:07:21 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.5643294758339006 on epoch=171
06/17/2022 12:07:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=172
06/17/2022 12:07:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=173
06/17/2022 12:07:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=173
06/17/2022 12:07:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=174
06/17/2022 12:07:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.36 on epoch=174
06/17/2022 12:07:37 - INFO - __main__ - Global step 2800 Train loss 0.35 Classification-F1 0.350463149416029 on epoch=174
06/17/2022 12:07:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=175
06/17/2022 12:07:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.35 on epoch=176
06/17/2022 12:07:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=176
06/17/2022 12:07:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.37 on epoch=177
06/17/2022 12:07:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=178
06/17/2022 12:07:54 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.5681776360179107 on epoch=178
06/17/2022 12:07:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.37 on epoch=178
06/17/2022 12:08:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=179
06/17/2022 12:08:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=179
06/17/2022 12:08:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=180
06/17/2022 12:08:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=181
06/17/2022 12:08:11 - INFO - __main__ - Global step 2900 Train loss 0.39 Classification-F1 0.5431964249056485 on epoch=181
06/17/2022 12:08:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=181
06/17/2022 12:08:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.35 on epoch=182
06/17/2022 12:08:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=183
06/17/2022 12:08:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.36 on epoch=183
06/17/2022 12:08:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=184
06/17/2022 12:08:28 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.5037747398490104 on epoch=184
06/17/2022 12:08:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.40 on epoch=184
06/17/2022 12:08:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=185
06/17/2022 12:08:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.39 on epoch=186
06/17/2022 12:08:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=186
06/17/2022 12:08:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.38 on epoch=187
06/17/2022 12:08:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:08:42 - INFO - __main__ - Printing 3 examples
06/17/2022 12:08:42 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:08:42 - INFO - __main__ - ['false']
06/17/2022 12:08:42 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:08:42 - INFO - __main__ - ['false']
06/17/2022 12:08:43 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:08:43 - INFO - __main__ - ['false']
06/17/2022 12:08:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:08:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:08:43 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:08:43 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:08:43 - INFO - __main__ - Printing 3 examples
06/17/2022 12:08:43 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:08:43 - INFO - __main__ - ['false']
06/17/2022 12:08:43 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:08:43 - INFO - __main__ - ['false']
06/17/2022 12:08:43 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:08:43 - INFO - __main__ - ['false']
06/17/2022 12:08:43 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:08:43 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:08:43 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:08:45 - INFO - __main__ - Global step 3000 Train loss 0.38 Classification-F1 0.36318407960199 on epoch=187
06/17/2022 12:08:45 - INFO - __main__ - save last model!
06/17/2022 12:08:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 12:08:45 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 12:08:45 - INFO - __main__ - Printing 3 examples
06/17/2022 12:08:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 12:08:45 - INFO - __main__ - ['false']
06/17/2022 12:08:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 12:08:45 - INFO - __main__ - ['false']
06/17/2022 12:08:45 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 12:08:45 - INFO - __main__ - ['false']
06/17/2022 12:08:45 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:08:46 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:08:49 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 12:09:02 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:09:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:09:03 - INFO - __main__ - Starting training!
06/17/2022 12:09:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.5_8_predictions.txt
06/17/2022 12:09:28 - INFO - __main__ - Classification-F1 on test data: 0.5038
06/17/2022 12:09:28 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.5, bsz=8, dev_performance=0.5741602698124437, test_performance=0.5038047164917602
06/17/2022 12:09:28 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.4, bsz=8 ...
06/17/2022 12:09:29 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:09:29 - INFO - __main__ - Printing 3 examples
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:09:29 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:09:29 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:09:29 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:09:29 - INFO - __main__ - Printing 3 examples
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:09:29 - INFO - __main__ - ['false']
06/17/2022 12:09:29 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:09:30 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:09:30 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:09:44 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:09:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:09:45 - INFO - __main__ - Starting training!
06/17/2022 12:09:49 - INFO - __main__ - Step 10 Global step 10 Train loss 0.75 on epoch=0
06/17/2022 12:09:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=1
06/17/2022 12:09:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=1
06/17/2022 12:09:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=2
06/17/2022 12:09:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=3
06/17/2022 12:10:03 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 12:10:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 12:10:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=3
06/17/2022 12:10:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=4
06/17/2022 12:10:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=4
06/17/2022 12:10:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=5
06/17/2022 12:10:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=6
06/17/2022 12:10:20 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 12:10:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=6
06/17/2022 12:10:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
06/17/2022 12:10:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=8
06/17/2022 12:10:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=8
06/17/2022 12:10:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=9
06/17/2022 12:10:37 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 12:10:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=9
06/17/2022 12:10:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=10
06/17/2022 12:10:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=11
06/17/2022 12:10:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=11
06/17/2022 12:10:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=12
06/17/2022 12:10:53 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.350463149416029 on epoch=12
06/17/2022 12:10:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.350463149416029 on epoch=12, global_step=200
06/17/2022 12:10:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=13
06/17/2022 12:10:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=13
06/17/2022 12:11:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=14
06/17/2022 12:11:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=14
06/17/2022 12:11:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=15
06/17/2022 12:11:10 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=15
06/17/2022 12:11:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=16
06/17/2022 12:11:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=16
06/17/2022 12:11:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=17
06/17/2022 12:11:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
06/17/2022 12:11:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=18
06/17/2022 12:11:27 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 12:11:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=19
06/17/2022 12:11:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=19
06/17/2022 12:11:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=20
06/17/2022 12:11:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=21
06/17/2022 12:11:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
06/17/2022 12:11:43 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3771988921326446 on epoch=21
06/17/2022 12:11:43 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.3771988921326446 on epoch=21, global_step=350
06/17/2022 12:11:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=22
06/17/2022 12:11:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=23
06/17/2022 12:11:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
06/17/2022 12:11:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/17/2022 12:11:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 12:12:00 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 12:12:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=25
06/17/2022 12:12:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
06/17/2022 12:12:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
06/17/2022 12:12:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 12:12:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
06/17/2022 12:12:17 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 12:12:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=28
06/17/2022 12:12:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/17/2022 12:12:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
06/17/2022 12:12:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
06/17/2022 12:12:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=31
06/17/2022 12:12:34 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.35693779904306216 on epoch=31
06/17/2022 12:12:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=31
06/17/2022 12:12:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
06/17/2022 12:12:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
06/17/2022 12:12:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
06/17/2022 12:12:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
06/17/2022 12:12:51 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 12:12:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
06/17/2022 12:12:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=35
06/17/2022 12:12:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=36
06/17/2022 12:13:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=36
06/17/2022 12:13:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=37
06/17/2022 12:13:07 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.33159268929503916 on epoch=37
06/17/2022 12:13:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
06/17/2022 12:13:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/17/2022 12:13:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=39
06/17/2022 12:13:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=39
06/17/2022 12:13:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/17/2022 12:13:24 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=40
06/17/2022 12:13:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
06/17/2022 12:13:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
06/17/2022 12:13:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
06/17/2022 12:13:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
06/17/2022 12:13:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/17/2022 12:13:41 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 12:13:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=44
06/17/2022 12:13:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=44
06/17/2022 12:13:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
06/17/2022 12:13:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
06/17/2022 12:13:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
06/17/2022 12:13:58 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.5566219114959879 on epoch=46
06/17/2022 12:13:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3771988921326446 -> 0.5566219114959879 on epoch=46, global_step=750
06/17/2022 12:14:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
06/17/2022 12:14:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
06/17/2022 12:14:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
06/17/2022 12:14:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
06/17/2022 12:14:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/17/2022 12:14:14 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 12:14:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
06/17/2022 12:14:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/17/2022 12:14:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=51
06/17/2022 12:14:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
06/17/2022 12:14:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/17/2022 12:14:31 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 12:14:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
06/17/2022 12:14:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
06/17/2022 12:14:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
06/17/2022 12:14:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
06/17/2022 12:14:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
06/17/2022 12:14:47 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.350463149416029 on epoch=56
06/17/2022 12:14:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
06/17/2022 12:14:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 12:14:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=58
06/17/2022 12:14:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
06/17/2022 12:15:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
06/17/2022 12:15:04 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 12:15:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/17/2022 12:15:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=60
06/17/2022 12:15:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
06/17/2022 12:15:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
06/17/2022 12:15:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/17/2022 12:15:20 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3892365456821026 on epoch=62
06/17/2022 12:15:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 12:15:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
06/17/2022 12:15:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/17/2022 12:15:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/17/2022 12:15:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
06/17/2022 12:15:37 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5944082013047531 on epoch=65
06/17/2022 12:15:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5566219114959879 -> 0.5944082013047531 on epoch=65, global_step=1050
06/17/2022 12:15:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=66
06/17/2022 12:15:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
06/17/2022 12:15:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=67
06/17/2022 12:15:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=68
06/17/2022 12:15:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 12:15:53 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 12:15:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=69
06/17/2022 12:15:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=69
06/17/2022 12:16:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=70
06/17/2022 12:16:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=71
06/17/2022 12:16:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
06/17/2022 12:16:09 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.5166666666666666 on epoch=71
06/17/2022 12:16:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
06/17/2022 12:16:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=73
06/17/2022 12:16:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=73
06/17/2022 12:16:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=74
06/17/2022 12:16:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=74
06/17/2022 12:16:27 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 12:16:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=75
06/17/2022 12:16:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=76
06/17/2022 12:16:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=76
06/17/2022 12:16:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
06/17/2022 12:16:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=78
06/17/2022 12:16:43 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.35501021683496337 on epoch=78
06/17/2022 12:16:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=78
06/17/2022 12:16:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
06/17/2022 12:16:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
06/17/2022 12:16:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=80
06/17/2022 12:16:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
06/17/2022 12:17:00 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=81
06/17/2022 12:17:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=81
06/17/2022 12:17:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=82
06/17/2022 12:17:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
06/17/2022 12:17:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=83
06/17/2022 12:17:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=84
06/17/2022 12:17:16 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.5008299089517364 on epoch=84
06/17/2022 12:17:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/17/2022 12:17:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
06/17/2022 12:17:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=86
06/17/2022 12:17:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
06/17/2022 12:17:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
06/17/2022 12:17:33 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.5536145638367647 on epoch=87
06/17/2022 12:17:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=88
06/17/2022 12:17:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=88
06/17/2022 12:17:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=89
06/17/2022 12:17:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
06/17/2022 12:17:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
06/17/2022 12:17:49 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.580098414434117 on epoch=90
06/17/2022 12:17:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 12:17:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=91
06/17/2022 12:17:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
06/17/2022 12:17:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
06/17/2022 12:18:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
06/17/2022 12:18:06 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.34195559333697656 on epoch=93
06/17/2022 12:18:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=94
06/17/2022 12:18:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
06/17/2022 12:18:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/17/2022 12:18:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=96
06/17/2022 12:18:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=96
06/17/2022 12:18:22 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.45347565738089035 on epoch=96
06/17/2022 12:18:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=97
06/17/2022 12:18:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=98
06/17/2022 12:18:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=98
06/17/2022 12:18:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
06/17/2022 12:18:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=99
06/17/2022 12:18:39 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/17/2022 12:18:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
06/17/2022 12:18:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
06/17/2022 12:18:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
06/17/2022 12:18:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=102
06/17/2022 12:18:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=103
06/17/2022 12:18:55 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.4083606042117329 on epoch=103
06/17/2022 12:18:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=103
06/17/2022 12:19:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
06/17/2022 12:19:03 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=104
06/17/2022 12:19:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
06/17/2022 12:19:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=106
06/17/2022 12:19:12 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.5561040381160666 on epoch=106
06/17/2022 12:19:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
06/17/2022 12:19:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.39 on epoch=107
06/17/2022 12:19:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=108
06/17/2022 12:19:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=108
06/17/2022 12:19:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=109
06/17/2022 12:19:28 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=109
06/17/2022 12:19:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.33 on epoch=109
06/17/2022 12:19:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=110
06/17/2022 12:19:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=111
06/17/2022 12:19:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=111
06/17/2022 12:19:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=112
06/17/2022 12:19:45 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.5713342038559028 on epoch=112
06/17/2022 12:19:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=113
06/17/2022 12:19:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=113
06/17/2022 12:19:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=114
06/17/2022 12:19:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=114
06/17/2022 12:19:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=115
06/17/2022 12:20:01 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.5342424596484785 on epoch=115
06/17/2022 12:20:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=116
06/17/2022 12:20:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=116
06/17/2022 12:20:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=117
06/17/2022 12:20:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.34 on epoch=118
06/17/2022 12:20:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.35 on epoch=118
06/17/2022 12:20:18 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3849492366116407 on epoch=118
06/17/2022 12:20:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=119
06/17/2022 12:20:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=119
06/17/2022 12:20:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=120
06/17/2022 12:20:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=121
06/17/2022 12:20:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=121
06/17/2022 12:20:34 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.6484330396407796 on epoch=121
06/17/2022 12:20:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5944082013047531 -> 0.6484330396407796 on epoch=121, global_step=1950
06/17/2022 12:20:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=122
06/17/2022 12:20:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.34 on epoch=123
06/17/2022 12:20:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=123
06/17/2022 12:20:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=124
06/17/2022 12:20:47 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=124
06/17/2022 12:20:51 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.3849492366116407 on epoch=124
06/17/2022 12:20:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=125
06/17/2022 12:20:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=126
06/17/2022 12:20:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.35 on epoch=126
06/17/2022 12:21:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=127
06/17/2022 12:21:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=128
06/17/2022 12:21:07 - INFO - __main__ - Global step 2050 Train loss 0.37 Classification-F1 0.4202898550724638 on epoch=128
06/17/2022 12:21:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=128
06/17/2022 12:21:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=129
06/17/2022 12:21:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.35 on epoch=129
06/17/2022 12:21:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.34 on epoch=130
06/17/2022 12:21:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=131
06/17/2022 12:21:24 - INFO - __main__ - Global step 2100 Train loss 0.35 Classification-F1 0.5865060685772301 on epoch=131
06/17/2022 12:21:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
06/17/2022 12:21:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=132
06/17/2022 12:21:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=133
06/17/2022 12:21:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=133
06/17/2022 12:21:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
06/17/2022 12:21:40 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.5859122260880181 on epoch=134
06/17/2022 12:21:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=134
06/17/2022 12:21:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=135
06/17/2022 12:21:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=136
06/17/2022 12:21:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=136
06/17/2022 12:21:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=137
06/17/2022 12:21:57 - INFO - __main__ - Global step 2200 Train loss 0.36 Classification-F1 0.6262193227710469 on epoch=137
06/17/2022 12:21:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.35 on epoch=138
06/17/2022 12:22:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.34 on epoch=138
06/17/2022 12:22:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=139
06/17/2022 12:22:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=139
06/17/2022 12:22:10 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.35 on epoch=140
06/17/2022 12:22:13 - INFO - __main__ - Global step 2250 Train loss 0.35 Classification-F1 0.5634920634920635 on epoch=140
06/17/2022 12:22:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=141
06/17/2022 12:22:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.33 on epoch=141
06/17/2022 12:22:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.36 on epoch=142
06/17/2022 12:22:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=143
06/17/2022 12:22:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.35 on epoch=143
06/17/2022 12:22:30 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3486005089058525 on epoch=143
06/17/2022 12:22:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=144
06/17/2022 12:22:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.37 on epoch=144
06/17/2022 12:22:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=145
06/17/2022 12:22:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
06/17/2022 12:22:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=146
06/17/2022 12:22:46 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.46997929606625255 on epoch=146
06/17/2022 12:22:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=147
06/17/2022 12:22:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=148
06/17/2022 12:22:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.33 on epoch=148
06/17/2022 12:22:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=149
06/17/2022 12:22:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=149
06/17/2022 12:23:03 - INFO - __main__ - Global step 2400 Train loss 0.35 Classification-F1 0.5651540888133948 on epoch=149
06/17/2022 12:23:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=150
06/17/2022 12:23:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=151
06/17/2022 12:23:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=151
06/17/2022 12:23:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.37 on epoch=152
06/17/2022 12:23:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=153
06/17/2022 12:23:19 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.4950166112956811 on epoch=153
06/17/2022 12:23:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=153
06/17/2022 12:23:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=154
06/17/2022 12:23:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=154
06/17/2022 12:23:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=155
06/17/2022 12:23:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.36 on epoch=156
06/17/2022 12:23:36 - INFO - __main__ - Global step 2500 Train loss 0.36 Classification-F1 0.5311355311355311 on epoch=156
06/17/2022 12:23:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
06/17/2022 12:23:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.34 on epoch=157
06/17/2022 12:23:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=158
06/17/2022 12:23:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=158
06/17/2022 12:23:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=159
06/17/2022 12:23:53 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.5975027094686388 on epoch=159
06/17/2022 12:23:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=159
06/17/2022 12:23:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=160
06/17/2022 12:24:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=161
06/17/2022 12:24:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.32 on epoch=161
06/17/2022 12:24:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=162
06/17/2022 12:24:09 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.6093511566868095 on epoch=162
06/17/2022 12:24:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=163
06/17/2022 12:24:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.37 on epoch=163
06/17/2022 12:24:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.35 on epoch=164
06/17/2022 12:24:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=164
06/17/2022 12:24:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.37 on epoch=165
06/17/2022 12:24:26 - INFO - __main__ - Global step 2650 Train loss 0.36 Classification-F1 0.4818181818181818 on epoch=165
06/17/2022 12:24:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.35 on epoch=166
06/17/2022 12:24:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.30 on epoch=166
06/17/2022 12:24:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.35 on epoch=167
06/17/2022 12:24:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.33 on epoch=168
06/17/2022 12:24:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=168
06/17/2022 12:24:43 - INFO - __main__ - Global step 2700 Train loss 0.33 Classification-F1 0.4693181192786777 on epoch=168
06/17/2022 12:24:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=169
06/17/2022 12:24:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=169
06/17/2022 12:24:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=170
06/17/2022 12:24:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.35 on epoch=171
06/17/2022 12:24:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=171
06/17/2022 12:25:00 - INFO - __main__ - Global step 2750 Train loss 0.35 Classification-F1 0.620459599703484 on epoch=171
06/17/2022 12:25:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.35 on epoch=172
06/17/2022 12:25:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.31 on epoch=173
06/17/2022 12:25:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.34 on epoch=173
06/17/2022 12:25:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.29 on epoch=174
06/17/2022 12:25:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=174
06/17/2022 12:25:18 - INFO - __main__ - Global step 2800 Train loss 0.32 Classification-F1 0.6526672833795868 on epoch=174
06/17/2022 12:25:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6484330396407796 -> 0.6526672833795868 on epoch=174, global_step=2800
06/17/2022 12:25:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=175
06/17/2022 12:25:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=176
06/17/2022 12:25:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=176
06/17/2022 12:25:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=177
06/17/2022 12:25:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.33 on epoch=178
06/17/2022 12:25:34 - INFO - __main__ - Global step 2850 Train loss 0.32 Classification-F1 0.4745030250648228 on epoch=178
06/17/2022 12:25:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.30 on epoch=178
06/17/2022 12:25:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.27 on epoch=179
06/17/2022 12:25:42 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=179
06/17/2022 12:25:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.28 on epoch=180
06/17/2022 12:25:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.28 on epoch=181
06/17/2022 12:25:51 - INFO - __main__ - Global step 2900 Train loss 0.29 Classification-F1 0.5416666666666666 on epoch=181
06/17/2022 12:25:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.37 on epoch=181
06/17/2022 12:25:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.33 on epoch=182
06/17/2022 12:25:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=183
06/17/2022 12:26:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.30 on epoch=183
06/17/2022 12:26:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.28 on epoch=184
06/17/2022 12:26:09 - INFO - __main__ - Global step 2950 Train loss 0.32 Classification-F1 0.6284470246734397 on epoch=184
06/17/2022 12:26:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.33 on epoch=184
06/17/2022 12:26:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.32 on epoch=185
06/17/2022 12:26:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.27 on epoch=186
06/17/2022 12:26:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.29 on epoch=186
06/17/2022 12:26:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.28 on epoch=187
06/17/2022 12:26:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:26:23 - INFO - __main__ - Printing 3 examples
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:26:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:26:23 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:26:23 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:26:23 - INFO - __main__ - Printing 3 examples
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:26:23 - INFO - __main__ - ['false']
06/17/2022 12:26:23 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:26:23 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:26:23 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:26:25 - INFO - __main__ - Global step 3000 Train loss 0.30 Classification-F1 0.5784035133040558 on epoch=187
06/17/2022 12:26:25 - INFO - __main__ - save last model!
06/17/2022 12:26:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 12:26:25 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 12:26:25 - INFO - __main__ - Printing 3 examples
06/17/2022 12:26:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 12:26:25 - INFO - __main__ - ['false']
06/17/2022 12:26:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 12:26:25 - INFO - __main__ - ['false']
06/17/2022 12:26:25 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 12:26:25 - INFO - __main__ - ['false']
06/17/2022 12:26:25 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:26:27 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:26:29 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 12:26:39 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:26:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:26:40 - INFO - __main__ - Starting training!
06/17/2022 12:27:13 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.4_8_predictions.txt
06/17/2022 12:27:13 - INFO - __main__ - Classification-F1 on test data: 0.3143
06/17/2022 12:27:13 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.4, bsz=8, dev_performance=0.6526672833795868, test_performance=0.3142738440447973
06/17/2022 12:27:13 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.3, bsz=8 ...
06/17/2022 12:27:14 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:27:14 - INFO - __main__ - Printing 3 examples
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:27:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:27:14 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:27:14 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:27:14 - INFO - __main__ - Printing 3 examples
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:27:14 - INFO - __main__ - ['false']
06/17/2022 12:27:14 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:27:14 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:27:15 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:27:29 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:27:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:27:30 - INFO - __main__ - Starting training!
06/17/2022 12:27:34 - INFO - __main__ - Step 10 Global step 10 Train loss 0.74 on epoch=0
06/17/2022 12:27:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
06/17/2022 12:27:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=1
06/17/2022 12:27:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=2
06/17/2022 12:27:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=3
06/17/2022 12:27:48 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 12:27:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 12:27:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=3
06/17/2022 12:27:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=4
06/17/2022 12:27:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=4
06/17/2022 12:27:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.41 on epoch=5
06/17/2022 12:28:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=6
06/17/2022 12:28:04 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=6
06/17/2022 12:28:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=6
06/17/2022 12:28:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=7
06/17/2022 12:28:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=8
06/17/2022 12:28:15 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
06/17/2022 12:28:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=9
06/17/2022 12:28:21 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 12:28:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=9
06/17/2022 12:28:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=10
06/17/2022 12:28:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=11
06/17/2022 12:28:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=11
06/17/2022 12:28:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
06/17/2022 12:28:37 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.43840100285535205 on epoch=12
06/17/2022 12:28:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43840100285535205 on epoch=12, global_step=200
06/17/2022 12:28:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=13
06/17/2022 12:28:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=13
06/17/2022 12:28:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=14
06/17/2022 12:28:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 12:28:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=15
06/17/2022 12:28:54 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.350463149416029 on epoch=15
06/17/2022 12:28:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=16
06/17/2022 12:28:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
06/17/2022 12:29:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=17
06/17/2022 12:29:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=18
06/17/2022 12:29:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=18
06/17/2022 12:29:10 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 12:29:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=19
06/17/2022 12:29:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=19
06/17/2022 12:29:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=20
06/17/2022 12:29:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
06/17/2022 12:29:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=21
06/17/2022 12:29:26 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.4174085604529648 on epoch=21
06/17/2022 12:29:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=22
06/17/2022 12:29:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=23
06/17/2022 12:29:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
06/17/2022 12:29:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
06/17/2022 12:29:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
06/17/2022 12:29:43 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 12:29:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
06/17/2022 12:29:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=26
06/17/2022 12:29:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
06/17/2022 12:29:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
06/17/2022 12:29:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=28
06/17/2022 12:29:59 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 12:30:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
06/17/2022 12:30:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=29
06/17/2022 12:30:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
06/17/2022 12:30:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=30
06/17/2022 12:30:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=31
06/17/2022 12:30:16 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.47124880673435743 on epoch=31
06/17/2022 12:30:16 - INFO - __main__ - Saving model with best Classification-F1: 0.43840100285535205 -> 0.47124880673435743 on epoch=31, global_step=500
06/17/2022 12:30:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.77 on epoch=31
06/17/2022 12:30:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=32
06/17/2022 12:30:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=33
06/17/2022 12:30:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.93 on epoch=33
06/17/2022 12:30:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=34
06/17/2022 12:30:32 - INFO - __main__ - Global step 550 Train loss 0.67 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 12:30:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.78 on epoch=34
06/17/2022 12:30:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=35
06/17/2022 12:30:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=36
06/17/2022 12:30:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=36
06/17/2022 12:30:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=37
06/17/2022 12:30:49 - INFO - __main__ - Global step 600 Train loss 0.63 Classification-F1 0.3333333333333333 on epoch=37
06/17/2022 12:30:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=38
06/17/2022 12:30:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=38
06/17/2022 12:30:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=39
06/17/2022 12:30:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
06/17/2022 12:31:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=40
06/17/2022 12:31:06 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=40
06/17/2022 12:31:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=41
06/17/2022 12:31:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
06/17/2022 12:31:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
06/17/2022 12:31:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=43
06/17/2022 12:31:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=43
06/17/2022 12:31:22 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.44279184502648194 on epoch=43
06/17/2022 12:31:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=44
06/17/2022 12:31:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=44
06/17/2022 12:31:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=45
06/17/2022 12:31:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=46
06/17/2022 12:31:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=46
06/17/2022 12:31:39 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.3813144709696433 on epoch=46
06/17/2022 12:31:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
06/17/2022 12:31:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
06/17/2022 12:31:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=48
06/17/2022 12:31:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
06/17/2022 12:31:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=49
06/17/2022 12:31:55 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 12:31:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/17/2022 12:32:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
06/17/2022 12:32:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=51
06/17/2022 12:32:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=52
06/17/2022 12:32:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
06/17/2022 12:32:12 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 12:32:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
06/17/2022 12:32:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
06/17/2022 12:32:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
06/17/2022 12:32:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
06/17/2022 12:32:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=56
06/17/2022 12:32:28 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.34195559333697656 on epoch=56
06/17/2022 12:32:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
06/17/2022 12:32:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
06/17/2022 12:32:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=58
06/17/2022 12:32:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/17/2022 12:32:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
06/17/2022 12:32:45 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 12:32:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
06/17/2022 12:32:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/17/2022 12:32:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
06/17/2022 12:32:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=61
06/17/2022 12:32:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
06/17/2022 12:33:01 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=62
06/17/2022 12:33:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=63
06/17/2022 12:33:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
06/17/2022 12:33:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
06/17/2022 12:33:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/17/2022 12:33:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
06/17/2022 12:33:18 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=65
06/17/2022 12:33:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=66
06/17/2022 12:33:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=66
06/17/2022 12:33:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=67
06/17/2022 12:33:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/17/2022 12:33:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/17/2022 12:33:34 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.34195559333697656 on epoch=68
06/17/2022 12:33:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
06/17/2022 12:33:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/17/2022 12:33:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
06/17/2022 12:33:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
06/17/2022 12:33:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
06/17/2022 12:33:51 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=71
06/17/2022 12:33:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/17/2022 12:33:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
06/17/2022 12:33:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
06/17/2022 12:34:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
06/17/2022 12:34:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=74
06/17/2022 12:34:07 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 12:34:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=75
06/17/2022 12:34:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
06/17/2022 12:34:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=76
06/17/2022 12:34:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=77
06/17/2022 12:34:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
06/17/2022 12:34:24 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.49401232414835483 on epoch=78
06/17/2022 12:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.47124880673435743 -> 0.49401232414835483 on epoch=78, global_step=1250
06/17/2022 12:34:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=78
06/17/2022 12:34:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=79
06/17/2022 12:34:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
06/17/2022 12:34:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=80
06/17/2022 12:34:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=81
06/17/2022 12:34:40 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.4801353814110908 on epoch=81
06/17/2022 12:34:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
06/17/2022 12:34:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=82
06/17/2022 12:34:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=83
06/17/2022 12:34:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
06/17/2022 12:34:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
06/17/2022 12:34:57 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.36516753625488524 on epoch=84
06/17/2022 12:35:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
06/17/2022 12:35:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
06/17/2022 12:35:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=86
06/17/2022 12:35:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/17/2022 12:35:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
06/17/2022 12:35:14 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=87
06/17/2022 12:35:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=88
06/17/2022 12:35:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=88
06/17/2022 12:35:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=89
06/17/2022 12:35:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=89
06/17/2022 12:35:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
06/17/2022 12:35:30 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=90
06/17/2022 12:35:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
06/17/2022 12:35:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=91
06/17/2022 12:35:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=92
06/17/2022 12:35:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=93
06/17/2022 12:35:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
06/17/2022 12:35:46 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.47333733864948024 on epoch=93
06/17/2022 12:35:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
06/17/2022 12:35:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
06/17/2022 12:35:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=95
06/17/2022 12:35:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
06/17/2022 12:35:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=96
06/17/2022 12:36:03 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.4864732989970182 on epoch=96
06/17/2022 12:36:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=97
06/17/2022 12:36:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=98
06/17/2022 12:36:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=98
06/17/2022 12:36:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=99
06/17/2022 12:36:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/17/2022 12:36:19 - INFO - __main__ - Global step 1600 Train loss 0.38 Classification-F1 0.38279939051439815 on epoch=99
06/17/2022 12:36:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=100
06/17/2022 12:36:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=101
06/17/2022 12:36:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
06/17/2022 12:36:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=102
06/17/2022 12:36:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=103
06/17/2022 12:36:36 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.5651282051282052 on epoch=103
06/17/2022 12:36:36 - INFO - __main__ - Saving model with best Classification-F1: 0.49401232414835483 -> 0.5651282051282052 on epoch=103, global_step=1650
06/17/2022 12:36:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=103
06/17/2022 12:36:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=104
06/17/2022 12:36:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=104
06/17/2022 12:36:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=105
06/17/2022 12:36:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=106
06/17/2022 12:36:52 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.35885876860812244 on epoch=106
06/17/2022 12:36:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=106
06/17/2022 12:36:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=107
06/17/2022 12:37:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=108
06/17/2022 12:37:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=108
06/17/2022 12:37:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=109
06/17/2022 12:37:09 - INFO - __main__ - Global step 1750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=109
06/17/2022 12:37:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=109
06/17/2022 12:37:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=110
06/17/2022 12:37:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=111
06/17/2022 12:37:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=111
06/17/2022 12:37:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=112
06/17/2022 12:37:25 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=112
06/17/2022 12:37:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=113
06/17/2022 12:37:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=113
06/17/2022 12:37:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=114
06/17/2022 12:37:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=114
06/17/2022 12:37:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=115
06/17/2022 12:37:42 - INFO - __main__ - Global step 1850 Train loss 0.38 Classification-F1 0.35307589038932324 on epoch=115
06/17/2022 12:37:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=116
06/17/2022 12:37:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=116
06/17/2022 12:37:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=117
06/17/2022 12:37:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=118
06/17/2022 12:37:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=118
06/17/2022 12:37:58 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=118
06/17/2022 12:38:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=119
06/17/2022 12:38:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=119
06/17/2022 12:38:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=120
06/17/2022 12:38:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=121
06/17/2022 12:38:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=121
06/17/2022 12:38:15 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.34673046251993617 on epoch=121
06/17/2022 12:38:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=122
06/17/2022 12:38:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=123
06/17/2022 12:38:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=123
06/17/2022 12:38:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=124
06/17/2022 12:38:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=124
06/17/2022 12:38:31 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.35307589038932324 on epoch=124
06/17/2022 12:38:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=125
06/17/2022 12:38:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.38 on epoch=126
06/17/2022 12:38:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=126
06/17/2022 12:38:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=127
06/17/2022 12:38:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=128
06/17/2022 12:38:47 - INFO - __main__ - Global step 2050 Train loss 0.39 Classification-F1 0.5324393253805019 on epoch=128
06/17/2022 12:38:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=128
06/17/2022 12:38:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=129
06/17/2022 12:38:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=129
06/17/2022 12:38:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=130
06/17/2022 12:39:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=131
06/17/2022 12:39:04 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.5765978823711261 on epoch=131
06/17/2022 12:39:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5651282051282052 -> 0.5765978823711261 on epoch=131, global_step=2100
06/17/2022 12:39:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
06/17/2022 12:39:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=132
06/17/2022 12:39:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.33 on epoch=133
06/17/2022 12:39:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=133
06/17/2022 12:39:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=134
06/17/2022 12:39:21 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=134
06/17/2022 12:39:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=134
06/17/2022 12:39:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=135
06/17/2022 12:39:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=136
06/17/2022 12:39:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=136
06/17/2022 12:39:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.36 on epoch=137
06/17/2022 12:39:37 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=137
06/17/2022 12:39:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=138
06/17/2022 12:39:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=138
06/17/2022 12:39:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=139
06/17/2022 12:39:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=139
06/17/2022 12:39:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=140
06/17/2022 12:39:53 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.5713342038559028 on epoch=140
06/17/2022 12:39:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=141
06/17/2022 12:39:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=141
06/17/2022 12:40:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=142
06/17/2022 12:40:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.35 on epoch=143
06/17/2022 12:40:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=143
06/17/2022 12:40:10 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=143
06/17/2022 12:40:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=144
06/17/2022 12:40:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=144
06/17/2022 12:40:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=145
06/17/2022 12:40:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=146
06/17/2022 12:40:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=146
06/17/2022 12:40:26 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.5365268875079512 on epoch=146
06/17/2022 12:40:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=147
06/17/2022 12:40:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=148
06/17/2022 12:40:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=148
06/17/2022 12:40:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=149
06/17/2022 12:40:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=149
06/17/2022 12:40:43 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=149
06/17/2022 12:40:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.39 on epoch=150
06/17/2022 12:40:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.41 on epoch=151
06/17/2022 12:40:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=151
06/17/2022 12:40:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=152
06/17/2022 12:40:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=153
06/17/2022 12:40:59 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.44622552554218203 on epoch=153
06/17/2022 12:41:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=153
06/17/2022 12:41:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=154
06/17/2022 12:41:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=154
06/17/2022 12:41:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=155
06/17/2022 12:41:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=156
06/17/2022 12:41:16 - INFO - __main__ - Global step 2500 Train loss 0.39 Classification-F1 0.5418072384572741 on epoch=156
06/17/2022 12:41:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=156
06/17/2022 12:41:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.38 on epoch=157
06/17/2022 12:41:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.38 on epoch=158
06/17/2022 12:41:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=158
06/17/2022 12:41:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=159
06/17/2022 12:41:32 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.350463149416029 on epoch=159
06/17/2022 12:41:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=159
06/17/2022 12:41:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=160
06/17/2022 12:41:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=161
06/17/2022 12:41:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=161
06/17/2022 12:41:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.38 on epoch=162
06/17/2022 12:41:49 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.3732922688146569 on epoch=162
06/17/2022 12:41:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=163
06/17/2022 12:41:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=163
06/17/2022 12:41:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=164
06/17/2022 12:41:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.37 on epoch=164
06/17/2022 12:42:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=165
06/17/2022 12:42:05 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=165
06/17/2022 12:42:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=166
06/17/2022 12:42:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=166
06/17/2022 12:42:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=167
06/17/2022 12:42:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=168
06/17/2022 12:42:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.39 on epoch=168
06/17/2022 12:42:22 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=168
06/17/2022 12:42:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=169
06/17/2022 12:42:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=169
06/17/2022 12:42:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=170
06/17/2022 12:42:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.34 on epoch=171
06/17/2022 12:42:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=171
06/17/2022 12:42:38 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.5699956807601861 on epoch=171
06/17/2022 12:42:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=172
06/17/2022 12:42:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=173
06/17/2022 12:42:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=173
06/17/2022 12:42:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=174
06/17/2022 12:42:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=174
06/17/2022 12:42:55 - INFO - __main__ - Global step 2800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=174
06/17/2022 12:42:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.35 on epoch=175
06/17/2022 12:43:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.36 on epoch=176
06/17/2022 12:43:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.38 on epoch=176
06/17/2022 12:43:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.37 on epoch=177
06/17/2022 12:43:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=178
06/17/2022 12:43:11 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.5781862392218969 on epoch=178
06/17/2022 12:43:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5765978823711261 -> 0.5781862392218969 on epoch=178, global_step=2850
06/17/2022 12:43:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=178
06/17/2022 12:43:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=179
06/17/2022 12:43:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=179
06/17/2022 12:43:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=180
06/17/2022 12:43:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=181
06/17/2022 12:43:28 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.4556917558747943 on epoch=181
06/17/2022 12:43:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=181
06/17/2022 12:43:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.35 on epoch=182
06/17/2022 12:43:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=183
06/17/2022 12:43:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=183
06/17/2022 12:43:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=184
06/17/2022 12:43:45 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.33159268929503916 on epoch=184
06/17/2022 12:43:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.41 on epoch=184
06/17/2022 12:43:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=185
06/17/2022 12:43:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=186
06/17/2022 12:43:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=186
06/17/2022 12:43:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=187
06/17/2022 12:43:59 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:43:59 - INFO - __main__ - Printing 3 examples
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:43:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:43:59 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:43:59 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:43:59 - INFO - __main__ - Printing 3 examples
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:43:59 - INFO - __main__ - ['false']
06/17/2022 12:43:59 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:43:59 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:44:00 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:44:01 - INFO - __main__ - Global step 3000 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=187
06/17/2022 12:44:01 - INFO - __main__ - save last model!
06/17/2022 12:44:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 12:44:01 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 12:44:01 - INFO - __main__ - Printing 3 examples
06/17/2022 12:44:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 12:44:01 - INFO - __main__ - ['false']
06/17/2022 12:44:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 12:44:01 - INFO - __main__ - ['false']
06/17/2022 12:44:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 12:44:01 - INFO - __main__ - ['false']
06/17/2022 12:44:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:44:03 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:44:05 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 12:44:15 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:44:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:44:16 - INFO - __main__ - Starting training!
06/17/2022 12:44:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.3_8_predictions.txt
06/17/2022 12:44:44 - INFO - __main__ - Classification-F1 on test data: 0.4868
06/17/2022 12:44:44 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.3, bsz=8, dev_performance=0.5781862392218969, test_performance=0.4867605633802817
06/17/2022 12:44:44 - INFO - __main__ - Running ... prefix=wiki_qa_128_87, lr=0.2, bsz=8 ...
06/17/2022 12:44:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:44:45 - INFO - __main__ - Printing 3 examples
06/17/2022 12:44:45 - INFO - __main__ -  [wiki_qa] question: what are the 4 eras of time which one do we live in [SEP] answer: The geologic time scale is a system of chronological measurement that relates stratigraphy to time, and is used by geologists , paleontologists , and other earth scientists to describe the timing and relationships between events that have occurred throughout Earth's history .
06/17/2022 12:44:45 - INFO - __main__ - ['false']
06/17/2022 12:44:45 - INFO - __main__ -  [wiki_qa] question: what is soulja boy's latest song [SEP] answer: However, his next two albums, iSouljaBoyTellem (2008) and The DeAndre Way (2010) did not match the commercial success of his debut, the latter only selling 100,000 copies, despite the success of several singles across both albums, such as " Kiss Me Thru the Phone " and " Turn My Swag On " (iSouljaBoyTellem) and " Pretty Boy Swag " (The DeAndre Way).
06/17/2022 12:44:45 - INFO - __main__ - ['false']
06/17/2022 12:44:45 - INFO - __main__ -  [wiki_qa] question: what does a roman numeral L stand for? [SEP] answer: The Roman numeral system is a cousin of Etruscan numerals .
06/17/2022 12:44:45 - INFO - __main__ - ['false']
06/17/2022 12:44:45 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:44:45 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:44:46 - INFO - __main__ - Loaded 256 examples from train data
06/17/2022 12:44:46 - INFO - __main__ - Start tokenizing ... 256 instances
06/17/2022 12:44:46 - INFO - __main__ - Printing 3 examples
06/17/2022 12:44:46 - INFO - __main__ -  [wiki_qa] question: what year did aerosmith i dont want to miss a thing [SEP] answer: In the UK, the song peaked at number four, becoming Aerosmith's highest charting song in the UK, where it was the 17th best-selling single of 1998 , and has sold over a million copies.
06/17/2022 12:44:46 - INFO - __main__ - ['false']
06/17/2022 12:44:46 - INFO - __main__ -  [wiki_qa] question: How did the pendulum improve upon earlier clocks? [SEP] answer: When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth.
06/17/2022 12:44:46 - INFO - __main__ - ['false']
06/17/2022 12:44:46 - INFO - __main__ -  [wiki_qa] question: what is puerto rico currency [SEP] answer: However, printing of these banknotes ceased after 1815.
06/17/2022 12:44:46 - INFO - __main__ - ['false']
06/17/2022 12:44:46 - INFO - __main__ - Tokenizing Input ...
06/17/2022 12:44:46 - INFO - __main__ - Tokenizing Output ...
06/17/2022 12:44:46 - INFO - __main__ - Loaded 256 examples from dev data
06/17/2022 12:45:04 - INFO - __main__ - load prompt embedding from ckpt
06/17/2022 12:45:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/17/2022 12:45:05 - INFO - __main__ - Starting training!
06/17/2022 12:45:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.83 on epoch=0
06/17/2022 12:45:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=1
06/17/2022 12:45:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=1
06/17/2022 12:45:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=2
06/17/2022 12:45:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=3
06/17/2022 12:45:23 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=3
06/17/2022 12:45:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=3, global_step=50
06/17/2022 12:45:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=3
06/17/2022 12:45:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=4
06/17/2022 12:45:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=4
06/17/2022 12:45:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=5
06/17/2022 12:45:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=6
06/17/2022 12:45:40 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.34195559333697656 on epoch=6
06/17/2022 12:45:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34195559333697656 on epoch=6, global_step=100
06/17/2022 12:45:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.41 on epoch=6
06/17/2022 12:45:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=7
06/17/2022 12:45:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=8
06/17/2022 12:45:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=8
06/17/2022 12:45:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=9
06/17/2022 12:45:57 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=9
06/17/2022 12:45:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=9
06/17/2022 12:46:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=10
06/17/2022 12:46:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=11
06/17/2022 12:46:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=11
06/17/2022 12:46:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=12
06/17/2022 12:46:13 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=12
06/17/2022 12:46:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=13
06/17/2022 12:46:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=13
06/17/2022 12:46:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=14
06/17/2022 12:46:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=14
06/17/2022 12:46:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=15
06/17/2022 12:46:30 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.5780068093235353 on epoch=15
06/17/2022 12:46:30 - INFO - __main__ - Saving model with best Classification-F1: 0.34195559333697656 -> 0.5780068093235353 on epoch=15, global_step=250
06/17/2022 12:46:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
06/17/2022 12:46:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=16
06/17/2022 12:46:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=17
06/17/2022 12:46:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=18
06/17/2022 12:46:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=18
06/17/2022 12:46:47 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=18
06/17/2022 12:46:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
06/17/2022 12:46:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=19
06/17/2022 12:46:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=20
06/17/2022 12:46:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
06/17/2022 12:47:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
06/17/2022 12:47:04 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.35885876860812244 on epoch=21
06/17/2022 12:47:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=22
06/17/2022 12:47:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
06/17/2022 12:47:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=23
06/17/2022 12:47:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=24
06/17/2022 12:47:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
06/17/2022 12:47:21 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=24
06/17/2022 12:47:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
06/17/2022 12:47:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=26
06/17/2022 12:47:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=26
06/17/2022 12:47:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=27
06/17/2022 12:47:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=28
06/17/2022 12:47:38 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=28
06/17/2022 12:47:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
06/17/2022 12:47:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=29
06/17/2022 12:47:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=29
06/17/2022 12:47:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=30
06/17/2022 12:47:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
06/17/2022 12:47:55 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.41763682590659706 on epoch=31
06/17/2022 12:47:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
06/17/2022 12:48:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=32
06/17/2022 12:48:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
06/17/2022 12:48:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=33
06/17/2022 12:48:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/17/2022 12:48:11 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=34
06/17/2022 12:48:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=34
06/17/2022 12:48:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
06/17/2022 12:48:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/17/2022 12:48:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
06/17/2022 12:48:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=37
06/17/2022 12:48:28 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=37
06/17/2022 12:48:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
06/17/2022 12:48:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/17/2022 12:48:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
06/17/2022 12:48:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=39
06/17/2022 12:48:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
06/17/2022 12:48:45 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.573436868074092 on epoch=40
06/17/2022 12:48:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=41
06/17/2022 12:48:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
06/17/2022 12:48:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/17/2022 12:48:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
06/17/2022 12:48:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/17/2022 12:49:02 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=43
06/17/2022 12:49:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
06/17/2022 12:49:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
06/17/2022 12:49:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
06/17/2022 12:49:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
06/17/2022 12:49:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=46
06/17/2022 12:49:19 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.4613654501922658 on epoch=46
06/17/2022 12:49:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
06/17/2022 12:49:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
06/17/2022 12:49:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
06/17/2022 12:49:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
06/17/2022 12:49:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=49
06/17/2022 12:49:36 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
06/17/2022 12:49:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=50
06/17/2022 12:49:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
06/17/2022 12:49:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=51
06/17/2022 12:49:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=52
06/17/2022 12:49:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/17/2022 12:49:53 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=53
06/17/2022 12:49:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/17/2022 12:49:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/17/2022 12:50:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
06/17/2022 12:50:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=55
06/17/2022 12:50:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=56
06/17/2022 12:50:11 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.42235512098475536 on epoch=56
06/17/2022 12:50:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=56
06/17/2022 12:50:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/17/2022 12:50:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/17/2022 12:50:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
06/17/2022 12:50:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=59
06/17/2022 12:50:28 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=59
06/17/2022 12:50:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=59
06/17/2022 12:50:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
06/17/2022 12:50:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=61
06/17/2022 12:50:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/17/2022 12:50:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
06/17/2022 12:50:44 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.47065225277515776 on epoch=62
06/17/2022 12:50:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
06/17/2022 12:50:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=63
06/17/2022 12:50:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/17/2022 12:50:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=64
06/17/2022 12:50:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=65
06/17/2022 12:51:01 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.5124804461674488 on epoch=65
06/17/2022 12:51:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
06/17/2022 12:51:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=66
06/17/2022 12:51:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
06/17/2022 12:51:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=68
06/17/2022 12:51:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=68
06/17/2022 12:51:18 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=68
06/17/2022 12:51:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
06/17/2022 12:51:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=69
06/17/2022 12:51:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=70
06/17/2022 12:51:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=71
06/17/2022 12:51:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=71
06/17/2022 12:51:35 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.4625998026374809 on epoch=71
06/17/2022 12:51:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
06/17/2022 12:51:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
06/17/2022 12:51:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
06/17/2022 12:51:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=74
06/17/2022 12:51:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=74
06/17/2022 12:51:52 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
06/17/2022 12:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=75
06/17/2022 12:51:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=76
06/17/2022 12:52:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/17/2022 12:52:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=77
06/17/2022 12:52:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
06/17/2022 12:52:08 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.3401530406766009 on epoch=78
06/17/2022 12:52:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=78
06/17/2022 12:52:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=79
06/17/2022 12:52:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=79
06/17/2022 12:52:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/17/2022 12:52:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=81
06/17/2022 12:52:25 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.5372549019607842 on epoch=81
06/17/2022 12:52:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=81
06/17/2022 12:52:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=82
06/17/2022 12:52:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=83
06/17/2022 12:52:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=83
06/17/2022 12:52:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=84
06/17/2022 12:52:42 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=84
06/17/2022 12:52:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
06/17/2022 12:52:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=85
06/17/2022 12:52:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=86
06/17/2022 12:52:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=86
06/17/2022 12:52:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=87
06/17/2022 12:52:59 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=87
06/17/2022 12:53:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
06/17/2022 12:53:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=88
06/17/2022 12:53:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=89
06/17/2022 12:53:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=89
06/17/2022 12:53:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
06/17/2022 12:53:16 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.45053291996110884 on epoch=90
06/17/2022 12:53:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=91
06/17/2022 12:53:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=91
06/17/2022 12:53:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
06/17/2022 12:53:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
06/17/2022 12:53:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=93
06/17/2022 12:53:33 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=93
06/17/2022 12:53:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=94
06/17/2022 12:53:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
06/17/2022 12:53:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/17/2022 12:53:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=96
06/17/2022 12:53:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=96
06/17/2022 12:53:49 - INFO - __main__ - Global step 1550 Train loss 0.38 Classification-F1 0.5062200279591583 on epoch=96
06/17/2022 12:53:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=97
06/17/2022 12:53:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=98
06/17/2022 12:53:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=98
06/17/2022 12:54:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=99
06/17/2022 12:54:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=99
06/17/2022 12:54:06 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
06/17/2022 12:54:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=100
06/17/2022 12:54:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=101
06/17/2022 12:54:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=101
06/17/2022 12:54:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=102
06/17/2022 12:54:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.36 on epoch=103
06/17/2022 12:54:23 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.35501021683496337 on epoch=103
06/17/2022 12:54:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=103
06/17/2022 12:54:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=104
06/17/2022 12:54:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=104
06/17/2022 12:54:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=105
06/17/2022 12:54:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=106
06/17/2022 12:54:40 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.4556917558747943 on epoch=106
06/17/2022 12:54:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
06/17/2022 12:54:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=107
06/17/2022 12:54:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=108
06/17/2022 12:54:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=108
06/17/2022 12:54:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=109
06/17/2022 12:54:57 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.45827145827145827 on epoch=109
06/17/2022 12:55:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=109
06/17/2022 12:55:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=110
06/17/2022 12:55:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=111
06/17/2022 12:55:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=111
06/17/2022 12:55:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=112
06/17/2022 12:55:14 - INFO - __main__ - Global step 1800 Train loss 0.37 Classification-F1 0.49188662163103286 on epoch=112
06/17/2022 12:55:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=113
06/17/2022 12:55:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=113
06/17/2022 12:55:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=114
06/17/2022 12:55:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=114
06/17/2022 12:55:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=115
06/17/2022 12:55:31 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.5813836477987422 on epoch=115
06/17/2022 12:55:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5780068093235353 -> 0.5813836477987422 on epoch=115, global_step=1850
06/17/2022 12:55:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=116
06/17/2022 12:55:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=116
06/17/2022 12:55:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=117
06/17/2022 12:55:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=118
06/17/2022 12:55:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=118
06/17/2022 12:55:48 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.36119461636703015 on epoch=118
06/17/2022 12:55:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
06/17/2022 12:55:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=119
06/17/2022 12:55:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=120
06/17/2022 12:55:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=121
06/17/2022 12:56:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=121
06/17/2022 12:56:05 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.6111392405063292 on epoch=121
06/17/2022 12:56:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5813836477987422 -> 0.6111392405063292 on epoch=121, global_step=1950
06/17/2022 12:56:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=122
06/17/2022 12:56:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=123
06/17/2022 12:56:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=123
06/17/2022 12:56:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=124
06/17/2022 12:56:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=124
06/17/2022 12:56:21 - INFO - __main__ - Global step 2000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=124
06/17/2022 12:56:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=125
06/17/2022 12:56:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.34 on epoch=126
06/17/2022 12:56:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=126
06/17/2022 12:56:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=127
06/17/2022 12:56:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.35 on epoch=128
06/17/2022 12:56:38 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.42266947144995926 on epoch=128
06/17/2022 12:56:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=128
06/17/2022 12:56:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=129
06/17/2022 12:56:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=129
06/17/2022 12:56:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=130
06/17/2022 12:56:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=131
06/17/2022 12:56:55 - INFO - __main__ - Global step 2100 Train loss 0.36 Classification-F1 0.605414561936301 on epoch=131
06/17/2022 12:56:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=131
06/17/2022 12:57:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.38 on epoch=132
06/17/2022 12:57:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.35 on epoch=133
06/17/2022 12:57:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=133
06/17/2022 12:57:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.36 on epoch=134
06/17/2022 12:57:12 - INFO - __main__ - Global step 2150 Train loss 0.36 Classification-F1 0.48902195608782434 on epoch=134
06/17/2022 12:57:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=134
06/17/2022 12:57:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.36 on epoch=135
06/17/2022 12:57:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=136
06/17/2022 12:57:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=136
06/17/2022 12:57:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=137
06/17/2022 12:57:29 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.5894339622641509 on epoch=137
06/17/2022 12:57:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.33 on epoch=138
06/17/2022 12:57:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.39 on epoch=138
06/17/2022 12:57:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.36 on epoch=139
06/17/2022 12:57:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.34 on epoch=139
06/17/2022 12:57:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=140
06/17/2022 12:57:46 - INFO - __main__ - Global step 2250 Train loss 0.36 Classification-F1 0.5520092475155329 on epoch=140
06/17/2022 12:57:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=141
06/17/2022 12:57:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=141
06/17/2022 12:57:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=142
06/17/2022 12:57:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=143
06/17/2022 12:57:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.39 on epoch=143
06/17/2022 12:58:02 - INFO - __main__ - Global step 2300 Train loss 0.37 Classification-F1 0.34195559333697656 on epoch=143
06/17/2022 12:58:05 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=144
06/17/2022 12:58:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=144
06/17/2022 12:58:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=145
06/17/2022 12:58:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=146
06/17/2022 12:58:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=146
06/17/2022 12:58:19 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.4613654501922658 on epoch=146
06/17/2022 12:58:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=147
06/17/2022 12:58:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=148
06/17/2022 12:58:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.35 on epoch=148
06/17/2022 12:58:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=149
06/17/2022 12:58:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=149
06/17/2022 12:58:36 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.5870650587602735 on epoch=149
06/17/2022 12:58:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.39 on epoch=150
06/17/2022 12:58:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=151
06/17/2022 12:58:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=151
06/17/2022 12:58:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=152
06/17/2022 12:58:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=153
06/17/2022 12:58:53 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.3980615931639617 on epoch=153
06/17/2022 12:58:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=153
06/17/2022 12:58:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.33 on epoch=154
06/17/2022 12:59:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.39 on epoch=154
06/17/2022 12:59:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=155
06/17/2022 12:59:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=156
06/17/2022 12:59:10 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.5564563582870219 on epoch=156
06/17/2022 12:59:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=156
06/17/2022 12:59:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=157
06/17/2022 12:59:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=158
06/17/2022 12:59:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=158
06/17/2022 12:59:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=159
06/17/2022 12:59:27 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.5823552704522056 on epoch=159
06/17/2022 12:59:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=159
06/17/2022 12:59:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=160
06/17/2022 12:59:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=161
06/17/2022 12:59:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=161
06/17/2022 12:59:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=162
06/17/2022 12:59:44 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.6074342145617371 on epoch=162
06/17/2022 12:59:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.38 on epoch=163
06/17/2022 12:59:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.40 on epoch=163
06/17/2022 12:59:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=164
06/17/2022 12:59:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=164
06/17/2022 12:59:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.35 on epoch=165
06/17/2022 13:00:00 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.4693181192786777 on epoch=165
06/17/2022 13:00:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=166
06/17/2022 13:00:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=166
06/17/2022 13:00:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.35 on epoch=167
06/17/2022 13:00:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=168
06/17/2022 13:00:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=168
06/17/2022 13:00:17 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.35885876860812244 on epoch=168
06/17/2022 13:00:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.33 on epoch=169
06/17/2022 13:00:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.36 on epoch=169
06/17/2022 13:00:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=170
06/17/2022 13:00:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.32 on epoch=171
06/17/2022 13:00:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.33 on epoch=171
06/17/2022 13:00:34 - INFO - __main__ - Global step 2750 Train loss 0.33 Classification-F1 0.5535402860132543 on epoch=171
06/17/2022 13:00:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.36 on epoch=172
06/17/2022 13:00:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=173
06/17/2022 13:00:42 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=173
06/17/2022 13:00:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.34 on epoch=174
06/17/2022 13:00:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=174
06/17/2022 13:00:51 - INFO - __main__ - Global step 2800 Train loss 0.36 Classification-F1 0.5244582043343653 on epoch=174
06/17/2022 13:00:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=175
06/17/2022 13:00:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.32 on epoch=176
06/17/2022 13:00:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=176
06/17/2022 13:01:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=177
06/17/2022 13:01:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.32 on epoch=178
06/17/2022 13:01:08 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.41763682590659706 on epoch=178
06/17/2022 13:01:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.37 on epoch=178
06/17/2022 13:01:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=179
06/17/2022 13:01:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=179
06/17/2022 13:01:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=180
06/17/2022 13:01:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.37 on epoch=181
06/17/2022 13:01:26 - INFO - __main__ - Global step 2900 Train loss 0.36 Classification-F1 0.6168132942326491 on epoch=181
06/17/2022 13:01:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6111392405063292 -> 0.6168132942326491 on epoch=181, global_step=2900
06/17/2022 13:01:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.39 on epoch=181
06/17/2022 13:01:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.33 on epoch=182
06/17/2022 13:01:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=183
06/17/2022 13:01:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.36 on epoch=183
06/17/2022 13:01:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=184
06/17/2022 13:01:44 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.5215789014059907 on epoch=184
06/17/2022 13:01:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=184
06/17/2022 13:01:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=185
06/17/2022 13:01:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.35 on epoch=186
06/17/2022 13:01:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.35 on epoch=186
06/17/2022 13:01:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=187
06/17/2022 13:02:01 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.6549019607843137 on epoch=187
06/17/2022 13:02:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6168132942326491 -> 0.6549019607843137 on epoch=187, global_step=3000
06/17/2022 13:02:01 - INFO - __main__ - save last model!
06/17/2022 13:02:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/17/2022 13:02:01 - INFO - __main__ - Start tokenizing ... 2733 instances
06/17/2022 13:02:01 - INFO - __main__ - Printing 3 examples
06/17/2022 13:02:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Cross section of sclerenchyma fibers in plant ground tissue
06/17/2022 13:02:01 - INFO - __main__ - ['false']
06/17/2022 13:02:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: Microscopic view of a histologic specimen of human lung tissue stained with hematoxylin and eosin .
06/17/2022 13:02:01 - INFO - __main__ - ['false']
06/17/2022 13:02:01 - INFO - __main__ -  [wiki_qa] question: How are epithelial tissues joined together? [SEP] answer: In Biology , Tissue is a cellular organizational level intermediate between cells and a complete organism .
06/17/2022 13:02:01 - INFO - __main__ - ['false']
06/17/2022 13:02:01 - INFO - __main__ - Tokenizing Input ...
06/17/2022 13:02:02 - INFO - __main__ - Tokenizing Output ...
06/17/2022 13:02:05 - INFO - __main__ - Loaded 2733 examples from test data
06/17/2022 13:02:45 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down128shot/singletask-wiki_qa/wiki_qa_128_87_0.2_8_predictions.txt
06/17/2022 13:02:45 - INFO - __main__ - Classification-F1 on test data: 0.5124
06/17/2022 13:02:46 - INFO - __main__ - prefix=wiki_qa_128_87, lr=0.2, bsz=8, dev_performance=0.6549019607843137, test_performance=0.5124367837749533
