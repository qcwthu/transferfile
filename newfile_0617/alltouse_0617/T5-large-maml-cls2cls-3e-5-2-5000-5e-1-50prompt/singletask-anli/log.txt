05/24/2022 16:19:30 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-50prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='0,1')
05/24/2022 16:19:30 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli
05/24/2022 16:19:30 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-50prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='0,1')
05/24/2022 16:19:30 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli
05/24/2022 16:19:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/24/2022 16:19:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/24/2022 16:19:31 - INFO - __main__ - args.device: cuda:0
05/24/2022 16:19:31 - INFO - __main__ - Using 2 gpus
05/24/2022 16:19:31 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/24/2022 16:19:31 - INFO - __main__ - args.device: cuda:1
05/24/2022 16:19:31 - INFO - __main__ - Using 2 gpus
05/24/2022 16:19:31 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
05/24/2022 16:19:36 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
05/24/2022 16:19:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:19:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:19:37 - INFO - __main__ - Printing 3 examples
05/24/2022 16:19:37 - INFO - __main__ - Printing 3 examples
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:19:37 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:19:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:19:37 - INFO - __main__ - Printing 3 examples
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:19:37 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:19:37 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:19:37 - INFO - __main__ - Printing 3 examples
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:19:37 - INFO - __main__ - ['neutral']
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:19:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:19:37 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:19:37 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:19:55 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:19:56 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:19:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:19:57 - INFO - __main__ - Starting training!
05/24/2022 16:19:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:19:59 - INFO - __main__ - Starting training!
05/24/2022 16:20:03 - INFO - __main__ - Step 10 Global step 10 Train loss 0.89 on epoch=3
05/24/2022 16:20:05 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=6
05/24/2022 16:20:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=9
05/24/2022 16:20:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=13
05/24/2022 16:20:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=16
05/24/2022 16:20:14 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.3532369578881207 on epoch=16
05/24/2022 16:20:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3532369578881207 on epoch=16, global_step=50
05/24/2022 16:20:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/24/2022 16:20:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
05/24/2022 16:20:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/24/2022 16:20:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=29
05/24/2022 16:20:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
05/24/2022 16:20:28 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 16:20:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/24/2022 16:20:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
05/24/2022 16:20:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
05/24/2022 16:20:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
05/24/2022 16:20:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
05/24/2022 16:20:41 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 16:20:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/24/2022 16:20:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=56
05/24/2022 16:20:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=59
05/24/2022 16:20:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
05/24/2022 16:20:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
05/24/2022 16:20:55 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 16:20:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=69
05/24/2022 16:21:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
05/24/2022 16:21:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=76
05/24/2022 16:21:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
05/24/2022 16:21:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=83
05/24/2022 16:21:09 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 16:21:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=86
05/24/2022 16:21:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
05/24/2022 16:21:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=93
05/24/2022 16:21:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
05/24/2022 16:21:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
05/24/2022 16:21:23 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=99
05/24/2022 16:21:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=103
05/24/2022 16:21:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
05/24/2022 16:21:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
05/24/2022 16:21:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
05/24/2022 16:21:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
05/24/2022 16:21:36 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=116
05/24/2022 16:21:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
05/24/2022 16:21:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
05/24/2022 16:21:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
05/24/2022 16:21:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
05/24/2022 16:21:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
05/24/2022 16:21:50 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=133
05/24/2022 16:21:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
05/24/2022 16:21:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=139
05/24/2022 16:21:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=143
05/24/2022 16:22:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
05/24/2022 16:22:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
05/24/2022 16:22:04 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.23737373737373735 on epoch=149
05/24/2022 16:22:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=153
05/24/2022 16:22:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
05/24/2022 16:22:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=159
05/24/2022 16:22:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=163
05/24/2022 16:22:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=166
05/24/2022 16:22:18 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.18379281537176273 on epoch=166
05/24/2022 16:22:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=169
05/24/2022 16:22:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=173
05/24/2022 16:22:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=176
05/24/2022 16:22:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
05/24/2022 16:22:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=183
05/24/2022 16:22:32 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.20282485875706216 on epoch=183
05/24/2022 16:22:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=186
05/24/2022 16:22:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
05/24/2022 16:22:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=193
05/24/2022 16:22:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=196
05/24/2022 16:22:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=199
05/24/2022 16:22:45 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.287001287001287 on epoch=199
05/24/2022 16:22:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=203
05/24/2022 16:22:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=206
05/24/2022 16:22:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
05/24/2022 16:22:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=213
05/24/2022 16:22:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=216
05/24/2022 16:22:59 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3169230769230769 on epoch=216
05/24/2022 16:23:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=219
05/24/2022 16:23:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=223
05/24/2022 16:23:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
05/24/2022 16:23:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=229
05/24/2022 16:23:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=233
05/24/2022 16:23:13 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.26190476190476186 on epoch=233
05/24/2022 16:23:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=236
05/24/2022 16:23:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=239
05/24/2022 16:23:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
05/24/2022 16:23:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=246
05/24/2022 16:23:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=249
05/24/2022 16:23:27 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.30416666666666664 on epoch=249
05/24/2022 16:23:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=253
05/24/2022 16:23:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
05/24/2022 16:23:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=259
05/24/2022 16:23:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=263
05/24/2022 16:23:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=266
05/24/2022 16:23:41 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.2922856753441461 on epoch=266
05/24/2022 16:23:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=269
05/24/2022 16:23:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=273
05/24/2022 16:23:49 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
05/24/2022 16:23:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=279
05/24/2022 16:23:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=283
05/24/2022 16:23:55 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.2541208791208791 on epoch=283
05/24/2022 16:23:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=286
05/24/2022 16:24:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
05/24/2022 16:24:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=293
05/24/2022 16:24:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/24/2022 16:24:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/24/2022 16:24:09 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.16245887445887447 on epoch=299
05/24/2022 16:24:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=303
05/24/2022 16:24:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
05/24/2022 16:24:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.08 on epoch=309
05/24/2022 16:24:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=313
05/24/2022 16:24:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=316
05/24/2022 16:24:23 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.20735785953177258 on epoch=316
05/24/2022 16:24:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
05/24/2022 16:24:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=323
05/24/2022 16:24:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=326
05/24/2022 16:24:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=329
05/24/2022 16:24:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
05/24/2022 16:24:37 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.21784448613716906 on epoch=333
05/24/2022 16:24:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=336
05/24/2022 16:24:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
05/24/2022 16:24:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=343
05/24/2022 16:24:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
05/24/2022 16:24:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=349
05/24/2022 16:24:51 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.20968286099865047 on epoch=349
05/24/2022 16:24:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
05/24/2022 16:24:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=356
05/24/2022 16:24:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=359
05/24/2022 16:25:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.05 on epoch=363
05/24/2022 16:25:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
05/24/2022 16:25:05 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.23286435786435786 on epoch=366
05/24/2022 16:25:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=369
05/24/2022 16:25:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=373
05/24/2022 16:25:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
05/24/2022 16:25:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
05/24/2022 16:25:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=383
05/24/2022 16:25:19 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.2208672086720867 on epoch=383
05/24/2022 16:25:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/24/2022 16:25:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=389
05/24/2022 16:25:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/24/2022 16:25:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
05/24/2022 16:25:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
05/24/2022 16:25:33 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.220138183296078 on epoch=399
05/24/2022 16:25:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/24/2022 16:25:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=406
05/24/2022 16:25:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=409
05/24/2022 16:25:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/24/2022 16:25:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/24/2022 16:25:47 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.1832674571805007 on epoch=416
05/24/2022 16:25:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
05/24/2022 16:25:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
05/24/2022 16:25:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
05/24/2022 16:25:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/24/2022 16:25:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=433
05/24/2022 16:26:01 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.12475929867234214 on epoch=433
05/24/2022 16:26:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
05/24/2022 16:26:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=439
05/24/2022 16:26:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=443
05/24/2022 16:26:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/24/2022 16:26:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
05/24/2022 16:26:15 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.2916605527023722 on epoch=449
05/24/2022 16:26:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
05/24/2022 16:26:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
05/24/2022 16:26:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=459
05/24/2022 16:26:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=463
05/24/2022 16:26:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
05/24/2022 16:26:29 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.2284090909090909 on epoch=466
05/24/2022 16:26:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/24/2022 16:26:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
05/24/2022 16:26:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
05/24/2022 16:26:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=479
05/24/2022 16:26:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/24/2022 16:26:43 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.2389233954451346 on epoch=483
05/24/2022 16:26:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
05/24/2022 16:26:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=489
05/24/2022 16:26:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=493
05/24/2022 16:26:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/24/2022 16:26:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/24/2022 16:26:58 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.22573529411764706 on epoch=499
05/24/2022 16:27:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
05/24/2022 16:27:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/24/2022 16:27:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
05/24/2022 16:27:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
05/24/2022 16:27:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/24/2022 16:27:12 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.1688235294117647 on epoch=516
05/24/2022 16:27:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=519
05/24/2022 16:27:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=523
05/24/2022 16:27:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
05/24/2022 16:27:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/24/2022 16:27:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/24/2022 16:27:26 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2236842105263158 on epoch=533
05/24/2022 16:27:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/24/2022 16:27:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/24/2022 16:27:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=543
05/24/2022 16:27:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
05/24/2022 16:27:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=549
05/24/2022 16:27:40 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.28086419753086417 on epoch=549
05/24/2022 16:27:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/24/2022 16:27:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/24/2022 16:27:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
05/24/2022 16:27:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/24/2022 16:27:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
05/24/2022 16:27:54 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.20698805838124726 on epoch=566
05/24/2022 16:27:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/24/2022 16:27:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/24/2022 16:28:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/24/2022 16:28:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
05/24/2022 16:28:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=583
05/24/2022 16:28:08 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3018641565153193 on epoch=583
05/24/2022 16:28:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=586
05/24/2022 16:28:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 16:28:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/24/2022 16:28:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/24/2022 16:28:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
05/24/2022 16:28:22 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.1260952380952381 on epoch=599
05/24/2022 16:28:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/24/2022 16:28:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/24/2022 16:28:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=609
05/24/2022 16:28:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/24/2022 16:28:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/24/2022 16:28:36 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3484612431980853 on epoch=616
05/24/2022 16:28:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/24/2022 16:28:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/24/2022 16:28:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/24/2022 16:28:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/24/2022 16:28:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=633
05/24/2022 16:28:51 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.34920634920634924 on epoch=633
05/24/2022 16:28:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=636
05/24/2022 16:28:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/24/2022 16:28:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/24/2022 16:29:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=646
05/24/2022 16:29:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/24/2022 16:29:05 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2517543859649123 on epoch=649
05/24/2022 16:29:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/24/2022 16:29:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=656
05/24/2022 16:29:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/24/2022 16:29:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/24/2022 16:29:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/24/2022 16:29:19 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.2925402938239908 on epoch=666
05/24/2022 16:29:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/24/2022 16:29:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/24/2022 16:29:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/24/2022 16:29:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
05/24/2022 16:29:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/24/2022 16:29:33 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.15358511220580187 on epoch=683
05/24/2022 16:29:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/24/2022 16:29:38 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/24/2022 16:29:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 16:29:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
05/24/2022 16:29:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=699
05/24/2022 16:29:47 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.18581399735898887 on epoch=699
05/24/2022 16:29:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/24/2022 16:29:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/24/2022 16:29:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 16:29:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/24/2022 16:29:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 16:30:00 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.24453138456238774 on epoch=716
05/24/2022 16:30:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/24/2022 16:30:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/24/2022 16:30:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/24/2022 16:30:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/24/2022 16:30:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/24/2022 16:30:13 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.15484234234234234 on epoch=733
05/24/2022 16:30:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
05/24/2022 16:30:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/24/2022 16:30:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/24/2022 16:30:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/24/2022 16:30:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/24/2022 16:30:27 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.20623100303951364 on epoch=749
05/24/2022 16:30:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/24/2022 16:30:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/24/2022 16:30:34 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
05/24/2022 16:30:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/24/2022 16:30:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/24/2022 16:30:41 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.21673992673992673 on epoch=766
05/24/2022 16:30:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 16:30:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
05/24/2022 16:30:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/24/2022 16:30:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 16:30:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 16:30:54 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.19974340175953081 on epoch=783
05/24/2022 16:30:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 16:30:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 16:31:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/24/2022 16:31:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 16:31:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/24/2022 16:31:08 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.13035475617672893 on epoch=799
05/24/2022 16:31:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/24/2022 16:31:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
05/24/2022 16:31:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 16:31:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/24/2022 16:31:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/24/2022 16:31:22 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.21460892049127342 on epoch=816
05/24/2022 16:31:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/24/2022 16:31:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/24/2022 16:31:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/24/2022 16:31:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 16:31:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/24/2022 16:31:36 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.2057951987996999 on epoch=833
05/24/2022 16:31:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/24/2022 16:31:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/24/2022 16:31:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 16:31:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/24/2022 16:31:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
05/24/2022 16:31:49 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.1874489795918367 on epoch=849
05/24/2022 16:31:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/24/2022 16:31:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
05/24/2022 16:31:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=859
05/24/2022 16:31:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/24/2022 16:32:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 16:32:03 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.23464912280701752 on epoch=866
05/24/2022 16:32:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 16:32:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/24/2022 16:32:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 16:32:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 16:32:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 16:32:17 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.22324002053144862 on epoch=883
05/24/2022 16:32:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 16:32:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 16:32:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 16:32:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 16:32:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/24/2022 16:32:31 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.2794301994301995 on epoch=899
05/24/2022 16:32:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
05/24/2022 16:32:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 16:32:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 16:32:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 16:32:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 16:32:45 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.19702521449177626 on epoch=916
05/24/2022 16:32:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 16:32:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 16:32:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 16:32:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 16:32:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=933
05/24/2022 16:32:59 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.22705108359133128 on epoch=933
05/24/2022 16:33:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 16:33:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 16:33:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/24/2022 16:33:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 16:33:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 16:33:12 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.21114484930274405 on epoch=949
05/24/2022 16:33:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 16:33:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/24/2022 16:33:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
05/24/2022 16:33:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/24/2022 16:33:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 16:33:26 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.15911409729998754 on epoch=966
05/24/2022 16:33:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 16:33:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 16:33:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 16:33:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 16:33:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 16:33:41 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.24871794871794872 on epoch=983
05/24/2022 16:33:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 16:33:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 16:33:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 16:33:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 16:33:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:33:55 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:33:55 - INFO - __main__ - ['neutral']
05/24/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:33:55 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:33:55 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.18742690058479533 on epoch=999
05/24/2022 16:33:55 - INFO - __main__ - save last model!
05/24/2022 16:33:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 16:33:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 16:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 16:33:55 - INFO - __main__ - ['contradiction']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 16:33:55 - INFO - __main__ - ['entailment']
05/24/2022 16:33:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 16:33:55 - INFO - __main__ - ['contradiction']
05/24/2022 16:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:33:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:33:57 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 16:34:10 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:34:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:34:10 - INFO - __main__ - Starting training!
05/24/2022 16:34:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_100_0.5_8_predictions.txt
05/24/2022 16:34:35 - INFO - __main__ - Classification-F1 on test data: 0.1467
05/24/2022 16:34:35 - INFO - __main__ - prefix=anli_16_100, lr=0.5, bsz=8, dev_performance=0.3532369578881207, test_performance=0.14668903680296533
05/24/2022 16:34:35 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.4, bsz=8 ...
05/24/2022 16:34:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:34:36 - INFO - __main__ - Printing 3 examples
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:34:36 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:34:36 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:34:36 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:34:36 - INFO - __main__ - Printing 3 examples
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:34:36 - INFO - __main__ - ['neutral']
05/24/2022 16:34:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:34:36 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:34:36 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:34:54 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:34:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:34:55 - INFO - __main__ - Starting training!
05/24/2022 16:34:58 - INFO - __main__ - Step 10 Global step 10 Train loss 0.91 on epoch=3
05/24/2022 16:35:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=6
05/24/2022 16:35:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=9
05/24/2022 16:35:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
05/24/2022 16:35:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=16
05/24/2022 16:35:09 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.2796847190439868 on epoch=16
05/24/2022 16:35:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2796847190439868 on epoch=16, global_step=50
05/24/2022 16:35:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=19
05/24/2022 16:35:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=23
05/24/2022 16:35:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=26
05/24/2022 16:35:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=29
05/24/2022 16:35:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
05/24/2022 16:35:23 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.15873015873015875 on epoch=33
05/24/2022 16:35:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/24/2022 16:35:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=39
05/24/2022 16:35:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
05/24/2022 16:35:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
05/24/2022 16:35:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
05/24/2022 16:35:36 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 16:35:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
05/24/2022 16:35:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
05/24/2022 16:35:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 16:35:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/24/2022 16:35:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
05/24/2022 16:35:50 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 16:35:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=69
05/24/2022 16:35:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
05/24/2022 16:35:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=76
05/24/2022 16:36:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
05/24/2022 16:36:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=83
05/24/2022 16:36:04 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 16:36:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=86
05/24/2022 16:36:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=89
05/24/2022 16:36:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
05/24/2022 16:36:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
05/24/2022 16:36:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
05/24/2022 16:36:17 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=99
05/24/2022 16:36:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=103
05/24/2022 16:36:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=106
05/24/2022 16:36:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
05/24/2022 16:36:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
05/24/2022 16:36:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=116
05/24/2022 16:36:31 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
05/24/2022 16:36:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
05/24/2022 16:36:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
05/24/2022 16:36:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=126
05/24/2022 16:36:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
05/24/2022 16:36:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
05/24/2022 16:36:45 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.2044241037376049 on epoch=133
05/24/2022 16:36:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=136
05/24/2022 16:36:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
05/24/2022 16:36:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
05/24/2022 16:36:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
05/24/2022 16:36:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
05/24/2022 16:36:59 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=149
05/24/2022 16:37:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=153
05/24/2022 16:37:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=156
05/24/2022 16:37:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=159
05/24/2022 16:37:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=163
05/24/2022 16:37:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=166
05/24/2022 16:37:12 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.15873015873015875 on epoch=166
05/24/2022 16:37:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=169
05/24/2022 16:37:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
05/24/2022 16:37:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=176
05/24/2022 16:37:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=179
05/24/2022 16:37:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=183
05/24/2022 16:37:26 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.23410986482599946 on epoch=183
05/24/2022 16:37:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
05/24/2022 16:37:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=189
05/24/2022 16:37:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=193
05/24/2022 16:37:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=196
05/24/2022 16:37:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=199
05/24/2022 16:37:40 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.16129032258064516 on epoch=199
05/24/2022 16:37:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=203
05/24/2022 16:37:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
05/24/2022 16:37:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=209
05/24/2022 16:37:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=213
05/24/2022 16:37:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=216
05/24/2022 16:37:54 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.20652856246076587 on epoch=216
05/24/2022 16:37:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=219
05/24/2022 16:37:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=223
05/24/2022 16:38:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=226
05/24/2022 16:38:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=229
05/24/2022 16:38:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=233
05/24/2022 16:38:08 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.17204301075268816 on epoch=233
05/24/2022 16:38:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
05/24/2022 16:38:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=239
05/24/2022 16:38:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=243
05/24/2022 16:38:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=246
05/24/2022 16:38:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=249
05/24/2022 16:38:21 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.16374269005847955 on epoch=249
05/24/2022 16:38:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=253
05/24/2022 16:38:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=256
05/24/2022 16:38:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=259
05/24/2022 16:38:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=263
05/24/2022 16:38:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=266
05/24/2022 16:38:35 - INFO - __main__ - Global step 800 Train loss 0.25 Classification-F1 0.1693121693121693 on epoch=266
05/24/2022 16:38:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=269
05/24/2022 16:38:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=273
05/24/2022 16:38:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=276
05/24/2022 16:38:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=279
05/24/2022 16:38:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=283
05/24/2022 16:38:49 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.15204678362573099 on epoch=283
05/24/2022 16:38:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=286
05/24/2022 16:38:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=289
05/24/2022 16:38:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=293
05/24/2022 16:38:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=296
05/24/2022 16:39:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=299
05/24/2022 16:39:03 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.27254870733131603 on epoch=299
05/24/2022 16:39:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=303
05/24/2022 16:39:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=306
05/24/2022 16:39:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=309
05/24/2022 16:39:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=313
05/24/2022 16:39:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=316
05/24/2022 16:39:17 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.1361111111111111 on epoch=316
05/24/2022 16:39:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=319
05/24/2022 16:39:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=323
05/24/2022 16:39:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=326
05/24/2022 16:39:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=329
05/24/2022 16:39:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=333
05/24/2022 16:39:31 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.23438792390405291 on epoch=333
05/24/2022 16:39:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=336
05/24/2022 16:39:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=339
05/24/2022 16:39:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=343
05/24/2022 16:39:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
05/24/2022 16:39:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=349
05/24/2022 16:39:44 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.16638297872340427 on epoch=349
05/24/2022 16:39:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=353
05/24/2022 16:39:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/24/2022 16:39:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=359
05/24/2022 16:39:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=363
05/24/2022 16:39:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
05/24/2022 16:39:59 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.22291306084409532 on epoch=366
05/24/2022 16:40:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
05/24/2022 16:40:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=373
05/24/2022 16:40:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=376
05/24/2022 16:40:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=379
05/24/2022 16:40:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
05/24/2022 16:40:12 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.18056038647342992 on epoch=383
05/24/2022 16:40:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
05/24/2022 16:40:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=389
05/24/2022 16:40:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
05/24/2022 16:40:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=396
05/24/2022 16:40:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
05/24/2022 16:40:26 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.2103623188405797 on epoch=399
05/24/2022 16:40:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
05/24/2022 16:40:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
05/24/2022 16:40:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=409
05/24/2022 16:40:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
05/24/2022 16:40:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=416
05/24/2022 16:40:40 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.22219987567192015 on epoch=416
05/24/2022 16:40:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
05/24/2022 16:40:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
05/24/2022 16:40:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
05/24/2022 16:40:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
05/24/2022 16:40:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=433
05/24/2022 16:40:54 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.2126701800614844 on epoch=433
05/24/2022 16:40:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=436
05/24/2022 16:40:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
05/24/2022 16:41:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=443
05/24/2022 16:41:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
05/24/2022 16:41:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
05/24/2022 16:41:08 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.24555555555555555 on epoch=449
05/24/2022 16:41:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/24/2022 16:41:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/24/2022 16:41:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=459
05/24/2022 16:41:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
05/24/2022 16:41:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
05/24/2022 16:41:22 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.24999999999999997 on epoch=466
05/24/2022 16:41:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/24/2022 16:41:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=473
05/24/2022 16:41:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/24/2022 16:41:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
05/24/2022 16:41:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
05/24/2022 16:41:36 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.1570909090909091 on epoch=483
05/24/2022 16:41:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
05/24/2022 16:41:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=489
05/24/2022 16:41:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=493
05/24/2022 16:41:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
05/24/2022 16:41:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/24/2022 16:41:50 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.21151716500553708 on epoch=499
05/24/2022 16:41:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
05/24/2022 16:41:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
05/24/2022 16:41:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
05/24/2022 16:42:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=513
05/24/2022 16:42:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/24/2022 16:42:04 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.2777777777777778 on epoch=516
05/24/2022 16:42:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
05/24/2022 16:42:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/24/2022 16:42:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
05/24/2022 16:42:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
05/24/2022 16:42:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/24/2022 16:42:18 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.2732021466905188 on epoch=533
05/24/2022 16:42:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/24/2022 16:42:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/24/2022 16:42:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
05/24/2022 16:42:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=546
05/24/2022 16:42:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/24/2022 16:42:32 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3302768549280177 on epoch=549
05/24/2022 16:42:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2796847190439868 -> 0.3302768549280177 on epoch=549, global_step=1650
05/24/2022 16:42:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
05/24/2022 16:42:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/24/2022 16:42:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
05/24/2022 16:42:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/24/2022 16:42:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
05/24/2022 16:42:46 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.2126701800614844 on epoch=566
05/24/2022 16:42:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/24/2022 16:42:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/24/2022 16:42:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
05/24/2022 16:42:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=579
05/24/2022 16:42:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=583
05/24/2022 16:43:00 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.2052669552669553 on epoch=583
05/24/2022 16:43:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 16:43:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 16:43:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/24/2022 16:43:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
05/24/2022 16:43:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=599
05/24/2022 16:43:14 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.3024691358024691 on epoch=599
05/24/2022 16:43:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
05/24/2022 16:43:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
05/24/2022 16:43:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/24/2022 16:43:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/24/2022 16:43:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/24/2022 16:43:28 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.21217391304347827 on epoch=616
05/24/2022 16:43:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/24/2022 16:43:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/24/2022 16:43:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/24/2022 16:43:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/24/2022 16:43:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/24/2022 16:43:42 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.24982478273058592 on epoch=633
05/24/2022 16:43:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/24/2022 16:43:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/24/2022 16:43:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=643
05/24/2022 16:43:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/24/2022 16:43:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/24/2022 16:43:57 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.15914423740510697 on epoch=649
05/24/2022 16:43:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=653
05/24/2022 16:44:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
05/24/2022 16:44:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/24/2022 16:44:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
05/24/2022 16:44:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/24/2022 16:44:11 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.25397137745974957 on epoch=666
05/24/2022 16:44:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
05/24/2022 16:44:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
05/24/2022 16:44:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/24/2022 16:44:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
05/24/2022 16:44:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/24/2022 16:44:25 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.26282734347250475 on epoch=683
05/24/2022 16:44:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/24/2022 16:44:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/24/2022 16:44:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=693
05/24/2022 16:44:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
05/24/2022 16:44:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
05/24/2022 16:44:39 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.2504761904761905 on epoch=699
05/24/2022 16:44:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/24/2022 16:44:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/24/2022 16:44:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 16:44:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/24/2022 16:44:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/24/2022 16:44:53 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.28812997347480107 on epoch=716
05/24/2022 16:44:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=719
05/24/2022 16:44:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/24/2022 16:45:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/24/2022 16:45:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
05/24/2022 16:45:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/24/2022 16:45:07 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.35367268746579095 on epoch=733
05/24/2022 16:45:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3302768549280177 -> 0.35367268746579095 on epoch=733, global_step=2200
05/24/2022 16:45:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/24/2022 16:45:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
05/24/2022 16:45:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/24/2022 16:45:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 16:45:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 16:45:21 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.3426881720430108 on epoch=749
05/24/2022 16:45:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
05/24/2022 16:45:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/24/2022 16:45:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
05/24/2022 16:45:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/24/2022 16:45:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/24/2022 16:45:35 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.3060359106870734 on epoch=766
05/24/2022 16:45:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
05/24/2022 16:45:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/24/2022 16:45:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/24/2022 16:45:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 16:45:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 16:45:49 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.2918989223337049 on epoch=783
05/24/2022 16:45:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=786
05/24/2022 16:45:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 16:45:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/24/2022 16:45:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 16:46:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
05/24/2022 16:46:03 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.24970394736842105 on epoch=799
05/24/2022 16:46:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/24/2022 16:46:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 16:46:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 16:46:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/24/2022 16:46:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
05/24/2022 16:46:17 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.3275268817204301 on epoch=816
05/24/2022 16:46:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=819
05/24/2022 16:46:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/24/2022 16:46:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/24/2022 16:46:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 16:46:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
05/24/2022 16:46:31 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.30272267859925645 on epoch=833
05/24/2022 16:46:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
05/24/2022 16:46:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/24/2022 16:46:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 16:46:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=846
05/24/2022 16:46:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=849
05/24/2022 16:46:46 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.3060359106870734 on epoch=849
05/24/2022 16:46:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/24/2022 16:46:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/24/2022 16:46:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 16:46:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 16:46:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 16:47:00 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.3306841817186645 on epoch=866
05/24/2022 16:47:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 16:47:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/24/2022 16:47:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/24/2022 16:47:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 16:47:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 16:47:13 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.3161567364465915 on epoch=883
05/24/2022 16:47:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
05/24/2022 16:47:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 16:47:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 16:47:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 16:47:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 16:47:27 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.31876543209876546 on epoch=899
05/24/2022 16:47:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/24/2022 16:47:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 16:47:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 16:47:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 16:47:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 16:47:41 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.3270460704607046 on epoch=916
05/24/2022 16:47:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 16:47:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 16:47:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 16:47:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 16:47:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 16:47:56 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.35539012958367794 on epoch=933
05/24/2022 16:47:56 - INFO - __main__ - Saving model with best Classification-F1: 0.35367268746579095 -> 0.35539012958367794 on epoch=933, global_step=2800
05/24/2022 16:47:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/24/2022 16:48:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 16:48:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 16:48:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 16:48:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
05/24/2022 16:48:09 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3547008547008547 on epoch=949
05/24/2022 16:48:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 16:48:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/24/2022 16:48:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 16:48:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/24/2022 16:48:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 16:48:23 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.3565283400809716 on epoch=966
05/24/2022 16:48:23 - INFO - __main__ - Saving model with best Classification-F1: 0.35539012958367794 -> 0.3565283400809716 on epoch=966, global_step=2900
05/24/2022 16:48:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 16:48:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 16:48:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 16:48:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 16:48:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 16:48:37 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.285096831196122 on epoch=983
05/24/2022 16:48:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 16:48:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 16:48:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 16:48:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 16:48:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 16:48:51 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.285096831196122 on epoch=999
05/24/2022 16:48:51 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:48:51 - INFO - __main__ - Printing 3 examples
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:48:51 - INFO - __main__ - save last model!
05/24/2022 16:48:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:48:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 16:48:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 16:48:51 - INFO - __main__ - Printing 3 examples
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 16:48:51 - INFO - __main__ - ['contradiction']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 16:48:51 - INFO - __main__ - ['entailment']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 16:48:51 - INFO - __main__ - ['contradiction']
05/24/2022 16:48:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:48:51 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:48:51 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:48:51 - INFO - __main__ - Printing 3 examples
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:48:51 - INFO - __main__ - ['neutral']
05/24/2022 16:48:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:48:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:48:51 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:48:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:48:53 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 16:49:10 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:49:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:49:11 - INFO - __main__ - Starting training!
05/24/2022 16:49:21 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_100_0.4_8_predictions.txt
05/24/2022 16:49:21 - INFO - __main__ - Classification-F1 on test data: 0.1889
05/24/2022 16:49:22 - INFO - __main__ - prefix=anli_16_100, lr=0.4, bsz=8, dev_performance=0.3565283400809716, test_performance=0.18894055106319257
05/24/2022 16:49:22 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.3, bsz=8 ...
05/24/2022 16:49:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:49:23 - INFO - __main__ - Printing 3 examples
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:49:23 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:49:23 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 16:49:23 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 16:49:23 - INFO - __main__ - Printing 3 examples
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 16:49:23 - INFO - __main__ - ['neutral']
05/24/2022 16:49:23 - INFO - __main__ - Tokenizing Input ...
05/24/2022 16:49:23 - INFO - __main__ - Tokenizing Output ...
05/24/2022 16:49:23 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 16:49:40 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 16:49:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 16:49:41 - INFO - __main__ - Starting training!
05/24/2022 16:49:44 - INFO - __main__ - Step 10 Global step 10 Train loss 1.11 on epoch=3
05/24/2022 16:49:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.75 on epoch=6
05/24/2022 16:49:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=9
05/24/2022 16:49:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=13
05/24/2022 16:49:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
05/24/2022 16:49:55 - INFO - __main__ - Global step 50 Train loss 0.72 Classification-F1 0.28571428571428575 on epoch=16
05/24/2022 16:49:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28571428571428575 on epoch=16, global_step=50
05/24/2022 16:49:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
05/24/2022 16:50:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
05/24/2022 16:50:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
05/24/2022 16:50:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=29
05/24/2022 16:50:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=33
05/24/2022 16:50:09 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 16:50:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/24/2022 16:50:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
05/24/2022 16:50:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
05/24/2022 16:50:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=46
05/24/2022 16:50:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=49
05/24/2022 16:50:22 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.3680555555555555 on epoch=49
05/24/2022 16:50:22 - INFO - __main__ - Saving model with best Classification-F1: 0.28571428571428575 -> 0.3680555555555555 on epoch=49, global_step=150
05/24/2022 16:50:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
05/24/2022 16:50:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/24/2022 16:50:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 16:50:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
05/24/2022 16:50:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
05/24/2022 16:50:36 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.23410986482599946 on epoch=66
05/24/2022 16:50:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
05/24/2022 16:50:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=73
05/24/2022 16:50:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=76
05/24/2022 16:50:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
05/24/2022 16:50:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
05/24/2022 16:50:49 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 16:50:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
05/24/2022 16:50:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=89
05/24/2022 16:50:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
05/24/2022 16:51:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
05/24/2022 16:51:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=99
05/24/2022 16:51:03 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.3172009218520846 on epoch=99
05/24/2022 16:51:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/24/2022 16:51:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
05/24/2022 16:51:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
05/24/2022 16:51:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=113
05/24/2022 16:51:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/24/2022 16:51:17 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.3659147869674186 on epoch=116
05/24/2022 16:51:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=119
05/24/2022 16:51:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
05/24/2022 16:51:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
05/24/2022 16:51:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
05/24/2022 16:51:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
05/24/2022 16:51:30 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.1983273596176822 on epoch=133
05/24/2022 16:51:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=136
05/24/2022 16:51:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
05/24/2022 16:51:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=143
05/24/2022 16:51:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=146
05/24/2022 16:51:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=149
05/24/2022 16:51:44 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1983273596176822 on epoch=149
05/24/2022 16:51:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=153
05/24/2022 16:51:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=156
05/24/2022 16:51:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/24/2022 16:51:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
05/24/2022 16:51:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=166
05/24/2022 16:51:57 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.2109090909090909 on epoch=166
05/24/2022 16:52:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=169
05/24/2022 16:52:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=173
05/24/2022 16:52:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=176
05/24/2022 16:52:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=179
05/24/2022 16:52:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=183
05/24/2022 16:52:11 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=183
05/24/2022 16:52:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=186
05/24/2022 16:52:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=189
05/24/2022 16:52:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=193
05/24/2022 16:52:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=196
05/24/2022 16:52:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=199
05/24/2022 16:52:25 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=199
05/24/2022 16:52:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=203
05/24/2022 16:52:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=206
05/24/2022 16:52:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=209
05/24/2022 16:52:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=213
05/24/2022 16:52:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=216
05/24/2022 16:52:38 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.16666666666666666 on epoch=216
05/24/2022 16:52:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=219
05/24/2022 16:52:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
05/24/2022 16:52:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=226
05/24/2022 16:52:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=229
05/24/2022 16:52:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=233
05/24/2022 16:52:52 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.1693121693121693 on epoch=233
05/24/2022 16:52:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
05/24/2022 16:52:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=239
05/24/2022 16:52:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=243
05/24/2022 16:53:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
05/24/2022 16:53:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=249
05/24/2022 16:53:05 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.16666666666666666 on epoch=249
05/24/2022 16:53:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=253
05/24/2022 16:53:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=256
05/24/2022 16:53:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=259
05/24/2022 16:53:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=263
05/24/2022 16:53:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=266
05/24/2022 16:53:19 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.1693121693121693 on epoch=266
05/24/2022 16:53:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=269
05/24/2022 16:53:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=273
05/24/2022 16:53:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=276
05/24/2022 16:53:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=279
05/24/2022 16:53:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=283
05/24/2022 16:53:33 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.17486338797814208 on epoch=283
05/24/2022 16:53:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=286
05/24/2022 16:53:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=289
05/24/2022 16:53:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=293
05/24/2022 16:53:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.89 on epoch=296
05/24/2022 16:53:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.71 on epoch=299
05/24/2022 16:53:47 - INFO - __main__ - Global step 900 Train loss 0.51 Classification-F1 0.1693121693121693 on epoch=299
05/24/2022 16:53:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=303
05/24/2022 16:53:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=306
05/24/2022 16:53:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=309
05/24/2022 16:53:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=313
05/24/2022 16:53:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=316
05/24/2022 16:54:00 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.1693121693121693 on epoch=316
05/24/2022 16:54:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=319
05/24/2022 16:54:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=323
05/24/2022 16:54:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=326
05/24/2022 16:54:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=329
05/24/2022 16:54:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=333
05/24/2022 16:54:14 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.17204301075268816 on epoch=333
05/24/2022 16:54:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=336
05/24/2022 16:54:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=339
05/24/2022 16:54:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=343
05/24/2022 16:54:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=346
05/24/2022 16:54:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=349
05/24/2022 16:54:27 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.1693121693121693 on epoch=349
05/24/2022 16:54:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=353
05/24/2022 16:54:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=356
05/24/2022 16:54:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=359
05/24/2022 16:54:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=363
05/24/2022 16:54:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=366
05/24/2022 16:54:41 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.1693121693121693 on epoch=366
05/24/2022 16:54:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=369
05/24/2022 16:54:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=373
05/24/2022 16:54:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=376
05/24/2022 16:54:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=379
05/24/2022 16:54:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=383
05/24/2022 16:54:55 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.16666666666666666 on epoch=383
05/24/2022 16:54:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=386
05/24/2022 16:55:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=389
05/24/2022 16:55:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=393
05/24/2022 16:55:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=396
05/24/2022 16:55:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=399
05/24/2022 16:55:08 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.1693121693121693 on epoch=399
05/24/2022 16:55:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=403
05/24/2022 16:55:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=406
05/24/2022 16:55:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=409
05/24/2022 16:55:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=413
05/24/2022 16:55:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=416
05/24/2022 16:55:22 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.1693121693121693 on epoch=416
05/24/2022 16:55:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=419
05/24/2022 16:55:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=423
05/24/2022 16:55:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=426
05/24/2022 16:55:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=429
05/24/2022 16:55:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=433
05/24/2022 16:55:35 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.17204301075268816 on epoch=433
05/24/2022 16:55:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=436
05/24/2022 16:55:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=439
05/24/2022 16:55:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=443
05/24/2022 16:55:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=446
05/24/2022 16:55:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=449
05/24/2022 16:55:49 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.17204301075268816 on epoch=449
05/24/2022 16:55:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=453
05/24/2022 16:55:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=456
05/24/2022 16:55:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=459
05/24/2022 16:55:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=463
05/24/2022 16:56:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=466
05/24/2022 16:56:03 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.17204301075268816 on epoch=466
05/24/2022 16:56:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=469
05/24/2022 16:56:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=473
05/24/2022 16:56:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=476
05/24/2022 16:56:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=479
05/24/2022 16:56:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=483
05/24/2022 16:56:16 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.17204301075268816 on epoch=483
05/24/2022 16:56:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=486
05/24/2022 16:56:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=489
05/24/2022 16:56:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=493
05/24/2022 16:56:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=496
05/24/2022 16:56:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=499
05/24/2022 16:56:30 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.17204301075268816 on epoch=499
05/24/2022 16:56:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=503
05/24/2022 16:56:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=506
05/24/2022 16:56:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=509
05/24/2022 16:56:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=513
05/24/2022 16:56:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=516
05/24/2022 16:56:43 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.17204301075268816 on epoch=516
05/24/2022 16:56:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=519
05/24/2022 16:56:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=523
05/24/2022 16:56:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=526
05/24/2022 16:56:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=529
05/24/2022 16:56:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=533
05/24/2022 16:56:57 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.1693121693121693 on epoch=533
05/24/2022 16:56:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=536
05/24/2022 16:57:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=539
05/24/2022 16:57:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=543
05/24/2022 16:57:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=546
05/24/2022 16:57:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=549
05/24/2022 16:57:10 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.16129032258064516 on epoch=549
05/24/2022 16:57:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=553
05/24/2022 16:57:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=556
05/24/2022 16:57:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=559
05/24/2022 16:57:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=563
05/24/2022 16:57:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=566
05/24/2022 16:57:24 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.1693121693121693 on epoch=566
05/24/2022 16:57:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=569
05/24/2022 16:57:29 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=573
05/24/2022 16:57:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=576
05/24/2022 16:57:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=579
05/24/2022 16:57:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=583
05/24/2022 16:57:38 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.16666666666666666 on epoch=583
05/24/2022 16:57:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=586
05/24/2022 16:57:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=589
05/24/2022 16:57:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=593
05/24/2022 16:57:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=596
05/24/2022 16:57:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=599
05/24/2022 16:57:51 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.16949152542372883 on epoch=599
05/24/2022 16:57:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=603
05/24/2022 16:57:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=606
05/24/2022 16:57:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=609
05/24/2022 16:58:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=613
05/24/2022 16:58:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=616
05/24/2022 16:58:05 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.1724137931034483 on epoch=616
05/24/2022 16:58:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=619
05/24/2022 16:58:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=623
05/24/2022 16:58:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=626
05/24/2022 16:58:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=629
05/24/2022 16:58:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=633
05/24/2022 16:58:19 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.16949152542372883 on epoch=633
05/24/2022 16:58:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=636
05/24/2022 16:58:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=639
05/24/2022 16:58:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=643
05/24/2022 16:58:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=646
05/24/2022 16:58:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=649
05/24/2022 16:58:33 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.1639344262295082 on epoch=649
05/24/2022 16:58:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=653
05/24/2022 16:58:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=656
05/24/2022 16:58:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=659
05/24/2022 16:58:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=663
05/24/2022 16:58:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=666
05/24/2022 16:58:47 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.2087719298245614 on epoch=666
05/24/2022 16:58:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=669
05/24/2022 16:58:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=673
05/24/2022 16:58:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=676
05/24/2022 16:58:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=679
05/24/2022 16:58:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=683
05/24/2022 16:59:01 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.2555555555555556 on epoch=683
05/24/2022 16:59:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=686
05/24/2022 16:59:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=689
05/24/2022 16:59:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=693
05/24/2022 16:59:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=696
05/24/2022 16:59:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=699
05/24/2022 16:59:15 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.23841946951052775 on epoch=699
05/24/2022 16:59:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=703
05/24/2022 16:59:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=706
05/24/2022 16:59:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=709
05/24/2022 16:59:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=713
05/24/2022 16:59:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=716
05/24/2022 16:59:29 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.24564102564102563 on epoch=716
05/24/2022 16:59:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=719
05/24/2022 16:59:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=723
05/24/2022 16:59:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=726
05/24/2022 16:59:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=729
05/24/2022 16:59:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=733
05/24/2022 16:59:43 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.23978919631093543 on epoch=733
05/24/2022 16:59:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=736
05/24/2022 16:59:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=739
05/24/2022 16:59:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=743
05/24/2022 16:59:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=746
05/24/2022 16:59:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=749
05/24/2022 16:59:57 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.20415982484948003 on epoch=749
05/24/2022 16:59:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=753
05/24/2022 17:00:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=756
05/24/2022 17:00:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=759
05/24/2022 17:00:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=763
05/24/2022 17:00:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=766
05/24/2022 17:00:11 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.16969696969696968 on epoch=766
05/24/2022 17:00:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=769
05/24/2022 17:00:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=773
05/24/2022 17:00:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=776
05/24/2022 17:00:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=779
05/24/2022 17:00:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=783
05/24/2022 17:00:25 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.3172727272727273 on epoch=783
05/24/2022 17:00:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=786
05/24/2022 17:00:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=789
05/24/2022 17:00:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=793
05/24/2022 17:00:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=796
05/24/2022 17:00:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=799
05/24/2022 17:00:38 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.211258697027198 on epoch=799
05/24/2022 17:00:41 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=803
05/24/2022 17:00:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=806
05/24/2022 17:00:46 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=809
05/24/2022 17:00:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=813
05/24/2022 17:00:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=816
05/24/2022 17:00:52 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.27682623334797246 on epoch=816
05/24/2022 17:00:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=819
05/24/2022 17:00:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=823
05/24/2022 17:01:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=826
05/24/2022 17:01:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=829
05/24/2022 17:01:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=833
05/24/2022 17:01:06 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.2003261122757978 on epoch=833
05/24/2022 17:01:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=836
05/24/2022 17:01:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=839
05/24/2022 17:01:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=843
05/24/2022 17:01:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=846
05/24/2022 17:01:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=849
05/24/2022 17:01:21 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.22798815645940468 on epoch=849
05/24/2022 17:01:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=853
05/24/2022 17:01:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=856
05/24/2022 17:01:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=859
05/24/2022 17:01:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=863
05/24/2022 17:01:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=866
05/24/2022 17:01:34 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.27995642701525053 on epoch=866
05/24/2022 17:01:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=869
05/24/2022 17:01:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=873
05/24/2022 17:01:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=876
05/24/2022 17:01:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=879
05/24/2022 17:01:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
05/24/2022 17:01:48 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.27486606102958305 on epoch=883
05/24/2022 17:01:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=886
05/24/2022 17:01:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=889
05/24/2022 17:01:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=893
05/24/2022 17:01:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=896
05/24/2022 17:02:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
05/24/2022 17:02:02 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.275758129338692 on epoch=899
05/24/2022 17:02:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=903
05/24/2022 17:02:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=906
05/24/2022 17:02:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=909
05/24/2022 17:02:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=913
05/24/2022 17:02:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
05/24/2022 17:02:16 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.1988888888888889 on epoch=916
05/24/2022 17:02:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=919
05/24/2022 17:02:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=923
05/24/2022 17:02:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=926
05/24/2022 17:02:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=929
05/24/2022 17:02:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=933
05/24/2022 17:02:30 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.2025196552050772 on epoch=933
05/24/2022 17:02:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=936
05/24/2022 17:02:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
05/24/2022 17:02:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=943
05/24/2022 17:02:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=946
05/24/2022 17:02:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=949
05/24/2022 17:02:44 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.3058117405943493 on epoch=949
05/24/2022 17:02:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=953
05/24/2022 17:02:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=956
05/24/2022 17:02:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=959
05/24/2022 17:02:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=963
05/24/2022 17:02:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=966
05/24/2022 17:02:58 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.2802308802308802 on epoch=966
05/24/2022 17:03:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=969
05/24/2022 17:03:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=973
05/24/2022 17:03:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=976
05/24/2022 17:03:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/24/2022 17:03:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=983
05/24/2022 17:03:12 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.2799835931091058 on epoch=983
05/24/2022 17:03:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
05/24/2022 17:03:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
05/24/2022 17:03:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=993
05/24/2022 17:03:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=996
05/24/2022 17:03:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=999
05/24/2022 17:03:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:03:26 - INFO - __main__ - Printing 3 examples
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:03:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:03:26 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.19721739130434784 on epoch=999
05/24/2022 17:03:26 - INFO - __main__ - save last model!
05/24/2022 17:03:26 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:03:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:03:26 - INFO - __main__ - Printing 3 examples
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 17:03:26 - INFO - __main__ - ['neutral']
05/24/2022 17:03:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:03:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 17:03:26 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 17:03:26 - INFO - __main__ - Printing 3 examples
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 17:03:26 - INFO - __main__ - ['contradiction']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 17:03:26 - INFO - __main__ - ['entailment']
05/24/2022 17:03:26 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 17:03:26 - INFO - __main__ - ['contradiction']
05/24/2022 17:03:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:03:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:03:26 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:03:27 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:03:28 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 17:03:44 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:03:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:03:45 - INFO - __main__ - Starting training!
05/24/2022 17:03:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_100_0.3_8_predictions.txt
05/24/2022 17:03:57 - INFO - __main__ - Classification-F1 on test data: 0.1292
05/24/2022 17:03:57 - INFO - __main__ - prefix=anli_16_100, lr=0.3, bsz=8, dev_performance=0.3680555555555555, test_performance=0.12919239146935305
05/24/2022 17:03:57 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.2, bsz=8 ...
05/24/2022 17:03:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:03:58 - INFO - __main__ - Printing 3 examples
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:03:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:03:58 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:03:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:03:58 - INFO - __main__ - Printing 3 examples
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series "The Champions". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate. [SEP] hypothesis: Sharron Macready was a popular character through the 1980's.
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: The Maysville Micropolitan Statistical Area (μSA), as defined by the United States Census Bureau, is coterminous with Mason County, Kentucky, whose county seat and largest city is Maysville. As of the 2010 census, the population of Mason County and the current µSA was 17,490, and 2014 Census Bureau estimates place the population at 17,166. [SEP] hypothesis: The Census Bureau was made up of 17,166 people. 
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ -  [anli] premise: Fail Safe is a 1964 Cold War thriller film directed by Sidney Lumet, based on the 1962 novel of the same name by Eugene Burdick and Harvey Wheeler. It portrays a fictional account of a nuclear crisis. The film features performances by actors Henry Fonda, Dan O'Herlihy, Walter Matthau and Frank Overton. Larry Hagman, Fritz Weaver, Dom DeLuise and Sorrell Booke appeared in early film roles. [SEP] hypothesis: Fail Safe was more popular than the novel with the same name.
05/24/2022 17:03:58 - INFO - __main__ - ['neutral']
05/24/2022 17:03:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:03:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:03:58 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:04:16 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:04:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:04:17 - INFO - __main__ - Starting training!
05/24/2022 17:04:20 - INFO - __main__ - Step 10 Global step 10 Train loss 1.22 on epoch=3
05/24/2022 17:04:22 - INFO - __main__ - Step 20 Global step 20 Train loss 0.81 on epoch=6
05/24/2022 17:04:25 - INFO - __main__ - Step 30 Global step 30 Train loss 0.72 on epoch=9
05/24/2022 17:04:27 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=13
05/24/2022 17:04:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=16
05/24/2022 17:04:31 - INFO - __main__ - Global step 50 Train loss 0.81 Classification-F1 0.2085278555866791 on epoch=16
05/24/2022 17:04:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2085278555866791 on epoch=16, global_step=50
05/24/2022 17:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=19
05/24/2022 17:04:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
05/24/2022 17:04:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=26
05/24/2022 17:04:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=29
05/24/2022 17:04:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
05/24/2022 17:04:45 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 17:04:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=36
05/24/2022 17:04:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=39
05/24/2022 17:04:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
05/24/2022 17:04:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=46
05/24/2022 17:04:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=49
05/24/2022 17:04:58 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.23410986482599946 on epoch=49
05/24/2022 17:04:58 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.23410986482599946 on epoch=49, global_step=150
05/24/2022 17:05:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=53
05/24/2022 17:05:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
05/24/2022 17:05:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.71 on epoch=59
05/24/2022 17:05:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
05/24/2022 17:05:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=66
05/24/2022 17:05:12 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.1983273596176822 on epoch=66
05/24/2022 17:05:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=69
05/24/2022 17:05:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
05/24/2022 17:05:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=76
05/24/2022 17:05:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=79
05/24/2022 17:05:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=83
05/24/2022 17:05:25 - INFO - __main__ - Global step 250 Train loss 0.64 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 17:05:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=86
05/24/2022 17:05:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=89
05/24/2022 17:05:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=93
05/24/2022 17:05:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=96
05/24/2022 17:05:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=99
05/24/2022 17:05:39 - INFO - __main__ - Global step 300 Train loss 0.76 Classification-F1 0.16666666666666666 on epoch=99
05/24/2022 17:05:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=103
05/24/2022 17:05:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
05/24/2022 17:05:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.58 on epoch=109
05/24/2022 17:05:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=113
05/24/2022 17:05:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=116
05/24/2022 17:05:52 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.15873015873015875 on epoch=116
05/24/2022 17:05:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=119
05/24/2022 17:05:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=123
05/24/2022 17:06:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
05/24/2022 17:06:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
05/24/2022 17:06:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=133
05/24/2022 17:06:06 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=133
05/24/2022 17:06:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=136
05/24/2022 17:06:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=139
05/24/2022 17:06:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=143
05/24/2022 17:06:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=146
05/24/2022 17:06:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=149
05/24/2022 17:06:20 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=149
05/24/2022 17:06:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=153
05/24/2022 17:06:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=156
05/24/2022 17:06:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=159
05/24/2022 17:06:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=163
05/24/2022 17:06:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=166
05/24/2022 17:06:33 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.2909090909090909 on epoch=166
05/24/2022 17:06:33 - INFO - __main__ - Saving model with best Classification-F1: 0.23410986482599946 -> 0.2909090909090909 on epoch=166, global_step=500
05/24/2022 17:06:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=169
05/24/2022 17:06:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=173
05/24/2022 17:06:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=176
05/24/2022 17:06:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=179
05/24/2022 17:06:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=183
05/24/2022 17:06:47 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.3318289275736084 on epoch=183
05/24/2022 17:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2909090909090909 -> 0.3318289275736084 on epoch=183, global_step=550
05/24/2022 17:06:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=186
05/24/2022 17:06:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=189
05/24/2022 17:06:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=193
05/24/2022 17:06:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=196
05/24/2022 17:07:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=199
05/24/2022 17:07:01 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.28869895536562207 on epoch=199
05/24/2022 17:07:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=203
05/24/2022 17:07:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=206
05/24/2022 17:07:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=209
05/24/2022 17:07:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=213
05/24/2022 17:07:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=216
05/24/2022 17:07:15 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.2796847190439868 on epoch=216
05/24/2022 17:07:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=219
05/24/2022 17:07:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=223
05/24/2022 17:07:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=226
05/24/2022 17:07:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=229
05/24/2022 17:07:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=233
05/24/2022 17:07:28 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2647296206618241 on epoch=233
05/24/2022 17:07:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=236
05/24/2022 17:07:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=239
05/24/2022 17:07:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=243
05/24/2022 17:07:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=246
05/24/2022 17:07:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=249
05/24/2022 17:07:42 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.3116701607267645 on epoch=249
05/24/2022 17:07:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=253
05/24/2022 17:07:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=256
05/24/2022 17:07:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=259
05/24/2022 17:07:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=263
05/24/2022 17:07:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=266
05/24/2022 17:07:55 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.1983273596176822 on epoch=266
05/24/2022 17:07:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=269
05/24/2022 17:08:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=273
05/24/2022 17:08:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=276
05/24/2022 17:08:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=279
05/24/2022 17:08:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=283
05/24/2022 17:08:09 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.15873015873015875 on epoch=283
05/24/2022 17:08:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=286
05/24/2022 17:08:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=289
05/24/2022 17:08:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=293
05/24/2022 17:08:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=296
05/24/2022 17:08:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=299
05/24/2022 17:08:23 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.1983273596176822 on epoch=299
05/24/2022 17:08:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=303
05/24/2022 17:08:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=306
05/24/2022 17:08:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=309
05/24/2022 17:08:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=313
05/24/2022 17:08:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=316
05/24/2022 17:08:37 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.32424877707896577 on epoch=316
05/24/2022 17:08:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=319
05/24/2022 17:08:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=323
05/24/2022 17:08:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=326
05/24/2022 17:08:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=329
05/24/2022 17:08:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=333
05/24/2022 17:08:51 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=333
05/24/2022 17:08:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=336
05/24/2022 17:08:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=339
05/24/2022 17:08:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=343
05/24/2022 17:09:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=346
05/24/2022 17:09:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=349
05/24/2022 17:09:04 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.15873015873015875 on epoch=349
05/24/2022 17:09:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=353
05/24/2022 17:09:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=356
05/24/2022 17:09:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=359
05/24/2022 17:09:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=363
05/24/2022 17:09:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=366
05/24/2022 17:09:18 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.15873015873015875 on epoch=366
05/24/2022 17:09:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=369
05/24/2022 17:09:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=373
05/24/2022 17:09:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=376
05/24/2022 17:09:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=379
05/24/2022 17:09:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=383
05/24/2022 17:09:31 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=383
05/24/2022 17:09:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=386
05/24/2022 17:09:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=389
05/24/2022 17:09:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=393
05/24/2022 17:09:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=396
05/24/2022 17:09:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=399
05/24/2022 17:09:44 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.15873015873015875 on epoch=399
05/24/2022 17:09:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=403
05/24/2022 17:09:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=406
05/24/2022 17:09:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=409
05/24/2022 17:09:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=413
05/24/2022 17:09:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=416
05/24/2022 17:09:58 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.28869895536562207 on epoch=416
05/24/2022 17:10:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=419
05/24/2022 17:10:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=423
05/24/2022 17:10:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=426
05/24/2022 17:10:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=429
05/24/2022 17:10:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=433
05/24/2022 17:10:12 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.15873015873015875 on epoch=433
05/24/2022 17:10:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=436
05/24/2022 17:10:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=439
05/24/2022 17:10:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=443
05/24/2022 17:10:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=446
05/24/2022 17:10:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=449
05/24/2022 17:10:25 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.15873015873015875 on epoch=449
05/24/2022 17:10:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=453
05/24/2022 17:10:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=456
05/24/2022 17:10:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=459
05/24/2022 17:10:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=463
05/24/2022 17:10:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=466
05/24/2022 17:10:39 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.15873015873015875 on epoch=466
05/24/2022 17:10:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=469
05/24/2022 17:10:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=473
05/24/2022 17:10:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=476
05/24/2022 17:10:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=479
05/24/2022 17:10:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.46 on epoch=483
05/24/2022 17:10:52 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.15873015873015875 on epoch=483
05/24/2022 17:10:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=486
05/24/2022 17:10:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=489
05/24/2022 17:11:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=493
05/24/2022 17:11:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=496
05/24/2022 17:11:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=499
05/24/2022 17:11:05 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.15873015873015875 on epoch=499
05/24/2022 17:11:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=503
05/24/2022 17:11:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=506
05/24/2022 17:11:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=509
05/24/2022 17:11:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=513
05/24/2022 17:11:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=516
05/24/2022 17:11:19 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.15053763440860216 on epoch=516
05/24/2022 17:11:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=519
05/24/2022 17:11:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=523
05/24/2022 17:11:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=526
05/24/2022 17:11:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=529
05/24/2022 17:11:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=533
05/24/2022 17:11:32 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=533
05/24/2022 17:11:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=536
05/24/2022 17:11:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=539
05/24/2022 17:11:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=543
05/24/2022 17:11:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=546
05/24/2022 17:11:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=549
05/24/2022 17:11:46 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=549
05/24/2022 17:11:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=553
05/24/2022 17:11:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=556
05/24/2022 17:11:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=559
05/24/2022 17:11:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=563
05/24/2022 17:11:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=566
05/24/2022 17:11:59 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=566
05/24/2022 17:12:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=569
05/24/2022 17:12:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=573
05/24/2022 17:12:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=576
05/24/2022 17:12:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=579
05/24/2022 17:12:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.37 on epoch=583
05/24/2022 17:12:13 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=583
05/24/2022 17:12:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=586
05/24/2022 17:12:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=589
05/24/2022 17:12:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=593
05/24/2022 17:12:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=596
05/24/2022 17:12:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=599
05/24/2022 17:12:26 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.15873015873015875 on epoch=599
05/24/2022 17:12:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=603
05/24/2022 17:12:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.35 on epoch=606
05/24/2022 17:12:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=609
05/24/2022 17:12:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=613
05/24/2022 17:12:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=616
05/24/2022 17:12:40 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.16666666666666666 on epoch=616
05/24/2022 17:12:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=619
05/24/2022 17:12:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=623
05/24/2022 17:12:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.35 on epoch=626
05/24/2022 17:12:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=629
05/24/2022 17:12:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=633
05/24/2022 17:12:53 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.16666666666666666 on epoch=633
05/24/2022 17:12:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=636
05/24/2022 17:12:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=639
05/24/2022 17:13:01 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=643
05/24/2022 17:13:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=646
05/24/2022 17:13:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=649
05/24/2022 17:13:07 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.2085278555866791 on epoch=649
05/24/2022 17:13:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=653
05/24/2022 17:13:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=656
05/24/2022 17:13:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=659
05/24/2022 17:13:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=663
05/24/2022 17:13:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.32 on epoch=666
05/24/2022 17:13:21 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.14285714285714282 on epoch=666
05/24/2022 17:13:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=669
05/24/2022 17:13:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.29 on epoch=673
05/24/2022 17:13:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=676
05/24/2022 17:13:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.30 on epoch=679
05/24/2022 17:13:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=683
05/24/2022 17:13:35 - INFO - __main__ - Global step 2050 Train loss 0.31 Classification-F1 0.16129032258064516 on epoch=683
05/24/2022 17:13:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.32 on epoch=686
05/24/2022 17:13:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=689
05/24/2022 17:13:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=693
05/24/2022 17:13:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.28 on epoch=696
05/24/2022 17:13:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.28 on epoch=699
05/24/2022 17:13:49 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.15555555555555556 on epoch=699
05/24/2022 17:13:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.32 on epoch=703
05/24/2022 17:13:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=706
05/24/2022 17:13:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=709
05/24/2022 17:13:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=713
05/24/2022 17:14:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=716
05/24/2022 17:14:03 - INFO - __main__ - Global step 2150 Train loss 0.27 Classification-F1 0.14942528735632185 on epoch=716
05/24/2022 17:14:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.32 on epoch=719
05/24/2022 17:14:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.30 on epoch=723
05/24/2022 17:14:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=726
05/24/2022 17:14:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=729
05/24/2022 17:14:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=733
05/24/2022 17:14:17 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.1926655719759168 on epoch=733
05/24/2022 17:14:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.33 on epoch=736
05/24/2022 17:14:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.24 on epoch=739
05/24/2022 17:14:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=743
05/24/2022 17:14:27 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=746
05/24/2022 17:14:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=749
05/24/2022 17:14:31 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.17184265010351965 on epoch=749
05/24/2022 17:14:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=753
05/24/2022 17:14:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=756
05/24/2022 17:14:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=759
05/24/2022 17:14:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.26 on epoch=763
05/24/2022 17:14:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.25 on epoch=766
05/24/2022 17:14:44 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.15555555555555556 on epoch=766
05/24/2022 17:14:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=769
05/24/2022 17:14:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=773
05/24/2022 17:14:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=776
05/24/2022 17:14:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=779
05/24/2022 17:14:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=783
05/24/2022 17:14:58 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.17323232323232327 on epoch=783
05/24/2022 17:15:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=786
05/24/2022 17:15:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=789
05/24/2022 17:15:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=793
05/24/2022 17:15:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=796
05/24/2022 17:15:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=799
05/24/2022 17:15:12 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.1823498139287613 on epoch=799
05/24/2022 17:15:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=803
05/24/2022 17:15:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=806
05/24/2022 17:15:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=809
05/24/2022 17:15:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=813
05/24/2022 17:15:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=816
05/24/2022 17:15:26 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.14035087719298242 on epoch=816
05/24/2022 17:15:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=819
05/24/2022 17:15:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=823
05/24/2022 17:15:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=826
05/24/2022 17:15:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=829
05/24/2022 17:15:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=833
05/24/2022 17:15:40 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.1811713191023536 on epoch=833
05/24/2022 17:15:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=836
05/24/2022 17:15:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=839
05/24/2022 17:15:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=843
05/24/2022 17:15:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=846
05/24/2022 17:15:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=849
05/24/2022 17:15:53 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.16666666666666666 on epoch=849
05/24/2022 17:15:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=853
05/24/2022 17:15:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=856
05/24/2022 17:16:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=859
05/24/2022 17:16:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=863
05/24/2022 17:16:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=866
05/24/2022 17:16:07 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.2435897435897436 on epoch=866
05/24/2022 17:16:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=869
05/24/2022 17:16:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=873
05/24/2022 17:16:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=876
05/24/2022 17:16:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=879
05/24/2022 17:16:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=883
05/24/2022 17:16:21 - INFO - __main__ - Global step 2650 Train loss 0.09 Classification-F1 0.2287351502445842 on epoch=883
05/24/2022 17:16:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=886
05/24/2022 17:16:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=889
05/24/2022 17:16:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=893
05/24/2022 17:16:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=896
05/24/2022 17:16:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=899
05/24/2022 17:16:35 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.23222610722610723 on epoch=899
05/24/2022 17:16:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=903
05/24/2022 17:16:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=906
05/24/2022 17:16:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=909
05/24/2022 17:16:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
05/24/2022 17:16:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=916
05/24/2022 17:16:49 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.18881118881118883 on epoch=916
05/24/2022 17:16:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=919
05/24/2022 17:16:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=923
05/24/2022 17:16:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=926
05/24/2022 17:16:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=929
05/24/2022 17:17:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=933
05/24/2022 17:17:03 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.2000844683771513 on epoch=933
05/24/2022 17:17:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=936
05/24/2022 17:17:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=939
05/24/2022 17:17:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=943
05/24/2022 17:17:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=946
05/24/2022 17:17:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=949
05/24/2022 17:17:17 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.18376068376068375 on epoch=949
05/24/2022 17:17:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=953
05/24/2022 17:17:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=956
05/24/2022 17:17:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=959
05/24/2022 17:17:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=963
05/24/2022 17:17:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=966
05/24/2022 17:17:31 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.24094202898550723 on epoch=966
05/24/2022 17:17:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=969
05/24/2022 17:17:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=973
05/24/2022 17:17:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=976
05/24/2022 17:17:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=979
05/24/2022 17:17:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=983
05/24/2022 17:17:44 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.19747794091379986 on epoch=983
05/24/2022 17:17:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
05/24/2022 17:17:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=989
05/24/2022 17:17:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=993
05/24/2022 17:17:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=996
05/24/2022 17:17:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=999
05/24/2022 17:17:58 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.10714285714285712 on epoch=999
05/24/2022 17:17:58 - INFO - __main__ - save last model!
05/24/2022 17:17:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 17:17:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 17:17:58 - INFO - __main__ - Printing 3 examples
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 17:17:58 - INFO - __main__ - ['entailment']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:17:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:17:58 - INFO - __main__ - Printing 3 examples
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:17:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:17:58 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:17:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:17:58 - INFO - __main__ - Printing 3 examples
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 17:17:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:17:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:17:58 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:17:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:18:00 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 17:18:17 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:18:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:18:18 - INFO - __main__ - Starting training!
05/24/2022 17:18:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_100_0.2_8_predictions.txt
05/24/2022 17:18:27 - INFO - __main__ - Classification-F1 on test data: 0.2113
05/24/2022 17:18:28 - INFO - __main__ - prefix=anli_16_100, lr=0.2, bsz=8, dev_performance=0.3318289275736084, test_performance=0.2112524420819343
05/24/2022 17:18:28 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.5, bsz=8 ...
05/24/2022 17:18:29 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:18:29 - INFO - __main__ - Printing 3 examples
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:18:29 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:18:29 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:18:29 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:18:29 - INFO - __main__ - Printing 3 examples
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:18:29 - INFO - __main__ - ['contradiction']
05/24/2022 17:18:29 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:18:29 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:18:29 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:18:45 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:18:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:18:46 - INFO - __main__ - Starting training!
05/24/2022 17:18:50 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=3
05/24/2022 17:18:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=6
05/24/2022 17:18:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=9
05/24/2022 17:18:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=13
05/24/2022 17:18:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=16
05/24/2022 17:19:00 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.28012654587287894 on epoch=16
05/24/2022 17:19:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28012654587287894 on epoch=16, global_step=50
05/24/2022 17:19:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/24/2022 17:19:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=23
05/24/2022 17:19:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=26
05/24/2022 17:19:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=29
05/24/2022 17:19:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=33
05/24/2022 17:19:14 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.24611708482676223 on epoch=33
05/24/2022 17:19:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/24/2022 17:19:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
05/24/2022 17:19:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=43
05/24/2022 17:19:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.62 on epoch=46
05/24/2022 17:19:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
05/24/2022 17:19:27 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 17:19:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
05/24/2022 17:19:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/24/2022 17:19:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/24/2022 17:19:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
05/24/2022 17:19:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
05/24/2022 17:19:40 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 17:19:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=69
05/24/2022 17:19:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
05/24/2022 17:19:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=76
05/24/2022 17:19:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=79
05/24/2022 17:19:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
05/24/2022 17:19:53 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.27602905569007263 on epoch=83
05/24/2022 17:19:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=86
05/24/2022 17:19:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/24/2022 17:20:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
05/24/2022 17:20:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=96
05/24/2022 17:20:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=99
05/24/2022 17:20:07 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.24444444444444446 on epoch=99
05/24/2022 17:20:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=103
05/24/2022 17:20:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
05/24/2022 17:20:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
05/24/2022 17:20:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=113
05/24/2022 17:20:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=116
05/24/2022 17:20:20 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.2450388265746333 on epoch=116
05/24/2022 17:20:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
05/24/2022 17:20:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=123
05/24/2022 17:20:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=126
05/24/2022 17:20:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
05/24/2022 17:20:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
05/24/2022 17:20:34 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.2884848484848485 on epoch=133
05/24/2022 17:20:34 - INFO - __main__ - Saving model with best Classification-F1: 0.28012654587287894 -> 0.2884848484848485 on epoch=133, global_step=400
05/24/2022 17:20:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=136
05/24/2022 17:20:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
05/24/2022 17:20:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
05/24/2022 17:20:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
05/24/2022 17:20:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
05/24/2022 17:20:47 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.2450388265746333 on epoch=149
05/24/2022 17:20:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=153
05/24/2022 17:20:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=156
05/24/2022 17:20:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/24/2022 17:20:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=163
05/24/2022 17:21:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
05/24/2022 17:21:01 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.30052910052910053 on epoch=166
05/24/2022 17:21:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2884848484848485 -> 0.30052910052910053 on epoch=166, global_step=500
05/24/2022 17:21:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=169
05/24/2022 17:21:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=173
05/24/2022 17:21:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.37 on epoch=176
05/24/2022 17:21:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=179
05/24/2022 17:21:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
05/24/2022 17:21:14 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.28774928774928776 on epoch=183
05/24/2022 17:21:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=186
05/24/2022 17:21:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
05/24/2022 17:21:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=193
05/24/2022 17:21:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=196
05/24/2022 17:21:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=199
05/24/2022 17:21:27 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.31339031339031337 on epoch=199
05/24/2022 17:21:27 - INFO - __main__ - Saving model with best Classification-F1: 0.30052910052910053 -> 0.31339031339031337 on epoch=199, global_step=600
05/24/2022 17:21:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=203
05/24/2022 17:21:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
05/24/2022 17:21:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=209
05/24/2022 17:21:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=213
05/24/2022 17:21:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=216
05/24/2022 17:21:41 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.24928681149879306 on epoch=216
05/24/2022 17:21:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=219
05/24/2022 17:21:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=223
05/24/2022 17:21:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=226
05/24/2022 17:21:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=229
05/24/2022 17:21:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=233
05/24/2022 17:21:54 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.2375960866526904 on epoch=233
05/24/2022 17:21:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=236
05/24/2022 17:21:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=239
05/24/2022 17:22:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=243
05/24/2022 17:22:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=246
05/24/2022 17:22:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=249
05/24/2022 17:22:07 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.3864173651407694 on epoch=249
05/24/2022 17:22:07 - INFO - __main__ - Saving model with best Classification-F1: 0.31339031339031337 -> 0.3864173651407694 on epoch=249, global_step=750
05/24/2022 17:22:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=253
05/24/2022 17:22:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=256
05/24/2022 17:22:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=259
05/24/2022 17:22:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=263
05/24/2022 17:22:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=266
05/24/2022 17:22:21 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.2622222222222222 on epoch=266
05/24/2022 17:22:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=269
05/24/2022 17:22:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=273
05/24/2022 17:22:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=276
05/24/2022 17:22:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=279
05/24/2022 17:22:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=283
05/24/2022 17:22:34 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.25 on epoch=283
05/24/2022 17:22:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=286
05/24/2022 17:22:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=289
05/24/2022 17:22:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=293
05/24/2022 17:22:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=296
05/24/2022 17:22:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=299
05/24/2022 17:22:48 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.2618181818181818 on epoch=299
05/24/2022 17:22:50 - INFO - __main__ - Step 910 Global step 910 Train loss 2.23 on epoch=303
05/24/2022 17:22:53 - INFO - __main__ - Step 920 Global step 920 Train loss 1.06 on epoch=306
05/24/2022 17:22:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=309
05/24/2022 17:22:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.82 on epoch=313
05/24/2022 17:23:00 - INFO - __main__ - Step 950 Global step 950 Train loss 2.65 on epoch=316
05/24/2022 17:23:01 - INFO - __main__ - Global step 950 Train loss 1.44 Classification-F1 0.2747252747252748 on epoch=316
05/24/2022 17:23:03 - INFO - __main__ - Step 960 Global step 960 Train loss 1.87 on epoch=319
05/24/2022 17:23:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=323
05/24/2022 17:23:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=326
05/24/2022 17:23:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=329
05/24/2022 17:23:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=333
05/24/2022 17:23:14 - INFO - __main__ - Global step 1000 Train loss 0.68 Classification-F1 0.27555555555555555 on epoch=333
05/24/2022 17:23:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=336
05/24/2022 17:23:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=339
05/24/2022 17:23:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=343
05/24/2022 17:23:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=346
05/24/2022 17:23:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=349
05/24/2022 17:23:28 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.25102084676552755 on epoch=349
05/24/2022 17:23:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=353
05/24/2022 17:23:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=356
05/24/2022 17:23:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=359
05/24/2022 17:23:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=363
05/24/2022 17:23:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=366
05/24/2022 17:23:41 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.25102084676552755 on epoch=366
05/24/2022 17:23:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=369
05/24/2022 17:23:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=373
05/24/2022 17:23:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=376
05/24/2022 17:23:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=379
05/24/2022 17:23:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=383
05/24/2022 17:23:55 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.25102084676552755 on epoch=383
05/24/2022 17:23:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=386
05/24/2022 17:24:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=389
05/24/2022 17:24:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=393
05/24/2022 17:24:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=396
05/24/2022 17:24:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=399
05/24/2022 17:24:08 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.25102084676552755 on epoch=399
05/24/2022 17:24:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=403
05/24/2022 17:24:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=406
05/24/2022 17:24:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=409
05/24/2022 17:24:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=413
05/24/2022 17:24:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=416
05/24/2022 17:24:22 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.2719522591645354 on epoch=416
05/24/2022 17:24:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=419
05/24/2022 17:24:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=423
05/24/2022 17:24:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=426
05/24/2022 17:24:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=429
05/24/2022 17:24:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=433
05/24/2022 17:24:35 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.25102084676552755 on epoch=433
05/24/2022 17:24:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=436
05/24/2022 17:24:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=439
05/24/2022 17:24:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=443
05/24/2022 17:24:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=446
05/24/2022 17:24:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=449
05/24/2022 17:24:49 - INFO - __main__ - Global step 1350 Train loss 0.29 Classification-F1 0.2523444160272805 on epoch=449
05/24/2022 17:24:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=453
05/24/2022 17:24:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=456
05/24/2022 17:24:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=459
05/24/2022 17:24:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=463
05/24/2022 17:25:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=466
05/24/2022 17:25:02 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.2523444160272805 on epoch=466
05/24/2022 17:25:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=469
05/24/2022 17:25:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=473
05/24/2022 17:25:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=476
05/24/2022 17:25:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=479
05/24/2022 17:25:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=483
05/24/2022 17:25:16 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.22916666666666666 on epoch=483
05/24/2022 17:25:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=486
05/24/2022 17:25:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=489
05/24/2022 17:25:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=493
05/24/2022 17:25:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=496
05/24/2022 17:25:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=499
05/24/2022 17:25:30 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.28225974567437984 on epoch=499
05/24/2022 17:25:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=503
05/24/2022 17:25:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=506
05/24/2022 17:25:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=509
05/24/2022 17:25:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=513
05/24/2022 17:25:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=516
05/24/2022 17:25:43 - INFO - __main__ - Global step 1550 Train loss 0.25 Classification-F1 0.26289225367566377 on epoch=516
05/24/2022 17:25:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=519
05/24/2022 17:25:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=523
05/24/2022 17:25:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=526
05/24/2022 17:25:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=529
05/24/2022 17:25:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=533
05/24/2022 17:25:56 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.2638888888888889 on epoch=533
05/24/2022 17:25:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=536
05/24/2022 17:26:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=539
05/24/2022 17:26:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=543
05/24/2022 17:26:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=546
05/24/2022 17:26:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=549
05/24/2022 17:26:10 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.273015873015873 on epoch=549
05/24/2022 17:26:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=553
05/24/2022 17:26:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=556
05/24/2022 17:26:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=559
05/24/2022 17:26:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=563
05/24/2022 17:26:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=566
05/24/2022 17:26:23 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.25925925925925924 on epoch=566
05/24/2022 17:26:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=569
05/24/2022 17:26:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=573
05/24/2022 17:26:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.27 on epoch=576
05/24/2022 17:26:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=579
05/24/2022 17:26:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=583
05/24/2022 17:26:37 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.2523444160272805 on epoch=583
05/24/2022 17:26:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=586
05/24/2022 17:26:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=589
05/24/2022 17:26:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=593
05/24/2022 17:26:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=596
05/24/2022 17:26:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=599
05/24/2022 17:26:50 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.25925925925925924 on epoch=599
05/24/2022 17:26:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=603
05/24/2022 17:26:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=606
05/24/2022 17:26:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=609
05/24/2022 17:27:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=613
05/24/2022 17:27:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=616
05/24/2022 17:27:04 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.28225974567437984 on epoch=616
05/24/2022 17:27:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=619
05/24/2022 17:27:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=623
05/24/2022 17:27:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=626
05/24/2022 17:27:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=629
05/24/2022 17:27:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=633
05/24/2022 17:27:17 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.2719522591645354 on epoch=633
05/24/2022 17:27:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=636
05/24/2022 17:27:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.30 on epoch=639
05/24/2022 17:27:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=643
05/24/2022 17:27:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=646
05/24/2022 17:27:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=649
05/24/2022 17:27:31 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.25 on epoch=649
05/24/2022 17:27:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=653
05/24/2022 17:27:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=656
05/24/2022 17:27:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=659
05/24/2022 17:27:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=663
05/24/2022 17:27:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=666
05/24/2022 17:27:44 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.273015873015873 on epoch=666
05/24/2022 17:27:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=669
05/24/2022 17:27:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=673
05/24/2022 17:27:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=676
05/24/2022 17:27:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=679
05/24/2022 17:27:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=683
05/24/2022 17:27:57 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.2523444160272805 on epoch=683
05/24/2022 17:28:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=686
05/24/2022 17:28:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=689
05/24/2022 17:28:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=693
05/24/2022 17:28:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=696
05/24/2022 17:28:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=699
05/24/2022 17:28:11 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.25925925925925924 on epoch=699
05/24/2022 17:28:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=703
05/24/2022 17:28:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=706
05/24/2022 17:28:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=709
05/24/2022 17:28:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=713
05/24/2022 17:28:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=716
05/24/2022 17:28:24 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.26399331662489556 on epoch=716
05/24/2022 17:28:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=719
05/24/2022 17:28:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.24 on epoch=723
05/24/2022 17:28:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=726
05/24/2022 17:28:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=729
05/24/2022 17:28:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=733
05/24/2022 17:28:38 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.2534188034188034 on epoch=733
05/24/2022 17:28:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.29 on epoch=736
05/24/2022 17:28:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=739
05/24/2022 17:28:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=743
05/24/2022 17:28:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=746
05/24/2022 17:28:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=749
05/24/2022 17:28:52 - INFO - __main__ - Global step 2250 Train loss 0.25 Classification-F1 0.27865074376702276 on epoch=749
05/24/2022 17:28:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=753
05/24/2022 17:28:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=756
05/24/2022 17:28:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=759
05/24/2022 17:29:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=763
05/24/2022 17:29:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.22 on epoch=766
05/24/2022 17:29:05 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.26683826683826684 on epoch=766
05/24/2022 17:29:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=769
05/24/2022 17:29:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=773
05/24/2022 17:29:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=776
05/24/2022 17:29:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=779
05/24/2022 17:29:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=783
05/24/2022 17:29:19 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.27865074376702276 on epoch=783
05/24/2022 17:29:22 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=786
05/24/2022 17:29:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=789
05/24/2022 17:29:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=793
05/24/2022 17:29:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=796
05/24/2022 17:29:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=799
05/24/2022 17:29:33 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.25132275132275134 on epoch=799
05/24/2022 17:29:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=803
05/24/2022 17:29:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=806
05/24/2022 17:29:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=809
05/24/2022 17:29:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=813
05/24/2022 17:29:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=816
05/24/2022 17:29:47 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.2759259259259259 on epoch=816
05/24/2022 17:29:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=819
05/24/2022 17:29:52 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=823
05/24/2022 17:29:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=826
05/24/2022 17:29:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=829
05/24/2022 17:30:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=833
05/24/2022 17:30:01 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.245836265540699 on epoch=833
05/24/2022 17:30:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=836
05/24/2022 17:30:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=839
05/24/2022 17:30:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=843
05/24/2022 17:30:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=846
05/24/2022 17:30:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=849
05/24/2022 17:30:15 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.2591093117408907 on epoch=849
05/24/2022 17:30:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=853
05/24/2022 17:30:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=856
05/24/2022 17:30:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=859
05/24/2022 17:30:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=863
05/24/2022 17:30:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=866
05/24/2022 17:30:29 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.15588235294117647 on epoch=866
05/24/2022 17:30:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=869
05/24/2022 17:30:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.20 on epoch=873
05/24/2022 17:30:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=876
05/24/2022 17:30:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=879
05/24/2022 17:30:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.27 on epoch=883
05/24/2022 17:30:43 - INFO - __main__ - Global step 2650 Train loss 0.22 Classification-F1 0.19373219373219375 on epoch=883
05/24/2022 17:30:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=886
05/24/2022 17:30:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=889
05/24/2022 17:30:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=893
05/24/2022 17:30:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=896
05/24/2022 17:30:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=899
05/24/2022 17:30:57 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.2210526315789474 on epoch=899
05/24/2022 17:30:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=903
05/24/2022 17:31:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=906
05/24/2022 17:31:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=909
05/24/2022 17:31:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=913
05/24/2022 17:31:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=916
05/24/2022 17:31:11 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.23492063492063495 on epoch=916
05/24/2022 17:31:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=919
05/24/2022 17:31:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=923
05/24/2022 17:31:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=926
05/24/2022 17:31:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=929
05/24/2022 17:31:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=933
05/24/2022 17:31:25 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.21052631578947367 on epoch=933
05/24/2022 17:31:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=936
05/24/2022 17:31:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=939
05/24/2022 17:31:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=943
05/24/2022 17:31:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=946
05/24/2022 17:31:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=949
05/24/2022 17:31:39 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.19670542635658914 on epoch=949
05/24/2022 17:31:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.18 on epoch=953
05/24/2022 17:31:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=956
05/24/2022 17:31:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=959
05/24/2022 17:31:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=963
05/24/2022 17:31:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=966
05/24/2022 17:31:53 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.19899749373433584 on epoch=966
05/24/2022 17:31:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=969
05/24/2022 17:31:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=973
05/24/2022 17:32:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=976
05/24/2022 17:32:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=979
05/24/2022 17:32:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=983
05/24/2022 17:32:07 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.16929698708751792 on epoch=983
05/24/2022 17:32:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=986
05/24/2022 17:32:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=989
05/24/2022 17:32:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=993
05/24/2022 17:32:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=996
05/24/2022 17:32:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=999
05/24/2022 17:32:20 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.1994832041343669 on epoch=999
05/24/2022 17:32:20 - INFO - __main__ - save last model!
05/24/2022 17:32:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 17:32:20 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 17:32:20 - INFO - __main__ - Printing 3 examples
05/24/2022 17:32:20 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 17:32:20 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:20 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 17:32:20 - INFO - __main__ - ['entailment']
05/24/2022 17:32:20 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 17:32:20 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:20 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:32:21 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:32:21 - INFO - __main__ - Printing 3 examples
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:32:21 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:32:21 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:32:21 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:32:21 - INFO - __main__ - Printing 3 examples
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:32:21 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:21 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:32:21 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:32:21 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:32:21 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:32:22 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 17:32:39 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:32:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:32:40 - INFO - __main__ - Starting training!
05/24/2022 17:32:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_13_0.5_8_predictions.txt
05/24/2022 17:32:50 - INFO - __main__ - Classification-F1 on test data: 0.1538
05/24/2022 17:32:51 - INFO - __main__ - prefix=anli_16_13, lr=0.5, bsz=8, dev_performance=0.3864173651407694, test_performance=0.1538239615096115
05/24/2022 17:32:51 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.4, bsz=8 ...
05/24/2022 17:32:52 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:32:52 - INFO - __main__ - Printing 3 examples
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:32:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:32:52 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:32:52 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:32:52 - INFO - __main__ - Printing 3 examples
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:32:52 - INFO - __main__ - ['contradiction']
05/24/2022 17:32:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:32:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:32:52 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:33:09 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:33:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:33:10 - INFO - __main__ - Starting training!
05/24/2022 17:33:13 - INFO - __main__ - Step 10 Global step 10 Train loss 0.92 on epoch=3
05/24/2022 17:33:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=6
05/24/2022 17:33:18 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=9
05/24/2022 17:33:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=13
05/24/2022 17:33:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
05/24/2022 17:33:24 - INFO - __main__ - Global step 50 Train loss 0.66 Classification-F1 0.24611708482676223 on epoch=16
05/24/2022 17:33:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24611708482676223 on epoch=16, global_step=50
05/24/2022 17:33:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
05/24/2022 17:33:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=23
05/24/2022 17:33:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
05/24/2022 17:33:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/24/2022 17:33:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
05/24/2022 17:33:38 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.43274853801169594 on epoch=33
05/24/2022 17:33:38 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.43274853801169594 on epoch=33, global_step=100
05/24/2022 17:33:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/24/2022 17:33:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=39
05/24/2022 17:33:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=43
05/24/2022 17:33:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
05/24/2022 17:33:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
05/24/2022 17:33:51 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.29794906468334464 on epoch=49
05/24/2022 17:33:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/24/2022 17:33:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
05/24/2022 17:33:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
05/24/2022 17:34:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=63
05/24/2022 17:34:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=66
05/24/2022 17:34:05 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.32 on epoch=66
05/24/2022 17:34:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/24/2022 17:34:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/24/2022 17:34:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=76
05/24/2022 17:34:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
05/24/2022 17:34:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
05/24/2022 17:34:18 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.3342528735632184 on epoch=83
05/24/2022 17:34:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=86
05/24/2022 17:34:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=89
05/24/2022 17:34:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=93
05/24/2022 17:34:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
05/24/2022 17:34:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=99
05/24/2022 17:34:31 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.32093757043047105 on epoch=99
05/24/2022 17:34:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=103
05/24/2022 17:34:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
05/24/2022 17:34:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=109
05/24/2022 17:34:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=113
05/24/2022 17:34:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
05/24/2022 17:34:45 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.2626262626262626 on epoch=116
05/24/2022 17:34:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=119
05/24/2022 17:34:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
05/24/2022 17:34:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=126
05/24/2022 17:34:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
05/24/2022 17:34:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=133
05/24/2022 17:34:58 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.22666666666666666 on epoch=133
05/24/2022 17:35:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
05/24/2022 17:35:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
05/24/2022 17:35:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=143
05/24/2022 17:35:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/24/2022 17:35:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=149
05/24/2022 17:35:12 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.22222222222222224 on epoch=149
05/24/2022 17:35:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=153
05/24/2022 17:35:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=156
05/24/2022 17:35:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=159
05/24/2022 17:35:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=163
05/24/2022 17:35:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=166
05/24/2022 17:35:25 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.23757575757575758 on epoch=166
05/24/2022 17:35:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.28 on epoch=169
05/24/2022 17:35:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=173
05/24/2022 17:35:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=176
05/24/2022 17:35:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=179
05/24/2022 17:35:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=183
05/24/2022 17:35:39 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.3259080268821264 on epoch=183
05/24/2022 17:35:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=186
05/24/2022 17:35:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=189
05/24/2022 17:35:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=193
05/24/2022 17:35:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=196
05/24/2022 17:35:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
05/24/2022 17:35:52 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.24888888888888885 on epoch=199
05/24/2022 17:35:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=203
05/24/2022 17:35:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=206
05/24/2022 17:36:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
05/24/2022 17:36:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
05/24/2022 17:36:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=216
05/24/2022 17:36:06 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.17694444444444446 on epoch=216
05/24/2022 17:36:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=219
05/24/2022 17:36:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=223
05/24/2022 17:36:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=226
05/24/2022 17:36:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
05/24/2022 17:36:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=233
05/24/2022 17:36:20 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.1185897435897436 on epoch=233
05/24/2022 17:36:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=236
05/24/2022 17:36:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
05/24/2022 17:36:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=243
05/24/2022 17:36:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.14 on epoch=246
05/24/2022 17:36:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=249
05/24/2022 17:36:34 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.15668527379053696 on epoch=249
05/24/2022 17:36:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
05/24/2022 17:36:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
05/24/2022 17:36:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=259
05/24/2022 17:36:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=263
05/24/2022 17:36:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=266
05/24/2022 17:36:47 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.1390424987199181 on epoch=266
05/24/2022 17:36:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=269
05/24/2022 17:36:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.08 on epoch=273
05/24/2022 17:36:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.08 on epoch=276
05/24/2022 17:36:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=279
05/24/2022 17:37:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=283
05/24/2022 17:37:01 - INFO - __main__ - Global step 850 Train loss 0.11 Classification-F1 0.12875885289678393 on epoch=283
05/24/2022 17:37:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=286
05/24/2022 17:37:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=289
05/24/2022 17:37:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
05/24/2022 17:37:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
05/24/2022 17:37:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/24/2022 17:37:15 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.11400703211048038 on epoch=299
05/24/2022 17:37:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=303
05/24/2022 17:37:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
05/24/2022 17:37:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=309
05/24/2022 17:37:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=313
05/24/2022 17:37:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=316
05/24/2022 17:37:29 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.1518181818181818 on epoch=316
05/24/2022 17:37:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
05/24/2022 17:37:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=323
05/24/2022 17:37:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
05/24/2022 17:37:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=329
05/24/2022 17:37:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=333
05/24/2022 17:37:42 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.15370370370370368 on epoch=333
05/24/2022 17:37:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=336
05/24/2022 17:37:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=339
05/24/2022 17:37:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=343
05/24/2022 17:37:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=346
05/24/2022 17:37:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
05/24/2022 17:37:56 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.1766081871345029 on epoch=349
05/24/2022 17:37:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=353
05/24/2022 17:38:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
05/24/2022 17:38:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=359
05/24/2022 17:38:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=363
05/24/2022 17:38:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
05/24/2022 17:38:10 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.11010284331518451 on epoch=366
05/24/2022 17:38:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
05/24/2022 17:38:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
05/24/2022 17:38:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
05/24/2022 17:38:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=379
05/24/2022 17:38:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=383
05/24/2022 17:38:24 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.12979037943974828 on epoch=383
05/24/2022 17:38:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=386
05/24/2022 17:38:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=389
05/24/2022 17:38:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
05/24/2022 17:38:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
05/24/2022 17:38:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=399
05/24/2022 17:38:38 - INFO - __main__ - Global step 1200 Train loss 0.04 Classification-F1 0.09421950598421187 on epoch=399
05/24/2022 17:38:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=403
05/24/2022 17:38:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=406
05/24/2022 17:38:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=409
05/24/2022 17:38:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=413
05/24/2022 17:38:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
05/24/2022 17:38:52 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.10555555555555556 on epoch=416
05/24/2022 17:38:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
05/24/2022 17:38:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
05/24/2022 17:38:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
05/24/2022 17:39:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=429
05/24/2022 17:39:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
05/24/2022 17:39:06 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.20812908496732024 on epoch=433
05/24/2022 17:39:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
05/24/2022 17:39:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
05/24/2022 17:39:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
05/24/2022 17:39:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
05/24/2022 17:39:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=449
05/24/2022 17:39:20 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.14377777777777778 on epoch=449
05/24/2022 17:39:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/24/2022 17:39:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/24/2022 17:39:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=459
05/24/2022 17:39:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 17:39:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
05/24/2022 17:39:33 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.19130434782608696 on epoch=466
05/24/2022 17:39:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=469
05/24/2022 17:39:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=473
05/24/2022 17:39:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
05/24/2022 17:39:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=479
05/24/2022 17:39:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=483
05/24/2022 17:39:47 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.11210317460317458 on epoch=483
05/24/2022 17:39:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
05/24/2022 17:39:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=489
05/24/2022 17:39:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/24/2022 17:39:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
05/24/2022 17:40:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=499
05/24/2022 17:40:01 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.17896551724137932 on epoch=499
05/24/2022 17:40:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
05/24/2022 17:40:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/24/2022 17:40:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/24/2022 17:40:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
05/24/2022 17:40:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/24/2022 17:40:15 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.15289968652037617 on epoch=516
05/24/2022 17:40:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/24/2022 17:40:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=523
05/24/2022 17:40:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=526
05/24/2022 17:40:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/24/2022 17:40:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/24/2022 17:40:29 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.12733143399810065 on epoch=533
05/24/2022 17:40:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/24/2022 17:40:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/24/2022 17:40:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=543
05/24/2022 17:40:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/24/2022 17:40:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=549
05/24/2022 17:40:42 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.12889983579638753 on epoch=549
05/24/2022 17:40:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
05/24/2022 17:40:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/24/2022 17:40:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/24/2022 17:40:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/24/2022 17:40:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
05/24/2022 17:40:56 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.16092436974789917 on epoch=566
05/24/2022 17:40:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/24/2022 17:41:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
05/24/2022 17:41:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/24/2022 17:41:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/24/2022 17:41:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
05/24/2022 17:41:10 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.27433780529755764 on epoch=583
05/24/2022 17:41:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/24/2022 17:41:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=589
05/24/2022 17:41:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/24/2022 17:41:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/24/2022 17:41:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/24/2022 17:41:24 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.19052419354838712 on epoch=599
05/24/2022 17:41:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=603
05/24/2022 17:41:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/24/2022 17:41:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=609
05/24/2022 17:41:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/24/2022 17:41:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=616
05/24/2022 17:41:38 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.24339080459770113 on epoch=616
05/24/2022 17:41:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/24/2022 17:41:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/24/2022 17:41:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/24/2022 17:41:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/24/2022 17:41:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/24/2022 17:41:52 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.33247863247863246 on epoch=633
05/24/2022 17:41:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/24/2022 17:41:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/24/2022 17:41:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
05/24/2022 17:42:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/24/2022 17:42:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/24/2022 17:42:05 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.31205629592726364 on epoch=649
05/24/2022 17:42:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/24/2022 17:42:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 17:42:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/24/2022 17:42:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=663
05/24/2022 17:42:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=666
05/24/2022 17:42:20 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.314703425229741 on epoch=666
05/24/2022 17:42:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
05/24/2022 17:42:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
05/24/2022 17:42:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/24/2022 17:42:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 17:42:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/24/2022 17:42:34 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.30044645664340064 on epoch=683
05/24/2022 17:42:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/24/2022 17:42:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/24/2022 17:42:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 17:42:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/24/2022 17:42:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/24/2022 17:42:47 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.2888888888888889 on epoch=699
05/24/2022 17:42:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/24/2022 17:42:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=706
05/24/2022 17:42:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 17:42:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=713
05/24/2022 17:43:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 17:43:01 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.2908256303842042 on epoch=716
05/24/2022 17:43:04 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/24/2022 17:43:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/24/2022 17:43:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/24/2022 17:43:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/24/2022 17:43:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/24/2022 17:43:15 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.2546365914786967 on epoch=733
05/24/2022 17:43:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
05/24/2022 17:43:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=739
05/24/2022 17:43:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/24/2022 17:43:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 17:43:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/24/2022 17:43:29 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.28008884460497363 on epoch=749
05/24/2022 17:43:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/24/2022 17:43:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/24/2022 17:43:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/24/2022 17:43:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/24/2022 17:43:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/24/2022 17:43:43 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2684302704463995 on epoch=766
05/24/2022 17:43:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 17:43:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 17:43:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=776
05/24/2022 17:43:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.34 on epoch=779
05/24/2022 17:43:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.23 on epoch=783
05/24/2022 17:43:57 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.2888888888888889 on epoch=783
05/24/2022 17:43:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/24/2022 17:44:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=789
05/24/2022 17:44:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=793
05/24/2022 17:44:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 17:44:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
05/24/2022 17:44:11 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.17420663482300597 on epoch=799
05/24/2022 17:44:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=803
05/24/2022 17:44:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=806
05/24/2022 17:44:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=809
05/24/2022 17:44:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=813
05/24/2022 17:44:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
05/24/2022 17:44:24 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.29648493994843744 on epoch=816
05/24/2022 17:44:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/24/2022 17:44:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/24/2022 17:44:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
05/24/2022 17:44:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 17:44:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/24/2022 17:44:38 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.18778409090909087 on epoch=833
05/24/2022 17:44:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=836
05/24/2022 17:44:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
05/24/2022 17:44:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/24/2022 17:44:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 17:44:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 17:44:52 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.31001700575737084 on epoch=849
05/24/2022 17:44:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/24/2022 17:44:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/24/2022 17:45:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 17:45:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 17:45:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 17:45:06 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2877873563218391 on epoch=866
05/24/2022 17:45:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 17:45:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 17:45:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 17:45:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/24/2022 17:45:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
05/24/2022 17:45:20 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.30588624338624343 on epoch=883
05/24/2022 17:45:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 17:45:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/24/2022 17:45:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=893
05/24/2022 17:45:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 17:45:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=899
05/24/2022 17:45:34 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.29264069264069265 on epoch=899
05/24/2022 17:45:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 17:45:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/24/2022 17:45:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/24/2022 17:45:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 17:45:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 17:45:47 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2349758085052203 on epoch=916
05/24/2022 17:45:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 17:45:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 17:45:55 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
05/24/2022 17:45:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/24/2022 17:46:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=933
05/24/2022 17:46:01 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.272875816993464 on epoch=933
05/24/2022 17:46:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/24/2022 17:46:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
05/24/2022 17:46:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
05/24/2022 17:46:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 17:46:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/24/2022 17:46:15 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.19764309764309762 on epoch=949
05/24/2022 17:46:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 17:46:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
05/24/2022 17:46:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 17:46:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 17:46:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 17:46:29 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.31512605042016806 on epoch=966
05/24/2022 17:46:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/24/2022 17:46:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 17:46:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 17:46:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 17:46:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 17:46:43 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.36614757137560244 on epoch=983
05/24/2022 17:46:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 17:46:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 17:46:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 17:46:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 17:46:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/24/2022 17:46:57 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.35956558061821214 on epoch=999
05/24/2022 17:46:57 - INFO - __main__ - save last model!
05/24/2022 17:46:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:46:57 - INFO - __main__ - Printing 3 examples
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:46:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 17:46:57 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 17:46:57 - INFO - __main__ - Printing 3 examples
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 17:46:57 - INFO - __main__ - ['entailment']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:46:57 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:46:57 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:46:57 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:46:57 - INFO - __main__ - Printing 3 examples
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:46:57 - INFO - __main__ - ['contradiction']
05/24/2022 17:46:57 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:46:57 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:46:57 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:46:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:46:59 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 17:47:12 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:47:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:47:13 - INFO - __main__ - Starting training!
05/24/2022 17:47:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_13_0.4_8_predictions.txt
05/24/2022 17:47:27 - INFO - __main__ - Classification-F1 on test data: 0.0778
05/24/2022 17:47:27 - INFO - __main__ - prefix=anli_16_13, lr=0.4, bsz=8, dev_performance=0.43274853801169594, test_performance=0.0777594311840209
05/24/2022 17:47:27 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.3, bsz=8 ...
05/24/2022 17:47:28 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:47:28 - INFO - __main__ - Printing 3 examples
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:47:28 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:47:28 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 17:47:28 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 17:47:28 - INFO - __main__ - Printing 3 examples
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 17:47:28 - INFO - __main__ - ['contradiction']
05/24/2022 17:47:28 - INFO - __main__ - Tokenizing Input ...
05/24/2022 17:47:28 - INFO - __main__ - Tokenizing Output ...
05/24/2022 17:47:28 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 17:47:45 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 17:47:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 17:47:46 - INFO - __main__ - Starting training!
05/24/2022 17:47:49 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=3
05/24/2022 17:47:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=6
05/24/2022 17:47:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=9
05/24/2022 17:47:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
05/24/2022 17:47:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.56 on epoch=16
05/24/2022 17:48:00 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.24611708482676223 on epoch=16
05/24/2022 17:48:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24611708482676223 on epoch=16, global_step=50
05/24/2022 17:48:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
05/24/2022 17:48:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
05/24/2022 17:48:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=26
05/24/2022 17:48:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=29
05/24/2022 17:48:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=33
05/24/2022 17:48:14 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.29777777777777775 on epoch=33
05/24/2022 17:48:14 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.29777777777777775 on epoch=33, global_step=100
05/24/2022 17:48:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/24/2022 17:48:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
05/24/2022 17:48:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=43
05/24/2022 17:48:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=46
05/24/2022 17:48:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/24/2022 17:48:27 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3320620391558607 on epoch=49
05/24/2022 17:48:27 - INFO - __main__ - Saving model with best Classification-F1: 0.29777777777777775 -> 0.3320620391558607 on epoch=49, global_step=150
05/24/2022 17:48:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
05/24/2022 17:48:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/24/2022 17:48:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 17:48:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/24/2022 17:48:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
05/24/2022 17:48:41 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.24611708482676223 on epoch=66
05/24/2022 17:48:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
05/24/2022 17:48:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=73
05/24/2022 17:48:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
05/24/2022 17:48:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.53 on epoch=79
05/24/2022 17:48:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=83
05/24/2022 17:48:55 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.30952380952380953 on epoch=83
05/24/2022 17:48:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
05/24/2022 17:49:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
05/24/2022 17:49:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
05/24/2022 17:49:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
05/24/2022 17:49:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
05/24/2022 17:49:08 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.2450388265746333 on epoch=99
05/24/2022 17:49:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
05/24/2022 17:49:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=106
05/24/2022 17:49:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
05/24/2022 17:49:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
05/24/2022 17:49:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=116
05/24/2022 17:49:22 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.2450388265746333 on epoch=116
05/24/2022 17:49:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
05/24/2022 17:49:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=123
05/24/2022 17:49:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
05/24/2022 17:49:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
05/24/2022 17:49:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
05/24/2022 17:49:35 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.2450388265746333 on epoch=133
05/24/2022 17:49:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=136
05/24/2022 17:49:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
05/24/2022 17:49:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=143
05/24/2022 17:49:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=146
05/24/2022 17:49:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=149
05/24/2022 17:49:48 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2450388265746333 on epoch=149
05/24/2022 17:49:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=153
05/24/2022 17:49:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=156
05/24/2022 17:49:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=159
05/24/2022 17:49:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
05/24/2022 17:50:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=166
05/24/2022 17:50:02 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.24444444444444446 on epoch=166
05/24/2022 17:50:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=169
05/24/2022 17:50:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=173
05/24/2022 17:50:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
05/24/2022 17:50:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=179
05/24/2022 17:50:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
05/24/2022 17:50:15 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.2450388265746333 on epoch=183
05/24/2022 17:50:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=186
05/24/2022 17:50:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
05/24/2022 17:50:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=193
05/24/2022 17:50:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=196
05/24/2022 17:50:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=199
05/24/2022 17:50:29 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3151515151515151 on epoch=199
05/24/2022 17:50:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=203
05/24/2022 17:50:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=206
05/24/2022 17:50:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=209
05/24/2022 17:50:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=213
05/24/2022 17:50:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=216
05/24/2022 17:50:43 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.2884848484848485 on epoch=216
05/24/2022 17:50:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=219
05/24/2022 17:50:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=223
05/24/2022 17:50:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=226
05/24/2022 17:50:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
05/24/2022 17:50:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=233
05/24/2022 17:50:57 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.30912698412698414 on epoch=233
05/24/2022 17:50:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=236
05/24/2022 17:51:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=239
05/24/2022 17:51:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=243
05/24/2022 17:51:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=246
05/24/2022 17:51:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=249
05/24/2022 17:51:10 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.30166880616174585 on epoch=249
05/24/2022 17:51:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=253
05/24/2022 17:51:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=256
05/24/2022 17:51:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=259
05/24/2022 17:51:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=263
05/24/2022 17:51:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=266
05/24/2022 17:51:24 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.2530237058538945 on epoch=266
05/24/2022 17:51:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=269
05/24/2022 17:51:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=273
05/24/2022 17:51:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=276
05/24/2022 17:51:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.29 on epoch=279
05/24/2022 17:51:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=283
05/24/2022 17:51:38 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.2790869112077074 on epoch=283
05/24/2022 17:51:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=286
05/24/2022 17:51:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=289
05/24/2022 17:51:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=293
05/24/2022 17:51:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=296
05/24/2022 17:51:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=299
05/24/2022 17:51:52 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.23359825463612283 on epoch=299
05/24/2022 17:51:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
05/24/2022 17:51:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=306
05/24/2022 17:52:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=309
05/24/2022 17:52:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=313
05/24/2022 17:52:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=316
05/24/2022 17:52:06 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.27812499999999996 on epoch=316
05/24/2022 17:52:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=319
05/24/2022 17:52:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=323
05/24/2022 17:52:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=326
05/24/2022 17:52:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=329
05/24/2022 17:52:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=333
05/24/2022 17:52:20 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.21758625930992553 on epoch=333
05/24/2022 17:52:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=336
05/24/2022 17:52:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=339
05/24/2022 17:52:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=343
05/24/2022 17:52:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=346
05/24/2022 17:52:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=349
05/24/2022 17:52:34 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.24210526315789474 on epoch=349
05/24/2022 17:52:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=353
05/24/2022 17:52:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
05/24/2022 17:52:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=359
05/24/2022 17:52:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
05/24/2022 17:52:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=366
05/24/2022 17:52:48 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.2287331701346389 on epoch=366
05/24/2022 17:52:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=369
05/24/2022 17:52:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=373
05/24/2022 17:52:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=376
05/24/2022 17:52:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=379
05/24/2022 17:53:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=383
05/24/2022 17:53:02 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.3121794871794871 on epoch=383
05/24/2022 17:53:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
05/24/2022 17:53:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=389
05/24/2022 17:53:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=393
05/24/2022 17:53:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=396
05/24/2022 17:53:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=399
05/24/2022 17:53:16 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.3649122807017544 on epoch=399
05/24/2022 17:53:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3320620391558607 -> 0.3649122807017544 on epoch=399, global_step=1200
05/24/2022 17:53:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=403
05/24/2022 17:53:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=406
05/24/2022 17:53:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=409
05/24/2022 17:53:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=413
05/24/2022 17:53:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=416
05/24/2022 17:53:30 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.13917748917748915 on epoch=416
05/24/2022 17:53:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=419
05/24/2022 17:53:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
05/24/2022 17:53:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=426
05/24/2022 17:53:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=429
05/24/2022 17:53:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=433
05/24/2022 17:53:44 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.20824586574039117 on epoch=433
05/24/2022 17:53:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/24/2022 17:53:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=439
05/24/2022 17:53:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=443
05/24/2022 17:53:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=446
05/24/2022 17:53:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=449
05/24/2022 17:53:59 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.18304761904761904 on epoch=449
05/24/2022 17:54:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=453
05/24/2022 17:54:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=456
05/24/2022 17:54:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=459
05/24/2022 17:54:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=463
05/24/2022 17:54:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=466
05/24/2022 17:54:13 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.18726190476190477 on epoch=466
05/24/2022 17:54:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=469
05/24/2022 17:54:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
05/24/2022 17:54:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
05/24/2022 17:54:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
05/24/2022 17:54:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
05/24/2022 17:54:27 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.1470494417862839 on epoch=483
05/24/2022 17:54:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=486
05/24/2022 17:54:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/24/2022 17:54:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=493
05/24/2022 17:54:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
05/24/2022 17:54:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=499
05/24/2022 17:54:41 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.15270493395493395 on epoch=499
05/24/2022 17:54:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
05/24/2022 17:54:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
05/24/2022 17:54:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
05/24/2022 17:54:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
05/24/2022 17:54:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=516
05/24/2022 17:54:55 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.17732562290283432 on epoch=516
05/24/2022 17:54:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=519
05/24/2022 17:55:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=523
05/24/2022 17:55:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=526
05/24/2022 17:55:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
05/24/2022 17:55:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
05/24/2022 17:55:09 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.15782312925170067 on epoch=533
05/24/2022 17:55:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/24/2022 17:55:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=539
05/24/2022 17:55:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=543
05/24/2022 17:55:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
05/24/2022 17:55:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
05/24/2022 17:55:24 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.2799506694855532 on epoch=549
05/24/2022 17:55:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=553
05/24/2022 17:55:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/24/2022 17:55:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=559
05/24/2022 17:55:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=563
05/24/2022 17:55:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=566
05/24/2022 17:55:38 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.1978494623655914 on epoch=566
05/24/2022 17:55:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/24/2022 17:55:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=573
05/24/2022 17:55:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=576
05/24/2022 17:55:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/24/2022 17:55:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
05/24/2022 17:55:52 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.16299436106974197 on epoch=583
05/24/2022 17:55:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 17:55:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=589
05/24/2022 17:56:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
05/24/2022 17:56:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/24/2022 17:56:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
05/24/2022 17:56:06 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.20819683856662613 on epoch=599
05/24/2022 17:56:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=603
05/24/2022 17:56:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
05/24/2022 17:56:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
05/24/2022 17:56:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=613
05/24/2022 17:56:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=616
05/24/2022 17:56:21 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.17065217391304346 on epoch=616
05/24/2022 17:56:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/24/2022 17:56:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=623
05/24/2022 17:56:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/24/2022 17:56:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/24/2022 17:56:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/24/2022 17:56:35 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.13876213876213875 on epoch=633
05/24/2022 17:56:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/24/2022 17:56:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/24/2022 17:56:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
05/24/2022 17:56:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/24/2022 17:56:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/24/2022 17:56:50 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.1721014492753623 on epoch=649
05/24/2022 17:56:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/24/2022 17:56:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=656
05/24/2022 17:56:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/24/2022 17:57:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
05/24/2022 17:57:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
05/24/2022 17:57:04 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.23118279569892475 on epoch=666
05/24/2022 17:57:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
05/24/2022 17:57:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=673
05/24/2022 17:57:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 17:57:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=679
05/24/2022 17:57:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/24/2022 17:57:19 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.12524196670538135 on epoch=683
05/24/2022 17:57:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/24/2022 17:57:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/24/2022 17:57:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 17:57:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/24/2022 17:57:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/24/2022 17:57:33 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.1168927548237893 on epoch=699
05/24/2022 17:57:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/24/2022 17:57:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/24/2022 17:57:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/24/2022 17:57:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/24/2022 17:57:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 17:57:48 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.3767884828349945 on epoch=716
05/24/2022 17:57:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3649122807017544 -> 0.3767884828349945 on epoch=716, global_step=2150
05/24/2022 17:57:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
05/24/2022 17:57:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=723
05/24/2022 17:57:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
05/24/2022 17:57:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/24/2022 17:58:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/24/2022 17:58:02 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.335092235092235 on epoch=733
05/24/2022 17:58:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/24/2022 17:58:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=739
05/24/2022 17:58:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/24/2022 17:58:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/24/2022 17:58:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
05/24/2022 17:58:16 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.12063932272660927 on epoch=749
05/24/2022 17:58:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/24/2022 17:58:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
05/24/2022 17:58:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/24/2022 17:58:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
05/24/2022 17:58:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
05/24/2022 17:58:31 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.16792929292929293 on epoch=766
05/24/2022 17:58:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/24/2022 17:58:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/24/2022 17:58:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
05/24/2022 17:58:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/24/2022 17:58:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/24/2022 17:58:45 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.23752775238343998 on epoch=783
05/24/2022 17:58:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/24/2022 17:58:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 17:58:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 17:58:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 17:58:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
05/24/2022 17:59:00 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.2515235824059353 on epoch=799
05/24/2022 17:59:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/24/2022 17:59:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 17:59:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 17:59:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/24/2022 17:59:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
05/24/2022 17:59:14 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.23252562907735322 on epoch=816
05/24/2022 17:59:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=819
05/24/2022 17:59:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=823
05/24/2022 17:59:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/24/2022 17:59:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 17:59:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
05/24/2022 17:59:29 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.18682012432012432 on epoch=833
05/24/2022 17:59:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=836
05/24/2022 17:59:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/24/2022 17:59:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 17:59:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 17:59:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 17:59:43 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.16821983273596178 on epoch=849
05/24/2022 17:59:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/24/2022 17:59:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=856
05/24/2022 17:59:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 17:59:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 17:59:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=866
05/24/2022 17:59:58 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.26638203668564203 on epoch=866
05/24/2022 18:00:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
05/24/2022 18:00:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
05/24/2022 18:00:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/24/2022 18:00:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 18:00:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
05/24/2022 18:00:13 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.3092741935483871 on epoch=883
05/24/2022 18:00:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 18:00:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/24/2022 18:00:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
05/24/2022 18:00:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/24/2022 18:00:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
05/24/2022 18:00:27 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.38664421997755327 on epoch=899
05/24/2022 18:00:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3767884828349945 -> 0.38664421997755327 on epoch=899, global_step=2700
05/24/2022 18:00:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
05/24/2022 18:00:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 18:00:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/24/2022 18:00:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 18:00:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 18:00:42 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.33092948717948717 on epoch=916
05/24/2022 18:00:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 18:00:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=923
05/24/2022 18:00:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/24/2022 18:00:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 18:00:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 18:00:56 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.4041093006610248 on epoch=933
05/24/2022 18:00:56 - INFO - __main__ - Saving model with best Classification-F1: 0.38664421997755327 -> 0.4041093006610248 on epoch=933, global_step=2800
05/24/2022 18:00:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 18:01:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
05/24/2022 18:01:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 18:01:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=946
05/24/2022 18:01:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 18:01:10 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.36029736029736026 on epoch=949
05/24/2022 18:01:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
05/24/2022 18:01:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
05/24/2022 18:01:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 18:01:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/24/2022 18:01:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
05/24/2022 18:01:25 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.3446623093681917 on epoch=966
05/24/2022 18:01:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=969
05/24/2022 18:01:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
05/24/2022 18:01:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 18:01:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 18:01:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 18:01:39 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.23316692268305172 on epoch=983
05/24/2022 18:01:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 18:01:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 18:01:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/24/2022 18:01:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 18:01:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/24/2022 18:01:54 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.25219941348973607 on epoch=999
05/24/2022 18:01:54 - INFO - __main__ - save last model!
05/24/2022 18:01:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:01:54 - INFO - __main__ - Printing 3 examples
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:01:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 18:01:54 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 18:01:54 - INFO - __main__ - Printing 3 examples
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 18:01:54 - INFO - __main__ - ['entailment']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:01:54 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:01:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:01:54 - INFO - __main__ - Printing 3 examples
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 18:01:54 - INFO - __main__ - ['contradiction']
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:01:54 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:01:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:01:55 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 18:02:12 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:02:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:02:13 - INFO - __main__ - Starting training!
05/24/2022 18:02:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_13_0.3_8_predictions.txt
05/24/2022 18:02:25 - INFO - __main__ - Classification-F1 on test data: 0.0534
05/24/2022 18:02:25 - INFO - __main__ - prefix=anli_16_13, lr=0.3, bsz=8, dev_performance=0.4041093006610248, test_performance=0.053385675045799066
05/24/2022 18:02:25 - INFO - __main__ - Running ... prefix=anli_16_13, lr=0.2, bsz=8 ...
05/24/2022 18:02:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:02:26 - INFO - __main__ - Printing 3 examples
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:02:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:02:26 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:02:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:02:26 - INFO - __main__ - Printing 3 examples
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: Jake Roberts is an English film editor. He is best known for his works on films "Citadel" (2012), "Starred Up" (2013), "The Riot Club" (2014) and "Brooklyn" (2015). For "Hell or High Water" (2016), Roberts was nominated (among several honors) for an Independent Spirit Award and the Academy Award for Best Film Editing at the 89th Academy Awards. [SEP] hypothesis: Jake Roberts has never won any type of accolade for his works.
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: Wye Bridge Ward was one of four wards in the town of Monmouth, Monmouthshire, Wales. Streets in the ward included St Mary's Street, Almshouse Street, St James Street, St James Square, Whitecross Street and Monk Street. The ward existed as a division of the town by the early seventeenth century, and continued into the twentieth century. [SEP] hypothesis: These wards exist in the present day.
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ -  [anli] premise: On 12 March 2007, Frank Newbery was beaten to death inside his convenience store, Franks Ham & Beef, in the inner-city suburb of Cooks Hill in the Australian city of Newcastle. The murder remains unsolved and the New South Wales Government offers a reward of $100,000 for any information leading to an arrest and conviction. [SEP] hypothesis: Frank Newbery had a mall store, Franks Ham & Beef
05/24/2022 18:02:26 - INFO - __main__ - ['contradiction']
05/24/2022 18:02:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:02:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:02:26 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:02:44 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:02:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:02:45 - INFO - __main__ - Starting training!
05/24/2022 18:02:48 - INFO - __main__ - Step 10 Global step 10 Train loss 1.09 on epoch=3
05/24/2022 18:02:50 - INFO - __main__ - Step 20 Global step 20 Train loss 0.77 on epoch=6
05/24/2022 18:02:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.68 on epoch=9
05/24/2022 18:02:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=13
05/24/2022 18:02:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=16
05/24/2022 18:02:59 - INFO - __main__ - Global step 50 Train loss 0.76 Classification-F1 0.3077750874361044 on epoch=16
05/24/2022 18:02:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3077750874361044 on epoch=16, global_step=50
05/24/2022 18:03:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=19
05/24/2022 18:03:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
05/24/2022 18:03:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/24/2022 18:03:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
05/24/2022 18:03:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=33
05/24/2022 18:03:13 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.34722222222222215 on epoch=33
05/24/2022 18:03:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3077750874361044 -> 0.34722222222222215 on epoch=33, global_step=100
05/24/2022 18:03:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=36
05/24/2022 18:03:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
05/24/2022 18:03:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=43
05/24/2022 18:03:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
05/24/2022 18:03:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=49
05/24/2022 18:03:27 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.24611708482676223 on epoch=49
05/24/2022 18:03:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=53
05/24/2022 18:03:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=56
05/24/2022 18:03:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=59
05/24/2022 18:03:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=63
05/24/2022 18:03:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
05/24/2022 18:03:40 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.2884848484848485 on epoch=66
05/24/2022 18:03:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=69
05/24/2022 18:03:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/24/2022 18:03:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=76
05/24/2022 18:03:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=79
05/24/2022 18:03:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/24/2022 18:03:54 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.35072281583909487 on epoch=83
05/24/2022 18:03:54 - INFO - __main__ - Saving model with best Classification-F1: 0.34722222222222215 -> 0.35072281583909487 on epoch=83, global_step=250
05/24/2022 18:03:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=86
05/24/2022 18:03:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=89
05/24/2022 18:04:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=93
05/24/2022 18:04:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=96
05/24/2022 18:04:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
05/24/2022 18:04:08 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.3006060606060606 on epoch=99
05/24/2022 18:04:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=103
05/24/2022 18:04:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=106
05/24/2022 18:04:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
05/24/2022 18:04:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=113
05/24/2022 18:04:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/24/2022 18:04:22 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.3015873015873016 on epoch=116
05/24/2022 18:04:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=119
05/24/2022 18:04:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=123
05/24/2022 18:04:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
05/24/2022 18:04:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=129
05/24/2022 18:04:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
05/24/2022 18:04:35 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.2111111111111111 on epoch=133
05/24/2022 18:04:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/24/2022 18:04:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=139
05/24/2022 18:04:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=143
05/24/2022 18:04:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=146
05/24/2022 18:04:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=149
05/24/2022 18:04:49 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.35038363171355497 on epoch=149
05/24/2022 18:04:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=153
05/24/2022 18:04:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=156
05/24/2022 18:04:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=159
05/24/2022 18:04:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
05/24/2022 18:05:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=166
05/24/2022 18:05:02 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.2995574190542744 on epoch=166
05/24/2022 18:05:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=169
05/24/2022 18:05:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=173
05/24/2022 18:05:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=176
05/24/2022 18:05:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=179
05/24/2022 18:05:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=183
05/24/2022 18:05:16 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.3138029405310511 on epoch=183
05/24/2022 18:05:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=186
05/24/2022 18:05:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
05/24/2022 18:05:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=193
05/24/2022 18:05:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=196
05/24/2022 18:05:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=199
05/24/2022 18:05:29 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.2770673486786019 on epoch=199
05/24/2022 18:05:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=203
05/24/2022 18:05:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=206
05/24/2022 18:05:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
05/24/2022 18:05:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=213
05/24/2022 18:05:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=216
05/24/2022 18:05:43 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.24451410658307213 on epoch=216
05/24/2022 18:05:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=219
05/24/2022 18:05:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=223
05/24/2022 18:05:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=226
05/24/2022 18:05:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=229
05/24/2022 18:05:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=233
05/24/2022 18:05:56 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.27079218784287906 on epoch=233
05/24/2022 18:05:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=236
05/24/2022 18:06:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=239
05/24/2022 18:06:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=243
05/24/2022 18:06:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
05/24/2022 18:06:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=249
05/24/2022 18:06:09 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.2747252747252748 on epoch=249
05/24/2022 18:06:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=253
05/24/2022 18:06:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=256
05/24/2022 18:06:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=259
05/24/2022 18:06:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=263
05/24/2022 18:06:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=266
05/24/2022 18:06:23 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.2747252747252748 on epoch=266
05/24/2022 18:06:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=269
05/24/2022 18:06:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=273
05/24/2022 18:06:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=276
05/24/2022 18:06:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=279
05/24/2022 18:06:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=283
05/24/2022 18:06:36 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.27724867724867724 on epoch=283
05/24/2022 18:06:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
05/24/2022 18:06:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=289
05/24/2022 18:06:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=293
05/24/2022 18:06:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=296
05/24/2022 18:06:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=299
05/24/2022 18:06:50 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.27122286696754777 on epoch=299
05/24/2022 18:06:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=303
05/24/2022 18:06:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=306
05/24/2022 18:06:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=309
05/24/2022 18:07:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=313
05/24/2022 18:07:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=316
05/24/2022 18:07:03 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.2719522591645354 on epoch=316
05/24/2022 18:07:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=319
05/24/2022 18:07:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=323
05/24/2022 18:07:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=326
05/24/2022 18:07:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=329
05/24/2022 18:07:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=333
05/24/2022 18:07:17 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.24928681149879306 on epoch=333
05/24/2022 18:07:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=336
05/24/2022 18:07:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
05/24/2022 18:07:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=343
05/24/2022 18:07:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=346
05/24/2022 18:07:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=349
05/24/2022 18:07:30 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.25 on epoch=349
05/24/2022 18:07:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=353
05/24/2022 18:07:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=356
05/24/2022 18:07:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=359
05/24/2022 18:07:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=363
05/24/2022 18:07:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=366
05/24/2022 18:07:44 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.31116794543904525 on epoch=366
05/24/2022 18:07:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=369
05/24/2022 18:07:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=373
05/24/2022 18:07:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=376
05/24/2022 18:07:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=379
05/24/2022 18:07:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=383
05/24/2022 18:07:57 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.25 on epoch=383
05/24/2022 18:08:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=386
05/24/2022 18:08:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=389
05/24/2022 18:08:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=393
05/24/2022 18:08:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=396
05/24/2022 18:08:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=399
05/24/2022 18:08:11 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.2990810359231412 on epoch=399
05/24/2022 18:08:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=403
05/24/2022 18:08:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=406
05/24/2022 18:08:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=409
05/24/2022 18:08:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=413
05/24/2022 18:08:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=416
05/24/2022 18:08:24 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.29777777777777775 on epoch=416
05/24/2022 18:08:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
05/24/2022 18:08:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=423
05/24/2022 18:08:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=426
05/24/2022 18:08:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=429
05/24/2022 18:08:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=433
05/24/2022 18:08:38 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.27122286696754777 on epoch=433
05/24/2022 18:08:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=436
05/24/2022 18:08:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=439
05/24/2022 18:08:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=443
05/24/2022 18:08:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=446
05/24/2022 18:08:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=449
05/24/2022 18:08:51 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.2719522591645354 on epoch=449
05/24/2022 18:08:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=453
05/24/2022 18:08:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=456
05/24/2022 18:08:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=459
05/24/2022 18:09:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=463
05/24/2022 18:09:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=466
05/24/2022 18:09:05 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.18437719915552425 on epoch=466
05/24/2022 18:09:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=469
05/24/2022 18:09:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=473
05/24/2022 18:09:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=476
05/24/2022 18:09:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=479
05/24/2022 18:09:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=483
05/24/2022 18:09:18 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.26683826683826684 on epoch=483
05/24/2022 18:09:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=486
05/24/2022 18:09:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=489
05/24/2022 18:09:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=493
05/24/2022 18:09:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=496
05/24/2022 18:09:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=499
05/24/2022 18:09:32 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.2095238095238095 on epoch=499
05/24/2022 18:09:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=503
05/24/2022 18:09:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=506
05/24/2022 18:09:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=509
05/24/2022 18:09:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=513
05/24/2022 18:09:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
05/24/2022 18:09:45 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.2121212121212121 on epoch=516
05/24/2022 18:09:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=519
05/24/2022 18:09:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=523
05/24/2022 18:09:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=526
05/24/2022 18:09:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
05/24/2022 18:09:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=533
05/24/2022 18:09:59 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.26657706093189965 on epoch=533
05/24/2022 18:10:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=536
05/24/2022 18:10:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=539
05/24/2022 18:10:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=543
05/24/2022 18:10:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=546
05/24/2022 18:10:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=549
05/24/2022 18:10:13 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.14804597701149425 on epoch=549
05/24/2022 18:10:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=553
05/24/2022 18:10:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=556
05/24/2022 18:10:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=559
05/24/2022 18:10:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=563
05/24/2022 18:10:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=566
05/24/2022 18:10:27 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.29647874475460684 on epoch=566
05/24/2022 18:10:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=569
05/24/2022 18:10:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
05/24/2022 18:10:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
05/24/2022 18:10:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=579
05/24/2022 18:10:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
05/24/2022 18:10:40 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.255632183908046 on epoch=583
05/24/2022 18:10:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=586
05/24/2022 18:10:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=589
05/24/2022 18:10:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=593
05/24/2022 18:10:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=596
05/24/2022 18:10:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=599
05/24/2022 18:10:54 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.14434782608695654 on epoch=599
05/24/2022 18:10:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=603
05/24/2022 18:10:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=606
05/24/2022 18:11:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=609
05/24/2022 18:11:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=613
05/24/2022 18:11:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=616
05/24/2022 18:11:08 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.2415795586527294 on epoch=616
05/24/2022 18:11:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=619
05/24/2022 18:11:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=623
05/24/2022 18:11:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=626
05/24/2022 18:11:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=629
05/24/2022 18:11:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=633
05/24/2022 18:11:22 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.1911355311355311 on epoch=633
05/24/2022 18:11:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=636
05/24/2022 18:11:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=639
05/24/2022 18:11:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
05/24/2022 18:11:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=646
05/24/2022 18:11:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/24/2022 18:11:36 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.30033222591362124 on epoch=649
05/24/2022 18:11:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=653
05/24/2022 18:11:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=656
05/24/2022 18:11:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
05/24/2022 18:11:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
05/24/2022 18:11:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=666
05/24/2022 18:11:49 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.29619883040935674 on epoch=666
05/24/2022 18:11:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
05/24/2022 18:11:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=673
05/24/2022 18:11:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
05/24/2022 18:11:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=679
05/24/2022 18:12:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=683
05/24/2022 18:12:03 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.27173913043478265 on epoch=683
05/24/2022 18:12:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=686
05/24/2022 18:12:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
05/24/2022 18:12:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=693
05/24/2022 18:12:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=696
05/24/2022 18:12:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=699
05/24/2022 18:12:17 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.18544636159039757 on epoch=699
05/24/2022 18:12:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/24/2022 18:12:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=706
05/24/2022 18:12:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/24/2022 18:12:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=713
05/24/2022 18:12:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=716
05/24/2022 18:12:31 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.21224340175953077 on epoch=716
05/24/2022 18:12:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=719
05/24/2022 18:12:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=723
05/24/2022 18:12:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/24/2022 18:12:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=729
05/24/2022 18:12:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
05/24/2022 18:12:45 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.2222222222222222 on epoch=733
05/24/2022 18:12:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
05/24/2022 18:12:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=739
05/24/2022 18:12:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
05/24/2022 18:12:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=746
05/24/2022 18:12:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=749
05/24/2022 18:12:58 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.31023965141612203 on epoch=749
05/24/2022 18:13:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=753
05/24/2022 18:13:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=756
05/24/2022 18:13:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=759
05/24/2022 18:13:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=763
05/24/2022 18:13:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
05/24/2022 18:13:12 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.18544636159039757 on epoch=766
05/24/2022 18:13:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
05/24/2022 18:13:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=773
05/24/2022 18:13:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
05/24/2022 18:13:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=779
05/24/2022 18:13:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/24/2022 18:13:26 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.19862078977932635 on epoch=783
05/24/2022 18:13:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 18:13:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=789
05/24/2022 18:13:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 18:13:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/24/2022 18:13:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=799
05/24/2022 18:13:40 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.2705128205128205 on epoch=799
05/24/2022 18:13:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=803
05/24/2022 18:13:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=806
05/24/2022 18:13:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=809
05/24/2022 18:13:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
05/24/2022 18:13:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
05/24/2022 18:13:53 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.19178885630498535 on epoch=816
05/24/2022 18:13:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=819
05/24/2022 18:13:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
05/24/2022 18:14:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
05/24/2022 18:14:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
05/24/2022 18:14:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
05/24/2022 18:14:07 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.2215367965367965 on epoch=833
05/24/2022 18:14:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
05/24/2022 18:14:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
05/24/2022 18:14:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/24/2022 18:14:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=846
05/24/2022 18:14:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=849
05/24/2022 18:14:21 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.17777777777777776 on epoch=849
05/24/2022 18:14:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/24/2022 18:14:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=856
05/24/2022 18:14:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
05/24/2022 18:14:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
05/24/2022 18:14:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=866
05/24/2022 18:14:35 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.2130642954856361 on epoch=866
05/24/2022 18:14:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=869
05/24/2022 18:14:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 18:14:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/24/2022 18:14:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=879
05/24/2022 18:14:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
05/24/2022 18:14:48 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.22 on epoch=883
05/24/2022 18:14:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
05/24/2022 18:14:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=889
05/24/2022 18:14:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/24/2022 18:14:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/24/2022 18:15:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=899
05/24/2022 18:15:02 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.2798573975044563 on epoch=899
05/24/2022 18:15:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/24/2022 18:15:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=906
05/24/2022 18:15:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/24/2022 18:15:12 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/24/2022 18:15:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 18:15:16 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.22617354196301565 on epoch=916
05/24/2022 18:15:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=919
05/24/2022 18:15:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/24/2022 18:15:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/24/2022 18:15:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/24/2022 18:15:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
05/24/2022 18:15:30 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.18068181818181817 on epoch=933
05/24/2022 18:15:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/24/2022 18:15:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/24/2022 18:15:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/24/2022 18:15:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=946
05/24/2022 18:15:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=949
05/24/2022 18:15:44 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.1462962962962963 on epoch=949
05/24/2022 18:15:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/24/2022 18:15:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=956
05/24/2022 18:15:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
05/24/2022 18:15:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/24/2022 18:15:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/24/2022 18:15:58 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.20402930402930403 on epoch=966
05/24/2022 18:16:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=969
05/24/2022 18:16:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=973
05/24/2022 18:16:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 18:16:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/24/2022 18:16:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/24/2022 18:16:11 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2232142857142857 on epoch=983
05/24/2022 18:16:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=986
05/24/2022 18:16:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=989
05/24/2022 18:16:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/24/2022 18:16:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
05/24/2022 18:16:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=999
05/24/2022 18:16:25 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.16324081020255066 on epoch=999
05/24/2022 18:16:25 - INFO - __main__ - save last model!
05/24/2022 18:16:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 18:16:25 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 18:16:25 - INFO - __main__ - Printing 3 examples
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 18:16:25 - INFO - __main__ - ['contradiction']
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 18:16:25 - INFO - __main__ - ['entailment']
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 18:16:25 - INFO - __main__ - ['contradiction']
05/24/2022 18:16:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:16:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:16:25 - INFO - __main__ - Printing 3 examples
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:16:25 - INFO - __main__ - ['entailment']
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:16:25 - INFO - __main__ - ['entailment']
05/24/2022 18:16:25 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:16:25 - INFO - __main__ - ['entailment']
05/24/2022 18:16:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:16:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:16:26 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:16:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:16:26 - INFO - __main__ - Printing 3 examples
05/24/2022 18:16:26 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:16:26 - INFO - __main__ - ['entailment']
05/24/2022 18:16:26 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:16:26 - INFO - __main__ - ['entailment']
05/24/2022 18:16:26 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:16:26 - INFO - __main__ - ['entailment']
05/24/2022 18:16:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:16:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:16:26 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:16:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:16:27 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 18:16:41 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:16:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:16:41 - INFO - __main__ - Starting training!
05/24/2022 18:16:52 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_13_0.2_8_predictions.txt
05/24/2022 18:16:52 - INFO - __main__ - Classification-F1 on test data: 0.1195
05/24/2022 18:16:53 - INFO - __main__ - prefix=anli_16_13, lr=0.2, bsz=8, dev_performance=0.35072281583909487, test_performance=0.11953234119087777
05/24/2022 18:16:53 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.5, bsz=8 ...
05/24/2022 18:16:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:16:54 - INFO - __main__ - Printing 3 examples
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:16:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:16:54 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:16:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:16:54 - INFO - __main__ - Printing 3 examples
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:16:54 - INFO - __main__ - ['entailment']
05/24/2022 18:16:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:16:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:16:54 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:17:11 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:17:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:17:12 - INFO - __main__ - Starting training!
05/24/2022 18:17:15 - INFO - __main__ - Step 10 Global step 10 Train loss 1.04 on epoch=3
05/24/2022 18:17:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=6
05/24/2022 18:17:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
05/24/2022 18:17:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=13
05/24/2022 18:17:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
05/24/2022 18:17:26 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.3666666666666667 on epoch=16
05/24/2022 18:17:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3666666666666667 on epoch=16, global_step=50
05/24/2022 18:17:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=19
05/24/2022 18:17:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=23
05/24/2022 18:17:33 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
05/24/2022 18:17:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=29
05/24/2022 18:17:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
05/24/2022 18:17:39 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.2334096109839817 on epoch=33
05/24/2022 18:17:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=36
05/24/2022 18:17:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=39
05/24/2022 18:17:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
05/24/2022 18:17:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=46
05/24/2022 18:17:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=49
05/24/2022 18:17:53 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 18:17:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
05/24/2022 18:17:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/24/2022 18:18:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 18:18:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
05/24/2022 18:18:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.41 on epoch=66
05/24/2022 18:18:07 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.1693121693121693 on epoch=66
05/24/2022 18:18:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=69
05/24/2022 18:18:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=73
05/24/2022 18:18:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
05/24/2022 18:18:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=79
05/24/2022 18:18:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=83
05/24/2022 18:18:20 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.27636363636363637 on epoch=83
05/24/2022 18:18:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
05/24/2022 18:18:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/24/2022 18:18:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=93
05/24/2022 18:18:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
05/24/2022 18:18:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
05/24/2022 18:18:34 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.19999999999999998 on epoch=99
05/24/2022 18:18:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
05/24/2022 18:18:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=106
05/24/2022 18:18:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=109
05/24/2022 18:18:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
05/24/2022 18:18:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/24/2022 18:18:48 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.3151515151515151 on epoch=116
05/24/2022 18:18:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=119
05/24/2022 18:18:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=123
05/24/2022 18:18:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=126
05/24/2022 18:18:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=129
05/24/2022 18:19:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=133
05/24/2022 18:19:01 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.2995574190542744 on epoch=133
05/24/2022 18:19:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=136
05/24/2022 18:19:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=139
05/24/2022 18:19:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=143
05/24/2022 18:19:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=146
05/24/2022 18:19:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=149
05/24/2022 18:19:15 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.34290271132376393 on epoch=149
05/24/2022 18:19:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=153
05/24/2022 18:19:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=156
05/24/2022 18:19:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=159
05/24/2022 18:19:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=163
05/24/2022 18:19:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=166
05/24/2022 18:19:29 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.34208998914881267 on epoch=166
05/24/2022 18:19:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=169
05/24/2022 18:19:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=173
05/24/2022 18:19:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=176
05/24/2022 18:19:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=179
05/24/2022 18:19:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=183
05/24/2022 18:19:43 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.3886868686868687 on epoch=183
05/24/2022 18:19:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3666666666666667 -> 0.3886868686868687 on epoch=183, global_step=550
05/24/2022 18:19:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=186
05/24/2022 18:19:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=189
05/24/2022 18:19:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=193
05/24/2022 18:19:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=196
05/24/2022 18:19:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=199
05/24/2022 18:19:57 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.2980859010270775 on epoch=199
05/24/2022 18:19:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=203
05/24/2022 18:20:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=206
05/24/2022 18:20:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=209
05/24/2022 18:20:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=213
05/24/2022 18:20:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=216
05/24/2022 18:20:11 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.35044283413848637 on epoch=216
05/24/2022 18:20:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=219
05/24/2022 18:20:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=223
05/24/2022 18:20:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=226
05/24/2022 18:20:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=229
05/24/2022 18:20:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=233
05/24/2022 18:20:25 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.3669655620532813 on epoch=233
05/24/2022 18:20:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=236
05/24/2022 18:20:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=239
05/24/2022 18:20:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=243
05/24/2022 18:20:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=246
05/24/2022 18:20:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=249
05/24/2022 18:20:39 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.2776769509981851 on epoch=249
05/24/2022 18:20:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=253
05/24/2022 18:20:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=256
05/24/2022 18:20:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=259
05/24/2022 18:20:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=263
05/24/2022 18:20:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=266
05/24/2022 18:20:52 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.20908004778972522 on epoch=266
05/24/2022 18:20:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=269
05/24/2022 18:20:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=273
05/24/2022 18:21:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=276
05/24/2022 18:21:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=279
05/24/2022 18:21:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=283
05/24/2022 18:21:06 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.20295837633298933 on epoch=283
05/24/2022 18:21:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=286
05/24/2022 18:21:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=289
05/24/2022 18:21:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=293
05/24/2022 18:21:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=296
05/24/2022 18:21:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=299
05/24/2022 18:21:20 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.3238095238095238 on epoch=299
05/24/2022 18:21:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=303
05/24/2022 18:21:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=306
05/24/2022 18:21:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=309
05/24/2022 18:21:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=313
05/24/2022 18:21:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=316
05/24/2022 18:21:34 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.3058117405943493 on epoch=316
05/24/2022 18:21:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=319
05/24/2022 18:21:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=323
05/24/2022 18:21:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=326
05/24/2022 18:21:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=329
05/24/2022 18:21:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=333
05/24/2022 18:21:48 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.2754036087369421 on epoch=333
05/24/2022 18:21:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=336
05/24/2022 18:21:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=339
05/24/2022 18:21:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=343
05/24/2022 18:21:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=346
05/24/2022 18:22:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=349
05/24/2022 18:22:01 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.1935672514619883 on epoch=349
05/24/2022 18:22:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=353
05/24/2022 18:22:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=356
05/24/2022 18:22:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=359
05/24/2022 18:22:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=363
05/24/2022 18:22:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=366
05/24/2022 18:22:15 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.16614060258249644 on epoch=366
05/24/2022 18:22:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=369
05/24/2022 18:22:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=373
05/24/2022 18:22:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=376
05/24/2022 18:22:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=379
05/24/2022 18:22:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=383
05/24/2022 18:22:29 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.21067031463748292 on epoch=383
05/24/2022 18:22:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=386
05/24/2022 18:22:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=389
05/24/2022 18:22:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=393
05/24/2022 18:22:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=396
05/24/2022 18:22:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=399
05/24/2022 18:22:42 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.2772404900064474 on epoch=399
05/24/2022 18:22:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
05/24/2022 18:22:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
05/24/2022 18:22:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=409
05/24/2022 18:22:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=413
05/24/2022 18:22:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=416
05/24/2022 18:22:56 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.14829268292682926 on epoch=416
05/24/2022 18:22:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=419
05/24/2022 18:23:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=423
05/24/2022 18:23:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=426
05/24/2022 18:23:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=429
05/24/2022 18:23:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=433
05/24/2022 18:23:10 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.20266116941529236 on epoch=433
05/24/2022 18:23:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=436
05/24/2022 18:23:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=439
05/24/2022 18:23:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=443
05/24/2022 18:23:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=446
05/24/2022 18:23:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
05/24/2022 18:23:24 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.22670807453416147 on epoch=449
05/24/2022 18:23:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=453
05/24/2022 18:23:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=456
05/24/2022 18:23:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/24/2022 18:23:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
05/24/2022 18:23:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=466
05/24/2022 18:23:37 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.25098039215686274 on epoch=466
05/24/2022 18:23:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
05/24/2022 18:23:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=473
05/24/2022 18:23:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=476
05/24/2022 18:23:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
05/24/2022 18:23:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=483
05/24/2022 18:23:51 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.14666666666666667 on epoch=483
05/24/2022 18:23:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=486
05/24/2022 18:23:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=489
05/24/2022 18:23:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=493
05/24/2022 18:24:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
05/24/2022 18:24:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=499
05/24/2022 18:24:05 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.12261904761904763 on epoch=499
05/24/2022 18:24:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
05/24/2022 18:24:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/24/2022 18:24:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=509
05/24/2022 18:24:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=513
05/24/2022 18:24:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
05/24/2022 18:24:18 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.10374149659863947 on epoch=516
05/24/2022 18:24:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=519
05/24/2022 18:24:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=523
05/24/2022 18:24:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=526
05/24/2022 18:24:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=529
05/24/2022 18:24:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=533
05/24/2022 18:24:32 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.1664008506113769 on epoch=533
05/24/2022 18:24:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
05/24/2022 18:24:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
05/24/2022 18:24:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=543
05/24/2022 18:24:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
05/24/2022 18:24:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=549
05/24/2022 18:24:46 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.19 on epoch=549
05/24/2022 18:24:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=553
05/24/2022 18:24:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=556
05/24/2022 18:24:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
05/24/2022 18:24:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=563
05/24/2022 18:24:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=566
05/24/2022 18:24:59 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.11461988304093568 on epoch=566
05/24/2022 18:25:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/24/2022 18:25:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=573
05/24/2022 18:25:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=576
05/24/2022 18:25:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/24/2022 18:25:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=583
05/24/2022 18:25:13 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.17691209827458776 on epoch=583
05/24/2022 18:25:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 18:25:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
05/24/2022 18:25:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=593
05/24/2022 18:25:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
05/24/2022 18:25:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=599
05/24/2022 18:25:27 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.10726310726310727 on epoch=599
05/24/2022 18:25:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=603
05/24/2022 18:25:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
05/24/2022 18:25:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=609
05/24/2022 18:25:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/24/2022 18:25:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=616
05/24/2022 18:25:40 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.13633241758241757 on epoch=616
05/24/2022 18:25:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=619
05/24/2022 18:25:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/24/2022 18:25:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/24/2022 18:25:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=629
05/24/2022 18:25:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/24/2022 18:25:54 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.1243401759530792 on epoch=633
05/24/2022 18:25:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/24/2022 18:25:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=639
05/24/2022 18:26:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=643
05/24/2022 18:26:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/24/2022 18:26:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/24/2022 18:26:08 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.15602513428600384 on epoch=649
05/24/2022 18:26:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=653
05/24/2022 18:26:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=656
05/24/2022 18:26:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
05/24/2022 18:26:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
05/24/2022 18:26:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
05/24/2022 18:26:22 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.3434572093315727 on epoch=666
05/24/2022 18:26:24 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/24/2022 18:26:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
05/24/2022 18:26:29 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=676
05/24/2022 18:26:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
05/24/2022 18:26:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
05/24/2022 18:26:35 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.1835151515151515 on epoch=683
05/24/2022 18:26:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=686
05/24/2022 18:26:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=689
05/24/2022 18:26:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/24/2022 18:26:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/24/2022 18:26:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/24/2022 18:26:50 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.18067345783814376 on epoch=699
05/24/2022 18:26:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/24/2022 18:26:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/24/2022 18:26:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/24/2022 18:26:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/24/2022 18:27:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
05/24/2022 18:27:04 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.16767241379310344 on epoch=716
05/24/2022 18:27:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=719
05/24/2022 18:27:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
05/24/2022 18:27:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=726
05/24/2022 18:27:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/24/2022 18:27:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/24/2022 18:27:19 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.3167907361455749 on epoch=733
05/24/2022 18:27:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/24/2022 18:27:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=739
05/24/2022 18:27:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/24/2022 18:27:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
05/24/2022 18:27:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/24/2022 18:27:32 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.26037798408488066 on epoch=749
05/24/2022 18:27:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=753
05/24/2022 18:27:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=756
05/24/2022 18:27:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
05/24/2022 18:27:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/24/2022 18:27:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/24/2022 18:27:46 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.2775857654889913 on epoch=766
05/24/2022 18:27:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 18:27:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/24/2022 18:27:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/24/2022 18:27:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/24/2022 18:27:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/24/2022 18:28:00 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.3373137505321413 on epoch=783
05/24/2022 18:28:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 18:28:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
05/24/2022 18:28:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 18:28:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 18:28:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=799
05/24/2022 18:28:13 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.20245943204868155 on epoch=799
05/24/2022 18:28:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
05/24/2022 18:28:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=806
05/24/2022 18:28:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 18:28:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=813
05/24/2022 18:28:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=816
05/24/2022 18:28:27 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.1499414434197043 on epoch=816
05/24/2022 18:28:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=819
05/24/2022 18:28:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/24/2022 18:28:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/24/2022 18:28:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=829
05/24/2022 18:28:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
05/24/2022 18:28:41 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.17982608695652172 on epoch=833
05/24/2022 18:28:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 18:28:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=839
05/24/2022 18:28:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
05/24/2022 18:28:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=846
05/24/2022 18:28:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 18:28:55 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.21364221364221364 on epoch=849
05/24/2022 18:28:58 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
05/24/2022 18:29:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 18:29:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
05/24/2022 18:29:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 18:29:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 18:29:09 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.16254052460949012 on epoch=866
05/24/2022 18:29:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=869
05/24/2022 18:29:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 18:29:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 18:29:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 18:29:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=883
05/24/2022 18:29:23 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.1831196581196581 on epoch=883
05/24/2022 18:29:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
05/24/2022 18:29:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/24/2022 18:29:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 18:29:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=896
05/24/2022 18:29:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 18:29:36 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.13864692218350755 on epoch=899
05/24/2022 18:29:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/24/2022 18:29:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/24/2022 18:29:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/24/2022 18:29:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=913
05/24/2022 18:29:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/24/2022 18:29:50 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.22007575757575754 on epoch=916
05/24/2022 18:29:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 18:29:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/24/2022 18:29:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/24/2022 18:30:00 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/24/2022 18:30:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 18:30:04 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.25170331621944525 on epoch=933
05/24/2022 18:30:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 18:30:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 18:30:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 18:30:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 18:30:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=949
05/24/2022 18:30:18 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.24656084656084656 on epoch=949
05/24/2022 18:30:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=953
05/24/2022 18:30:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/24/2022 18:30:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 18:30:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 18:30:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
05/24/2022 18:30:32 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.2286472148541114 on epoch=966
05/24/2022 18:30:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 18:30:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 18:30:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
05/24/2022 18:30:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 18:30:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 18:30:46 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.3220773220773221 on epoch=983
05/24/2022 18:30:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=986
05/24/2022 18:30:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/24/2022 18:30:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/24/2022 18:30:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
05/24/2022 18:30:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 18:31:00 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:31:00 - INFO - __main__ - Printing 3 examples
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:31:00 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:31:00 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.24688083308772962 on epoch=999
05/24/2022 18:31:00 - INFO - __main__ - save last model!
05/24/2022 18:31:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 18:31:00 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:31:00 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:31:00 - INFO - __main__ - Printing 3 examples
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:31:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 18:31:00 - INFO - __main__ - Printing 3 examples
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 18:31:00 - INFO - __main__ - ['contradiction']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 18:31:00 - INFO - __main__ - ['entailment']
05/24/2022 18:31:00 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 18:31:00 - INFO - __main__ - ['contradiction']
05/24/2022 18:31:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:31:00 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:31:00 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:31:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:31:02 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 18:31:15 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:31:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:31:16 - INFO - __main__ - Starting training!
05/24/2022 18:31:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_21_0.5_8_predictions.txt
05/24/2022 18:31:30 - INFO - __main__ - Classification-F1 on test data: 0.0864
05/24/2022 18:31:31 - INFO - __main__ - prefix=anli_16_21, lr=0.5, bsz=8, dev_performance=0.3886868686868687, test_performance=0.08638568318955728
05/24/2022 18:31:31 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.4, bsz=8 ...
05/24/2022 18:31:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:31:32 - INFO - __main__ - Printing 3 examples
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:31:32 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:31:32 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:31:32 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:31:32 - INFO - __main__ - Printing 3 examples
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:31:32 - INFO - __main__ - ['entailment']
05/24/2022 18:31:32 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:31:32 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:31:32 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:31:49 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:31:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:31:50 - INFO - __main__ - Starting training!
05/24/2022 18:31:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=3
05/24/2022 18:31:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
05/24/2022 18:31:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=9
05/24/2022 18:32:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=13
05/24/2022 18:32:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
05/24/2022 18:32:04 - INFO - __main__ - Global step 50 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 18:32:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/24/2022 18:32:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=19
05/24/2022 18:32:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
05/24/2022 18:32:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=26
05/24/2022 18:32:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=29
05/24/2022 18:32:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/24/2022 18:32:17 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.22507122507122504 on epoch=33
05/24/2022 18:32:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22507122507122504 on epoch=33, global_step=100
05/24/2022 18:32:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
05/24/2022 18:32:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=39
05/24/2022 18:32:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
05/24/2022 18:32:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
05/24/2022 18:32:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=49
05/24/2022 18:32:31 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 18:32:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=53
05/24/2022 18:32:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=56
05/24/2022 18:32:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=59
05/24/2022 18:32:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=63
05/24/2022 18:32:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=66
05/24/2022 18:32:44 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 18:32:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=69
05/24/2022 18:32:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=73
05/24/2022 18:32:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=76
05/24/2022 18:32:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=79
05/24/2022 18:32:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
05/24/2022 18:32:58 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.2920634920634921 on epoch=83
05/24/2022 18:32:58 - INFO - __main__ - Saving model with best Classification-F1: 0.22507122507122504 -> 0.2920634920634921 on epoch=83, global_step=250
05/24/2022 18:33:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
05/24/2022 18:33:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
05/24/2022 18:33:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=93
05/24/2022 18:33:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=96
05/24/2022 18:33:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=99
05/24/2022 18:33:11 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.22222222222222218 on epoch=99
05/24/2022 18:33:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
05/24/2022 18:33:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=106
05/24/2022 18:33:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
05/24/2022 18:33:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=113
05/24/2022 18:33:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=116
05/24/2022 18:33:25 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.2099511072763877 on epoch=116
05/24/2022 18:33:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=119
05/24/2022 18:33:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
05/24/2022 18:33:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
05/24/2022 18:33:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=129
05/24/2022 18:33:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=133
05/24/2022 18:33:38 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.3153439153439153 on epoch=133
05/24/2022 18:33:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2920634920634921 -> 0.3153439153439153 on epoch=133, global_step=400
05/24/2022 18:33:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=136
05/24/2022 18:33:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=139
05/24/2022 18:33:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=143
05/24/2022 18:33:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=146
05/24/2022 18:33:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=149
05/24/2022 18:33:52 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.24999999999999997 on epoch=149
05/24/2022 18:33:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=153
05/24/2022 18:33:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=156
05/24/2022 18:33:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
05/24/2022 18:34:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=163
05/24/2022 18:34:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=166
05/24/2022 18:34:05 - INFO - __main__ - Global step 500 Train loss 0.34 Classification-F1 0.25290697674418605 on epoch=166
05/24/2022 18:34:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=169
05/24/2022 18:34:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.29 on epoch=173
05/24/2022 18:34:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=176
05/24/2022 18:34:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=179
05/24/2022 18:34:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=183
05/24/2022 18:34:19 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.24305555555555558 on epoch=183
05/24/2022 18:34:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=186
05/24/2022 18:34:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=189
05/24/2022 18:34:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=193
05/24/2022 18:34:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=196
05/24/2022 18:34:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=199
05/24/2022 18:34:32 - INFO - __main__ - Global step 600 Train loss 0.26 Classification-F1 0.24888888888888885 on epoch=199
05/24/2022 18:34:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=203
05/24/2022 18:34:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=206
05/24/2022 18:34:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=209
05/24/2022 18:34:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=213
05/24/2022 18:34:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=216
05/24/2022 18:34:46 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.22265205243928646 on epoch=216
05/24/2022 18:34:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=219
05/24/2022 18:34:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=223
05/24/2022 18:34:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=226
05/24/2022 18:34:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
05/24/2022 18:34:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=233
05/24/2022 18:34:59 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.19943019943019938 on epoch=233
05/24/2022 18:35:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=236
05/24/2022 18:35:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=239
05/24/2022 18:35:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=243
05/24/2022 18:35:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=246
05/24/2022 18:35:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.15 on epoch=249
05/24/2022 18:35:13 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.29223939212840433 on epoch=249
05/24/2022 18:35:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=253
05/24/2022 18:35:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
05/24/2022 18:35:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.11 on epoch=259
05/24/2022 18:35:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=263
05/24/2022 18:35:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
05/24/2022 18:35:27 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.3463015647226173 on epoch=266
05/24/2022 18:35:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3153439153439153 -> 0.3463015647226173 on epoch=266, global_step=800
05/24/2022 18:35:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=269
05/24/2022 18:35:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
05/24/2022 18:35:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=276
05/24/2022 18:35:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.09 on epoch=279
05/24/2022 18:35:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=283
05/24/2022 18:35:41 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.3011244802047194 on epoch=283
05/24/2022 18:35:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=286
05/24/2022 18:35:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
05/24/2022 18:35:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
05/24/2022 18:35:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=296
05/24/2022 18:35:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.08 on epoch=299
05/24/2022 18:35:55 - INFO - __main__ - Global step 900 Train loss 0.11 Classification-F1 0.2573099415204678 on epoch=299
05/24/2022 18:35:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=303
05/24/2022 18:36:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=306
05/24/2022 18:36:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
05/24/2022 18:36:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/24/2022 18:36:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=316
05/24/2022 18:36:09 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.25984405458089666 on epoch=316
05/24/2022 18:36:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=319
05/24/2022 18:36:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.06 on epoch=323
05/24/2022 18:36:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=326
05/24/2022 18:36:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=329
05/24/2022 18:36:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=333
05/24/2022 18:36:22 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.2819430652101953 on epoch=333
05/24/2022 18:36:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
05/24/2022 18:36:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=339
05/24/2022 18:36:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=343
05/24/2022 18:36:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/24/2022 18:36:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
05/24/2022 18:36:36 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.22980030721966208 on epoch=349
05/24/2022 18:36:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
05/24/2022 18:36:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/24/2022 18:36:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=359
05/24/2022 18:36:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=363
05/24/2022 18:36:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=366
05/24/2022 18:36:50 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.3389396456256921 on epoch=366
05/24/2022 18:36:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
05/24/2022 18:36:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.06 on epoch=373
05/24/2022 18:36:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=376
05/24/2022 18:37:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=379
05/24/2022 18:37:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=383
05/24/2022 18:37:04 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.2868131868131868 on epoch=383
05/24/2022 18:37:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=386
05/24/2022 18:37:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
05/24/2022 18:37:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=393
05/24/2022 18:37:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=396
05/24/2022 18:37:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=399
05/24/2022 18:37:18 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.3209415158667715 on epoch=399
05/24/2022 18:37:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=403
05/24/2022 18:37:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
05/24/2022 18:37:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
05/24/2022 18:37:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=413
05/24/2022 18:37:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=416
05/24/2022 18:37:32 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.3075491759702286 on epoch=416
05/24/2022 18:37:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=419
05/24/2022 18:37:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
05/24/2022 18:37:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=426
05/24/2022 18:37:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=429
05/24/2022 18:37:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=433
05/24/2022 18:37:46 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.3542801284736769 on epoch=433
05/24/2022 18:37:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3463015647226173 -> 0.3542801284736769 on epoch=433, global_step=1300
05/24/2022 18:37:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=436
05/24/2022 18:37:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/24/2022 18:37:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=443
05/24/2022 18:37:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/24/2022 18:37:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
05/24/2022 18:38:00 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.32905822489155817 on epoch=449
05/24/2022 18:38:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=453
05/24/2022 18:38:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
05/24/2022 18:38:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/24/2022 18:38:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 18:38:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/24/2022 18:38:14 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.3376025165711718 on epoch=466
05/24/2022 18:38:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
05/24/2022 18:38:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
05/24/2022 18:38:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
05/24/2022 18:38:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=479
05/24/2022 18:38:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
05/24/2022 18:38:29 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.34271227749488614 on epoch=483
05/24/2022 18:38:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=486
05/24/2022 18:38:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/24/2022 18:38:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
05/24/2022 18:38:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/24/2022 18:38:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=499
05/24/2022 18:38:43 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.3551559551559551 on epoch=499
05/24/2022 18:38:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3542801284736769 -> 0.3551559551559551 on epoch=499, global_step=1500
05/24/2022 18:38:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=503
05/24/2022 18:38:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=506
05/24/2022 18:38:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=509
05/24/2022 18:38:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/24/2022 18:38:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/24/2022 18:38:58 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.3703412245611734 on epoch=516
05/24/2022 18:38:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3551559551559551 -> 0.3703412245611734 on epoch=516, global_step=1550
05/24/2022 18:39:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
05/24/2022 18:39:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/24/2022 18:39:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/24/2022 18:39:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
05/24/2022 18:39:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/24/2022 18:39:12 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.2586722488038277 on epoch=533
05/24/2022 18:39:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
05/24/2022 18:39:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/24/2022 18:39:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/24/2022 18:39:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=546
05/24/2022 18:39:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=549
05/24/2022 18:39:26 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.35568181818181815 on epoch=549
05/24/2022 18:39:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/24/2022 18:39:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/24/2022 18:39:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=559
05/24/2022 18:39:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
05/24/2022 18:39:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=566
05/24/2022 18:39:40 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.3899490676984283 on epoch=566
05/24/2022 18:39:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3703412245611734 -> 0.3899490676984283 on epoch=566, global_step=1700
05/24/2022 18:39:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/24/2022 18:39:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/24/2022 18:39:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=576
05/24/2022 18:39:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/24/2022 18:39:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
05/24/2022 18:39:56 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.32205988455988455 on epoch=583
05/24/2022 18:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 18:40:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=589
05/24/2022 18:40:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=593
05/24/2022 18:40:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=596
05/24/2022 18:40:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=599
05/24/2022 18:40:10 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.40494227994227994 on epoch=599
05/24/2022 18:40:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3899490676984283 -> 0.40494227994227994 on epoch=599, global_step=1800
05/24/2022 18:40:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/24/2022 18:40:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=606
05/24/2022 18:40:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/24/2022 18:40:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
05/24/2022 18:40:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=616
05/24/2022 18:40:24 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.3515229515229515 on epoch=616
05/24/2022 18:40:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/24/2022 18:40:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/24/2022 18:40:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=626
05/24/2022 18:40:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/24/2022 18:40:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
05/24/2022 18:40:38 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.38814814814814813 on epoch=633
05/24/2022 18:40:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/24/2022 18:40:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/24/2022 18:40:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
05/24/2022 18:40:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=646
05/24/2022 18:40:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=649
05/24/2022 18:40:52 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.35380591630591635 on epoch=649
05/24/2022 18:40:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/24/2022 18:40:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 18:40:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/24/2022 18:41:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/24/2022 18:41:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/24/2022 18:41:06 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.3072833599149388 on epoch=666
05/24/2022 18:41:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=669
05/24/2022 18:41:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/24/2022 18:41:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 18:41:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 18:41:19 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
05/24/2022 18:41:20 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.388327721661055 on epoch=683
05/24/2022 18:41:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/24/2022 18:41:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/24/2022 18:41:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/24/2022 18:41:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/24/2022 18:41:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=699
05/24/2022 18:41:34 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.369109047127623 on epoch=699
05/24/2022 18:41:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/24/2022 18:41:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/24/2022 18:41:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
05/24/2022 18:41:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=713
05/24/2022 18:41:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 18:41:48 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.2685786435786436 on epoch=716
05/24/2022 18:41:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/24/2022 18:41:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/24/2022 18:41:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/24/2022 18:41:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/24/2022 18:42:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/24/2022 18:42:03 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.26045548654244305 on epoch=733
05/24/2022 18:42:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=736
05/24/2022 18:42:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=739
05/24/2022 18:42:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
05/24/2022 18:42:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 18:42:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 18:42:17 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.24532988871224165 on epoch=749
05/24/2022 18:42:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/24/2022 18:42:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/24/2022 18:42:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/24/2022 18:42:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/24/2022 18:42:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=766
05/24/2022 18:42:31 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.22836921850079744 on epoch=766
05/24/2022 18:42:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/24/2022 18:42:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 18:42:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/24/2022 18:42:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 18:42:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=783
05/24/2022 18:42:45 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.3694486341545165 on epoch=783
05/24/2022 18:42:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/24/2022 18:42:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=789
05/24/2022 18:42:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/24/2022 18:42:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 18:42:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=799
05/24/2022 18:43:00 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.34960718294051624 on epoch=799
05/24/2022 18:43:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/24/2022 18:43:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/24/2022 18:43:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
05/24/2022 18:43:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/24/2022 18:43:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/24/2022 18:43:14 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.36935366739288306 on epoch=816
05/24/2022 18:43:16 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/24/2022 18:43:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
05/24/2022 18:43:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/24/2022 18:43:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 18:43:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/24/2022 18:43:28 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.3403449907514948 on epoch=833
05/24/2022 18:43:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 18:43:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/24/2022 18:43:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/24/2022 18:43:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/24/2022 18:43:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 18:43:42 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.34877618617456013 on epoch=849
05/24/2022 18:43:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/24/2022 18:43:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 18:43:49 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/24/2022 18:43:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 18:43:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 18:43:56 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.38840128644050215 on epoch=866
05/24/2022 18:43:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 18:44:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/24/2022 18:44:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 18:44:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 18:44:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 18:44:10 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.4066666666666667 on epoch=883
05/24/2022 18:44:10 - INFO - __main__ - Saving model with best Classification-F1: 0.40494227994227994 -> 0.4066666666666667 on epoch=883, global_step=2650
05/24/2022 18:44:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 18:44:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 18:44:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=893
05/24/2022 18:44:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 18:44:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 18:44:26 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.38728349240014137 on epoch=899
05/24/2022 18:44:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=903
05/24/2022 18:44:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 18:44:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=909
05/24/2022 18:44:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/24/2022 18:44:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 18:44:42 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.2962394212394212 on epoch=916
05/24/2022 18:44:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 18:44:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/24/2022 18:44:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 18:44:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=929
05/24/2022 18:44:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 18:44:58 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.389626239511823 on epoch=933
05/24/2022 18:45:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 18:45:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/24/2022 18:45:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 18:45:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 18:45:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 18:45:13 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.38888888888888884 on epoch=949
05/24/2022 18:45:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 18:45:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
05/24/2022 18:45:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 18:45:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 18:45:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 18:45:28 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.3700476947535771 on epoch=966
05/24/2022 18:45:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 18:45:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 18:45:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 18:45:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/24/2022 18:45:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 18:45:44 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.33225589225589225 on epoch=983
05/24/2022 18:45:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 18:45:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 18:45:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
05/24/2022 18:45:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 18:45:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
05/24/2022 18:45:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:45:58 - INFO - __main__ - Printing 3 examples
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:45:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:45:58 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:45:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:45:58 - INFO - __main__ - Printing 3 examples
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:45:58 - INFO - __main__ - ['entailment']
05/24/2022 18:45:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:45:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:45:58 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:45:59 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.35037112010796223 on epoch=999
05/24/2022 18:45:59 - INFO - __main__ - save last model!
05/24/2022 18:45:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 18:45:59 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 18:45:59 - INFO - __main__ - Printing 3 examples
05/24/2022 18:45:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 18:45:59 - INFO - __main__ - ['contradiction']
05/24/2022 18:45:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 18:45:59 - INFO - __main__ - ['entailment']
05/24/2022 18:45:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 18:45:59 - INFO - __main__ - ['contradiction']
05/24/2022 18:45:59 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:45:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:46:00 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 18:46:14 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:46:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:46:15 - INFO - __main__ - Starting training!
05/24/2022 18:47:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_21_0.4_8_predictions.txt
05/24/2022 18:47:09 - INFO - __main__ - Classification-F1 on test data: 0.1535
05/24/2022 18:47:09 - INFO - __main__ - prefix=anli_16_21, lr=0.4, bsz=8, dev_performance=0.4066666666666667, test_performance=0.15345860486965826
05/24/2022 18:47:09 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.3, bsz=8 ...
05/24/2022 18:47:10 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:47:10 - INFO - __main__ - Printing 3 examples
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:47:10 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:47:10 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 18:47:10 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 18:47:10 - INFO - __main__ - Printing 3 examples
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 18:47:10 - INFO - __main__ - ['entailment']
05/24/2022 18:47:10 - INFO - __main__ - Tokenizing Input ...
05/24/2022 18:47:10 - INFO - __main__ - Tokenizing Output ...
05/24/2022 18:47:10 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 18:47:28 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 18:47:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 18:47:29 - INFO - __main__ - Starting training!
05/24/2022 18:47:32 - INFO - __main__ - Step 10 Global step 10 Train loss 1.11 on epoch=3
05/24/2022 18:47:34 - INFO - __main__ - Step 20 Global step 20 Train loss 0.75 on epoch=6
05/24/2022 18:47:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.65 on epoch=9
05/24/2022 18:47:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=13
05/24/2022 18:47:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=16
05/24/2022 18:47:43 - INFO - __main__ - Global step 50 Train loss 0.74 Classification-F1 0.3642439431913116 on epoch=16
05/24/2022 18:47:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3642439431913116 on epoch=16, global_step=50
05/24/2022 18:47:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
05/24/2022 18:47:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.61 on epoch=23
05/24/2022 18:47:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=26
05/24/2022 18:47:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=29
05/24/2022 18:47:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=33
05/24/2022 18:47:57 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.31213603540647566 on epoch=33
05/24/2022 18:47:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=36
05/24/2022 18:48:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
05/24/2022 18:48:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=43
05/24/2022 18:48:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=46
05/24/2022 18:48:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=49
05/24/2022 18:48:10 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 18:48:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/24/2022 18:48:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=56
05/24/2022 18:48:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=59
05/24/2022 18:48:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
05/24/2022 18:48:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=66
05/24/2022 18:48:24 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.1693121693121693 on epoch=66
05/24/2022 18:48:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=69
05/24/2022 18:48:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=73
05/24/2022 18:48:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
05/24/2022 18:48:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
05/24/2022 18:48:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/24/2022 18:48:37 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.3263888888888889 on epoch=83
05/24/2022 18:48:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
05/24/2022 18:48:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=89
05/24/2022 18:48:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=93
05/24/2022 18:48:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=96
05/24/2022 18:48:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=99
05/24/2022 18:48:51 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.17204301075268816 on epoch=99
05/24/2022 18:48:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=103
05/24/2022 18:48:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
05/24/2022 18:48:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=109
05/24/2022 18:49:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=113
05/24/2022 18:49:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/24/2022 18:49:04 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.31666666666666665 on epoch=116
05/24/2022 18:49:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=119
05/24/2022 18:49:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=123
05/24/2022 18:49:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=126
05/24/2022 18:49:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=129
05/24/2022 18:49:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=133
05/24/2022 18:49:18 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.31666666666666665 on epoch=133
05/24/2022 18:49:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=136
05/24/2022 18:49:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=139
05/24/2022 18:49:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=143
05/24/2022 18:49:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=146
05/24/2022 18:49:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=149
05/24/2022 18:49:31 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.24999999999999997 on epoch=149
05/24/2022 18:49:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=153
05/24/2022 18:49:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=156
05/24/2022 18:49:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/24/2022 18:49:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
05/24/2022 18:49:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=166
05/24/2022 18:49:45 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.300187617260788 on epoch=166
05/24/2022 18:49:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=169
05/24/2022 18:49:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=173
05/24/2022 18:49:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=176
05/24/2022 18:49:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=179
05/24/2022 18:49:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=183
05/24/2022 18:49:59 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.2896825396825397 on epoch=183
05/24/2022 18:50:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=186
05/24/2022 18:50:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=189
05/24/2022 18:50:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=193
05/24/2022 18:50:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=196
05/24/2022 18:50:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=199
05/24/2022 18:50:12 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.27777777777777773 on epoch=199
05/24/2022 18:50:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=203
05/24/2022 18:50:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=206
05/24/2022 18:50:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=209
05/24/2022 18:50:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=213
05/24/2022 18:50:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=216
05/24/2022 18:50:26 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.27636363636363637 on epoch=216
05/24/2022 18:50:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=219
05/24/2022 18:50:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=223
05/24/2022 18:50:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=226
05/24/2022 18:50:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=229
05/24/2022 18:50:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=233
05/24/2022 18:50:39 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.22171370455123318 on epoch=233
05/24/2022 18:50:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=236
05/24/2022 18:50:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=239
05/24/2022 18:50:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=243
05/24/2022 18:50:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=246
05/24/2022 18:50:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=249
05/24/2022 18:50:53 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.23757575757575758 on epoch=249
05/24/2022 18:50:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=253
05/24/2022 18:50:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=256
05/24/2022 18:51:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=259
05/24/2022 18:51:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=263
05/24/2022 18:51:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=266
05/24/2022 18:51:07 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.31628303495311166 on epoch=266
05/24/2022 18:51:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=269
05/24/2022 18:51:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=273
05/24/2022 18:51:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=276
05/24/2022 18:51:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=279
05/24/2022 18:51:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=283
05/24/2022 18:51:21 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.2947937795807978 on epoch=283
05/24/2022 18:51:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=286
05/24/2022 18:51:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=289
05/24/2022 18:51:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=293
05/24/2022 18:51:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=296
05/24/2022 18:51:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=299
05/24/2022 18:51:34 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.23757575757575758 on epoch=299
05/24/2022 18:51:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=303
05/24/2022 18:51:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=306
05/24/2022 18:51:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=309
05/24/2022 18:51:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=313
05/24/2022 18:51:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=316
05/24/2022 18:51:48 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.27289377289377287 on epoch=316
05/24/2022 18:51:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=319
05/24/2022 18:51:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=323
05/24/2022 18:51:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=326
05/24/2022 18:51:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=329
05/24/2022 18:52:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=333
05/24/2022 18:52:01 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.23301985370950887 on epoch=333
05/24/2022 18:52:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=336
05/24/2022 18:52:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=339
05/24/2022 18:52:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=343
05/24/2022 18:52:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=346
05/24/2022 18:52:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=349
05/24/2022 18:52:15 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.25777777777777783 on epoch=349
05/24/2022 18:52:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=353
05/24/2022 18:52:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=356
05/24/2022 18:52:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=359
05/24/2022 18:52:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=363
05/24/2022 18:52:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=366
05/24/2022 18:52:28 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.2832080200501253 on epoch=366
05/24/2022 18:52:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=369
05/24/2022 18:52:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=373
05/24/2022 18:52:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=376
05/24/2022 18:52:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=379
05/24/2022 18:52:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=383
05/24/2022 18:52:42 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.2814814814814815 on epoch=383
05/24/2022 18:52:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=386
05/24/2022 18:52:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=389
05/24/2022 18:52:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=393
05/24/2022 18:52:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=396
05/24/2022 18:52:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=399
05/24/2022 18:52:56 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.3050108932461874 on epoch=399
05/24/2022 18:52:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=403
05/24/2022 18:53:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=406
05/24/2022 18:53:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=409
05/24/2022 18:53:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=413
05/24/2022 18:53:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=416
05/24/2022 18:53:09 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.3498019595580571 on epoch=416
05/24/2022 18:53:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=419
05/24/2022 18:53:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=423
05/24/2022 18:53:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=426
05/24/2022 18:53:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=429
05/24/2022 18:53:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=433
05/24/2022 18:53:23 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.27777777777777773 on epoch=433
05/24/2022 18:53:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=436
05/24/2022 18:53:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=439
05/24/2022 18:53:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=443
05/24/2022 18:53:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=446
05/24/2022 18:53:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=449
05/24/2022 18:53:36 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.320244055068836 on epoch=449
05/24/2022 18:53:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=453
05/24/2022 18:53:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=456
05/24/2022 18:53:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=459
05/24/2022 18:53:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=463
05/24/2022 18:53:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=466
05/24/2022 18:53:49 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.37812602518484867 on epoch=466
05/24/2022 18:53:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3642439431913116 -> 0.37812602518484867 on epoch=466, global_step=1400
05/24/2022 18:53:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=469
05/24/2022 18:53:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=473
05/24/2022 18:53:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=476
05/24/2022 18:53:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=479
05/24/2022 18:54:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=483
05/24/2022 18:54:03 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.33921568627450976 on epoch=483
05/24/2022 18:54:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=486
05/24/2022 18:54:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=489
05/24/2022 18:54:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=493
05/24/2022 18:54:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=496
05/24/2022 18:54:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=499
05/24/2022 18:54:17 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.2814814814814815 on epoch=499
05/24/2022 18:54:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=503
05/24/2022 18:54:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=506
05/24/2022 18:54:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=509
05/24/2022 18:54:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=513
05/24/2022 18:54:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=516
05/24/2022 18:54:30 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.34263448969331317 on epoch=516
05/24/2022 18:54:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=519
05/24/2022 18:54:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=523
05/24/2022 18:54:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=526
05/24/2022 18:54:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=529
05/24/2022 18:54:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=533
05/24/2022 18:54:44 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.24834864178229432 on epoch=533
05/24/2022 18:54:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=536
05/24/2022 18:54:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=539
05/24/2022 18:54:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=543
05/24/2022 18:54:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=546
05/24/2022 18:54:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
05/24/2022 18:54:58 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.3257647891794233 on epoch=549
05/24/2022 18:55:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=553
05/24/2022 18:55:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=556
05/24/2022 18:55:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=559
05/24/2022 18:55:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=563
05/24/2022 18:55:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=566
05/24/2022 18:55:11 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.2352676563202879 on epoch=566
05/24/2022 18:55:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=569
05/24/2022 18:55:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=573
05/24/2022 18:55:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=576
05/24/2022 18:55:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=579
05/24/2022 18:55:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=583
05/24/2022 18:55:25 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.12255344775669978 on epoch=583
05/24/2022 18:55:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
05/24/2022 18:55:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=589
05/24/2022 18:55:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=593
05/24/2022 18:55:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=596
05/24/2022 18:55:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=599
05/24/2022 18:55:39 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.21313131313131312 on epoch=599
05/24/2022 18:55:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=603
05/24/2022 18:55:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
05/24/2022 18:55:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=609
05/24/2022 18:55:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=613
05/24/2022 18:55:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=616
05/24/2022 18:55:52 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.2772404900064474 on epoch=616
05/24/2022 18:55:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
05/24/2022 18:55:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=623
05/24/2022 18:56:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=626
05/24/2022 18:56:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=629
05/24/2022 18:56:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=633
05/24/2022 18:56:06 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.291005291005291 on epoch=633
05/24/2022 18:56:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
05/24/2022 18:56:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=639
05/24/2022 18:56:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
05/24/2022 18:56:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=646
05/24/2022 18:56:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=649
05/24/2022 18:56:20 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.3220976638958794 on epoch=649
05/24/2022 18:56:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
05/24/2022 18:56:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=656
05/24/2022 18:56:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=659
05/24/2022 18:56:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=663
05/24/2022 18:56:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=666
05/24/2022 18:56:33 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.25095982448923626 on epoch=666
05/24/2022 18:56:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=669
05/24/2022 18:56:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=673
05/24/2022 18:56:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 18:56:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=679
05/24/2022 18:56:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=683
05/24/2022 18:56:47 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.2910477322242029 on epoch=683
05/24/2022 18:56:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=686
05/24/2022 18:56:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
05/24/2022 18:56:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=693
05/24/2022 18:56:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=696
05/24/2022 18:57:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=699
05/24/2022 18:57:01 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.3624931000551996 on epoch=699
05/24/2022 18:57:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
05/24/2022 18:57:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=706
05/24/2022 18:57:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
05/24/2022 18:57:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/24/2022 18:57:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 18:57:15 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.2922305764411028 on epoch=716
05/24/2022 18:57:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=719
05/24/2022 18:57:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/24/2022 18:57:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
05/24/2022 18:57:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/24/2022 18:57:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=733
05/24/2022 18:57:28 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.20921622237411713 on epoch=733
05/24/2022 18:57:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=736
05/24/2022 18:57:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=739
05/24/2022 18:57:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
05/24/2022 18:57:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
05/24/2022 18:57:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 18:57:42 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.3063973063973064 on epoch=749
05/24/2022 18:57:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/24/2022 18:57:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=756
05/24/2022 18:57:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=759
05/24/2022 18:57:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
05/24/2022 18:57:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=766
05/24/2022 18:57:56 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.353106392540164 on epoch=766
05/24/2022 18:57:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=769
05/24/2022 18:58:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=773
05/24/2022 18:58:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=776
05/24/2022 18:58:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=779
05/24/2022 18:58:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
05/24/2022 18:58:10 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.20224358974358975 on epoch=783
05/24/2022 18:58:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
05/24/2022 18:58:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
05/24/2022 18:58:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=793
05/24/2022 18:58:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=796
05/24/2022 18:58:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/24/2022 18:58:24 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.1353192559074912 on epoch=799
05/24/2022 18:58:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=803
05/24/2022 18:58:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/24/2022 18:58:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=809
05/24/2022 18:58:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
05/24/2022 18:58:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=816
05/24/2022 18:58:37 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.22823255176196355 on epoch=816
05/24/2022 18:58:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=819
05/24/2022 18:58:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
05/24/2022 18:58:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
05/24/2022 18:58:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=829
05/24/2022 18:58:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=833
05/24/2022 18:58:51 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.14590347923681257 on epoch=833
05/24/2022 18:58:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 18:58:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/24/2022 18:58:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
05/24/2022 18:59:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/24/2022 18:59:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 18:59:05 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.26638847758783146 on epoch=849
05/24/2022 18:59:07 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
05/24/2022 18:59:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=856
05/24/2022 18:59:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
05/24/2022 18:59:15 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
05/24/2022 18:59:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 18:59:19 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.17791208791208796 on epoch=866
05/24/2022 18:59:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
05/24/2022 18:59:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=873
05/24/2022 18:59:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=876
05/24/2022 18:59:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/24/2022 18:59:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
05/24/2022 18:59:33 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.17334445139323187 on epoch=883
05/24/2022 18:59:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=886
05/24/2022 18:59:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/24/2022 18:59:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/24/2022 18:59:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 18:59:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 18:59:47 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.3252002002002002 on epoch=899
05/24/2022 18:59:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 18:59:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=906
05/24/2022 18:59:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/24/2022 18:59:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=913
05/24/2022 19:00:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=916
05/24/2022 19:00:02 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.29207349730152843 on epoch=916
05/24/2022 19:00:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 19:00:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/24/2022 19:00:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=926
05/24/2022 19:00:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/24/2022 19:00:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 19:00:17 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.3315696649029982 on epoch=933
05/24/2022 19:00:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/24/2022 19:00:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 19:00:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 19:00:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 19:00:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 19:00:31 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.3323784614107195 on epoch=949
05/24/2022 19:00:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=953
05/24/2022 19:00:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=956
05/24/2022 19:00:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=959
05/24/2022 19:00:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=963
05/24/2022 19:00:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
05/24/2022 19:00:45 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.29365079365079366 on epoch=966
05/24/2022 19:00:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=969
05/24/2022 19:00:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
05/24/2022 19:00:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 19:00:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=979
05/24/2022 19:00:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/24/2022 19:00:59 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.38095238095238093 on epoch=983
05/24/2022 19:00:59 - INFO - __main__ - Saving model with best Classification-F1: 0.37812602518484867 -> 0.38095238095238093 on epoch=983, global_step=2950
05/24/2022 19:01:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 19:01:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=989
05/24/2022 19:01:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
05/24/2022 19:01:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 19:01:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/24/2022 19:01:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:01:13 - INFO - __main__ - Printing 3 examples
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:01:13 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:01:13 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:01:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:01:13 - INFO - __main__ - Printing 3 examples
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:01:13 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:01:13 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:01:13 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.4041514041514042 on epoch=999
05/24/2022 19:01:13 - INFO - __main__ - Saving model with best Classification-F1: 0.38095238095238093 -> 0.4041514041514042 on epoch=999, global_step=3000
05/24/2022 19:01:13 - INFO - __main__ - save last model!
05/24/2022 19:01:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 19:01:13 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 19:01:13 - INFO - __main__ - Printing 3 examples
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 19:01:13 - INFO - __main__ - ['contradiction']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 19:01:13 - INFO - __main__ - ['entailment']
05/24/2022 19:01:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 19:01:13 - INFO - __main__ - ['contradiction']
05/24/2022 19:01:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:01:14 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:01:15 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 19:01:28 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:01:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:01:29 - INFO - __main__ - Starting training!
05/24/2022 19:01:46 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_21_0.3_8_predictions.txt
05/24/2022 19:01:46 - INFO - __main__ - Classification-F1 on test data: 0.1533
05/24/2022 19:01:46 - INFO - __main__ - prefix=anli_16_21, lr=0.3, bsz=8, dev_performance=0.4041514041514042, test_performance=0.1532820835535836
05/24/2022 19:01:46 - INFO - __main__ - Running ... prefix=anli_16_21, lr=0.2, bsz=8 ...
05/24/2022 19:01:47 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:01:47 - INFO - __main__ - Printing 3 examples
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:01:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:01:47 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:01:47 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:01:47 - INFO - __main__ - Printing 3 examples
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: Herrlisheim is a commune in the Bas-Rhin department in Grand Est in north-eastern France. The town dates from the 8th century. Herrlisheim was the scene of very heavy fighting during "Operation Nordwind", an offensive launched by the German Army during World War II that inflicted considerable damage to the town. [SEP] hypothesis: Operation Nordwind was a Germany operation that damaged this French town
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: Fulletby is a village and a civil parish in the East Lindsey district of Lincolnshire, England. It is situated in the Lincolnshire Wolds, and 3 mi north-east from Horncastle, 9 mi south from Louth, and 8 mi north-west from Spilsby. The parish covers approximately 1950 acre . At the time of the 2011 census the population remained less than 100 and is included in the civil parish of Low Toynton. [SEP] hypothesis: Fulletby is not a city in England 
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ -  [anli] premise: Jia Zhangke (born 24 May 1970) is a Chinese film director and screenwriter. He is generally regarded as a leading figure of the "Sixth Generation" movement of Chinese cinema, a group that also includes such figures as Wang Xiaoshuai, Lou Ye, Wang Quan'an and Zhang Yuan. [SEP] hypothesis: Jia Zhangke was not alive in 1960.
05/24/2022 19:01:47 - INFO - __main__ - ['entailment']
05/24/2022 19:01:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:01:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:01:47 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:02:05 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:02:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:02:05 - INFO - __main__ - Starting training!
05/24/2022 19:02:08 - INFO - __main__ - Step 10 Global step 10 Train loss 1.22 on epoch=3
05/24/2022 19:02:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.82 on epoch=6
05/24/2022 19:02:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=9
05/24/2022 19:02:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=13
05/24/2022 19:02:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=16
05/24/2022 19:02:19 - INFO - __main__ - Global step 50 Train loss 0.77 Classification-F1 0.26332288401253917 on epoch=16
05/24/2022 19:02:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.26332288401253917 on epoch=16, global_step=50
05/24/2022 19:02:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=19
05/24/2022 19:02:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=23
05/24/2022 19:02:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.60 on epoch=26
05/24/2022 19:02:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/24/2022 19:02:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.62 on epoch=33
05/24/2022 19:02:33 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.3149540517961571 on epoch=33
05/24/2022 19:02:33 - INFO - __main__ - Saving model with best Classification-F1: 0.26332288401253917 -> 0.3149540517961571 on epoch=33, global_step=100
05/24/2022 19:02:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
05/24/2022 19:02:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
05/24/2022 19:02:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=43
05/24/2022 19:02:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=46
05/24/2022 19:02:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=49
05/24/2022 19:02:46 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 19:02:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/24/2022 19:02:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=56
05/24/2022 19:02:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 19:02:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=63
05/24/2022 19:02:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=66
05/24/2022 19:03:00 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.2754036087369421 on epoch=66
05/24/2022 19:03:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=69
05/24/2022 19:03:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=73
05/24/2022 19:03:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
05/24/2022 19:03:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
05/24/2022 19:03:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=83
05/24/2022 19:03:14 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.23301985370950887 on epoch=83
05/24/2022 19:03:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=86
05/24/2022 19:03:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=89
05/24/2022 19:03:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=93
05/24/2022 19:03:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=96
05/24/2022 19:03:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=99
05/24/2022 19:03:27 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.2623951182303585 on epoch=99
05/24/2022 19:03:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=103
05/24/2022 19:03:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=106
05/24/2022 19:03:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
05/24/2022 19:03:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=113
05/24/2022 19:03:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=116
05/24/2022 19:03:41 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.28444444444444444 on epoch=116
05/24/2022 19:03:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=119
05/24/2022 19:03:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=123
05/24/2022 19:03:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=126
05/24/2022 19:03:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=129
05/24/2022 19:03:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=133
05/24/2022 19:03:54 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.27777777777777773 on epoch=133
05/24/2022 19:03:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/24/2022 19:03:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=139
05/24/2022 19:04:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=143
05/24/2022 19:04:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=146
05/24/2022 19:04:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=149
05/24/2022 19:04:08 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.27636363636363637 on epoch=149
05/24/2022 19:04:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=153
05/24/2022 19:04:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=156
05/24/2022 19:04:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/24/2022 19:04:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=163
05/24/2022 19:04:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=166
05/24/2022 19:04:21 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.24969696969696967 on epoch=166
05/24/2022 19:04:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
05/24/2022 19:04:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=173
05/24/2022 19:04:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=176
05/24/2022 19:04:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=179
05/24/2022 19:04:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=183
05/24/2022 19:04:35 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.3340094658553076 on epoch=183
05/24/2022 19:04:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3149540517961571 -> 0.3340094658553076 on epoch=183, global_step=550
05/24/2022 19:04:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=186
05/24/2022 19:04:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=189
05/24/2022 19:04:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=193
05/24/2022 19:04:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=196
05/24/2022 19:04:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=199
05/24/2022 19:04:48 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.2334096109839817 on epoch=199
05/24/2022 19:04:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=203
05/24/2022 19:04:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=206
05/24/2022 19:04:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
05/24/2022 19:04:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=213
05/24/2022 19:05:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=216
05/24/2022 19:05:02 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.2334096109839817 on epoch=216
05/24/2022 19:05:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=219
05/24/2022 19:05:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=223
05/24/2022 19:05:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=226
05/24/2022 19:05:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=229
05/24/2022 19:05:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=233
05/24/2022 19:05:16 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.2622874446773818 on epoch=233
05/24/2022 19:05:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=236
05/24/2022 19:05:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=239
05/24/2022 19:05:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=243
05/24/2022 19:05:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=246
05/24/2022 19:05:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=249
05/24/2022 19:05:29 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.2770673486786019 on epoch=249
05/24/2022 19:05:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=253
05/24/2022 19:05:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=256
05/24/2022 19:05:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=259
05/24/2022 19:05:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=263
05/24/2022 19:05:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
05/24/2022 19:05:42 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.2490842490842491 on epoch=266
05/24/2022 19:05:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=269
05/24/2022 19:05:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=273
05/24/2022 19:05:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=276
05/24/2022 19:05:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=279
05/24/2022 19:05:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
05/24/2022 19:05:56 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.22589725545390568 on epoch=283
05/24/2022 19:05:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=286
05/24/2022 19:06:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=289
05/24/2022 19:06:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=293
05/24/2022 19:06:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=296
05/24/2022 19:06:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=299
05/24/2022 19:06:10 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.31628303495311166 on epoch=299
05/24/2022 19:06:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
05/24/2022 19:06:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=306
05/24/2022 19:06:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=309
05/24/2022 19:06:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=313
05/24/2022 19:06:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=316
05/24/2022 19:06:23 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.3499999999999999 on epoch=316
05/24/2022 19:06:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3340094658553076 -> 0.3499999999999999 on epoch=316, global_step=950
05/24/2022 19:06:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=319
05/24/2022 19:06:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=323
05/24/2022 19:06:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=326
05/24/2022 19:06:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
05/24/2022 19:06:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=333
05/24/2022 19:06:37 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.3499999999999999 on epoch=333
05/24/2022 19:06:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=336
05/24/2022 19:06:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=339
05/24/2022 19:06:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=343
05/24/2022 19:06:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=346
05/24/2022 19:06:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=349
05/24/2022 19:06:50 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.2599765487590385 on epoch=349
05/24/2022 19:06:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=353
05/24/2022 19:06:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=356
05/24/2022 19:06:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=359
05/24/2022 19:07:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=363
05/24/2022 19:07:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=366
05/24/2022 19:07:04 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.35972850678733037 on epoch=366
05/24/2022 19:07:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3499999999999999 -> 0.35972850678733037 on epoch=366, global_step=1100
05/24/2022 19:07:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=369
05/24/2022 19:07:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=373
05/24/2022 19:07:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=376
05/24/2022 19:07:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=379
05/24/2022 19:07:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=383
05/24/2022 19:07:18 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.362565196775723 on epoch=383
05/24/2022 19:07:18 - INFO - __main__ - Saving model with best Classification-F1: 0.35972850678733037 -> 0.362565196775723 on epoch=383, global_step=1150
05/24/2022 19:07:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=386
05/24/2022 19:07:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=389
05/24/2022 19:07:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=393
05/24/2022 19:07:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=396
05/24/2022 19:07:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=399
05/24/2022 19:07:32 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.3304284676833696 on epoch=399
05/24/2022 19:07:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=403
05/24/2022 19:07:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=406
05/24/2022 19:07:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=409
05/24/2022 19:07:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=413
05/24/2022 19:07:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=416
05/24/2022 19:07:46 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.2890083632019116 on epoch=416
05/24/2022 19:07:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=419
05/24/2022 19:07:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=423
05/24/2022 19:07:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=426
05/24/2022 19:07:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=429
05/24/2022 19:07:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=433
05/24/2022 19:08:00 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.2763157894736842 on epoch=433
05/24/2022 19:08:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=436
05/24/2022 19:08:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=439
05/24/2022 19:08:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=443
05/24/2022 19:08:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=446
05/24/2022 19:08:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=449
05/24/2022 19:08:13 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.2534875419433141 on epoch=449
05/24/2022 19:08:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=453
05/24/2022 19:08:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=456
05/24/2022 19:08:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=459
05/24/2022 19:08:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=463
05/24/2022 19:08:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=466
05/24/2022 19:08:27 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.3458319458319458 on epoch=466
05/24/2022 19:08:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=469
05/24/2022 19:08:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=473
05/24/2022 19:08:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=476
05/24/2022 19:08:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=479
05/24/2022 19:08:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
05/24/2022 19:08:41 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.38054695562435503 on epoch=483
05/24/2022 19:08:42 - INFO - __main__ - Saving model with best Classification-F1: 0.362565196775723 -> 0.38054695562435503 on epoch=483, global_step=1450
05/24/2022 19:08:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=486
05/24/2022 19:08:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/24/2022 19:08:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=493
05/24/2022 19:08:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=496
05/24/2022 19:08:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=499
05/24/2022 19:08:56 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.3283678062372736 on epoch=499
05/24/2022 19:08:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=503
05/24/2022 19:09:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=506
05/24/2022 19:09:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=509
05/24/2022 19:09:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
05/24/2022 19:09:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=516
05/24/2022 19:09:09 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.39155822489155817 on epoch=516
05/24/2022 19:09:10 - INFO - __main__ - Saving model with best Classification-F1: 0.38054695562435503 -> 0.39155822489155817 on epoch=516, global_step=1550
05/24/2022 19:09:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=519
05/24/2022 19:09:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
05/24/2022 19:09:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
05/24/2022 19:09:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=529
05/24/2022 19:09:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=533
05/24/2022 19:09:23 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.3479870848291901 on epoch=533
05/24/2022 19:09:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=536
05/24/2022 19:09:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=539
05/24/2022 19:09:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=543
05/24/2022 19:09:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=546
05/24/2022 19:09:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
05/24/2022 19:09:37 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.2756862745098039 on epoch=549
05/24/2022 19:09:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=553
05/24/2022 19:09:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/24/2022 19:09:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=559
05/24/2022 19:09:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=563
05/24/2022 19:09:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=566
05/24/2022 19:09:51 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.21377091377091376 on epoch=566
05/24/2022 19:09:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=569
05/24/2022 19:09:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=573
05/24/2022 19:09:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=576
05/24/2022 19:10:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=579
05/24/2022 19:10:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=583
05/24/2022 19:10:05 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.315398293029872 on epoch=583
05/24/2022 19:10:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 19:10:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=589
05/24/2022 19:10:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
05/24/2022 19:10:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=596
05/24/2022 19:10:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=599
05/24/2022 19:10:19 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.31696564305259955 on epoch=599
05/24/2022 19:10:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=603
05/24/2022 19:10:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=606
05/24/2022 19:10:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=609
05/24/2022 19:10:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
05/24/2022 19:10:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=616
05/24/2022 19:10:34 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.30256410256410254 on epoch=616
05/24/2022 19:10:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
05/24/2022 19:10:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/24/2022 19:10:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=626
05/24/2022 19:10:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=629
05/24/2022 19:10:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/24/2022 19:10:48 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.2754673283705542 on epoch=633
05/24/2022 19:10:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/24/2022 19:10:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=639
05/24/2022 19:10:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=643
05/24/2022 19:10:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=646
05/24/2022 19:11:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=649
05/24/2022 19:11:02 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.3190180878552971 on epoch=649
05/24/2022 19:11:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=653
05/24/2022 19:11:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=656
05/24/2022 19:11:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/24/2022 19:11:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
05/24/2022 19:11:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=666
05/24/2022 19:11:16 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.3087365591397849 on epoch=666
05/24/2022 19:11:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=669
05/24/2022 19:11:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=673
05/24/2022 19:11:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
05/24/2022 19:11:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=679
05/24/2022 19:11:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=683
05/24/2022 19:11:29 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.2656657068421774 on epoch=683
05/24/2022 19:11:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=686
05/24/2022 19:11:34 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
05/24/2022 19:11:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
05/24/2022 19:11:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=696
05/24/2022 19:11:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=699
05/24/2022 19:11:43 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.24454545454545454 on epoch=699
05/24/2022 19:11:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=703
05/24/2022 19:11:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=706
05/24/2022 19:11:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/24/2022 19:11:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=713
05/24/2022 19:11:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=716
05/24/2022 19:11:57 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.2650034176349966 on epoch=716
05/24/2022 19:12:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/24/2022 19:12:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
05/24/2022 19:12:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/24/2022 19:12:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=729
05/24/2022 19:12:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=733
05/24/2022 19:12:11 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.4161838161838161 on epoch=733
05/24/2022 19:12:11 - INFO - __main__ - Saving model with best Classification-F1: 0.39155822489155817 -> 0.4161838161838161 on epoch=733, global_step=2200
05/24/2022 19:12:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
05/24/2022 19:12:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=739
05/24/2022 19:12:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
05/24/2022 19:12:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
05/24/2022 19:12:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
05/24/2022 19:12:25 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.39815718157181573 on epoch=749
05/24/2022 19:12:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
05/24/2022 19:12:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=756
05/24/2022 19:12:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=759
05/24/2022 19:12:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
05/24/2022 19:12:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=766
05/24/2022 19:12:39 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.3918070034750067 on epoch=766
05/24/2022 19:12:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=769
05/24/2022 19:12:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=773
05/24/2022 19:12:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/24/2022 19:12:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/24/2022 19:12:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=783
05/24/2022 19:12:53 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.3625854700854701 on epoch=783
05/24/2022 19:12:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
05/24/2022 19:12:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 19:13:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=793
05/24/2022 19:13:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/24/2022 19:13:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=799
05/24/2022 19:13:07 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.40079772079772075 on epoch=799
05/24/2022 19:13:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/24/2022 19:13:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/24/2022 19:13:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 19:13:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/24/2022 19:13:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=816
05/24/2022 19:13:21 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.30646551724137927 on epoch=816
05/24/2022 19:13:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/24/2022 19:13:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=823
05/24/2022 19:13:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=826
05/24/2022 19:13:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/24/2022 19:13:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=833
05/24/2022 19:13:35 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.34259259259259256 on epoch=833
05/24/2022 19:13:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 19:13:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
05/24/2022 19:13:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=843
05/24/2022 19:13:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
05/24/2022 19:13:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 19:13:49 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.32658695561921364 on epoch=849
05/24/2022 19:13:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=853
05/24/2022 19:13:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=856
05/24/2022 19:13:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 19:13:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 19:14:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 19:14:03 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.3444633444633445 on epoch=866
05/24/2022 19:14:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=869
05/24/2022 19:14:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 19:14:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=876
05/24/2022 19:14:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 19:14:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/24/2022 19:14:17 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.32083333333333336 on epoch=883
05/24/2022 19:14:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 19:14:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=889
05/24/2022 19:14:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/24/2022 19:14:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/24/2022 19:14:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/24/2022 19:14:31 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.36149961149961146 on epoch=899
05/24/2022 19:14:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=903
05/24/2022 19:14:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/24/2022 19:14:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=909
05/24/2022 19:14:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 19:14:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
05/24/2022 19:14:45 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.3444633444633445 on epoch=916
05/24/2022 19:14:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 19:14:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=923
05/24/2022 19:14:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=926
05/24/2022 19:14:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
05/24/2022 19:14:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/24/2022 19:14:59 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.34338102808691046 on epoch=933
05/24/2022 19:15:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/24/2022 19:15:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=939
05/24/2022 19:15:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
05/24/2022 19:15:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=946
05/24/2022 19:15:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/24/2022 19:15:13 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.34339525283797734 on epoch=949
05/24/2022 19:15:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 19:15:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=956
05/24/2022 19:15:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
05/24/2022 19:15:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/24/2022 19:15:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/24/2022 19:15:27 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.36073029551290414 on epoch=966
05/24/2022 19:15:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=969
05/24/2022 19:15:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
05/24/2022 19:15:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 19:15:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/24/2022 19:15:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/24/2022 19:15:41 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.30522486772486773 on epoch=983
05/24/2022 19:15:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 19:15:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/24/2022 19:15:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=993
05/24/2022 19:15:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 19:15:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 19:15:55 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:15:55 - INFO - __main__ - Printing 3 examples
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:15:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:15:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.3098544973544974 on epoch=999
05/24/2022 19:15:55 - INFO - __main__ - save last model!
05/24/2022 19:15:55 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:15:55 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:15:55 - INFO - __main__ - Printing 3 examples
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:15:55 - INFO - __main__ - ['neutral']
05/24/2022 19:15:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:15:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 19:15:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:15:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 19:15:55 - INFO - __main__ - Printing 3 examples
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 19:15:55 - INFO - __main__ - ['contradiction']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 19:15:55 - INFO - __main__ - ['entailment']
05/24/2022 19:15:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 19:15:55 - INFO - __main__ - ['contradiction']
05/24/2022 19:15:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:15:55 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:15:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:15:57 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 19:16:14 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:16:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:16:15 - INFO - __main__ - Starting training!
05/24/2022 19:16:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_21_0.2_8_predictions.txt
05/24/2022 19:16:25 - INFO - __main__ - Classification-F1 on test data: 0.0534
05/24/2022 19:16:25 - INFO - __main__ - prefix=anli_16_21, lr=0.2, bsz=8, dev_performance=0.4161838161838161, test_performance=0.053362189023583556
05/24/2022 19:16:25 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.5, bsz=8 ...
05/24/2022 19:16:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:16:26 - INFO - __main__ - Printing 3 examples
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:16:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:16:26 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:16:26 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:16:26 - INFO - __main__ - Printing 3 examples
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:16:26 - INFO - __main__ - ['neutral']
05/24/2022 19:16:26 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:16:26 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:16:27 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:16:44 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:16:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:16:45 - INFO - __main__ - Starting training!
05/24/2022 19:16:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.95 on epoch=3
05/24/2022 19:16:50 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=6
05/24/2022 19:16:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=9
05/24/2022 19:16:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=13
05/24/2022 19:16:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=16
05/24/2022 19:16:59 - INFO - __main__ - Global step 50 Train loss 0.67 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 19:16:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/24/2022 19:17:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=19
05/24/2022 19:17:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=23
05/24/2022 19:17:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=26
05/24/2022 19:17:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/24/2022 19:17:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=33
05/24/2022 19:17:13 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.24611708482676223 on epoch=33
05/24/2022 19:17:13 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.24611708482676223 on epoch=33, global_step=100
05/24/2022 19:17:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=36
05/24/2022 19:17:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=39
05/24/2022 19:17:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
05/24/2022 19:17:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=46
05/24/2022 19:17:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=49
05/24/2022 19:17:27 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.27777777777777773 on epoch=49
05/24/2022 19:17:27 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.27777777777777773 on epoch=49, global_step=150
05/24/2022 19:17:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=53
05/24/2022 19:17:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=56
05/24/2022 19:17:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=59
05/24/2022 19:17:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/24/2022 19:17:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=66
05/24/2022 19:17:40 - INFO - __main__ - Global step 200 Train loss 0.42 Classification-F1 0.27636363636363637 on epoch=66
05/24/2022 19:17:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/24/2022 19:17:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/24/2022 19:17:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=76
05/24/2022 19:17:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.36 on epoch=79
05/24/2022 19:17:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.38 on epoch=83
05/24/2022 19:17:54 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.23301985370950887 on epoch=83
05/24/2022 19:17:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=86
05/24/2022 19:17:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=89
05/24/2022 19:18:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=93
05/24/2022 19:18:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=96
05/24/2022 19:18:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.34 on epoch=99
05/24/2022 19:18:08 - INFO - __main__ - Global step 300 Train loss 0.36 Classification-F1 0.19878787878787882 on epoch=99
05/24/2022 19:18:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=103
05/24/2022 19:18:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=106
05/24/2022 19:18:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=109
05/24/2022 19:18:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=113
05/24/2022 19:18:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=116
05/24/2022 19:18:22 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.22501747030048916 on epoch=116
05/24/2022 19:18:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.34 on epoch=119
05/24/2022 19:18:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=123
05/24/2022 19:18:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=126
05/24/2022 19:18:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=129
05/24/2022 19:18:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=133
05/24/2022 19:18:36 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.20886615515771526 on epoch=133
05/24/2022 19:18:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=136
05/24/2022 19:18:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=139
05/24/2022 19:18:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=143
05/24/2022 19:18:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/24/2022 19:18:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=149
05/24/2022 19:18:50 - INFO - __main__ - Global step 450 Train loss 0.29 Classification-F1 0.2229630982827914 on epoch=149
05/24/2022 19:18:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=153
05/24/2022 19:18:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=156
05/24/2022 19:18:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=159
05/24/2022 19:19:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/24/2022 19:19:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=166
05/24/2022 19:19:04 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.2127563669145819 on epoch=166
05/24/2022 19:19:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=169
05/24/2022 19:19:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=173
05/24/2022 19:19:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
05/24/2022 19:19:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.14 on epoch=179
05/24/2022 19:19:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=183
05/24/2022 19:19:17 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.23391812865497075 on epoch=183
05/24/2022 19:19:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=186
05/24/2022 19:19:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=189
05/24/2022 19:19:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=193
05/24/2022 19:19:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=196
05/24/2022 19:19:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=199
05/24/2022 19:19:31 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.20050125313283207 on epoch=199
05/24/2022 19:19:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.10 on epoch=203
05/24/2022 19:19:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.31 on epoch=206
05/24/2022 19:19:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=209
05/24/2022 19:19:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=213
05/24/2022 19:19:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=216
05/24/2022 19:19:45 - INFO - __main__ - Global step 650 Train loss 0.17 Classification-F1 0.26566416040100255 on epoch=216
05/24/2022 19:19:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=219
05/24/2022 19:19:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=223
05/24/2022 19:19:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
05/24/2022 19:19:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=229
05/24/2022 19:19:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=233
05/24/2022 19:19:59 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.23632478632478635 on epoch=233
05/24/2022 19:20:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=236
05/24/2022 19:20:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=239
05/24/2022 19:20:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=243
05/24/2022 19:20:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=246
05/24/2022 19:20:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
05/24/2022 19:20:12 - INFO - __main__ - Global step 750 Train loss 0.11 Classification-F1 0.25092592592592594 on epoch=249
05/24/2022 19:20:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=253
05/24/2022 19:20:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=256
05/24/2022 19:20:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=259
05/24/2022 19:20:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=263
05/24/2022 19:20:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=266
05/24/2022 19:20:27 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.26294756294756294 on epoch=266
05/24/2022 19:20:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=269
05/24/2022 19:20:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=273
05/24/2022 19:20:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=276
05/24/2022 19:20:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=279
05/24/2022 19:20:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
05/24/2022 19:20:41 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.2771030139451192 on epoch=283
05/24/2022 19:20:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
05/24/2022 19:20:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
05/24/2022 19:20:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=293
05/24/2022 19:20:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=296
05/24/2022 19:20:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=299
05/24/2022 19:20:55 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.27698430137454527 on epoch=299
05/24/2022 19:20:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=303
05/24/2022 19:21:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=306
05/24/2022 19:21:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
05/24/2022 19:21:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=313
05/24/2022 19:21:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=316
05/24/2022 19:21:09 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.2589662833565272 on epoch=316
05/24/2022 19:21:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=319
05/24/2022 19:21:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
05/24/2022 19:21:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=326
05/24/2022 19:21:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=329
05/24/2022 19:21:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
05/24/2022 19:21:23 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.2056690837178642 on epoch=333
05/24/2022 19:21:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=336
05/24/2022 19:21:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=339
05/24/2022 19:21:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
05/24/2022 19:21:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
05/24/2022 19:21:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=349
05/24/2022 19:21:37 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.24230019493177388 on epoch=349
05/24/2022 19:21:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=353
05/24/2022 19:21:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=356
05/24/2022 19:21:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/24/2022 19:21:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=363
05/24/2022 19:21:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
05/24/2022 19:21:51 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.24143692564745198 on epoch=366
05/24/2022 19:21:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=369
05/24/2022 19:21:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=373
05/24/2022 19:21:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=376
05/24/2022 19:22:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
05/24/2022 19:22:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=383
05/24/2022 19:22:05 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.2767417860657115 on epoch=383
05/24/2022 19:22:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
05/24/2022 19:22:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
05/24/2022 19:22:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
05/24/2022 19:22:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=396
05/24/2022 19:22:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=399
05/24/2022 19:22:19 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.18614718614718614 on epoch=399
05/24/2022 19:22:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=403
05/24/2022 19:22:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
05/24/2022 19:22:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
05/24/2022 19:22:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/24/2022 19:22:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/24/2022 19:22:33 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.24203574203574205 on epoch=416
05/24/2022 19:22:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=419
05/24/2022 19:22:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=423
05/24/2022 19:22:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=426
05/24/2022 19:22:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
05/24/2022 19:22:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=433
05/24/2022 19:22:47 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.25884531147689044 on epoch=433
05/24/2022 19:22:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=436
05/24/2022 19:22:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=439
05/24/2022 19:22:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
05/24/2022 19:22:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
05/24/2022 19:22:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
05/24/2022 19:23:01 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.18859649122807018 on epoch=449
05/24/2022 19:23:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=453
05/24/2022 19:23:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/24/2022 19:23:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
05/24/2022 19:23:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 19:23:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
05/24/2022 19:23:15 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.24400554400554397 on epoch=466
05/24/2022 19:23:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=469
05/24/2022 19:23:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/24/2022 19:23:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/24/2022 19:23:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=479
05/24/2022 19:23:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=483
05/24/2022 19:23:29 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.29168973791066816 on epoch=483
05/24/2022 19:23:29 - INFO - __main__ - Saving model with best Classification-F1: 0.27777777777777773 -> 0.29168973791066816 on epoch=483, global_step=1450
05/24/2022 19:23:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/24/2022 19:23:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
05/24/2022 19:23:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/24/2022 19:23:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=496
05/24/2022 19:23:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
05/24/2022 19:23:43 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.2425925925925926 on epoch=499
05/24/2022 19:23:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=503
05/24/2022 19:23:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
05/24/2022 19:23:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
05/24/2022 19:23:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
05/24/2022 19:23:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/24/2022 19:23:58 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.2589662833565272 on epoch=516
05/24/2022 19:24:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=519
05/24/2022 19:24:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
05/24/2022 19:24:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=526
05/24/2022 19:24:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=529
05/24/2022 19:24:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/24/2022 19:24:12 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.243859649122807 on epoch=533
05/24/2022 19:24:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/24/2022 19:24:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/24/2022 19:24:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
05/24/2022 19:24:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/24/2022 19:24:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=549
05/24/2022 19:24:26 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2600199686207388 on epoch=549
05/24/2022 19:24:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
05/24/2022 19:24:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/24/2022 19:24:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/24/2022 19:24:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
05/24/2022 19:24:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/24/2022 19:24:40 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.2192192192192192 on epoch=566
05/24/2022 19:24:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=569
05/24/2022 19:24:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/24/2022 19:24:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/24/2022 19:24:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=579
05/24/2022 19:24:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/24/2022 19:24:55 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.27698032961190855 on epoch=583
05/24/2022 19:24:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/24/2022 19:25:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=589
05/24/2022 19:25:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
05/24/2022 19:25:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/24/2022 19:25:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=599
05/24/2022 19:25:09 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.243859649122807 on epoch=599
05/24/2022 19:25:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/24/2022 19:25:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/24/2022 19:25:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=609
05/24/2022 19:25:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/24/2022 19:25:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/24/2022 19:25:23 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.242914979757085 on epoch=616
05/24/2022 19:25:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/24/2022 19:25:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/24/2022 19:25:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=626
05/24/2022 19:25:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/24/2022 19:25:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/24/2022 19:25:37 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.24365079365079367 on epoch=633
05/24/2022 19:25:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/24/2022 19:25:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=639
05/24/2022 19:25:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/24/2022 19:25:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=646
05/24/2022 19:25:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/24/2022 19:25:51 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.2765107212475633 on epoch=649
05/24/2022 19:25:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/24/2022 19:25:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=656
05/24/2022 19:25:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=659
05/24/2022 19:26:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
05/24/2022 19:26:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/24/2022 19:26:05 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.24400554400554397 on epoch=666
05/24/2022 19:26:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/24/2022 19:26:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/24/2022 19:26:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 19:26:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 19:26:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/24/2022 19:26:19 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.31296823138928404 on epoch=683
05/24/2022 19:26:19 - INFO - __main__ - Saving model with best Classification-F1: 0.29168973791066816 -> 0.31296823138928404 on epoch=683, global_step=2050
05/24/2022 19:26:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/24/2022 19:26:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/24/2022 19:26:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 19:26:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/24/2022 19:26:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/24/2022 19:26:33 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.2751322751322751 on epoch=699
05/24/2022 19:26:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=703
05/24/2022 19:26:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/24/2022 19:26:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 19:26:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/24/2022 19:26:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/24/2022 19:26:47 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.24143692564745198 on epoch=716
05/24/2022 19:26:50 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/24/2022 19:26:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/24/2022 19:26:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/24/2022 19:26:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/24/2022 19:27:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=733
05/24/2022 19:27:01 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.20404704944178625 on epoch=733
05/24/2022 19:27:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/24/2022 19:27:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/24/2022 19:27:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/24/2022 19:27:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 19:27:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/24/2022 19:27:15 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2768633294949084 on epoch=749
05/24/2022 19:27:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/24/2022 19:27:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/24/2022 19:27:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/24/2022 19:27:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/24/2022 19:27:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/24/2022 19:27:29 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.2430815430815431 on epoch=766
05/24/2022 19:27:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 19:27:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 19:27:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/24/2022 19:27:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 19:27:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 19:27:43 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.27535241738972016 on epoch=783
05/24/2022 19:27:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=786
05/24/2022 19:27:48 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 19:27:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 19:27:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 19:27:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/24/2022 19:27:57 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.22757343809975386 on epoch=799
05/24/2022 19:28:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/24/2022 19:28:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 19:28:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 19:28:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=813
05/24/2022 19:28:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/24/2022 19:28:12 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.2425925925925926 on epoch=816
05/24/2022 19:28:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/24/2022 19:28:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/24/2022 19:28:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/24/2022 19:28:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 19:28:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/24/2022 19:28:26 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.19647696476964768 on epoch=833
05/24/2022 19:28:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/24/2022 19:28:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/24/2022 19:28:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 19:28:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 19:28:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 19:28:40 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.17082467082467082 on epoch=849
05/24/2022 19:28:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/24/2022 19:28:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 19:28:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 19:28:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/24/2022 19:28:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 19:28:54 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2768633294949084 on epoch=866
05/24/2022 19:28:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 19:29:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 19:29:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 19:29:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/24/2022 19:29:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 19:29:09 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.20337301587301587 on epoch=883
05/24/2022 19:29:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/24/2022 19:29:14 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=889
05/24/2022 19:29:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 19:29:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 19:29:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 19:29:23 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.1869345045239638 on epoch=899
05/24/2022 19:29:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 19:29:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 19:29:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 19:29:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 19:29:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=916
05/24/2022 19:29:37 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2413014518277676 on epoch=916
05/24/2022 19:29:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 19:29:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 19:29:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=926
05/24/2022 19:29:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 19:29:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/24/2022 19:29:52 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.22582096266306792 on epoch=933
05/24/2022 19:29:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 19:29:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 19:29:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/24/2022 19:30:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 19:30:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/24/2022 19:30:06 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.2596491228070175 on epoch=949
05/24/2022 19:30:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 19:30:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/24/2022 19:30:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 19:30:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=963
05/24/2022 19:30:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 19:30:20 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.17076023391812867 on epoch=966
05/24/2022 19:30:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 19:30:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 19:30:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=976
05/24/2022 19:30:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 19:30:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 19:30:35 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.24444444444444438 on epoch=983
05/24/2022 19:30:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 19:30:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/24/2022 19:30:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 19:30:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 19:30:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 19:30:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:30:49 - INFO - __main__ - Printing 3 examples
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:30:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:30:49 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:30:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:30:49 - INFO - __main__ - Printing 3 examples
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:30:49 - INFO - __main__ - ['neutral']
05/24/2022 19:30:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:30:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:30:49 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:30:49 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.15656789031402035 on epoch=999
05/24/2022 19:30:49 - INFO - __main__ - save last model!
05/24/2022 19:30:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 19:30:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 19:30:49 - INFO - __main__ - Printing 3 examples
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 19:30:49 - INFO - __main__ - ['contradiction']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 19:30:49 - INFO - __main__ - ['entailment']
05/24/2022 19:30:49 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 19:30:49 - INFO - __main__ - ['contradiction']
05/24/2022 19:30:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:30:50 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:30:51 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 19:31:04 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:31:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:31:05 - INFO - __main__ - Starting training!
05/24/2022 19:31:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_42_0.5_8_predictions.txt
05/24/2022 19:31:24 - INFO - __main__ - Classification-F1 on test data: 0.2497
05/24/2022 19:31:24 - INFO - __main__ - prefix=anli_16_42, lr=0.5, bsz=8, dev_performance=0.31296823138928404, test_performance=0.24971478822842172
05/24/2022 19:31:24 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.4, bsz=8 ...
05/24/2022 19:31:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:31:25 - INFO - __main__ - Printing 3 examples
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:31:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:31:25 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:31:25 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:31:25 - INFO - __main__ - Printing 3 examples
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:31:25 - INFO - __main__ - ['neutral']
05/24/2022 19:31:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:31:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:31:25 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:31:42 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:31:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:31:43 - INFO - __main__ - Starting training!
05/24/2022 19:31:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.06 on epoch=3
05/24/2022 19:31:48 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=6
05/24/2022 19:31:51 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=9
05/24/2022 19:31:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=13
05/24/2022 19:31:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=16
05/24/2022 19:31:57 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.20123755716976058 on epoch=16
05/24/2022 19:31:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20123755716976058 on epoch=16, global_step=50
05/24/2022 19:31:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
05/24/2022 19:32:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=23
05/24/2022 19:32:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=26
05/24/2022 19:32:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
05/24/2022 19:32:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=33
05/24/2022 19:32:10 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 19:32:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=36
05/24/2022 19:32:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
05/24/2022 19:32:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=43
05/24/2022 19:32:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=46
05/24/2022 19:32:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/24/2022 19:32:24 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.23298358891579232 on epoch=49
05/24/2022 19:32:24 - INFO - __main__ - Saving model with best Classification-F1: 0.20123755716976058 -> 0.23298358891579232 on epoch=49, global_step=150
05/24/2022 19:32:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=53
05/24/2022 19:32:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=56
05/24/2022 19:32:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=59
05/24/2022 19:32:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=63
05/24/2022 19:32:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=66
05/24/2022 19:32:37 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.22527472527472528 on epoch=66
05/24/2022 19:32:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
05/24/2022 19:32:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=73
05/24/2022 19:32:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=76
05/24/2022 19:32:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
05/24/2022 19:32:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=83
05/24/2022 19:32:51 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.23298358891579232 on epoch=83
05/24/2022 19:32:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=86
05/24/2022 19:32:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/24/2022 19:32:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=93
05/24/2022 19:33:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=96
05/24/2022 19:33:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
05/24/2022 19:33:04 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.23757575757575758 on epoch=99
05/24/2022 19:33:04 - INFO - __main__ - Saving model with best Classification-F1: 0.23298358891579232 -> 0.23757575757575758 on epoch=99, global_step=300
05/24/2022 19:33:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=103
05/24/2022 19:33:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=106
05/24/2022 19:33:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=109
05/24/2022 19:33:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=113
05/24/2022 19:33:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=116
05/24/2022 19:33:18 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.20886615515771526 on epoch=116
05/24/2022 19:33:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=119
05/24/2022 19:33:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=123
05/24/2022 19:33:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=126
05/24/2022 19:33:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
05/24/2022 19:33:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=133
05/24/2022 19:33:32 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.28672390195346525 on epoch=133
05/24/2022 19:33:32 - INFO - __main__ - Saving model with best Classification-F1: 0.23757575757575758 -> 0.28672390195346525 on epoch=133, global_step=400
05/24/2022 19:33:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=136
05/24/2022 19:33:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=139
05/24/2022 19:33:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
05/24/2022 19:33:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=146
05/24/2022 19:33:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=149
05/24/2022 19:33:46 - INFO - __main__ - Global step 450 Train loss 0.31 Classification-F1 0.2504585496912863 on epoch=149
05/24/2022 19:33:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=153
05/24/2022 19:33:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=156
05/24/2022 19:33:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
05/24/2022 19:33:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=163
05/24/2022 19:33:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=166
05/24/2022 19:33:59 - INFO - __main__ - Global step 500 Train loss 0.27 Classification-F1 0.2914866085597793 on epoch=166
05/24/2022 19:33:59 - INFO - __main__ - Saving model with best Classification-F1: 0.28672390195346525 -> 0.2914866085597793 on epoch=166, global_step=500
05/24/2022 19:34:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.17 on epoch=169
05/24/2022 19:34:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=173
05/24/2022 19:34:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
05/24/2022 19:34:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=179
05/24/2022 19:34:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.18 on epoch=183
05/24/2022 19:34:13 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.2708174178762414 on epoch=183
05/24/2022 19:34:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=186
05/24/2022 19:34:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=189
05/24/2022 19:34:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.17 on epoch=193
05/24/2022 19:34:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=196
05/24/2022 19:34:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=199
05/24/2022 19:34:27 - INFO - __main__ - Global step 600 Train loss 0.17 Classification-F1 0.25777290448343076 on epoch=199
05/24/2022 19:34:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=203
05/24/2022 19:34:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=206
05/24/2022 19:34:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
05/24/2022 19:34:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=213
05/24/2022 19:34:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.15 on epoch=216
05/24/2022 19:34:41 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.2712842712842713 on epoch=216
05/24/2022 19:34:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=219
05/24/2022 19:34:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=223
05/24/2022 19:34:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=226
05/24/2022 19:34:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=229
05/24/2022 19:34:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.13 on epoch=233
05/24/2022 19:34:55 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.28779018252702465 on epoch=233
05/24/2022 19:34:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=236
05/24/2022 19:35:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=239
05/24/2022 19:35:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=243
05/24/2022 19:35:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=246
05/24/2022 19:35:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
05/24/2022 19:35:08 - INFO - __main__ - Global step 750 Train loss 0.10 Classification-F1 0.2233953410423999 on epoch=249
05/24/2022 19:35:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=253
05/24/2022 19:35:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=256
05/24/2022 19:35:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
05/24/2022 19:35:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=263
05/24/2022 19:35:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=266
05/24/2022 19:35:22 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.2868421052631579 on epoch=266
05/24/2022 19:35:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=269
05/24/2022 19:35:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=273
05/24/2022 19:35:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=276
05/24/2022 19:35:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=279
05/24/2022 19:35:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.05 on epoch=283
05/24/2022 19:35:36 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.2546218487394958 on epoch=283
05/24/2022 19:35:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=286
05/24/2022 19:35:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=289
05/24/2022 19:35:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=293
05/24/2022 19:35:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/24/2022 19:35:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=299
05/24/2022 19:35:50 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.30109793588054456 on epoch=299
05/24/2022 19:35:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2914866085597793 -> 0.30109793588054456 on epoch=299, global_step=900
05/24/2022 19:35:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=303
05/24/2022 19:35:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=306
05/24/2022 19:35:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=309
05/24/2022 19:35:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=313
05/24/2022 19:36:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
05/24/2022 19:36:03 - INFO - __main__ - Global step 950 Train loss 0.06 Classification-F1 0.24774354186118894 on epoch=316
05/24/2022 19:36:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=319
05/24/2022 19:36:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=323
05/24/2022 19:36:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=326
05/24/2022 19:36:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
05/24/2022 19:36:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
05/24/2022 19:36:17 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.23896648960587577 on epoch=333
05/24/2022 19:36:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=336
05/24/2022 19:36:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=339
05/24/2022 19:36:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=343
05/24/2022 19:36:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/24/2022 19:36:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=349
05/24/2022 19:36:31 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.2763891711260132 on epoch=349
05/24/2022 19:36:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
05/24/2022 19:36:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=356
05/24/2022 19:36:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=359
05/24/2022 19:36:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
05/24/2022 19:36:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=366
05/24/2022 19:36:45 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.3113678505206104 on epoch=366
05/24/2022 19:36:45 - INFO - __main__ - Saving model with best Classification-F1: 0.30109793588054456 -> 0.3113678505206104 on epoch=366, global_step=1100
05/24/2022 19:36:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=369
05/24/2022 19:36:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=373
05/24/2022 19:36:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=376
05/24/2022 19:36:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=379
05/24/2022 19:36:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=383
05/24/2022 19:36:59 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.2906846240179573 on epoch=383
05/24/2022 19:37:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
05/24/2022 19:37:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=389
05/24/2022 19:37:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=393
05/24/2022 19:37:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=396
05/24/2022 19:37:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=399
05/24/2022 19:37:13 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.2560605066998929 on epoch=399
05/24/2022 19:37:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/24/2022 19:37:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=406
05/24/2022 19:37:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=409
05/24/2022 19:37:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/24/2022 19:37:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=416
05/24/2022 19:37:27 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.25570409982174686 on epoch=416
05/24/2022 19:37:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=419
05/24/2022 19:37:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=423
05/24/2022 19:37:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=426
05/24/2022 19:37:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=429
05/24/2022 19:37:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
05/24/2022 19:37:41 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.2757427757427758 on epoch=433
05/24/2022 19:37:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/24/2022 19:37:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/24/2022 19:37:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/24/2022 19:37:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=446
05/24/2022 19:37:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=449
05/24/2022 19:37:55 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.2560605066998929 on epoch=449
05/24/2022 19:37:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=453
05/24/2022 19:38:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/24/2022 19:38:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
05/24/2022 19:38:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 19:38:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=466
05/24/2022 19:38:09 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.2557445742524653 on epoch=466
05/24/2022 19:38:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
05/24/2022 19:38:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/24/2022 19:38:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=476
05/24/2022 19:38:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
05/24/2022 19:38:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=483
05/24/2022 19:38:23 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.25867171519345433 on epoch=483
05/24/2022 19:38:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/24/2022 19:38:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=489
05/24/2022 19:38:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/24/2022 19:38:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=496
05/24/2022 19:38:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=499
05/24/2022 19:38:37 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.24015319667493581 on epoch=499
05/24/2022 19:38:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
05/24/2022 19:38:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=506
05/24/2022 19:38:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
05/24/2022 19:38:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=513
05/24/2022 19:38:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=516
05/24/2022 19:38:51 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.28684966272624063 on epoch=516
05/24/2022 19:38:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=519
05/24/2022 19:38:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/24/2022 19:38:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/24/2022 19:39:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=529
05/24/2022 19:39:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/24/2022 19:39:05 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.22211640211640213 on epoch=533
05/24/2022 19:39:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/24/2022 19:39:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
05/24/2022 19:39:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
05/24/2022 19:39:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
05/24/2022 19:39:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
05/24/2022 19:39:19 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.25867171519345433 on epoch=549
05/24/2022 19:39:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=553
05/24/2022 19:39:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=556
05/24/2022 19:39:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=559
05/24/2022 19:39:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=563
05/24/2022 19:39:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
05/24/2022 19:39:33 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.22286572286572284 on epoch=566
05/24/2022 19:39:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/24/2022 19:39:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=573
05/24/2022 19:39:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/24/2022 19:39:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/24/2022 19:39:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/24/2022 19:39:47 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.2863492063492063 on epoch=583
05/24/2022 19:39:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/24/2022 19:39:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 19:39:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/24/2022 19:39:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/24/2022 19:40:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/24/2022 19:40:01 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.23987281399046104 on epoch=599
05/24/2022 19:40:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/24/2022 19:40:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/24/2022 19:40:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/24/2022 19:40:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=613
05/24/2022 19:40:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/24/2022 19:40:15 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.30500163452108525 on epoch=616
05/24/2022 19:40:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=619
05/24/2022 19:40:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/24/2022 19:40:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/24/2022 19:40:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/24/2022 19:40:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=633
05/24/2022 19:40:29 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.26519581782739676 on epoch=633
05/24/2022 19:40:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=636
05/24/2022 19:40:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/24/2022 19:40:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=643
05/24/2022 19:40:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
05/24/2022 19:40:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/24/2022 19:40:42 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.30630870104554314 on epoch=649
05/24/2022 19:40:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/24/2022 19:40:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 19:40:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/24/2022 19:40:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
05/24/2022 19:40:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/24/2022 19:40:56 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2658392658392658 on epoch=666
05/24/2022 19:40:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/24/2022 19:41:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=673
05/24/2022 19:41:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/24/2022 19:41:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 19:41:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=683
05/24/2022 19:41:10 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.2655195681511471 on epoch=683
05/24/2022 19:41:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/24/2022 19:41:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/24/2022 19:41:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 19:41:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=696
05/24/2022 19:41:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/24/2022 19:41:24 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.2756683498371478 on epoch=699
05/24/2022 19:41:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=703
05/24/2022 19:41:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/24/2022 19:41:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 19:41:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/24/2022 19:41:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=716
05/24/2022 19:41:38 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.1940216646098999 on epoch=716
05/24/2022 19:41:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=719
05/24/2022 19:41:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=723
05/24/2022 19:41:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=726
05/24/2022 19:41:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=729
05/24/2022 19:41:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=733
05/24/2022 19:41:52 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.24495798319327733 on epoch=733
05/24/2022 19:41:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=736
05/24/2022 19:41:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=739
05/24/2022 19:42:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
05/24/2022 19:42:02 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 19:42:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=749
05/24/2022 19:42:06 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.3072833599149388 on epoch=749
05/24/2022 19:42:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=753
05/24/2022 19:42:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/24/2022 19:42:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/24/2022 19:42:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=763
05/24/2022 19:42:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/24/2022 19:42:20 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.25846181109339006 on epoch=766
05/24/2022 19:42:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 19:42:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 19:42:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=776
05/24/2022 19:42:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 19:42:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 19:42:34 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.30500163452108525 on epoch=783
05/24/2022 19:42:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 19:42:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 19:42:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/24/2022 19:42:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 19:42:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/24/2022 19:42:48 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.304001554001554 on epoch=799
05/24/2022 19:42:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/24/2022 19:42:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 19:42:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 19:42:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/24/2022 19:43:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=816
05/24/2022 19:43:02 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.2876572224398311 on epoch=816
05/24/2022 19:43:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/24/2022 19:43:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/24/2022 19:43:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/24/2022 19:43:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=829
05/24/2022 19:43:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/24/2022 19:43:16 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.28673835125448033 on epoch=833
05/24/2022 19:43:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/24/2022 19:43:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=839
05/24/2022 19:43:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 19:43:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 19:43:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 19:43:30 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.24794704206468912 on epoch=849
05/24/2022 19:43:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/24/2022 19:43:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 19:43:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=859
05/24/2022 19:43:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/24/2022 19:43:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 19:43:44 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.2876572224398311 on epoch=866
05/24/2022 19:43:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 19:43:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/24/2022 19:43:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 19:43:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
05/24/2022 19:43:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/24/2022 19:43:59 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.30500163452108525 on epoch=883
05/24/2022 19:44:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 19:44:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 19:44:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 19:44:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 19:44:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 19:44:14 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.2684101558243434 on epoch=899
05/24/2022 19:44:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 19:44:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 19:44:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=909
05/24/2022 19:44:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=913
05/24/2022 19:44:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=916
05/24/2022 19:44:28 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.29426129426129427 on epoch=916
05/24/2022 19:44:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 19:44:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 19:44:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 19:44:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 19:44:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/24/2022 19:44:43 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2585743327431307 on epoch=933
05/24/2022 19:44:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 19:44:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=939
05/24/2022 19:44:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/24/2022 19:44:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 19:44:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 19:44:57 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.20421353030048678 on epoch=949
05/24/2022 19:45:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/24/2022 19:45:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=956
05/24/2022 19:45:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
05/24/2022 19:45:08 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/24/2022 19:45:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 19:45:12 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.28749060599849696 on epoch=966
05/24/2022 19:45:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 19:45:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 19:45:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 19:45:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=979
05/24/2022 19:45:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 19:45:26 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.1819942611190818 on epoch=983
05/24/2022 19:45:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 19:45:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 19:45:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 19:45:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 19:45:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
05/24/2022 19:45:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:45:40 - INFO - __main__ - Printing 3 examples
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:45:40 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:45:40 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:45:40 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:45:40 - INFO - __main__ - Printing 3 examples
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:45:40 - INFO - __main__ - ['neutral']
05/24/2022 19:45:40 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:45:40 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:45:40 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:45:41 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.2174004856931686 on epoch=999
05/24/2022 19:45:41 - INFO - __main__ - save last model!
05/24/2022 19:45:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 19:45:41 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 19:45:41 - INFO - __main__ - Printing 3 examples
05/24/2022 19:45:41 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 19:45:41 - INFO - __main__ - ['contradiction']
05/24/2022 19:45:41 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 19:45:41 - INFO - __main__ - ['entailment']
05/24/2022 19:45:41 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 19:45:41 - INFO - __main__ - ['contradiction']
05/24/2022 19:45:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:45:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:45:42 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 19:45:55 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:45:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:45:56 - INFO - __main__ - Starting training!
05/24/2022 19:46:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_42_0.4_8_predictions.txt
05/24/2022 19:46:14 - INFO - __main__ - Classification-F1 on test data: 0.0848
05/24/2022 19:46:14 - INFO - __main__ - prefix=anli_16_42, lr=0.4, bsz=8, dev_performance=0.3113678505206104, test_performance=0.08481345201968032
05/24/2022 19:46:14 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.3, bsz=8 ...
05/24/2022 19:46:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:46:15 - INFO - __main__ - Printing 3 examples
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:46:15 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:46:15 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 19:46:15 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 19:46:15 - INFO - __main__ - Printing 3 examples
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 19:46:15 - INFO - __main__ - ['neutral']
05/24/2022 19:46:15 - INFO - __main__ - Tokenizing Input ...
05/24/2022 19:46:15 - INFO - __main__ - Tokenizing Output ...
05/24/2022 19:46:15 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 19:46:32 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 19:46:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 19:46:32 - INFO - __main__ - Starting training!
05/24/2022 19:46:36 - INFO - __main__ - Step 10 Global step 10 Train loss 1.04 on epoch=3
05/24/2022 19:46:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=6
05/24/2022 19:46:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=9
05/24/2022 19:46:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=13
05/24/2022 19:46:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=16
05/24/2022 19:46:47 - INFO - __main__ - Global step 50 Train loss 0.74 Classification-F1 0.2099511072763877 on epoch=16
05/24/2022 19:46:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2099511072763877 on epoch=16, global_step=50
05/24/2022 19:46:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=19
05/24/2022 19:46:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=23
05/24/2022 19:46:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=26
05/24/2022 19:46:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=29
05/24/2022 19:46:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=33
05/24/2022 19:47:00 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.24444444444444446 on epoch=33
05/24/2022 19:47:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2099511072763877 -> 0.24444444444444446 on epoch=33, global_step=100
05/24/2022 19:47:02 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
05/24/2022 19:47:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=39
05/24/2022 19:47:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=43
05/24/2022 19:47:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=46
05/24/2022 19:47:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/24/2022 19:47:13 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.2375960866526904 on epoch=49
05/24/2022 19:47:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
05/24/2022 19:47:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=56
05/24/2022 19:47:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
05/24/2022 19:47:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=63
05/24/2022 19:47:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=66
05/24/2022 19:47:27 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.3116269073715882 on epoch=66
05/24/2022 19:47:27 - INFO - __main__ - Saving model with best Classification-F1: 0.24444444444444446 -> 0.3116269073715882 on epoch=66, global_step=200
05/24/2022 19:47:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/24/2022 19:47:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=73
05/24/2022 19:47:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
05/24/2022 19:47:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=79
05/24/2022 19:47:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/24/2022 19:47:40 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.2375960866526904 on epoch=83
05/24/2022 19:47:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=86
05/24/2022 19:47:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=89
05/24/2022 19:47:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=93
05/24/2022 19:47:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=96
05/24/2022 19:47:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=99
05/24/2022 19:47:54 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.22962497381102032 on epoch=99
05/24/2022 19:47:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=103
05/24/2022 19:47:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
05/24/2022 19:48:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=109
05/24/2022 19:48:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=113
05/24/2022 19:48:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=116
05/24/2022 19:48:07 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.22433862433862437 on epoch=116
05/24/2022 19:48:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
05/24/2022 19:48:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=123
05/24/2022 19:48:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/24/2022 19:48:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=129
05/24/2022 19:48:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=133
05/24/2022 19:48:20 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.24970882832518052 on epoch=133
05/24/2022 19:48:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=136
05/24/2022 19:48:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.33 on epoch=139
05/24/2022 19:48:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.37 on epoch=143
05/24/2022 19:48:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=146
05/24/2022 19:48:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=149
05/24/2022 19:48:34 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.24332351682649953 on epoch=149
05/24/2022 19:48:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=153
05/24/2022 19:48:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=156
05/24/2022 19:48:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=159
05/24/2022 19:48:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=163
05/24/2022 19:48:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=166
05/24/2022 19:48:47 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.21470342522974104 on epoch=166
05/24/2022 19:48:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
05/24/2022 19:48:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=173
05/24/2022 19:48:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=176
05/24/2022 19:48:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=179
05/24/2022 19:49:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=183
05/24/2022 19:49:01 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.22857142857142856 on epoch=183
05/24/2022 19:49:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=186
05/24/2022 19:49:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=189
05/24/2022 19:49:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=193
05/24/2022 19:49:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=196
05/24/2022 19:49:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=199
05/24/2022 19:49:14 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.18399657680787337 on epoch=199
05/24/2022 19:49:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=203
05/24/2022 19:49:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=206
05/24/2022 19:49:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=209
05/24/2022 19:49:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=213
05/24/2022 19:49:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=216
05/24/2022 19:49:28 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.21780059905862215 on epoch=216
05/24/2022 19:49:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=219
05/24/2022 19:49:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=223
05/24/2022 19:49:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=226
05/24/2022 19:49:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=229
05/24/2022 19:49:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=233
05/24/2022 19:49:42 - INFO - __main__ - Global step 700 Train loss 0.17 Classification-F1 0.23440285204991093 on epoch=233
05/24/2022 19:49:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=236
05/24/2022 19:49:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=239
05/24/2022 19:49:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=243
05/24/2022 19:49:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=246
05/24/2022 19:49:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=249
05/24/2022 19:49:55 - INFO - __main__ - Global step 750 Train loss 0.17 Classification-F1 0.20512820512820515 on epoch=249
05/24/2022 19:49:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=253
05/24/2022 19:50:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=256
05/24/2022 19:50:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=259
05/24/2022 19:50:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.15 on epoch=263
05/24/2022 19:50:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
05/24/2022 19:50:09 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.20737741790373368 on epoch=266
05/24/2022 19:50:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=269
05/24/2022 19:50:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=273
05/24/2022 19:50:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=276
05/24/2022 19:50:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=279
05/24/2022 19:50:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=283
05/24/2022 19:50:23 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.2816993464052288 on epoch=283
05/24/2022 19:50:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.07 on epoch=286
05/24/2022 19:50:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=289
05/24/2022 19:50:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
05/24/2022 19:50:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=296
05/24/2022 19:50:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=299
05/24/2022 19:50:37 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.2041553748870822 on epoch=299
05/24/2022 19:50:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=303
05/24/2022 19:50:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=306
05/24/2022 19:50:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=309
05/24/2022 19:50:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/24/2022 19:50:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
05/24/2022 19:50:50 - INFO - __main__ - Global step 950 Train loss 0.08 Classification-F1 0.22382645803698434 on epoch=316
05/24/2022 19:50:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=319
05/24/2022 19:50:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=323
05/24/2022 19:50:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
05/24/2022 19:51:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=329
05/24/2022 19:51:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=333
05/24/2022 19:51:04 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.23826232247284881 on epoch=333
05/24/2022 19:51:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=336
05/24/2022 19:51:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=339
05/24/2022 19:51:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=343
05/24/2022 19:51:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/24/2022 19:51:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=349
05/24/2022 19:51:18 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.25413533834586466 on epoch=349
05/24/2022 19:51:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=353
05/24/2022 19:51:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=356
05/24/2022 19:51:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=359
05/24/2022 19:51:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=363
05/24/2022 19:51:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=366
05/24/2022 19:51:32 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.2433648223121907 on epoch=366
05/24/2022 19:51:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=369
05/24/2022 19:51:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=373
05/24/2022 19:51:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=376
05/24/2022 19:51:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=379
05/24/2022 19:51:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=383
05/24/2022 19:51:46 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.2586527293844367 on epoch=383
05/24/2022 19:51:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=386
05/24/2022 19:51:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=389
05/24/2022 19:51:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=393
05/24/2022 19:51:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=396
05/24/2022 19:51:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=399
05/24/2022 19:52:00 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.2771929824561403 on epoch=399
05/24/2022 19:52:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=403
05/24/2022 19:52:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
05/24/2022 19:52:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
05/24/2022 19:52:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
05/24/2022 19:52:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=416
05/24/2022 19:52:13 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.3140173666489456 on epoch=416
05/24/2022 19:52:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3116269073715882 -> 0.3140173666489456 on epoch=416, global_step=1250
05/24/2022 19:52:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=419
05/24/2022 19:52:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=423
05/24/2022 19:52:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=426
05/24/2022 19:52:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=429
05/24/2022 19:52:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
05/24/2022 19:52:27 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.2920634920634921 on epoch=433
05/24/2022 19:52:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=436
05/24/2022 19:52:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=439
05/24/2022 19:52:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=443
05/24/2022 19:52:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=446
05/24/2022 19:52:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=449
05/24/2022 19:52:41 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.2424651372019793 on epoch=449
05/24/2022 19:52:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=453
05/24/2022 19:52:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=456
05/24/2022 19:52:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=459
05/24/2022 19:52:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 19:52:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/24/2022 19:52:55 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.30712530712530706 on epoch=466
05/24/2022 19:52:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
05/24/2022 19:53:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=473
05/24/2022 19:53:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=476
05/24/2022 19:53:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=479
05/24/2022 19:53:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
05/24/2022 19:53:09 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.24385964912280703 on epoch=483
05/24/2022 19:53:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/24/2022 19:53:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/24/2022 19:53:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/24/2022 19:53:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
05/24/2022 19:53:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=499
05/24/2022 19:53:23 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.243996743996744 on epoch=499
05/24/2022 19:53:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=503
05/24/2022 19:53:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=506
05/24/2022 19:53:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=509
05/24/2022 19:53:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=513
05/24/2022 19:53:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/24/2022 19:53:37 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.22612612612612612 on epoch=516
05/24/2022 19:53:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/24/2022 19:53:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=523
05/24/2022 19:53:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=526
05/24/2022 19:53:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=529
05/24/2022 19:53:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/24/2022 19:53:50 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.2455914561177719 on epoch=533
05/24/2022 19:53:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=536
05/24/2022 19:53:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=539
05/24/2022 19:53:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=543
05/24/2022 19:54:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
05/24/2022 19:54:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=549
05/24/2022 19:54:04 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.21159621159621156 on epoch=549
05/24/2022 19:54:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/24/2022 19:54:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/24/2022 19:54:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/24/2022 19:54:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/24/2022 19:54:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=566
05/24/2022 19:54:18 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2381724218933521 on epoch=566
05/24/2022 19:54:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=569
05/24/2022 19:54:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/24/2022 19:54:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/24/2022 19:54:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=579
05/24/2022 19:54:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=583
05/24/2022 19:54:33 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.1853840682788051 on epoch=583
05/24/2022 19:54:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=586
05/24/2022 19:54:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 19:54:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=593
05/24/2022 19:54:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/24/2022 19:54:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/24/2022 19:54:47 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.23297160797160793 on epoch=599
05/24/2022 19:54:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/24/2022 19:54:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=606
05/24/2022 19:54:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=609
05/24/2022 19:54:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/24/2022 19:54:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=616
05/24/2022 19:55:01 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.14754835807467387 on epoch=616
05/24/2022 19:55:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/24/2022 19:55:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=623
05/24/2022 19:55:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=626
05/24/2022 19:55:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=629
05/24/2022 19:55:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=633
05/24/2022 19:55:15 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.18214285714285713 on epoch=633
05/24/2022 19:55:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/24/2022 19:55:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/24/2022 19:55:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=643
05/24/2022 19:55:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/24/2022 19:55:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/24/2022 19:55:30 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.2723707664884136 on epoch=649
05/24/2022 19:55:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/24/2022 19:55:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 19:55:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=659
05/24/2022 19:55:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=663
05/24/2022 19:55:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/24/2022 19:55:44 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.2574871522239943 on epoch=666
05/24/2022 19:55:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=669
05/24/2022 19:55:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=673
05/24/2022 19:55:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=676
05/24/2022 19:55:54 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=679
05/24/2022 19:55:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/24/2022 19:55:58 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.1487908961593172 on epoch=683
05/24/2022 19:56:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=686
05/24/2022 19:56:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/24/2022 19:56:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=693
05/24/2022 19:56:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/24/2022 19:56:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=699
05/24/2022 19:56:12 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.20421353030048678 on epoch=699
05/24/2022 19:56:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/24/2022 19:56:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/24/2022 19:56:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 19:56:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=713
05/24/2022 19:56:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=716
05/24/2022 19:56:26 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.19621212121212123 on epoch=716
05/24/2022 19:56:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/24/2022 19:56:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/24/2022 19:56:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/24/2022 19:56:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=729
05/24/2022 19:56:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/24/2022 19:56:40 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.22211242863416777 on epoch=733
05/24/2022 19:56:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=736
05/24/2022 19:56:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/24/2022 19:56:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=743
05/24/2022 19:56:50 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=746
05/24/2022 19:56:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 19:56:54 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.1711593172119488 on epoch=749
05/24/2022 19:56:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/24/2022 19:56:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/24/2022 19:57:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/24/2022 19:57:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=763
05/24/2022 19:57:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/24/2022 19:57:08 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.2455914561177719 on epoch=766
05/24/2022 19:57:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=769
05/24/2022 19:57:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 19:57:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=776
05/24/2022 19:57:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
05/24/2022 19:57:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 19:57:23 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.2965675057208238 on epoch=783
05/24/2022 19:57:25 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=786
05/24/2022 19:57:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 19:57:30 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=793
05/24/2022 19:57:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 19:57:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=799
05/24/2022 19:57:37 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.23124055732751383 on epoch=799
05/24/2022 19:57:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/24/2022 19:57:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 19:57:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=809
05/24/2022 19:57:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=813
05/24/2022 19:57:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/24/2022 19:57:51 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.27719023371197277 on epoch=816
05/24/2022 19:57:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
05/24/2022 19:57:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=823
05/24/2022 19:57:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/24/2022 19:58:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/24/2022 19:58:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/24/2022 19:58:05 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.24126984126984127 on epoch=833
05/24/2022 19:58:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 19:58:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=839
05/24/2022 19:58:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=843
05/24/2022 19:58:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 19:58:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=849
05/24/2022 19:58:19 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.32989336085311316 on epoch=849
05/24/2022 19:58:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3140173666489456 -> 0.32989336085311316 on epoch=849, global_step=2550
05/24/2022 19:58:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=853
05/24/2022 19:58:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 19:58:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/24/2022 19:58:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=863
05/24/2022 19:58:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 19:58:33 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.24023837502098372 on epoch=866
05/24/2022 19:58:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 19:58:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/24/2022 19:58:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/24/2022 19:58:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/24/2022 19:58:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/24/2022 19:58:47 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.3340213278293773 on epoch=883
05/24/2022 19:58:47 - INFO - __main__ - Saving model with best Classification-F1: 0.32989336085311316 -> 0.3340213278293773 on epoch=883, global_step=2650
05/24/2022 19:58:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
05/24/2022 19:58:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 19:58:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/24/2022 19:58:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 19:59:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=899
05/24/2022 19:59:01 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.19626769626769625 on epoch=899
05/24/2022 19:59:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 19:59:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 19:59:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 19:59:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 19:59:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
05/24/2022 19:59:15 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2859540154734662 on epoch=916
05/24/2022 19:59:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 19:59:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 19:59:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 19:59:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 19:59:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/24/2022 19:59:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.170113433271328 on epoch=933
05/24/2022 19:59:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 19:59:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 19:59:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=943
05/24/2022 19:59:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 19:59:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
05/24/2022 19:59:44 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.32477477477477473 on epoch=949
05/24/2022 19:59:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 19:59:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/24/2022 19:59:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 19:59:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 19:59:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/24/2022 19:59:58 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.27795498848130423 on epoch=966
05/24/2022 20:00:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 20:00:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=973
05/24/2022 20:00:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 20:00:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 20:00:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 20:00:13 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.2574871522239943 on epoch=983
05/24/2022 20:00:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 20:00:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 20:00:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 20:00:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 20:00:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 20:00:27 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:00:27 - INFO - __main__ - Printing 3 examples
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:00:27 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:00:27 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:00:27 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:00:27 - INFO - __main__ - Printing 3 examples
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 20:00:27 - INFO - __main__ - ['neutral']
05/24/2022 20:00:27 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:00:27 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:00:27 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:00:28 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.26666666666666666 on epoch=999
05/24/2022 20:00:28 - INFO - __main__ - save last model!
05/24/2022 20:00:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 20:00:28 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 20:00:28 - INFO - __main__ - Printing 3 examples
05/24/2022 20:00:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 20:00:28 - INFO - __main__ - ['contradiction']
05/24/2022 20:00:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 20:00:28 - INFO - __main__ - ['entailment']
05/24/2022 20:00:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 20:00:28 - INFO - __main__ - ['contradiction']
05/24/2022 20:00:28 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:00:29 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:00:30 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 20:00:42 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:00:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:00:43 - INFO - __main__ - Starting training!
05/24/2022 20:01:12 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_42_0.3_8_predictions.txt
05/24/2022 20:01:12 - INFO - __main__ - Classification-F1 on test data: 0.0940
05/24/2022 20:01:12 - INFO - __main__ - prefix=anli_16_42, lr=0.3, bsz=8, dev_performance=0.3340213278293773, test_performance=0.09403036096006509
05/24/2022 20:01:12 - INFO - __main__ - Running ... prefix=anli_16_42, lr=0.2, bsz=8 ...
05/24/2022 20:01:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:01:13 - INFO - __main__ - Printing 3 examples
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:01:13 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:01:13 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:01:13 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:01:13 - INFO - __main__ - Printing 3 examples
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: William Gurstelle (born March 29, 1956) is an American nonfiction author, magazine writer, and inventor. He is a feature columnist for "Make" magazine and a columnist and contributing editor at "Popular Science" magazine. Previously, he was the Pyrotechnics and Ballistics Editor at "Popular Mechanics" magazine. [SEP] hypothesis: William Gurstelle  is known for all the scientists due to his contribution at "popular science"
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: Charlotte Anley (1796–1893) was a 19th-century English didactic novelist, social and religious writer, composer and lyricist. She was a Quaker, and spent the years 1836–38 in Australia, researching for a report on women's prisons commissioned by Elizabeth Fry. [SEP] hypothesis: Anley was against the treatment of women in prisons.
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ -  [anli] premise: Clerodendrum is a genus of flowering plants in the family Lamiaceae. Its common names include glorybower, bagflower and bleeding-heart. It is currently classified in the subfamily Ajugoideae, being one of several genera transferred from Verbenaceae to Lamiaceae in the 1990s, based on phylogenetic analysis of morphological and molecular data. [SEP] hypothesis: Clerodendrum is a genus of flowering plants in the family lamiaceae. It was one of several to be found in 1980. 
05/24/2022 20:01:13 - INFO - __main__ - ['neutral']
05/24/2022 20:01:13 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:01:13 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:01:13 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:01:30 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:01:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:01:30 - INFO - __main__ - Starting training!
05/24/2022 20:01:34 - INFO - __main__ - Step 10 Global step 10 Train loss 1.20 on epoch=3
05/24/2022 20:01:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.84 on epoch=6
05/24/2022 20:01:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.71 on epoch=9
05/24/2022 20:01:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=13
05/24/2022 20:01:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=16
05/24/2022 20:01:45 - INFO - __main__ - Global step 50 Train loss 0.78 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 20:01:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/24/2022 20:01:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=19
05/24/2022 20:01:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=23
05/24/2022 20:01:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=26
05/24/2022 20:01:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=29
05/24/2022 20:01:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=33
05/24/2022 20:01:58 - INFO - __main__ - Global step 100 Train loss 0.58 Classification-F1 0.17204301075268816 on epoch=33
05/24/2022 20:01:58 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17204301075268816 on epoch=33, global_step=100
05/24/2022 20:02:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=36
05/24/2022 20:02:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
05/24/2022 20:02:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=43
05/24/2022 20:02:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
05/24/2022 20:02:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
05/24/2022 20:02:12 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.1693121693121693 on epoch=49
05/24/2022 20:02:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/24/2022 20:02:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
05/24/2022 20:02:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=59
05/24/2022 20:02:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=63
05/24/2022 20:02:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=66
05/24/2022 20:02:25 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.23741690408357075 on epoch=66
05/24/2022 20:02:26 - INFO - __main__ - Saving model with best Classification-F1: 0.17204301075268816 -> 0.23741690408357075 on epoch=66, global_step=200
05/24/2022 20:02:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=69
05/24/2022 20:02:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=73
05/24/2022 20:02:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=76
05/24/2022 20:02:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=79
05/24/2022 20:02:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=83
05/24/2022 20:02:39 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.24969696969696967 on epoch=83
05/24/2022 20:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.23741690408357075 -> 0.24969696969696967 on epoch=83, global_step=250
05/24/2022 20:02:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=86
05/24/2022 20:02:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=89
05/24/2022 20:02:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=93
05/24/2022 20:02:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=96
05/24/2022 20:02:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=99
05/24/2022 20:02:53 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.2141759929778363 on epoch=99
05/24/2022 20:02:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=103
05/24/2022 20:02:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=106
05/24/2022 20:03:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=109
05/24/2022 20:03:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=113
05/24/2022 20:03:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=116
05/24/2022 20:03:06 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.24969696969696967 on epoch=116
05/24/2022 20:03:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=119
05/24/2022 20:03:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=123
05/24/2022 20:03:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=126
05/24/2022 20:03:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=129
05/24/2022 20:03:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=133
05/24/2022 20:03:20 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.24969696969696967 on epoch=133
05/24/2022 20:03:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/24/2022 20:03:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=139
05/24/2022 20:03:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=143
05/24/2022 20:03:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=146
05/24/2022 20:03:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=149
05/24/2022 20:03:34 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.2355555555555556 on epoch=149
05/24/2022 20:03:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=153
05/24/2022 20:03:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=156
05/24/2022 20:03:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=159
05/24/2022 20:03:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
05/24/2022 20:03:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
05/24/2022 20:03:47 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.21245421245421248 on epoch=166
05/24/2022 20:03:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=169
05/24/2022 20:03:52 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=173
05/24/2022 20:03:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=176
05/24/2022 20:03:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=179
05/24/2022 20:04:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=183
05/24/2022 20:04:01 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.22222222222222224 on epoch=183
05/24/2022 20:04:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=186
05/24/2022 20:04:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=189
05/24/2022 20:04:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=193
05/24/2022 20:04:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=196
05/24/2022 20:04:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=199
05/24/2022 20:04:15 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.24285407264130668 on epoch=199
05/24/2022 20:04:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=203
05/24/2022 20:04:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=206
05/24/2022 20:04:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=209
05/24/2022 20:04:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=213
05/24/2022 20:04:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=216
05/24/2022 20:04:28 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.2631468677980306 on epoch=216
05/24/2022 20:04:28 - INFO - __main__ - Saving model with best Classification-F1: 0.24969696969696967 -> 0.2631468677980306 on epoch=216, global_step=650
05/24/2022 20:04:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=219
05/24/2022 20:04:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=223
05/24/2022 20:04:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=226
05/24/2022 20:04:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=229
05/24/2022 20:04:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=233
05/24/2022 20:04:42 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.24285407264130668 on epoch=233
05/24/2022 20:04:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=236
05/24/2022 20:04:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=239
05/24/2022 20:04:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=243
05/24/2022 20:04:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=246
05/24/2022 20:04:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=249
05/24/2022 20:04:56 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.2318262411347518 on epoch=249
05/24/2022 20:04:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=253
05/24/2022 20:05:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=256
05/24/2022 20:05:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=259
05/24/2022 20:05:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=263
05/24/2022 20:05:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=266
05/24/2022 20:05:09 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.24285407264130668 on epoch=266
05/24/2022 20:05:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=269
05/24/2022 20:05:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=273
05/24/2022 20:05:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=276
05/24/2022 20:05:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=279
05/24/2022 20:05:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=283
05/24/2022 20:05:23 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.265993265993266 on epoch=283
05/24/2022 20:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2631468677980306 -> 0.265993265993266 on epoch=283, global_step=850
05/24/2022 20:05:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=286
05/24/2022 20:05:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=289
05/24/2022 20:05:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=293
05/24/2022 20:05:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=296
05/24/2022 20:05:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=299
05/24/2022 20:05:37 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.25545057252374326 on epoch=299
05/24/2022 20:05:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=303
05/24/2022 20:05:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=306
05/24/2022 20:05:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=309
05/24/2022 20:05:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=313
05/24/2022 20:05:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=316
05/24/2022 20:05:50 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.23057644110275688 on epoch=316
05/24/2022 20:05:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=319
05/24/2022 20:05:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=323
05/24/2022 20:05:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=326
05/24/2022 20:06:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=329
05/24/2022 20:06:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=333
05/24/2022 20:06:04 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.30021786492374725 on epoch=333
05/24/2022 20:06:04 - INFO - __main__ - Saving model with best Classification-F1: 0.265993265993266 -> 0.30021786492374725 on epoch=333, global_step=1000
05/24/2022 20:06:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=336
05/24/2022 20:06:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=339
05/24/2022 20:06:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=343
05/24/2022 20:06:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=346
05/24/2022 20:06:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=349
05/24/2022 20:06:17 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.28596893302775656 on epoch=349
05/24/2022 20:06:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=353
05/24/2022 20:06:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
05/24/2022 20:06:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=359
05/24/2022 20:06:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=363
05/24/2022 20:06:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=366
05/24/2022 20:06:31 - INFO - __main__ - Global step 1100 Train loss 0.15 Classification-F1 0.28684703855702487 on epoch=366
05/24/2022 20:06:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=369
05/24/2022 20:06:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=373
05/24/2022 20:06:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=376
05/24/2022 20:06:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=379
05/24/2022 20:06:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=383
05/24/2022 20:06:45 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.2431958274063537 on epoch=383
05/24/2022 20:06:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=386
05/24/2022 20:06:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=389
05/24/2022 20:06:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=393
05/24/2022 20:06:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=396
05/24/2022 20:06:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
05/24/2022 20:06:59 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.27216291922174274 on epoch=399
05/24/2022 20:07:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=403
05/24/2022 20:07:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=406
05/24/2022 20:07:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=409
05/24/2022 20:07:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=413
05/24/2022 20:07:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=416
05/24/2022 20:07:13 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.2908203075838808 on epoch=416
05/24/2022 20:07:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=419
05/24/2022 20:07:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=423
05/24/2022 20:07:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
05/24/2022 20:07:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=429
05/24/2022 20:07:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=433
05/24/2022 20:07:27 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.2917797888386124 on epoch=433
05/24/2022 20:07:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=436
05/24/2022 20:07:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=439
05/24/2022 20:07:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=443
05/24/2022 20:07:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=446
05/24/2022 20:07:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=449
05/24/2022 20:07:42 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.2922305764411028 on epoch=449
05/24/2022 20:07:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=453
05/24/2022 20:07:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=456
05/24/2022 20:07:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=459
05/24/2022 20:07:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=463
05/24/2022 20:07:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=466
05/24/2022 20:07:56 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.32135076252723316 on epoch=466
05/24/2022 20:07:56 - INFO - __main__ - Saving model with best Classification-F1: 0.30021786492374725 -> 0.32135076252723316 on epoch=466, global_step=1400
05/24/2022 20:07:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=469
05/24/2022 20:08:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=473
05/24/2022 20:08:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=476
05/24/2022 20:08:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=479
05/24/2022 20:08:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=483
05/24/2022 20:08:10 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.2797906602254428 on epoch=483
05/24/2022 20:08:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=486
05/24/2022 20:08:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
05/24/2022 20:08:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=493
05/24/2022 20:08:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=496
05/24/2022 20:08:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=499
05/24/2022 20:08:24 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.291005291005291 on epoch=499
05/24/2022 20:08:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=503
05/24/2022 20:08:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=506
05/24/2022 20:08:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=509
05/24/2022 20:08:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=513
05/24/2022 20:08:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=516
05/24/2022 20:08:38 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.2425925925925926 on epoch=516
05/24/2022 20:08:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=519
05/24/2022 20:08:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=523
05/24/2022 20:08:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
05/24/2022 20:08:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=529
05/24/2022 20:08:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=533
05/24/2022 20:08:52 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.2577616127038463 on epoch=533
05/24/2022 20:08:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=536
05/24/2022 20:08:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=539
05/24/2022 20:09:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=543
05/24/2022 20:09:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
05/24/2022 20:09:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=549
05/24/2022 20:09:06 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.26698356110120813 on epoch=549
05/24/2022 20:09:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
05/24/2022 20:09:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/24/2022 20:09:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=559
05/24/2022 20:09:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=563
05/24/2022 20:09:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
05/24/2022 20:09:20 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.25628990334872687 on epoch=566
05/24/2022 20:09:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=569
05/24/2022 20:09:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/24/2022 20:09:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=576
05/24/2022 20:09:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
05/24/2022 20:09:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=583
05/24/2022 20:09:34 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.2731829573934837 on epoch=583
05/24/2022 20:09:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 20:09:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=589
05/24/2022 20:09:42 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=593
05/24/2022 20:09:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=596
05/24/2022 20:09:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=599
05/24/2022 20:09:49 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.2577616127038463 on epoch=599
05/24/2022 20:09:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
05/24/2022 20:09:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=606
05/24/2022 20:09:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/24/2022 20:09:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=613
05/24/2022 20:10:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/24/2022 20:10:04 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.25628990334872687 on epoch=616
05/24/2022 20:10:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=619
05/24/2022 20:10:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/24/2022 20:10:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/24/2022 20:10:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=629
05/24/2022 20:10:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=633
05/24/2022 20:10:18 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.26109956109956106 on epoch=633
05/24/2022 20:10:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=636
05/24/2022 20:10:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=639
05/24/2022 20:10:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=643
05/24/2022 20:10:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=646
05/24/2022 20:10:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/24/2022 20:10:33 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.17187055476529162 on epoch=649
05/24/2022 20:10:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=653
05/24/2022 20:10:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 20:10:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=659
05/24/2022 20:10:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=663
05/24/2022 20:10:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/24/2022 20:10:48 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.29239766081871343 on epoch=666
05/24/2022 20:10:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/24/2022 20:10:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/24/2022 20:10:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 20:10:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 20:11:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=683
05/24/2022 20:11:03 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2572084279401352 on epoch=683
05/24/2022 20:11:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/24/2022 20:11:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=689
05/24/2022 20:11:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/24/2022 20:11:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/24/2022 20:11:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=699
05/24/2022 20:11:19 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.18037741412354416 on epoch=699
05/24/2022 20:11:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/24/2022 20:11:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=706
05/24/2022 20:11:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=709
05/24/2022 20:11:29 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
05/24/2022 20:11:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=716
05/24/2022 20:11:35 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.23826232247284881 on epoch=716
05/24/2022 20:11:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=719
05/24/2022 20:11:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=723
05/24/2022 20:11:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
05/24/2022 20:11:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=729
05/24/2022 20:11:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/24/2022 20:11:51 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.23755009959920317 on epoch=733
05/24/2022 20:11:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/24/2022 20:11:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/24/2022 20:11:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=743
05/24/2022 20:12:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=746
05/24/2022 20:12:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=749
05/24/2022 20:12:06 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.2665692007797271 on epoch=749
05/24/2022 20:12:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=753
05/24/2022 20:12:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/24/2022 20:12:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=759
05/24/2022 20:12:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/24/2022 20:12:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=766
05/24/2022 20:12:21 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.26741095162147793 on epoch=766
05/24/2022 20:12:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/24/2022 20:12:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/24/2022 20:12:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/24/2022 20:12:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/24/2022 20:12:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/24/2022 20:12:36 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.22298293158370183 on epoch=783
05/24/2022 20:12:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
05/24/2022 20:12:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=789
05/24/2022 20:12:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 20:12:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=796
05/24/2022 20:12:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=799
05/24/2022 20:12:51 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.26741095162147793 on epoch=799
05/24/2022 20:12:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/24/2022 20:12:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=806
05/24/2022 20:12:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/24/2022 20:13:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/24/2022 20:13:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=816
05/24/2022 20:13:06 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.22298293158370183 on epoch=816
05/24/2022 20:13:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=819
05/24/2022 20:13:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
05/24/2022 20:13:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=826
05/24/2022 20:13:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/24/2022 20:13:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=833
05/24/2022 20:13:21 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.29198606271777 on epoch=833
05/24/2022 20:13:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=836
05/24/2022 20:13:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/24/2022 20:13:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 20:13:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 20:13:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 20:13:36 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.29198606271777 on epoch=849
05/24/2022 20:13:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/24/2022 20:13:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/24/2022 20:13:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=859
05/24/2022 20:13:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
05/24/2022 20:13:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 20:13:51 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2557445742524653 on epoch=866
05/24/2022 20:13:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/24/2022 20:13:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=873
05/24/2022 20:13:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 20:14:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=879
05/24/2022 20:14:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 20:14:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.2557445742524653 on epoch=883
05/24/2022 20:14:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=886
05/24/2022 20:14:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 20:14:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 20:14:17 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 20:14:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 20:14:22 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.2396825396825397 on epoch=899
05/24/2022 20:14:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=903
05/24/2022 20:14:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=906
05/24/2022 20:14:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 20:14:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 20:14:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/24/2022 20:14:38 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.2557445742524653 on epoch=916
05/24/2022 20:14:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 20:14:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=923
05/24/2022 20:14:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 20:14:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 20:14:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=933
05/24/2022 20:14:53 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2863636363636364 on epoch=933
05/24/2022 20:14:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/24/2022 20:14:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 20:15:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=943
05/24/2022 20:15:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=946
05/24/2022 20:15:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 20:15:09 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.22239943292574874 on epoch=949
05/24/2022 20:15:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 20:15:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/24/2022 20:15:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
05/24/2022 20:15:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=963
05/24/2022 20:15:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=966
05/24/2022 20:15:24 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.2916125799913461 on epoch=966
05/24/2022 20:15:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/24/2022 20:15:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=973
05/24/2022 20:15:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=976
05/24/2022 20:15:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 20:15:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/24/2022 20:15:40 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.24155474155474155 on epoch=983
05/24/2022 20:15:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=986
05/24/2022 20:15:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/24/2022 20:15:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 20:15:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=996
05/24/2022 20:15:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=999
05/24/2022 20:15:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:15:54 - INFO - __main__ - Printing 3 examples
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:15:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:15:54 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:15:54 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:15:54 - INFO - __main__ - Printing 3 examples
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:15:54 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:54 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:15:54 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:15:54 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:15:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2557445742524653 on epoch=999
05/24/2022 20:15:55 - INFO - __main__ - save last model!
05/24/2022 20:15:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 20:15:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 20:15:55 - INFO - __main__ - Printing 3 examples
05/24/2022 20:15:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 20:15:55 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 20:15:55 - INFO - __main__ - ['entailment']
05/24/2022 20:15:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 20:15:55 - INFO - __main__ - ['contradiction']
05/24/2022 20:15:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:15:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:15:57 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 20:16:09 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:16:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:16:10 - INFO - __main__ - Starting training!
05/24/2022 20:16:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_42_0.2_8_predictions.txt
05/24/2022 20:16:55 - INFO - __main__ - Classification-F1 on test data: 0.0898
05/24/2022 20:16:55 - INFO - __main__ - prefix=anli_16_42, lr=0.2, bsz=8, dev_performance=0.32135076252723316, test_performance=0.08977721568504078
05/24/2022 20:16:55 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.5, bsz=8 ...
05/24/2022 20:16:56 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:16:56 - INFO - __main__ - Printing 3 examples
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:16:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:16:56 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:16:56 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:16:56 - INFO - __main__ - Printing 3 examples
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:16:56 - INFO - __main__ - ['contradiction']
05/24/2022 20:16:56 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:16:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:16:56 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:17:12 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:17:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:17:13 - INFO - __main__ - Starting training!
05/24/2022 20:17:17 - INFO - __main__ - Step 10 Global step 10 Train loss 0.91 on epoch=3
05/24/2022 20:17:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=6
05/24/2022 20:17:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=9
05/24/2022 20:17:24 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=13
05/24/2022 20:17:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=16
05/24/2022 20:17:28 - INFO - __main__ - Global step 50 Train loss 0.61 Classification-F1 0.20057055080096556 on epoch=16
05/24/2022 20:17:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20057055080096556 on epoch=16, global_step=50
05/24/2022 20:17:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=19
05/24/2022 20:17:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=23
05/24/2022 20:17:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=26
05/24/2022 20:17:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=29
05/24/2022 20:17:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
05/24/2022 20:17:41 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.18666666666666668 on epoch=33
05/24/2022 20:17:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=36
05/24/2022 20:17:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=39
05/24/2022 20:17:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=43
05/24/2022 20:17:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=46
05/24/2022 20:17:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=49
05/24/2022 20:17:55 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.2085278555866791 on epoch=49
05/24/2022 20:17:55 - INFO - __main__ - Saving model with best Classification-F1: 0.20057055080096556 -> 0.2085278555866791 on epoch=49, global_step=150
05/24/2022 20:17:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/24/2022 20:18:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=56
05/24/2022 20:18:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.40 on epoch=59
05/24/2022 20:18:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/24/2022 20:18:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=66
05/24/2022 20:18:09 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.29193899782135074 on epoch=66
05/24/2022 20:18:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.29193899782135074 on epoch=66, global_step=200
05/24/2022 20:18:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=69
05/24/2022 20:18:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=73
05/24/2022 20:18:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=76
05/24/2022 20:18:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=79
05/24/2022 20:18:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=83
05/24/2022 20:18:23 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.3312983312983313 on epoch=83
05/24/2022 20:18:23 - INFO - __main__ - Saving model with best Classification-F1: 0.29193899782135074 -> 0.3312983312983313 on epoch=83, global_step=250
05/24/2022 20:18:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=86
05/24/2022 20:18:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
05/24/2022 20:18:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=93
05/24/2022 20:18:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=96
05/24/2022 20:18:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=99
05/24/2022 20:18:37 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.23298358891579232 on epoch=99
05/24/2022 20:18:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.34 on epoch=103
05/24/2022 20:18:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
05/24/2022 20:18:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.33 on epoch=109
05/24/2022 20:18:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=113
05/24/2022 20:18:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=116
05/24/2022 20:18:51 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.24611708482676223 on epoch=116
05/24/2022 20:18:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=119
05/24/2022 20:18:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=123
05/24/2022 20:18:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=126
05/24/2022 20:19:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=129
05/24/2022 20:19:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=133
05/24/2022 20:19:05 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.21001779811848462 on epoch=133
05/24/2022 20:19:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.32 on epoch=136
05/24/2022 20:19:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=139
05/24/2022 20:19:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=143
05/24/2022 20:19:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=146
05/24/2022 20:19:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=149
05/24/2022 20:19:19 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.32677653730285305 on epoch=149
05/24/2022 20:19:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=153
05/24/2022 20:19:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=156
05/24/2022 20:19:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=159
05/24/2022 20:19:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=163
05/24/2022 20:19:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=166
05/24/2022 20:19:33 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.21360895779500433 on epoch=166
05/24/2022 20:19:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=169
05/24/2022 20:19:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.18 on epoch=173
05/24/2022 20:19:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=176
05/24/2022 20:19:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=179
05/24/2022 20:19:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=183
05/24/2022 20:19:47 - INFO - __main__ - Global step 550 Train loss 0.19 Classification-F1 0.26356443693904996 on epoch=183
05/24/2022 20:19:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=186
05/24/2022 20:19:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=189
05/24/2022 20:19:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=193
05/24/2022 20:19:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=196
05/24/2022 20:20:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=199
05/24/2022 20:20:01 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.2390289449112979 on epoch=199
05/24/2022 20:20:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.09 on epoch=203
05/24/2022 20:20:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=206
05/24/2022 20:20:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=209
05/24/2022 20:20:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=213
05/24/2022 20:20:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=216
05/24/2022 20:20:15 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.24317738791423002 on epoch=216
05/24/2022 20:20:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=219
05/24/2022 20:20:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=223
05/24/2022 20:20:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=226
05/24/2022 20:20:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=229
05/24/2022 20:20:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=233
05/24/2022 20:20:29 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.24275362318840576 on epoch=233
05/24/2022 20:20:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=236
05/24/2022 20:20:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=239
05/24/2022 20:20:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=243
05/24/2022 20:20:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=246
05/24/2022 20:20:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.09 on epoch=249
05/24/2022 20:20:43 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.22335890878090367 on epoch=249
05/24/2022 20:20:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=253
05/24/2022 20:20:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=256
05/24/2022 20:20:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=259
05/24/2022 20:20:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=263
05/24/2022 20:20:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=266
05/24/2022 20:20:57 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.19001589825119236 on epoch=266
05/24/2022 20:20:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=269
05/24/2022 20:21:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=273
05/24/2022 20:21:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=276
05/24/2022 20:21:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=279
05/24/2022 20:21:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=283
05/24/2022 20:21:11 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.17948717948717946 on epoch=283
05/24/2022 20:21:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=286
05/24/2022 20:21:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.07 on epoch=289
05/24/2022 20:21:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.06 on epoch=293
05/24/2022 20:21:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=296
05/24/2022 20:21:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=299
05/24/2022 20:21:26 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.21666666666666667 on epoch=299
05/24/2022 20:21:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=303
05/24/2022 20:21:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=306
05/24/2022 20:21:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=309
05/24/2022 20:21:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.06 on epoch=313
05/24/2022 20:21:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=316
05/24/2022 20:21:40 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.2385487528344671 on epoch=316
05/24/2022 20:21:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=319
05/24/2022 20:21:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=323
05/24/2022 20:21:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=326
05/24/2022 20:21:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=329
05/24/2022 20:21:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=333
05/24/2022 20:21:54 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.23264540337711068 on epoch=333
05/24/2022 20:21:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=336
05/24/2022 20:21:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=339
05/24/2022 20:22:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=343
05/24/2022 20:22:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=346
05/24/2022 20:22:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=349
05/24/2022 20:22:09 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.27845254161043637 on epoch=349
05/24/2022 20:22:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=353
05/24/2022 20:22:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=356
05/24/2022 20:22:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=359
05/24/2022 20:22:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=363
05/24/2022 20:22:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=366
05/24/2022 20:22:23 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.20215337288508017 on epoch=366
05/24/2022 20:22:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=369
05/24/2022 20:22:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=373
05/24/2022 20:22:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=376
05/24/2022 20:22:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=379
05/24/2022 20:22:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=383
05/24/2022 20:22:37 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.17962456300349597 on epoch=383
05/24/2022 20:22:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=386
05/24/2022 20:22:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=389
05/24/2022 20:22:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=393
05/24/2022 20:22:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=396
05/24/2022 20:22:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=399
05/24/2022 20:22:51 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.08918452820891846 on epoch=399
05/24/2022 20:22:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=403
05/24/2022 20:22:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=406
05/24/2022 20:22:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=409
05/24/2022 20:23:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=413
05/24/2022 20:23:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=416
05/24/2022 20:23:06 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.18015873015873016 on epoch=416
05/24/2022 20:23:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=419
05/24/2022 20:23:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=423
05/24/2022 20:23:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=426
05/24/2022 20:23:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=429
05/24/2022 20:23:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=433
05/24/2022 20:23:21 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.23145481969011383 on epoch=433
05/24/2022 20:23:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=436
05/24/2022 20:23:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=439
05/24/2022 20:23:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=443
05/24/2022 20:23:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=446
05/24/2022 20:23:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=449
05/24/2022 20:23:36 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.14676691729323307 on epoch=449
05/24/2022 20:23:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=453
05/24/2022 20:23:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=456
05/24/2022 20:23:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=459
05/24/2022 20:23:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=463
05/24/2022 20:23:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=466
05/24/2022 20:23:50 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.21428066454577585 on epoch=466
05/24/2022 20:23:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=469
05/24/2022 20:23:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=473
05/24/2022 20:23:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=476
05/24/2022 20:24:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=479
05/24/2022 20:24:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=483
05/24/2022 20:24:04 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.1578282828282828 on epoch=483
05/24/2022 20:24:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=486
05/24/2022 20:24:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=489
05/24/2022 20:24:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=493
05/24/2022 20:24:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=496
05/24/2022 20:24:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=499
05/24/2022 20:24:18 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.2526497085320615 on epoch=499
05/24/2022 20:24:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=503
05/24/2022 20:24:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=506
05/24/2022 20:24:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=509
05/24/2022 20:24:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=513
05/24/2022 20:24:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=516
05/24/2022 20:24:32 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.39305555555555555 on epoch=516
05/24/2022 20:24:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3312983312983313 -> 0.39305555555555555 on epoch=516, global_step=1550
05/24/2022 20:24:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=519
05/24/2022 20:24:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=523
05/24/2022 20:24:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=526
05/24/2022 20:24:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=529
05/24/2022 20:24:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=533
05/24/2022 20:24:47 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.22283072546230442 on epoch=533
05/24/2022 20:24:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=536
05/24/2022 20:24:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=539
05/24/2022 20:24:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=543
05/24/2022 20:24:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=546
05/24/2022 20:25:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=549
05/24/2022 20:25:01 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.26666666666666666 on epoch=549
05/24/2022 20:25:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=553
05/24/2022 20:25:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=556
05/24/2022 20:25:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=559
05/24/2022 20:25:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=563
05/24/2022 20:25:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=566
05/24/2022 20:25:15 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.26527820006080877 on epoch=566
05/24/2022 20:25:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/24/2022 20:25:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=573
05/24/2022 20:25:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=576
05/24/2022 20:25:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=579
05/24/2022 20:25:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=583
05/24/2022 20:25:29 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2714285714285714 on epoch=583
05/24/2022 20:25:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=586
05/24/2022 20:25:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 20:25:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=593
05/24/2022 20:25:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=596
05/24/2022 20:25:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=599
05/24/2022 20:25:44 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.1920940170940171 on epoch=599
05/24/2022 20:25:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=603
05/24/2022 20:25:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=606
05/24/2022 20:25:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=609
05/24/2022 20:25:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=613
05/24/2022 20:25:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/24/2022 20:25:58 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.17718013580082545 on epoch=616
05/24/2022 20:26:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=619
05/24/2022 20:26:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=623
05/24/2022 20:26:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/24/2022 20:26:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=629
05/24/2022 20:26:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/24/2022 20:26:12 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.18736398736398735 on epoch=633
05/24/2022 20:26:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=636
05/24/2022 20:26:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=639
05/24/2022 20:26:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/24/2022 20:26:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=646
05/24/2022 20:26:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=649
05/24/2022 20:26:27 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.23658694246929543 on epoch=649
05/24/2022 20:26:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=653
05/24/2022 20:26:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=656
05/24/2022 20:26:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=659
05/24/2022 20:26:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=663
05/24/2022 20:26:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=666
05/24/2022 20:26:41 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.21437908496732025 on epoch=666
05/24/2022 20:26:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/24/2022 20:26:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=673
05/24/2022 20:26:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=676
05/24/2022 20:26:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=679
05/24/2022 20:26:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=683
05/24/2022 20:26:57 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.2969114219114219 on epoch=683
05/24/2022 20:26:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=686
05/24/2022 20:27:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=689
05/24/2022 20:27:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/24/2022 20:27:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=696
05/24/2022 20:27:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=699
05/24/2022 20:27:12 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.28408119658119657 on epoch=699
05/24/2022 20:27:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=703
05/24/2022 20:27:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=706
05/24/2022 20:27:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=709
05/24/2022 20:27:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=713
05/24/2022 20:27:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=716
05/24/2022 20:27:28 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.17372313024486938 on epoch=716
05/24/2022 20:27:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/24/2022 20:27:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/24/2022 20:27:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=726
05/24/2022 20:27:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=729
05/24/2022 20:27:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=733
05/24/2022 20:27:43 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.241231884057971 on epoch=733
05/24/2022 20:27:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=736
05/24/2022 20:27:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/24/2022 20:27:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=743
05/24/2022 20:27:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=746
05/24/2022 20:27:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=749
05/24/2022 20:27:58 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.2408068783068783 on epoch=749
05/24/2022 20:28:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/24/2022 20:28:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=756
05/24/2022 20:28:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/24/2022 20:28:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=763
05/24/2022 20:28:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=766
05/24/2022 20:28:14 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.18946241165131722 on epoch=766
05/24/2022 20:28:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=769
05/24/2022 20:28:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=773
05/24/2022 20:28:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/24/2022 20:28:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=779
05/24/2022 20:28:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=783
05/24/2022 20:28:29 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.1914085914085914 on epoch=783
05/24/2022 20:28:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 20:28:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=789
05/24/2022 20:28:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=793
05/24/2022 20:28:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=796
05/24/2022 20:28:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/24/2022 20:28:44 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.19982342954159593 on epoch=799
05/24/2022 20:28:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=803
05/24/2022 20:28:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=806
05/24/2022 20:28:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=809
05/24/2022 20:28:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=813
05/24/2022 20:28:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=816
05/24/2022 20:28:59 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.24116161616161613 on epoch=816
05/24/2022 20:29:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=819
05/24/2022 20:29:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=823
05/24/2022 20:29:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=826
05/24/2022 20:29:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=829
05/24/2022 20:29:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=833
05/24/2022 20:29:15 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.19515151515151513 on epoch=833
05/24/2022 20:29:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 20:29:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=839
05/24/2022 20:29:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=843
05/24/2022 20:29:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=846
05/24/2022 20:29:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=849
05/24/2022 20:29:31 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.18858090827400287 on epoch=849
05/24/2022 20:29:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=853
05/24/2022 20:29:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=856
05/24/2022 20:29:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
05/24/2022 20:29:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/24/2022 20:29:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=866
05/24/2022 20:29:47 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.24171539961013644 on epoch=866
05/24/2022 20:29:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 20:29:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=873
05/24/2022 20:29:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 20:29:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 20:29:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=883
05/24/2022 20:30:02 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.22098214285714288 on epoch=883
05/24/2022 20:30:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=886
05/24/2022 20:30:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=889
05/24/2022 20:30:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 20:30:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=896
05/24/2022 20:30:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=899
05/24/2022 20:30:17 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.18850574712643678 on epoch=899
05/24/2022 20:30:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 20:30:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/24/2022 20:30:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=909
05/24/2022 20:30:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=913
05/24/2022 20:30:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/24/2022 20:30:33 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.1788235294117647 on epoch=916
05/24/2022 20:30:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=919
05/24/2022 20:30:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 20:30:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=926
05/24/2022 20:30:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=929
05/24/2022 20:30:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 20:30:48 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.2652788388082506 on epoch=933
05/24/2022 20:30:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=936
05/24/2022 20:30:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=939
05/24/2022 20:30:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 20:30:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 20:31:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=949
05/24/2022 20:31:04 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.24415204678362573 on epoch=949
05/24/2022 20:31:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=953
05/24/2022 20:31:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=956
05/24/2022 20:31:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=959
05/24/2022 20:31:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 20:31:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=966
05/24/2022 20:31:20 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.21146320346320344 on epoch=966
05/24/2022 20:31:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 20:31:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=973
05/24/2022 20:31:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
05/24/2022 20:31:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 20:31:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 20:31:35 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.24205128205128207 on epoch=983
05/24/2022 20:31:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 20:31:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 20:31:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 20:31:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=996
05/24/2022 20:31:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/24/2022 20:31:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:31:49 - INFO - __main__ - Printing 3 examples
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:31:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:31:49 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:31:49 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:31:49 - INFO - __main__ - Printing 3 examples
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:31:49 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:49 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:31:49 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:31:49 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:31:51 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.22108108108108104 on epoch=999
05/24/2022 20:31:51 - INFO - __main__ - save last model!
05/24/2022 20:31:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 20:31:51 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 20:31:51 - INFO - __main__ - Printing 3 examples
05/24/2022 20:31:51 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 20:31:51 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:51 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 20:31:51 - INFO - __main__ - ['entailment']
05/24/2022 20:31:51 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 20:31:51 - INFO - __main__ - ['contradiction']
05/24/2022 20:31:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:31:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:31:53 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 20:32:05 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:32:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:32:05 - INFO - __main__ - Starting training!
05/24/2022 20:32:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_87_0.5_8_predictions.txt
05/24/2022 20:32:57 - INFO - __main__ - Classification-F1 on test data: 0.0975
05/24/2022 20:32:57 - INFO - __main__ - prefix=anli_16_87, lr=0.5, bsz=8, dev_performance=0.39305555555555555, test_performance=0.09747611865258923
05/24/2022 20:32:57 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.4, bsz=8 ...
05/24/2022 20:32:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:32:58 - INFO - __main__ - Printing 3 examples
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:32:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:32:58 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:32:58 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:32:58 - INFO - __main__ - Printing 3 examples
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:32:58 - INFO - __main__ - ['contradiction']
05/24/2022 20:32:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:32:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:32:58 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:33:16 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:33:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:33:17 - INFO - __main__ - Starting training!
05/24/2022 20:33:20 - INFO - __main__ - Step 10 Global step 10 Train loss 0.93 on epoch=3
05/24/2022 20:33:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=6
05/24/2022 20:33:25 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=9
05/24/2022 20:33:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.56 on epoch=13
05/24/2022 20:33:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=16
05/24/2022 20:33:31 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 20:33:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/24/2022 20:33:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=19
05/24/2022 20:33:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=23
05/24/2022 20:33:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/24/2022 20:33:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=29
05/24/2022 20:33:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=33
05/24/2022 20:33:44 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.2974425102084676 on epoch=33
05/24/2022 20:33:44 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2974425102084676 on epoch=33, global_step=100
05/24/2022 20:33:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=36
05/24/2022 20:33:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=39
05/24/2022 20:33:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/24/2022 20:33:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=46
05/24/2022 20:33:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=49
05/24/2022 20:33:58 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 20:34:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=53
05/24/2022 20:34:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=56
05/24/2022 20:34:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=59
05/24/2022 20:34:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=63
05/24/2022 20:34:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
05/24/2022 20:34:11 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 20:34:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=69
05/24/2022 20:34:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=73
05/24/2022 20:34:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
05/24/2022 20:34:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=79
05/24/2022 20:34:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=83
05/24/2022 20:34:25 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.19999999999999998 on epoch=83
05/24/2022 20:34:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=86
05/24/2022 20:34:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=89
05/24/2022 20:34:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=93
05/24/2022 20:34:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
05/24/2022 20:34:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=99
05/24/2022 20:34:39 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.24444444444444446 on epoch=99
05/24/2022 20:34:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=103
05/24/2022 20:34:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=106
05/24/2022 20:34:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
05/24/2022 20:34:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/24/2022 20:34:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
05/24/2022 20:34:53 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.23757575757575758 on epoch=116
05/24/2022 20:34:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=119
05/24/2022 20:34:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=123
05/24/2022 20:35:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=126
05/24/2022 20:35:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=129
05/24/2022 20:35:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=133
05/24/2022 20:35:06 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.25777777777777783 on epoch=133
05/24/2022 20:35:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=136
05/24/2022 20:35:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=139
05/24/2022 20:35:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=143
05/24/2022 20:35:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=146
05/24/2022 20:35:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
05/24/2022 20:35:20 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.23298358891579232 on epoch=149
05/24/2022 20:35:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=153
05/24/2022 20:35:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=156
05/24/2022 20:35:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=159
05/24/2022 20:35:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=163
05/24/2022 20:35:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=166
05/24/2022 20:35:33 - INFO - __main__ - Global step 500 Train loss 0.29 Classification-F1 0.2333333333333333 on epoch=166
05/24/2022 20:35:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=169
05/24/2022 20:35:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=173
05/24/2022 20:35:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=176
05/24/2022 20:35:43 - INFO - __main__ - Step 540 Global step 540 Train loss 1.71 on epoch=179
05/24/2022 20:35:46 - INFO - __main__ - Step 550 Global step 550 Train loss 1.49 on epoch=183
05/24/2022 20:35:47 - INFO - __main__ - Global step 550 Train loss 0.91 Classification-F1 0.27970951343500366 on epoch=183
05/24/2022 20:35:49 - INFO - __main__ - Step 560 Global step 560 Train loss 1.55 on epoch=186
05/24/2022 20:35:52 - INFO - __main__ - Step 570 Global step 570 Train loss 1.15 on epoch=189
05/24/2022 20:35:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=193
05/24/2022 20:35:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=196
05/24/2022 20:35:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=199
05/24/2022 20:36:00 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.22222222222222218 on epoch=199
05/24/2022 20:36:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=203
05/24/2022 20:36:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=206
05/24/2022 20:36:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=209
05/24/2022 20:36:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=213
05/24/2022 20:36:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=216
05/24/2022 20:36:14 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.22222222222222218 on epoch=216
05/24/2022 20:36:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=219
05/24/2022 20:36:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=223
05/24/2022 20:36:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=226
05/24/2022 20:36:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=229
05/24/2022 20:36:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=233
05/24/2022 20:36:27 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.22222222222222218 on epoch=233
05/24/2022 20:36:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=236
05/24/2022 20:36:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=239
05/24/2022 20:36:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=243
05/24/2022 20:36:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=246
05/24/2022 20:36:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=249
05/24/2022 20:36:40 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.22222222222222218 on epoch=249
05/24/2022 20:36:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=253
05/24/2022 20:36:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=256
05/24/2022 20:36:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=259
05/24/2022 20:36:50 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=263
05/24/2022 20:36:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=266
05/24/2022 20:36:54 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.2334096109839817 on epoch=266
05/24/2022 20:36:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=269
05/24/2022 20:36:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=273
05/24/2022 20:37:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=276
05/24/2022 20:37:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=279
05/24/2022 20:37:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=283
05/24/2022 20:37:07 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.24969696969696967 on epoch=283
05/24/2022 20:37:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=286
05/24/2022 20:37:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=289
05/24/2022 20:37:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=293
05/24/2022 20:37:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=296
05/24/2022 20:37:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=299
05/24/2022 20:37:20 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.2754036087369421 on epoch=299
05/24/2022 20:37:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=303
05/24/2022 20:37:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=306
05/24/2022 20:37:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=309
05/24/2022 20:37:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=313
05/24/2022 20:37:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=316
05/24/2022 20:37:33 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.2985347985347986 on epoch=316
05/24/2022 20:37:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2974425102084676 -> 0.2985347985347986 on epoch=316, global_step=950
05/24/2022 20:37:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=319
05/24/2022 20:37:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=323
05/24/2022 20:37:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.28 on epoch=326
05/24/2022 20:37:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=329
05/24/2022 20:37:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=333
05/24/2022 20:37:47 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.29794906468334464 on epoch=333
05/24/2022 20:37:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=336
05/24/2022 20:37:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=339
05/24/2022 20:37:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=343
05/24/2022 20:37:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=346
05/24/2022 20:37:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=349
05/24/2022 20:38:00 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.2995574190542744 on epoch=349
05/24/2022 20:38:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2985347985347986 -> 0.2995574190542744 on epoch=349, global_step=1050
05/24/2022 20:38:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=353
05/24/2022 20:38:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=356
05/24/2022 20:38:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=359
05/24/2022 20:38:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=363
05/24/2022 20:38:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=366
05/24/2022 20:38:13 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.28487716925850803 on epoch=366
05/24/2022 20:38:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=369
05/24/2022 20:38:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=373
05/24/2022 20:38:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=376
05/24/2022 20:38:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=379
05/24/2022 20:38:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=383
05/24/2022 20:38:27 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.31532718489240236 on epoch=383
05/24/2022 20:38:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2995574190542744 -> 0.31532718489240236 on epoch=383, global_step=1150
05/24/2022 20:38:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=386
05/24/2022 20:38:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=389
05/24/2022 20:38:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=393
05/24/2022 20:38:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=396
05/24/2022 20:38:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=399
05/24/2022 20:38:40 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.30666666666666664 on epoch=399
05/24/2022 20:38:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=403
05/24/2022 20:38:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=406
05/24/2022 20:38:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=409
05/24/2022 20:38:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=413
05/24/2022 20:38:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=416
05/24/2022 20:38:54 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.31532718489240236 on epoch=416
05/24/2022 20:38:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=419
05/24/2022 20:38:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=423
05/24/2022 20:39:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=426
05/24/2022 20:39:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=429
05/24/2022 20:39:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=433
05/24/2022 20:39:07 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.26460239268121044 on epoch=433
05/24/2022 20:39:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=436
05/24/2022 20:39:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=439
05/24/2022 20:39:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=443
05/24/2022 20:39:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=446
05/24/2022 20:39:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=449
05/24/2022 20:39:21 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.31974637681159424 on epoch=449
05/24/2022 20:39:21 - INFO - __main__ - Saving model with best Classification-F1: 0.31532718489240236 -> 0.31974637681159424 on epoch=449, global_step=1350
05/24/2022 20:39:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=453
05/24/2022 20:39:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=456
05/24/2022 20:39:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=459
05/24/2022 20:39:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=463
05/24/2022 20:39:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=466
05/24/2022 20:39:34 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.2887700534759358 on epoch=466
05/24/2022 20:39:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=469
05/24/2022 20:39:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=473
05/24/2022 20:39:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=476
05/24/2022 20:39:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=479
05/24/2022 20:39:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=483
05/24/2022 20:39:48 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.2638888888888889 on epoch=483
05/24/2022 20:39:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=486
05/24/2022 20:39:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=489
05/24/2022 20:39:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=493
05/24/2022 20:39:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=496
05/24/2022 20:40:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=499
05/24/2022 20:40:03 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.2735042735042735 on epoch=499
05/24/2022 20:40:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=503
05/24/2022 20:40:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=506
05/24/2022 20:40:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=509
05/24/2022 20:40:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=513
05/24/2022 20:40:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=516
05/24/2022 20:40:17 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.28442028985507245 on epoch=516
05/24/2022 20:40:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=519
05/24/2022 20:40:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=523
05/24/2022 20:40:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=526
05/24/2022 20:40:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=529
05/24/2022 20:40:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=533
05/24/2022 20:40:32 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.303921568627451 on epoch=533
05/24/2022 20:40:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=536
05/24/2022 20:40:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=539
05/24/2022 20:40:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=543
05/24/2022 20:40:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=546
05/24/2022 20:40:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=549
05/24/2022 20:40:46 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.2635552505147563 on epoch=549
05/24/2022 20:40:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=553
05/24/2022 20:40:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=556
05/24/2022 20:40:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=559
05/24/2022 20:40:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=563
05/24/2022 20:40:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=566
05/24/2022 20:41:00 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.30437710437710436 on epoch=566
05/24/2022 20:41:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=569
05/24/2022 20:41:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=573
05/24/2022 20:41:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=576
05/24/2022 20:41:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=579
05/24/2022 20:41:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=583
05/24/2022 20:41:14 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.24166666666666667 on epoch=583
05/24/2022 20:41:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=586
05/24/2022 20:41:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=589
05/24/2022 20:41:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=593
05/24/2022 20:41:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=596
05/24/2022 20:41:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=599
05/24/2022 20:41:29 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.30437710437710436 on epoch=599
05/24/2022 20:41:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=603
05/24/2022 20:41:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=606
05/24/2022 20:41:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=609
05/24/2022 20:41:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=613
05/24/2022 20:41:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=616
05/24/2022 20:41:43 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.2887700534759358 on epoch=616
05/24/2022 20:41:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=619
05/24/2022 20:41:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=623
05/24/2022 20:41:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=626
05/24/2022 20:41:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=629
05/24/2022 20:41:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=633
05/24/2022 20:41:57 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.2381894089211162 on epoch=633
05/24/2022 20:41:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=636
05/24/2022 20:42:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=639
05/24/2022 20:42:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=643
05/24/2022 20:42:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=646
05/24/2022 20:42:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=649
05/24/2022 20:42:11 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.30437710437710436 on epoch=649
05/24/2022 20:42:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=653
05/24/2022 20:42:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=656
05/24/2022 20:42:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=659
05/24/2022 20:42:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=663
05/24/2022 20:42:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=666
05/24/2022 20:42:25 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.26992753623188404 on epoch=666
05/24/2022 20:42:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=669
05/24/2022 20:42:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=673
05/24/2022 20:42:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=676
05/24/2022 20:42:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=679
05/24/2022 20:42:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=683
05/24/2022 20:42:39 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.2564102564102564 on epoch=683
05/24/2022 20:42:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=686
05/24/2022 20:42:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=689
05/24/2022 20:42:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=693
05/24/2022 20:42:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=696
05/24/2022 20:42:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=699
05/24/2022 20:42:52 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.2381894089211162 on epoch=699
05/24/2022 20:42:55 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=703
05/24/2022 20:42:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=706
05/24/2022 20:43:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=709
05/24/2022 20:43:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=713
05/24/2022 20:43:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=716
05/24/2022 20:43:06 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.25614035087719295 on epoch=716
05/24/2022 20:43:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=719
05/24/2022 20:43:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=723
05/24/2022 20:43:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=726
05/24/2022 20:43:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=729
05/24/2022 20:43:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=733
05/24/2022 20:43:20 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.2724675895407603 on epoch=733
05/24/2022 20:43:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=736
05/24/2022 20:43:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=739
05/24/2022 20:43:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=743
05/24/2022 20:43:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=746
05/24/2022 20:43:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=749
05/24/2022 20:43:34 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.25614035087719295 on epoch=749
05/24/2022 20:43:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=753
05/24/2022 20:43:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=756
05/24/2022 20:43:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=759
05/24/2022 20:43:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=763
05/24/2022 20:43:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=766
05/24/2022 20:43:48 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.25661375661375657 on epoch=766
05/24/2022 20:43:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=769
05/24/2022 20:43:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=773
05/24/2022 20:43:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=776
05/24/2022 20:43:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=779
05/24/2022 20:44:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=783
05/24/2022 20:44:02 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.2724867724867725 on epoch=783
05/24/2022 20:44:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=786
05/24/2022 20:44:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=789
05/24/2022 20:44:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=793
05/24/2022 20:44:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=796
05/24/2022 20:44:15 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=799
05/24/2022 20:44:16 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.25620742693913423 on epoch=799
05/24/2022 20:44:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=803
05/24/2022 20:44:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=806
05/24/2022 20:44:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=809
05/24/2022 20:44:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=813
05/24/2022 20:44:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=816
05/24/2022 20:44:30 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.25620742693913423 on epoch=816
05/24/2022 20:44:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
05/24/2022 20:44:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=823
05/24/2022 20:44:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=826
05/24/2022 20:44:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=829
05/24/2022 20:44:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=833
05/24/2022 20:44:44 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.3322476322476322 on epoch=833
05/24/2022 20:44:44 - INFO - __main__ - Saving model with best Classification-F1: 0.31974637681159424 -> 0.3322476322476322 on epoch=833, global_step=2500
05/24/2022 20:44:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
05/24/2022 20:44:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=839
05/24/2022 20:44:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=843
05/24/2022 20:44:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=846
05/24/2022 20:44:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=849
05/24/2022 20:44:58 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.31105881234250937 on epoch=849
05/24/2022 20:45:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=853
05/24/2022 20:45:03 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=856
05/24/2022 20:45:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=859
05/24/2022 20:45:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=863
05/24/2022 20:45:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=866
05/24/2022 20:45:12 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.3117408906882591 on epoch=866
05/24/2022 20:45:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=869
05/24/2022 20:45:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=873
05/24/2022 20:45:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=876
05/24/2022 20:45:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=879
05/24/2022 20:45:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=883
05/24/2022 20:45:27 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.23603801169590644 on epoch=883
05/24/2022 20:45:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
05/24/2022 20:45:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=889
05/24/2022 20:45:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=893
05/24/2022 20:45:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=896
05/24/2022 20:45:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=899
05/24/2022 20:45:41 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.22353801169590642 on epoch=899
05/24/2022 20:45:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=903
05/24/2022 20:45:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=906
05/24/2022 20:45:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=909
05/24/2022 20:45:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=913
05/24/2022 20:45:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=916
05/24/2022 20:45:55 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.24060150375939848 on epoch=916
05/24/2022 20:45:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=919
05/24/2022 20:46:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=923
05/24/2022 20:46:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=926
05/24/2022 20:46:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=929
05/24/2022 20:46:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=933
05/24/2022 20:46:09 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.22416748732538205 on epoch=933
05/24/2022 20:46:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=936
05/24/2022 20:46:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=939
05/24/2022 20:46:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=943
05/24/2022 20:46:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=946
05/24/2022 20:46:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=949
05/24/2022 20:46:24 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.22308998302207134 on epoch=949
05/24/2022 20:46:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=953
05/24/2022 20:46:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=956
05/24/2022 20:46:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=959
05/24/2022 20:46:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=963
05/24/2022 20:46:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=966
05/24/2022 20:46:38 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.2363998230871296 on epoch=966
05/24/2022 20:46:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=969
05/24/2022 20:46:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
05/24/2022 20:46:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=976
05/24/2022 20:46:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/24/2022 20:46:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=983
05/24/2022 20:46:52 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.3290322580645162 on epoch=983
05/24/2022 20:46:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=986
05/24/2022 20:46:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=989
05/24/2022 20:46:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=993
05/24/2022 20:47:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=996
05/24/2022 20:47:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=999
05/24/2022 20:47:06 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.3817509817509817 on epoch=999
05/24/2022 20:47:06 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:47:06 - INFO - __main__ - Printing 3 examples
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:47:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3322476322476322 -> 0.3817509817509817 on epoch=999, global_step=3000
05/24/2022 20:47:06 - INFO - __main__ - save last model!
05/24/2022 20:47:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 20:47:06 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:47:06 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 20:47:06 - INFO - __main__ - Printing 3 examples
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 20:47:06 - INFO - __main__ - ['entailment']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:47:06 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:47:06 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:47:06 - INFO - __main__ - Printing 3 examples
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:47:06 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:06 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:47:06 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:47:06 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:47:07 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:47:08 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 20:47:22 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:47:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:47:23 - INFO - __main__ - Starting training!
05/24/2022 20:47:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_87_0.4_8_predictions.txt
05/24/2022 20:47:39 - INFO - __main__ - Classification-F1 on test data: 0.0443
05/24/2022 20:47:40 - INFO - __main__ - prefix=anli_16_87, lr=0.4, bsz=8, dev_performance=0.3817509817509817, test_performance=0.04428260886474587
05/24/2022 20:47:40 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.3, bsz=8 ...
05/24/2022 20:47:41 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:47:41 - INFO - __main__ - Printing 3 examples
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:47:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:47:41 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 20:47:41 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 20:47:41 - INFO - __main__ - Printing 3 examples
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 20:47:41 - INFO - __main__ - ['contradiction']
05/24/2022 20:47:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 20:47:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 20:47:41 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 20:47:57 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 20:47:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 20:47:58 - INFO - __main__ - Starting training!
05/24/2022 20:48:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.98 on epoch=3
05/24/2022 20:48:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=6
05/24/2022 20:48:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=9
05/24/2022 20:48:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=13
05/24/2022 20:48:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=16
05/24/2022 20:48:12 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.15873015873015875 on epoch=16
05/24/2022 20:48:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15873015873015875 on epoch=16, global_step=50
05/24/2022 20:48:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=19
05/24/2022 20:48:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=23
05/24/2022 20:48:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=26
05/24/2022 20:48:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=29
05/24/2022 20:48:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
05/24/2022 20:48:26 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.15873015873015875 on epoch=33
05/24/2022 20:48:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
05/24/2022 20:48:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=39
05/24/2022 20:48:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=43
05/24/2022 20:48:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=46
05/24/2022 20:48:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=49
05/24/2022 20:48:40 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 20:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.15873015873015875 -> 0.16666666666666666 on epoch=49, global_step=150
05/24/2022 20:48:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=53
05/24/2022 20:48:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=56
05/24/2022 20:48:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=59
05/24/2022 20:48:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=63
05/24/2022 20:48:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=66
05/24/2022 20:48:53 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.1693121693121693 on epoch=66
05/24/2022 20:48:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1693121693121693 on epoch=66, global_step=200
05/24/2022 20:48:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=69
05/24/2022 20:48:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=73
05/24/2022 20:49:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=76
05/24/2022 20:49:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/24/2022 20:49:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=83
05/24/2022 20:49:07 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.20908004778972522 on epoch=83
05/24/2022 20:49:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1693121693121693 -> 0.20908004778972522 on epoch=83, global_step=250
05/24/2022 20:49:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=86
05/24/2022 20:49:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=89
05/24/2022 20:49:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=93
05/24/2022 20:49:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=96
05/24/2022 20:49:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=99
05/24/2022 20:49:20 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.20908004778972522 on epoch=99
05/24/2022 20:49:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=103
05/24/2022 20:49:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=106
05/24/2022 20:49:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=109
05/24/2022 20:49:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/24/2022 20:49:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
05/24/2022 20:49:34 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.1693121693121693 on epoch=116
05/24/2022 20:49:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=119
05/24/2022 20:49:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=123
05/24/2022 20:49:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=126
05/24/2022 20:49:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=129
05/24/2022 20:49:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=133
05/24/2022 20:49:47 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.2450388265746333 on epoch=133
05/24/2022 20:49:47 - INFO - __main__ - Saving model with best Classification-F1: 0.20908004778972522 -> 0.2450388265746333 on epoch=133, global_step=400
05/24/2022 20:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=136
05/24/2022 20:49:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=139
05/24/2022 20:49:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=143
05/24/2022 20:49:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=146
05/24/2022 20:50:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=149
05/24/2022 20:50:01 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.2334096109839817 on epoch=149
05/24/2022 20:50:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.35 on epoch=153
05/24/2022 20:50:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=156
05/24/2022 20:50:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=159
05/24/2022 20:50:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=163
05/24/2022 20:50:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=166
05/24/2022 20:50:14 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.2111111111111111 on epoch=166
05/24/2022 20:50:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=169
05/24/2022 20:50:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=173
05/24/2022 20:50:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=176
05/24/2022 20:50:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=179
05/24/2022 20:50:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=183
05/24/2022 20:50:28 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.23301985370950887 on epoch=183
05/24/2022 20:50:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=186
05/24/2022 20:50:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=189
05/24/2022 20:50:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=193
05/24/2022 20:50:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=196
05/24/2022 20:50:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=199
05/24/2022 20:50:42 - INFO - __main__ - Global step 600 Train loss 0.28 Classification-F1 0.2119004250151791 on epoch=199
05/24/2022 20:50:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=203
05/24/2022 20:50:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=206
05/24/2022 20:50:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=209
05/24/2022 20:50:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=213
05/24/2022 20:50:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=216
05/24/2022 20:50:56 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.201010101010101 on epoch=216
05/24/2022 20:50:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=219
05/24/2022 20:51:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=223
05/24/2022 20:51:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=226
05/24/2022 20:51:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=229
05/24/2022 20:51:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=233
05/24/2022 20:51:10 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.21031746031746035 on epoch=233
05/24/2022 20:51:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=236
05/24/2022 20:51:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=239
05/24/2022 20:51:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=243
05/24/2022 20:51:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=246
05/24/2022 20:51:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=249
05/24/2022 20:51:23 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.19144736842105264 on epoch=249
05/24/2022 20:51:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=253
05/24/2022 20:51:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=256
05/24/2022 20:51:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=259
05/24/2022 20:51:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=263
05/24/2022 20:51:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=266
05/24/2022 20:51:37 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.23879928315412188 on epoch=266
05/24/2022 20:51:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=269
05/24/2022 20:51:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=273
05/24/2022 20:51:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=276
05/24/2022 20:51:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=279
05/24/2022 20:51:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=283
05/24/2022 20:51:51 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.26043039086517344 on epoch=283
05/24/2022 20:51:51 - INFO - __main__ - Saving model with best Classification-F1: 0.2450388265746333 -> 0.26043039086517344 on epoch=283, global_step=850
05/24/2022 20:51:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=286
05/24/2022 20:51:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=289
05/24/2022 20:51:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=293
05/24/2022 20:52:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=296
05/24/2022 20:52:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=299
05/24/2022 20:52:05 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.19140989729225025 on epoch=299
05/24/2022 20:52:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=303
05/24/2022 20:52:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=306
05/24/2022 20:52:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=309
05/24/2022 20:52:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=313
05/24/2022 20:52:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=316
05/24/2022 20:52:19 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.19410430839002268 on epoch=316
05/24/2022 20:52:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=319
05/24/2022 20:52:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=323
05/24/2022 20:52:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=326
05/24/2022 20:52:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=329
05/24/2022 20:52:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=333
05/24/2022 20:52:33 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.2854086435601198 on epoch=333
05/24/2022 20:52:33 - INFO - __main__ - Saving model with best Classification-F1: 0.26043039086517344 -> 0.2854086435601198 on epoch=333, global_step=1000
05/24/2022 20:52:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=336
05/24/2022 20:52:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=339
05/24/2022 20:52:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=343
05/24/2022 20:52:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=346
05/24/2022 20:52:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=349
05/24/2022 20:52:47 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.18421052631578946 on epoch=349
05/24/2022 20:52:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=353
05/24/2022 20:52:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=356
05/24/2022 20:52:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=359
05/24/2022 20:52:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.09 on epoch=363
05/24/2022 20:53:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=366
05/24/2022 20:53:01 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.3121907332433648 on epoch=366
05/24/2022 20:53:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2854086435601198 -> 0.3121907332433648 on epoch=366, global_step=1100
05/24/2022 20:53:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=369
05/24/2022 20:53:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=373
05/24/2022 20:53:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=376
05/24/2022 20:53:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=379
05/24/2022 20:53:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=383
05/24/2022 20:53:15 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.28016094932824115 on epoch=383
05/24/2022 20:53:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=386
05/24/2022 20:53:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=389
05/24/2022 20:53:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=393
05/24/2022 20:53:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=396
05/24/2022 20:53:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=399
05/24/2022 20:53:29 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.3686868686868687 on epoch=399
05/24/2022 20:53:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3121907332433648 -> 0.3686868686868687 on epoch=399, global_step=1200
05/24/2022 20:53:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=403
05/24/2022 20:53:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=406
05/24/2022 20:53:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=409
05/24/2022 20:53:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=413
05/24/2022 20:53:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=416
05/24/2022 20:53:43 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.29843489843489845 on epoch=416
05/24/2022 20:53:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=419
05/24/2022 20:53:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=423
05/24/2022 20:53:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=426
05/24/2022 20:53:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=429
05/24/2022 20:53:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=433
05/24/2022 20:53:57 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.287962962962963 on epoch=433
05/24/2022 20:54:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=436
05/24/2022 20:54:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=439
05/24/2022 20:54:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=443
05/24/2022 20:54:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=446
05/24/2022 20:54:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=449
05/24/2022 20:54:11 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.21230258331080817 on epoch=449
05/24/2022 20:54:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=453
05/24/2022 20:54:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=456
05/24/2022 20:54:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=459
05/24/2022 20:54:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=463
05/24/2022 20:54:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=466
05/24/2022 20:54:25 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.17015781922525108 on epoch=466
05/24/2022 20:54:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=469
05/24/2022 20:54:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=473
05/24/2022 20:54:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=476
05/24/2022 20:54:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=479
05/24/2022 20:54:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
05/24/2022 20:54:39 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.14179818268770925 on epoch=483
05/24/2022 20:54:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=486
05/24/2022 20:54:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=489
05/24/2022 20:54:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=493
05/24/2022 20:54:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=496
05/24/2022 20:54:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=499
05/24/2022 20:54:53 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.18362442155545605 on epoch=499
05/24/2022 20:54:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=503
05/24/2022 20:54:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=506
05/24/2022 20:55:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
05/24/2022 20:55:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=513
05/24/2022 20:55:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=516
05/24/2022 20:55:07 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.26002906976744183 on epoch=516
05/24/2022 20:55:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=519
05/24/2022 20:55:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=523
05/24/2022 20:55:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=526
05/24/2022 20:55:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=529
05/24/2022 20:55:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=533
05/24/2022 20:55:22 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.21541125541125544 on epoch=533
05/24/2022 20:55:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=536
05/24/2022 20:55:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=539
05/24/2022 20:55:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=543
05/24/2022 20:55:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=546
05/24/2022 20:55:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
05/24/2022 20:55:37 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.1951089845826688 on epoch=549
05/24/2022 20:55:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=553
05/24/2022 20:55:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=556
05/24/2022 20:55:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=559
05/24/2022 20:55:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=563
05/24/2022 20:55:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=566
05/24/2022 20:55:51 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.1297297297297297 on epoch=566
05/24/2022 20:55:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=569
05/24/2022 20:55:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=573
05/24/2022 20:55:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=576
05/24/2022 20:56:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=579
05/24/2022 20:56:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=583
05/24/2022 20:56:05 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.36307787186603097 on epoch=583
05/24/2022 20:56:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=586
05/24/2022 20:56:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=589
05/24/2022 20:56:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=593
05/24/2022 20:56:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=596
05/24/2022 20:56:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=599
05/24/2022 20:56:19 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.35399976779287123 on epoch=599
05/24/2022 20:56:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=603
05/24/2022 20:56:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=606
05/24/2022 20:56:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=609
05/24/2022 20:56:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=613
05/24/2022 20:56:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=616
05/24/2022 20:56:33 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.22187604317347276 on epoch=616
05/24/2022 20:56:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=619
05/24/2022 20:56:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=623
05/24/2022 20:56:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=626
05/24/2022 20:56:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=629
05/24/2022 20:56:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=633
05/24/2022 20:56:48 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.13863049095607236 on epoch=633
05/24/2022 20:56:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=636
05/24/2022 20:56:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=639
05/24/2022 20:56:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=643
05/24/2022 20:56:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=646
05/24/2022 20:57:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=649
05/24/2022 20:57:02 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.20718816067653273 on epoch=649
05/24/2022 20:57:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=653
05/24/2022 20:57:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=656
05/24/2022 20:57:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=659
05/24/2022 20:57:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=663
05/24/2022 20:57:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=666
05/24/2022 20:57:17 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.3060041407867495 on epoch=666
05/24/2022 20:57:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=669
05/24/2022 20:57:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=673
05/24/2022 20:57:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=676
05/24/2022 20:57:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=679
05/24/2022 20:57:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=683
05/24/2022 20:57:31 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.25564516129032255 on epoch=683
05/24/2022 20:57:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=686
05/24/2022 20:57:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=689
05/24/2022 20:57:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=693
05/24/2022 20:57:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=696
05/24/2022 20:57:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=699
05/24/2022 20:57:46 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.24021739130434783 on epoch=699
05/24/2022 20:57:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=703
05/24/2022 20:57:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=706
05/24/2022 20:57:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=709
05/24/2022 20:57:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=713
05/24/2022 20:57:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=716
05/24/2022 20:58:01 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.2394896178975815 on epoch=716
05/24/2022 20:58:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=719
05/24/2022 20:58:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=723
05/24/2022 20:58:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=726
05/24/2022 20:58:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/24/2022 20:58:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=733
05/24/2022 20:58:15 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.33088417031794176 on epoch=733
05/24/2022 20:58:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=736
05/24/2022 20:58:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=739
05/24/2022 20:58:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=743
05/24/2022 20:58:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/24/2022 20:58:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 20:58:30 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.2974748942490878 on epoch=749
05/24/2022 20:58:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=753
05/24/2022 20:58:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=756
05/24/2022 20:58:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=759
05/24/2022 20:58:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=763
05/24/2022 20:58:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=766
05/24/2022 20:58:44 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.12436868686868688 on epoch=766
05/24/2022 20:58:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=769
05/24/2022 20:58:50 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=773
05/24/2022 20:58:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=776
05/24/2022 20:58:55 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=779
05/24/2022 20:58:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=783
05/24/2022 20:58:59 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.20279304029304032 on epoch=783
05/24/2022 20:59:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=786
05/24/2022 20:59:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=789
05/24/2022 20:59:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 20:59:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/24/2022 20:59:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=799
05/24/2022 20:59:14 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.1894144144144144 on epoch=799
05/24/2022 20:59:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=803
05/24/2022 20:59:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
05/24/2022 20:59:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=809
05/24/2022 20:59:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=813
05/24/2022 20:59:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=816
05/24/2022 20:59:28 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.22952578499392035 on epoch=816
05/24/2022 20:59:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=819
05/24/2022 20:59:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=823
05/24/2022 20:59:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=826
05/24/2022 20:59:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/24/2022 20:59:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/24/2022 20:59:43 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.20116959064327486 on epoch=833
05/24/2022 20:59:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=836
05/24/2022 20:59:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=839
05/24/2022 20:59:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=843
05/24/2022 20:59:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=846
05/24/2022 20:59:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 20:59:57 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.18441558441558442 on epoch=849
05/24/2022 21:00:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=853
05/24/2022 21:00:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/24/2022 21:00:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=859
05/24/2022 21:00:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=863
05/24/2022 21:00:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=866
05/24/2022 21:00:11 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.2704761904761905 on epoch=866
05/24/2022 21:00:14 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=869
05/24/2022 21:00:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=873
05/24/2022 21:00:19 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=876
05/24/2022 21:00:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=879
05/24/2022 21:00:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=883
05/24/2022 21:00:26 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.25304020598138244 on epoch=883
05/24/2022 21:00:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=886
05/24/2022 21:00:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=889
05/24/2022 21:00:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=893
05/24/2022 21:00:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=896
05/24/2022 21:00:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/24/2022 21:00:41 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.17124907612712492 on epoch=899
05/24/2022 21:00:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=903
05/24/2022 21:00:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=906
05/24/2022 21:00:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/24/2022 21:00:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=913
05/24/2022 21:00:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=916
05/24/2022 21:00:56 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.2032428355957768 on epoch=916
05/24/2022 21:00:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=919
05/24/2022 21:01:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=923
05/24/2022 21:01:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
05/24/2022 21:01:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=929
05/24/2022 21:01:09 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=933
05/24/2022 21:01:12 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.29732142857142857 on epoch=933
05/24/2022 21:01:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=936
05/24/2022 21:01:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=939
05/24/2022 21:01:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=943
05/24/2022 21:01:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=946
05/24/2022 21:01:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 21:01:27 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.15277777777777776 on epoch=949
05/24/2022 21:01:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/24/2022 21:01:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/24/2022 21:01:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=959
05/24/2022 21:01:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=963
05/24/2022 21:01:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/24/2022 21:01:42 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.19619047619047617 on epoch=966
05/24/2022 21:01:44 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=969
05/24/2022 21:01:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
05/24/2022 21:01:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=976
05/24/2022 21:01:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=979
05/24/2022 21:01:55 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=983
05/24/2022 21:01:57 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.27953667953667954 on epoch=983
05/24/2022 21:01:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=986
05/24/2022 21:02:02 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=989
05/24/2022 21:02:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=993
05/24/2022 21:02:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=996
05/24/2022 21:02:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=999
05/24/2022 21:02:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 21:02:11 - INFO - __main__ - Printing 3 examples
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:02:11 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:02:11 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 21:02:11 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 21:02:11 - INFO - __main__ - Printing 3 examples
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:02:11 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:02:11 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 21:02:11 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.13160648874934588 on epoch=999
05/24/2022 21:02:11 - INFO - __main__ - save last model!
05/24/2022 21:02:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 21:02:11 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 21:02:11 - INFO - __main__ - Printing 3 examples
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 21:02:11 - INFO - __main__ - ['entailment']
05/24/2022 21:02:11 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 21:02:11 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:11 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:02:12 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:02:13 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 21:02:26 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 21:02:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 21:02:27 - INFO - __main__ - Starting training!
05/24/2022 21:02:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_87_0.3_8_predictions.txt
05/24/2022 21:02:58 - INFO - __main__ - Classification-F1 on test data: 0.0697
05/24/2022 21:02:58 - INFO - __main__ - prefix=anli_16_87, lr=0.3, bsz=8, dev_performance=0.3686868686868687, test_performance=0.06965224483361804
05/24/2022 21:02:58 - INFO - __main__ - Running ... prefix=anli_16_87, lr=0.2, bsz=8 ...
05/24/2022 21:02:59 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 21:02:59 - INFO - __main__ - Printing 3 examples
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:02:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:02:59 - INFO - __main__ - Loaded 48 examples from train data
05/24/2022 21:02:59 - INFO - __main__ - Start tokenizing ... 48 instances
05/24/2022 21:02:59 - INFO - __main__ - Printing 3 examples
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: MuscleCar is a television program whose hosts demonstrate how to rebuild muscle cars while sharing information about these cars and their history. It became a part of a group of shows called the Powerblock, currently shown on Spike TV, on January 7, 2006. [SEP] hypothesis: MuscleCar is a radio program whose hosts demonstrate how to rebuild fat bikes while sharing information about these bikes and their history. Currently shown on BBC2, since 2001.
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: Alexander Kartveli Batumi International Airport (IATA: BUS, ICAO: UGSB) is an airport located 2 km south of Batumi, a city on the Black Sea coast and capital of Adjara, an autonomous republic in southwest Georgia. The airport is 20 km northeast of Hopa, Turkey, and serves as a domestic and international airport for Georgia and northeastern Turkey. [SEP] hypothesis: The IATA is nowhere near the Black Sea.
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ -  [anli] premise: Tiffanie DeBartolo (born November 27, 1970) is an American novelist, filmmaker, and co-founder of independent record label Bright Antenna. She has written two novels, "God-Shaped Hole" and "How To Kill a Rock Star". She wrote and directed the film "Dream for an Insomniac", featuring Jennifer Aniston, but which had a very limited release in 1996. [SEP] hypothesis:  The film "Dream for an Insomniac" written and directed by Tiffanie DeBartolo was widely released in 1996
05/24/2022 21:02:59 - INFO - __main__ - ['contradiction']
05/24/2022 21:02:59 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:02:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:02:59 - INFO - __main__ - Loaded 48 examples from dev data
05/24/2022 21:03:17 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 21:03:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/24/2022 21:03:17 - INFO - __main__ - Starting training!
05/24/2022 21:03:20 - INFO - __main__ - Step 10 Global step 10 Train loss 1.14 on epoch=3
05/24/2022 21:03:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.78 on epoch=6
05/24/2022 21:03:26 - INFO - __main__ - Step 30 Global step 30 Train loss 0.70 on epoch=9
05/24/2022 21:03:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=13
05/24/2022 21:03:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=16
05/24/2022 21:03:32 - INFO - __main__ - Global step 50 Train loss 0.78 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 21:03:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=16, global_step=50
05/24/2022 21:03:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=19
05/24/2022 21:03:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=23
05/24/2022 21:03:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=26
05/24/2022 21:03:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=29
05/24/2022 21:03:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=33
05/24/2022 21:03:45 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 21:03:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=36
05/24/2022 21:03:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=39
05/24/2022 21:03:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=43
05/24/2022 21:03:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=46
05/24/2022 21:03:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=49
05/24/2022 21:03:59 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.17486338797814208 on epoch=49
05/24/2022 21:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17486338797814208 on epoch=49, global_step=150
05/24/2022 21:04:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=53
05/24/2022 21:04:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=56
05/24/2022 21:04:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=59
05/24/2022 21:04:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=63
05/24/2022 21:04:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=66
05/24/2022 21:04:13 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 21:04:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=69
05/24/2022 21:04:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=73
05/24/2022 21:04:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=76
05/24/2022 21:04:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=79
05/24/2022 21:04:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=83
05/24/2022 21:04:26 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 21:04:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=86
05/24/2022 21:04:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=89
05/24/2022 21:04:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=93
05/24/2022 21:04:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=96
05/24/2022 21:04:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=99
05/24/2022 21:04:40 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.2216841538875437 on epoch=99
05/24/2022 21:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.17486338797814208 -> 0.2216841538875437 on epoch=99, global_step=300
05/24/2022 21:04:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=103
05/24/2022 21:04:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=106
05/24/2022 21:04:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=109
05/24/2022 21:04:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=113
05/24/2022 21:04:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=116
05/24/2022 21:04:54 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=116
05/24/2022 21:04:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=119
05/24/2022 21:04:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=123
05/24/2022 21:05:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=126
05/24/2022 21:05:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=129
05/24/2022 21:05:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=133
05/24/2022 21:05:08 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.24611708482676223 on epoch=133
05/24/2022 21:05:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2216841538875437 -> 0.24611708482676223 on epoch=133, global_step=400
05/24/2022 21:05:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=136
05/24/2022 21:05:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=139
05/24/2022 21:05:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=143
05/24/2022 21:05:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=146
05/24/2022 21:05:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=149
05/24/2022 21:05:22 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.2450388265746333 on epoch=149
05/24/2022 21:05:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=153
05/24/2022 21:05:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=156
05/24/2022 21:05:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=159
05/24/2022 21:05:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=163
05/24/2022 21:05:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=166
05/24/2022 21:05:36 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.22171370455123318 on epoch=166
05/24/2022 21:05:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=169
05/24/2022 21:05:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=173
05/24/2022 21:05:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=176
05/24/2022 21:05:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=179
05/24/2022 21:05:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=183
05/24/2022 21:05:50 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.21001779811848462 on epoch=183
05/24/2022 21:05:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=186
05/24/2022 21:05:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=189
05/24/2022 21:05:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=193
05/24/2022 21:06:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=196
05/24/2022 21:06:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=199
05/24/2022 21:06:03 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.22152560083594564 on epoch=199
05/24/2022 21:06:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=203
05/24/2022 21:06:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=206
05/24/2022 21:06:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=209
05/24/2022 21:06:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=213
05/24/2022 21:06:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=216
05/24/2022 21:06:17 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.22152560083594564 on epoch=216
05/24/2022 21:06:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=219
05/24/2022 21:06:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=223
05/24/2022 21:06:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=226
05/24/2022 21:06:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=229
05/24/2022 21:06:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=233
05/24/2022 21:06:31 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.22501747030048916 on epoch=233
05/24/2022 21:06:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=236
05/24/2022 21:06:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=239
05/24/2022 21:06:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=243
05/24/2022 21:06:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=246
05/24/2022 21:06:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=249
05/24/2022 21:06:44 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.1996844714897453 on epoch=249
05/24/2022 21:06:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=253
05/24/2022 21:06:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=256
05/24/2022 21:06:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=259
05/24/2022 21:06:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=263
05/24/2022 21:06:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=266
05/24/2022 21:06:58 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.21031746031746035 on epoch=266
05/24/2022 21:07:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=269
05/24/2022 21:07:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=273
05/24/2022 21:07:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=276
05/24/2022 21:07:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=279
05/24/2022 21:07:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=283
05/24/2022 21:07:12 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.19943019943019938 on epoch=283
05/24/2022 21:07:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=286
05/24/2022 21:07:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=289
05/24/2022 21:07:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=293
05/24/2022 21:07:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=296
05/24/2022 21:07:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=299
05/24/2022 21:07:25 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.23757575757575758 on epoch=299
05/24/2022 21:07:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=303
05/24/2022 21:07:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=306
05/24/2022 21:07:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=309
05/24/2022 21:07:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=313
05/24/2022 21:07:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=316
05/24/2022 21:07:39 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.23514211886304906 on epoch=316
05/24/2022 21:07:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=319
05/24/2022 21:07:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=323
05/24/2022 21:07:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=326
05/24/2022 21:07:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=329
05/24/2022 21:07:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=333
05/24/2022 21:07:53 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.24973942047112777 on epoch=333
05/24/2022 21:07:53 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.24973942047112777 on epoch=333, global_step=1000
05/24/2022 21:07:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=336
05/24/2022 21:07:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=339
05/24/2022 21:08:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=343
05/24/2022 21:08:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=346
05/24/2022 21:08:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=349
05/24/2022 21:08:07 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.2323232323232323 on epoch=349
05/24/2022 21:08:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=353
05/24/2022 21:08:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=356
05/24/2022 21:08:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=359
05/24/2022 21:08:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=363
05/24/2022 21:08:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.15 on epoch=366
05/24/2022 21:08:21 - INFO - __main__ - Global step 1100 Train loss 0.15 Classification-F1 0.2323232323232323 on epoch=366
05/24/2022 21:08:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=369
05/24/2022 21:08:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=373
05/24/2022 21:08:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=376
05/24/2022 21:08:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=379
05/24/2022 21:08:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=383
05/24/2022 21:08:35 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.22866967547818615 on epoch=383
05/24/2022 21:08:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=386
05/24/2022 21:08:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=389
05/24/2022 21:08:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=393
05/24/2022 21:08:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=396
05/24/2022 21:08:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=399
05/24/2022 21:08:49 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.19157608695652173 on epoch=399
05/24/2022 21:08:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=403
05/24/2022 21:08:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=406
05/24/2022 21:08:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=409
05/24/2022 21:08:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=413
05/24/2022 21:09:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=416
05/24/2022 21:09:02 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.19960474308300394 on epoch=416
05/24/2022 21:09:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=419
05/24/2022 21:09:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=423
05/24/2022 21:09:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=426
05/24/2022 21:09:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=429
05/24/2022 21:09:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=433
05/24/2022 21:09:16 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.20315717227179136 on epoch=433
05/24/2022 21:09:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=436
05/24/2022 21:09:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=439
05/24/2022 21:09:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=443
05/24/2022 21:09:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=446
05/24/2022 21:09:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=449
05/24/2022 21:09:30 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.20512820512820512 on epoch=449
05/24/2022 21:09:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=453
05/24/2022 21:09:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=456
05/24/2022 21:09:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=459
05/24/2022 21:09:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=463
05/24/2022 21:09:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=466
05/24/2022 21:09:44 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.19090909090909092 on epoch=466
05/24/2022 21:09:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=469
05/24/2022 21:09:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=473
05/24/2022 21:09:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=476
05/24/2022 21:09:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=479
05/24/2022 21:09:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=483
05/24/2022 21:09:58 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.1717948717948718 on epoch=483
05/24/2022 21:10:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=486
05/24/2022 21:10:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=489
05/24/2022 21:10:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=493
05/24/2022 21:10:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=496
05/24/2022 21:10:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=499
05/24/2022 21:10:12 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.2626107889265784 on epoch=499
05/24/2022 21:10:12 - INFO - __main__ - Saving model with best Classification-F1: 0.24973942047112777 -> 0.2626107889265784 on epoch=499, global_step=1500
05/24/2022 21:10:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=503
05/24/2022 21:10:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=506
05/24/2022 21:10:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=509
05/24/2022 21:10:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=513
05/24/2022 21:10:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=516
05/24/2022 21:10:26 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.23140512018760992 on epoch=516
05/24/2022 21:10:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=519
05/24/2022 21:10:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=523
05/24/2022 21:10:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=526
05/24/2022 21:10:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=529
05/24/2022 21:10:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=533
05/24/2022 21:10:41 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.13859020310633216 on epoch=533
05/24/2022 21:10:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=536
05/24/2022 21:10:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=539
05/24/2022 21:10:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=543
05/24/2022 21:10:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=546
05/24/2022 21:10:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=549
05/24/2022 21:10:55 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.22353801169590642 on epoch=549
05/24/2022 21:10:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=553
05/24/2022 21:11:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=556
05/24/2022 21:11:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=559
05/24/2022 21:11:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=563
05/24/2022 21:11:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=566
05/24/2022 21:11:09 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.21977721321086574 on epoch=566
05/24/2022 21:11:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=569
05/24/2022 21:11:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=573
05/24/2022 21:11:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=576
05/24/2022 21:11:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=579
05/24/2022 21:11:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=583
05/24/2022 21:11:23 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.2482638888888889 on epoch=583
05/24/2022 21:11:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=586
05/24/2022 21:11:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=589
05/24/2022 21:11:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=593
05/24/2022 21:11:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=596
05/24/2022 21:11:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=599
05/24/2022 21:11:37 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.1897912201317774 on epoch=599
05/24/2022 21:11:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=603
05/24/2022 21:11:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=606
05/24/2022 21:11:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=609
05/24/2022 21:11:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=613
05/24/2022 21:11:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=616
05/24/2022 21:11:51 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.16517027863777087 on epoch=616
05/24/2022 21:11:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=619
05/24/2022 21:11:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=623
05/24/2022 21:11:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=626
05/24/2022 21:12:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=629
05/24/2022 21:12:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=633
05/24/2022 21:12:05 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.1595939831233949 on epoch=633
05/24/2022 21:12:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=636
05/24/2022 21:12:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=639
05/24/2022 21:12:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=643
05/24/2022 21:12:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=646
05/24/2022 21:12:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=649
05/24/2022 21:12:20 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.2405628002745367 on epoch=649
05/24/2022 21:12:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=653
05/24/2022 21:12:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=656
05/24/2022 21:12:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=659
05/24/2022 21:12:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=663
05/24/2022 21:12:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=666
05/24/2022 21:12:35 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.13333333333333336 on epoch=666
05/24/2022 21:12:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=669
05/24/2022 21:12:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=673
05/24/2022 21:12:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=676
05/24/2022 21:12:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=679
05/24/2022 21:12:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=683
05/24/2022 21:12:51 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.15014005602240893 on epoch=683
05/24/2022 21:12:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=686
05/24/2022 21:12:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=689
05/24/2022 21:12:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=693
05/24/2022 21:13:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=696
05/24/2022 21:13:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=699
05/24/2022 21:13:06 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.248191832647157 on epoch=699
05/24/2022 21:13:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=703
05/24/2022 21:13:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=706
05/24/2022 21:13:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=709
05/24/2022 21:13:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=713
05/24/2022 21:13:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=716
05/24/2022 21:13:21 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.16654921306084097 on epoch=716
05/24/2022 21:13:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=719
05/24/2022 21:13:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=723
05/24/2022 21:13:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=726
05/24/2022 21:13:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=729
05/24/2022 21:13:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=733
05/24/2022 21:13:37 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.14427133664883213 on epoch=733
05/24/2022 21:13:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=736
05/24/2022 21:13:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=739
05/24/2022 21:13:45 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=743
05/24/2022 21:13:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=746
05/24/2022 21:13:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=749
05/24/2022 21:13:52 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.15283797729618162 on epoch=749
05/24/2022 21:13:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=753
05/24/2022 21:13:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=756
05/24/2022 21:13:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=759
05/24/2022 21:14:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=763
05/24/2022 21:14:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=766
05/24/2022 21:14:06 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.1645887445887446 on epoch=766
05/24/2022 21:14:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=769
05/24/2022 21:14:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=773
05/24/2022 21:14:14 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=776
05/24/2022 21:14:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=779
05/24/2022 21:14:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=783
05/24/2022 21:14:22 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.12376283846872083 on epoch=783
05/24/2022 21:14:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=786
05/24/2022 21:14:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=789
05/24/2022 21:14:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=793
05/24/2022 21:14:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=796
05/24/2022 21:14:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=799
05/24/2022 21:14:37 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.22153846153846155 on epoch=799
05/24/2022 21:14:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=803
05/24/2022 21:14:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=806
05/24/2022 21:14:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=809
05/24/2022 21:14:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=813
05/24/2022 21:14:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=816
05/24/2022 21:14:52 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.16762843068261787 on epoch=816
05/24/2022 21:14:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=819
05/24/2022 21:14:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=823
05/24/2022 21:14:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=826
05/24/2022 21:15:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=829
05/24/2022 21:15:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=833
05/24/2022 21:15:06 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.19398496240601504 on epoch=833
05/24/2022 21:15:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=836
05/24/2022 21:15:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=839
05/24/2022 21:15:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=843
05/24/2022 21:15:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=846
05/24/2022 21:15:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=849
05/24/2022 21:15:21 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.2365079365079365 on epoch=849
05/24/2022 21:15:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=853
05/24/2022 21:15:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=856
05/24/2022 21:15:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=859
05/24/2022 21:15:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=863
05/24/2022 21:15:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=866
05/24/2022 21:15:36 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.2379515056147629 on epoch=866
05/24/2022 21:15:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=869
05/24/2022 21:15:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=873
05/24/2022 21:15:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=876
05/24/2022 21:15:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=879
05/24/2022 21:15:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=883
05/24/2022 21:15:52 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.24040100250626564 on epoch=883
05/24/2022 21:15:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=886
05/24/2022 21:15:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=889
05/24/2022 21:16:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=893
05/24/2022 21:16:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=896
05/24/2022 21:16:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=899
05/24/2022 21:16:07 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.27097338935574233 on epoch=899
05/24/2022 21:16:07 - INFO - __main__ - Saving model with best Classification-F1: 0.2626107889265784 -> 0.27097338935574233 on epoch=899, global_step=2700
05/24/2022 21:16:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=903
05/24/2022 21:16:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=906
05/24/2022 21:16:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=909
05/24/2022 21:16:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=913
05/24/2022 21:16:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=916
05/24/2022 21:16:23 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.23112326270221004 on epoch=916
05/24/2022 21:16:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=919
05/24/2022 21:16:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=923
05/24/2022 21:16:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=926
05/24/2022 21:16:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=929
05/24/2022 21:16:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=933
05/24/2022 21:16:38 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.2606177606177606 on epoch=933
05/24/2022 21:16:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=936
05/24/2022 21:16:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=939
05/24/2022 21:16:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=943
05/24/2022 21:16:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=946
05/24/2022 21:16:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=949
05/24/2022 21:16:54 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.26089743589743586 on epoch=949
05/24/2022 21:16:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=953
05/24/2022 21:16:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=956
05/24/2022 21:17:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=959
05/24/2022 21:17:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=963
05/24/2022 21:17:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=966
05/24/2022 21:17:09 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.26089743589743586 on epoch=966
05/24/2022 21:17:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=969
05/24/2022 21:17:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=973
05/24/2022 21:17:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=976
05/24/2022 21:17:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=979
05/24/2022 21:17:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=983
05/24/2022 21:17:24 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.23921375921375923 on epoch=983
05/24/2022 21:17:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=986
05/24/2022 21:17:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=989
05/24/2022 21:17:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=993
05/24/2022 21:17:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=996
05/24/2022 21:17:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=999
05/24/2022 21:17:39 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.2692815249266862 on epoch=999
05/24/2022 21:17:39 - INFO - __main__ - save last model!
05/24/2022 21:17:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 21:17:39 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 21:17:39 - INFO - __main__ - Printing 3 examples
05/24/2022 21:17:39 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 21:17:39 - INFO - __main__ - ['contradiction']
05/24/2022 21:17:39 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 21:17:39 - INFO - __main__ - ['entailment']
05/24/2022 21:17:39 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 21:17:39 - INFO - __main__ - ['contradiction']
05/24/2022 21:17:39 - INFO - __main__ - Tokenizing Input ...
05/24/2022 21:17:40 - INFO - __main__ - Tokenizing Output ...
05/24/2022 21:17:41 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 21:18:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-anli/anli_16_87_0.2_8_predictions.txt
05/24/2022 21:18:28 - INFO - __main__ - Classification-F1 on test data: 0.0386
05/24/2022 21:18:28 - INFO - __main__ - prefix=anli_16_87, lr=0.2, bsz=8, dev_performance=0.27097338935574233, test_performance=0.03856843590858936
