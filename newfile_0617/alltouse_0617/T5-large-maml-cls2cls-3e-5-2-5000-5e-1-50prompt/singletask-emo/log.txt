05/23/2022 06:36:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-50prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='0,1')
05/23/2022 06:36:58 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo
05/23/2022 06:36:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-50prompt-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=50, cuda='0,1')
05/23/2022 06:36:58 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo
05/23/2022 06:36:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/23/2022 06:36:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/23/2022 06:36:59 - INFO - __main__ - args.device: cuda:1
05/23/2022 06:36:59 - INFO - __main__ - args.device: cuda:0
05/23/2022 06:36:59 - INFO - __main__ - Using 2 gpus
05/23/2022 06:36:59 - INFO - __main__ - Using 2 gpus
05/23/2022 06:36:59 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/23/2022 06:36:59 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/23/2022 06:37:04 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/23/2022 06:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:37:05 - INFO - __main__ - Printing 3 examples
05/23/2022 06:37:05 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:37:05 - INFO - __main__ - Printing 3 examples
05/23/2022 06:37:05 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:37:05 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 06:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:37:05 - INFO - __main__ - Printing 3 examples
05/23/2022 06:37:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:37:05 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 06:37:05 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:37:05 - INFO - __main__ - Printing 3 examples
05/23/2022 06:37:05 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 06:37:05 - INFO - __main__ - ['others']
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:37:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:37:05 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 06:37:05 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 06:37:23 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 06:37:23 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 06:37:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 06:37:24 - INFO - __main__ - Starting training!
05/23/2022 06:37:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 06:37:28 - INFO - __main__ - Starting training!
05/23/2022 06:37:31 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=2
05/23/2022 06:37:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.48 on epoch=4
05/23/2022 06:37:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.14 on epoch=7
05/23/2022 06:37:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=9
05/23/2022 06:37:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=12
05/23/2022 06:37:42 - INFO - __main__ - Global step 50 Train loss 1.48 Classification-F1 0.45521889400921656 on epoch=12
05/23/2022 06:37:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.45521889400921656 on epoch=12, global_step=50
05/23/2022 06:37:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=14
05/23/2022 06:37:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
05/23/2022 06:37:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=19
05/23/2022 06:37:51 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=22
05/23/2022 06:37:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=24
05/23/2022 06:37:54 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.20894844424256187 on epoch=24
05/23/2022 06:37:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=27
05/23/2022 06:37:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.78 on epoch=29
05/23/2022 06:38:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.74 on epoch=32
05/23/2022 06:38:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.72 on epoch=34
05/23/2022 06:38:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=37
05/23/2022 06:38:07 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.6103219696969696 on epoch=37
05/23/2022 06:38:07 - INFO - __main__ - Saving model with best Classification-F1: 0.45521889400921656 -> 0.6103219696969696 on epoch=37, global_step=150
05/23/2022 06:38:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=39
05/23/2022 06:38:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.70 on epoch=42
05/23/2022 06:38:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=44
05/23/2022 06:38:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=47
05/23/2022 06:38:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=49
05/23/2022 06:38:20 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5139045736871823 on epoch=49
05/23/2022 06:38:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.71 on epoch=52
05/23/2022 06:38:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=54
05/23/2022 06:38:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.66 on epoch=57
05/23/2022 06:38:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=59
05/23/2022 06:38:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=62
05/23/2022 06:38:33 - INFO - __main__ - Global step 250 Train loss 0.64 Classification-F1 0.5769102148558856 on epoch=62
05/23/2022 06:38:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=64
05/23/2022 06:38:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=67
05/23/2022 06:38:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=69
05/23/2022 06:38:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=72
05/23/2022 06:38:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=74
05/23/2022 06:38:46 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.5852229780801209 on epoch=74
05/23/2022 06:38:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=77
05/23/2022 06:38:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=79
05/23/2022 06:38:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=82
05/23/2022 06:38:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=84
05/23/2022 06:38:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=87
05/23/2022 06:38:58 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.5406600356691101 on epoch=87
05/23/2022 06:39:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=89
05/23/2022 06:39:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=92
05/23/2022 06:39:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=94
05/23/2022 06:39:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=97
05/23/2022 06:39:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=99
05/23/2022 06:39:11 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.6541702993315895 on epoch=99
05/23/2022 06:39:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6103219696969696 -> 0.6541702993315895 on epoch=99, global_step=400
05/23/2022 06:39:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=102
05/23/2022 06:39:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=104
05/23/2022 06:39:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
05/23/2022 06:39:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=109
05/23/2022 06:39:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=112
05/23/2022 06:39:24 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.540527950310559 on epoch=112
05/23/2022 06:39:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=114
05/23/2022 06:39:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=117
05/23/2022 06:39:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/23/2022 06:39:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=122
05/23/2022 06:39:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=124
05/23/2022 06:39:38 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.696309412861137 on epoch=124
05/23/2022 06:39:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6541702993315895 -> 0.696309412861137 on epoch=124, global_step=500
05/23/2022 06:39:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=127
05/23/2022 06:39:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/23/2022 06:39:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=132
05/23/2022 06:39:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=134
05/23/2022 06:39:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=137
05/23/2022 06:39:52 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.6101330913490624 on epoch=137
05/23/2022 06:39:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=139
05/23/2022 06:39:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=142
05/23/2022 06:39:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=144
05/23/2022 06:40:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=147
05/23/2022 06:40:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.08 on epoch=149
05/23/2022 06:40:07 - INFO - __main__ - Global step 600 Train loss 0.16 Classification-F1 0.6098438033921905 on epoch=149
05/23/2022 06:40:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/23/2022 06:40:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
05/23/2022 06:40:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=157
05/23/2022 06:40:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=159
05/23/2022 06:40:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.07 on epoch=162
05/23/2022 06:40:21 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.5218500797448167 on epoch=162
05/23/2022 06:40:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
05/23/2022 06:40:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=167
05/23/2022 06:40:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=169
05/23/2022 06:40:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=172
05/23/2022 06:40:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
05/23/2022 06:40:35 - INFO - __main__ - Global step 700 Train loss 0.12 Classification-F1 0.56524064171123 on epoch=174
05/23/2022 06:40:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=177
05/23/2022 06:40:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
05/23/2022 06:40:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=182
05/23/2022 06:40:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=184
05/23/2022 06:40:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
05/23/2022 06:40:50 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.6078956582633054 on epoch=187
05/23/2022 06:40:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=189
05/23/2022 06:40:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.06 on epoch=192
05/23/2022 06:40:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
05/23/2022 06:41:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=197
05/23/2022 06:41:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=199
05/23/2022 06:41:06 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.6254279110815614 on epoch=199
05/23/2022 06:41:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
05/23/2022 06:41:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
05/23/2022 06:41:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=207
05/23/2022 06:41:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=209
05/23/2022 06:41:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/23/2022 06:41:21 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.5701884920634921 on epoch=212
05/23/2022 06:41:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=214
05/23/2022 06:41:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=217
05/23/2022 06:41:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/23/2022 06:41:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.06 on epoch=222
05/23/2022 06:41:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
05/23/2022 06:41:36 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.5480936819172113 on epoch=224
05/23/2022 06:41:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.08 on epoch=227
05/23/2022 06:41:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=229
05/23/2022 06:41:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/23/2022 06:41:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/23/2022 06:41:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
05/23/2022 06:41:52 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6201069518716578 on epoch=237
05/23/2022 06:41:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=239
05/23/2022 06:41:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
05/23/2022 06:41:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=244
05/23/2022 06:42:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
05/23/2022 06:42:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/23/2022 06:42:06 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.5022864959937463 on epoch=249
05/23/2022 06:42:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/23/2022 06:42:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/23/2022 06:42:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/23/2022 06:42:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/23/2022 06:42:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
05/23/2022 06:42:19 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.6557105492589363 on epoch=262
05/23/2022 06:42:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/23/2022 06:42:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 06:42:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 06:42:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 06:42:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/23/2022 06:42:34 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.5825943901133654 on epoch=274
05/23/2022 06:42:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/23/2022 06:42:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 06:42:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/23/2022 06:42:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/23/2022 06:42:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
05/23/2022 06:42:50 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6554154995331466 on epoch=287
05/23/2022 06:42:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/23/2022 06:42:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/23/2022 06:42:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/23/2022 06:42:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
05/23/2022 06:43:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/23/2022 06:43:04 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.5602014273327552 on epoch=299
05/23/2022 06:43:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 06:43:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/23/2022 06:43:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/23/2022 06:43:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/23/2022 06:43:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
05/23/2022 06:43:20 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6667582417582417 on epoch=312
05/23/2022 06:43:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/23/2022 06:43:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/23/2022 06:43:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/23/2022 06:43:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=322
05/23/2022 06:43:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=324
05/23/2022 06:43:35 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.5718621075157578 on epoch=324
05/23/2022 06:43:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=327
05/23/2022 06:43:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=329
05/23/2022 06:43:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 06:43:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/23/2022 06:43:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 06:43:50 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5701148031368745 on epoch=337
05/23/2022 06:43:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/23/2022 06:43:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 06:43:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 06:44:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/23/2022 06:44:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 06:44:06 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5543978079461951 on epoch=349
05/23/2022 06:44:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 06:44:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 06:44:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=357
05/23/2022 06:44:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 06:44:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 06:44:22 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.614552811921233 on epoch=362
05/23/2022 06:44:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/23/2022 06:44:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 06:44:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/23/2022 06:44:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 06:44:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/23/2022 06:44:38 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.642135989010989 on epoch=374
05/23/2022 06:44:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/23/2022 06:44:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 06:44:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/23/2022 06:44:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/23/2022 06:44:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/23/2022 06:44:52 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6163950991537198 on epoch=387
05/23/2022 06:44:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/23/2022 06:44:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 06:45:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 06:45:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 06:45:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 06:45:08 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.5816050672587175 on epoch=399
05/23/2022 06:45:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 06:45:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/23/2022 06:45:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/23/2022 06:45:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 06:45:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 06:45:23 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6828431372549019 on epoch=412
05/23/2022 06:45:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 06:45:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 06:45:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 06:45:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/23/2022 06:45:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=424
05/23/2022 06:45:38 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.5690090611143243 on epoch=424
05/23/2022 06:45:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 06:45:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 06:45:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/23/2022 06:45:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/23/2022 06:45:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/23/2022 06:45:52 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6112171612171612 on epoch=437
05/23/2022 06:45:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/23/2022 06:45:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/23/2022 06:45:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/23/2022 06:46:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 06:46:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/23/2022 06:46:07 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.5969778268760356 on epoch=449
05/23/2022 06:46:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 06:46:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 06:46:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 06:46:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 06:46:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 06:46:22 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.613507989594946 on epoch=462
05/23/2022 06:46:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/23/2022 06:46:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 06:46:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 06:46:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 06:46:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 06:46:36 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6471235941824177 on epoch=474
05/23/2022 06:46:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 06:46:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 06:46:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 06:46:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 06:46:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/23/2022 06:46:51 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5904511278195489 on epoch=487
05/23/2022 06:46:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=489
05/23/2022 06:46:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/23/2022 06:46:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/23/2022 06:47:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 06:47:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 06:47:06 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6465815651773905 on epoch=499
05/23/2022 06:47:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 06:47:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 06:47:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/23/2022 06:47:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 06:47:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/23/2022 06:47:21 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6658029878618114 on epoch=512
05/23/2022 06:47:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 06:47:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 06:47:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=519
05/23/2022 06:47:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 06:47:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 06:47:35 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.61661069936932 on epoch=524
05/23/2022 06:47:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 06:47:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 06:47:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 06:47:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 06:47:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 06:47:50 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.590509645153608 on epoch=537
05/23/2022 06:47:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 06:47:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=542
05/23/2022 06:47:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 06:48:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 06:48:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 06:48:05 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6123013173356422 on epoch=549
05/23/2022 06:48:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 06:48:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 06:48:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 06:48:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 06:48:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 06:48:20 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.635672514619883 on epoch=562
05/23/2022 06:48:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 06:48:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 06:48:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/23/2022 06:48:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 06:48:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 06:48:35 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6332399626517273 on epoch=574
05/23/2022 06:48:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 06:48:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/23/2022 06:48:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 06:48:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 06:48:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=587
05/23/2022 06:48:50 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.6112860112860113 on epoch=587
05/23/2022 06:48:52 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 06:48:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/23/2022 06:48:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=594
05/23/2022 06:48:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 06:49:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 06:49:04 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6390804184921832 on epoch=599
05/23/2022 06:49:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 06:49:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 06:49:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=607
05/23/2022 06:49:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 06:49:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 06:49:18 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6351009369264947 on epoch=612
05/23/2022 06:49:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 06:49:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 06:49:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 06:49:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 06:49:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 06:49:33 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6511092170713974 on epoch=624
05/23/2022 06:49:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/23/2022 06:49:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 06:49:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 06:49:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 06:49:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 06:49:47 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6584107052857053 on epoch=637
05/23/2022 06:49:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 06:49:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 06:49:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 06:49:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 06:49:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 06:50:00 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6430408660010178 on epoch=649
05/23/2022 06:50:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 06:50:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 06:50:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 06:50:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 06:50:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/23/2022 06:50:14 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6414248349732221 on epoch=662
05/23/2022 06:50:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
05/23/2022 06:50:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/23/2022 06:50:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=669
05/23/2022 06:50:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 06:50:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 06:50:28 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.6306113241597113 on epoch=674
05/23/2022 06:50:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 06:50:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 06:50:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 06:50:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 06:50:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 06:50:42 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6517676035931613 on epoch=687
05/23/2022 06:50:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 06:50:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 06:50:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 06:50:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 06:50:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 06:50:56 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.644030294030294 on epoch=699
05/23/2022 06:50:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 06:51:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 06:51:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 06:51:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 06:51:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/23/2022 06:51:12 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6178571428571429 on epoch=712
05/23/2022 06:51:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 06:51:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 06:51:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 06:51:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 06:51:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
05/23/2022 06:51:26 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5823297491039426 on epoch=724
05/23/2022 06:51:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 06:51:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 06:51:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 06:51:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 06:51:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 06:51:41 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6427083333333333 on epoch=737
05/23/2022 06:51:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 06:51:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 06:51:48 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 06:51:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 06:51:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 06:51:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:51:54 - INFO - __main__ - Printing 3 examples
05/23/2022 06:51:54 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:51:54 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:51:54 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 06:51:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:51:54 - INFO - __main__ - Printing 3 examples
05/23/2022 06:51:54 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 06:51:54 - INFO - __main__ - ['others']
05/23/2022 06:51:54 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:51:54 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:51:54 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 06:51:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.592156862745098 on epoch=749
05/23/2022 06:51:55 - INFO - __main__ - save last model!
05/23/2022 06:51:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 06:51:55 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 06:51:55 - INFO - __main__ - Printing 3 examples
05/23/2022 06:51:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 06:51:55 - INFO - __main__ - ['others']
05/23/2022 06:51:55 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 06:51:55 - INFO - __main__ - ['others']
05/23/2022 06:51:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 06:51:55 - INFO - __main__ - ['others']
05/23/2022 06:51:55 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:51:57 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:52:03 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 06:52:09 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 06:52:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 06:52:10 - INFO - __main__ - Starting training!
05/23/2022 06:55:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/23/2022 06:55:57 - INFO - __main__ - Classification-F1 on test data: 0.2775
05/23/2022 06:55:57 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.696309412861137, test_performance=0.27751360718405954
05/23/2022 06:55:57 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/23/2022 06:55:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:55:58 - INFO - __main__ - Printing 3 examples
05/23/2022 06:55:58 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:55:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:55:58 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 06:55:58 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 06:55:58 - INFO - __main__ - Printing 3 examples
05/23/2022 06:55:58 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 06:55:58 - INFO - __main__ - ['others']
05/23/2022 06:55:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 06:55:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 06:55:59 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 06:56:17 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 06:56:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 06:56:18 - INFO - __main__ - Starting training!
05/23/2022 06:56:21 - INFO - __main__ - Step 10 Global step 10 Train loss 3.04 on epoch=2
05/23/2022 06:56:23 - INFO - __main__ - Step 20 Global step 20 Train loss 1.71 on epoch=4
05/23/2022 06:56:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.40 on epoch=7
05/23/2022 06:56:28 - INFO - __main__ - Step 40 Global step 40 Train loss 1.13 on epoch=9
05/23/2022 06:56:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=12
05/23/2022 06:56:31 - INFO - __main__ - Global step 50 Train loss 1.65 Classification-F1 0.2694805194805195 on epoch=12
05/23/2022 06:56:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2694805194805195 on epoch=12, global_step=50
05/23/2022 06:56:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.90 on epoch=14
05/23/2022 06:56:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=17
05/23/2022 06:56:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=19
05/23/2022 06:56:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
05/23/2022 06:56:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
05/23/2022 06:56:44 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.4026226051061157 on epoch=24
05/23/2022 06:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.2694805194805195 -> 0.4026226051061157 on epoch=24, global_step=100
05/23/2022 06:56:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=27
05/23/2022 06:56:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
05/23/2022 06:56:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=32
05/23/2022 06:56:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.87 on epoch=34
05/23/2022 06:56:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=37
05/23/2022 06:56:57 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.5774551921655581 on epoch=37
05/23/2022 06:56:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4026226051061157 -> 0.5774551921655581 on epoch=37, global_step=150
05/23/2022 06:57:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=39
05/23/2022 06:57:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=42
05/23/2022 06:57:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=44
05/23/2022 06:57:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=47
05/23/2022 06:57:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.67 on epoch=49
05/23/2022 06:57:10 - INFO - __main__ - Global step 200 Train loss 0.70 Classification-F1 0.41292824822236585 on epoch=49
05/23/2022 06:57:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.69 on epoch=52
05/23/2022 06:57:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=54
05/23/2022 06:57:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=57
05/23/2022 06:57:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=59
05/23/2022 06:57:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=62
05/23/2022 06:57:23 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.5723332395159021 on epoch=62
05/23/2022 06:57:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=64
05/23/2022 06:57:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=67
05/23/2022 06:57:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=69
05/23/2022 06:57:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.60 on epoch=72
05/23/2022 06:57:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=74
05/23/2022 06:57:36 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.6735823934837093 on epoch=74
05/23/2022 06:57:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5774551921655581 -> 0.6735823934837093 on epoch=74, global_step=300
05/23/2022 06:57:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=77
05/23/2022 06:57:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=79
05/23/2022 06:57:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=82
05/23/2022 06:57:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=84
05/23/2022 06:57:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=87
05/23/2022 06:57:49 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.5866013071895425 on epoch=87
05/23/2022 06:57:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=89
05/23/2022 06:57:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=92
05/23/2022 06:57:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=94
05/23/2022 06:57:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=97
05/23/2022 06:58:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=99
05/23/2022 06:58:02 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.6020032051282052 on epoch=99
05/23/2022 06:58:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=102
05/23/2022 06:58:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=104
05/23/2022 06:58:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=107
05/23/2022 06:58:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
05/23/2022 06:58:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=112
05/23/2022 06:58:15 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.5826839826839827 on epoch=112
05/23/2022 06:58:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=114
05/23/2022 06:58:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=117
05/23/2022 06:58:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.32 on epoch=119
05/23/2022 06:58:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/23/2022 06:58:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=124
05/23/2022 06:58:27 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.6635931441300222 on epoch=124
05/23/2022 06:58:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=127
05/23/2022 06:58:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=129
05/23/2022 06:58:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/23/2022 06:58:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=134
05/23/2022 06:58:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=137
05/23/2022 06:58:40 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.6860197368421053 on epoch=137
05/23/2022 06:58:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6735823934837093 -> 0.6860197368421053 on epoch=137, global_step=550
05/23/2022 06:58:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=139
05/23/2022 06:58:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=142
05/23/2022 06:58:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=144
05/23/2022 06:58:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=147
05/23/2022 06:58:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=149
05/23/2022 06:58:53 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.6467329790113432 on epoch=149
05/23/2022 06:58:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=152
05/23/2022 06:58:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=154
05/23/2022 06:59:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.15 on epoch=157
05/23/2022 06:59:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=159
05/23/2022 06:59:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=162
05/23/2022 06:59:06 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.7174001205042639 on epoch=162
05/23/2022 06:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6860197368421053 -> 0.7174001205042639 on epoch=162, global_step=650
05/23/2022 06:59:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
05/23/2022 06:59:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/23/2022 06:59:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=169
05/23/2022 06:59:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.14 on epoch=172
05/23/2022 06:59:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=174
05/23/2022 06:59:20 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.6640567765567766 on epoch=174
05/23/2022 06:59:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=177
05/23/2022 06:59:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
05/23/2022 06:59:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/23/2022 06:59:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
05/23/2022 06:59:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=187
05/23/2022 06:59:33 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.7009926854754441 on epoch=187
05/23/2022 06:59:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=189
05/23/2022 06:59:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/23/2022 06:59:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=194
05/23/2022 06:59:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.09 on epoch=197
05/23/2022 06:59:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
05/23/2022 06:59:46 - INFO - __main__ - Global step 800 Train loss 0.12 Classification-F1 0.654888249005896 on epoch=199
05/23/2022 06:59:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=202
05/23/2022 06:59:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
05/23/2022 06:59:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
05/23/2022 06:59:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/23/2022 06:59:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=212
05/23/2022 07:00:03 - INFO - __main__ - Global step 850 Train loss 0.09 Classification-F1 0.700753083491461 on epoch=212
05/23/2022 07:00:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=214
05/23/2022 07:00:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.15 on epoch=217
05/23/2022 07:00:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.08 on epoch=219
05/23/2022 07:00:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/23/2022 07:00:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=224
05/23/2022 07:00:19 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7245767745767745 on epoch=224
05/23/2022 07:00:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7174001205042639 -> 0.7245767745767745 on epoch=224, global_step=900
05/23/2022 07:00:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
05/23/2022 07:00:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
05/23/2022 07:00:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/23/2022 07:00:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
05/23/2022 07:00:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=237
05/23/2022 07:00:34 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.668100358422939 on epoch=237
05/23/2022 07:00:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.05 on epoch=239
05/23/2022 07:00:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
05/23/2022 07:00:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
05/23/2022 07:00:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=247
05/23/2022 07:00:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=249
05/23/2022 07:00:49 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.6886716791979951 on epoch=249
05/23/2022 07:00:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/23/2022 07:00:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/23/2022 07:00:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/23/2022 07:00:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/23/2022 07:01:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=262
05/23/2022 07:01:05 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.6940197294798813 on epoch=262
05/23/2022 07:01:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=264
05/23/2022 07:01:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.06 on epoch=267
05/23/2022 07:01:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/23/2022 07:01:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/23/2022 07:01:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
05/23/2022 07:01:19 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.7140703438877881 on epoch=274
05/23/2022 07:01:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=277
05/23/2022 07:01:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=279
05/23/2022 07:01:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=282
05/23/2022 07:01:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=284
05/23/2022 07:01:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
05/23/2022 07:01:33 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.6252300437083045 on epoch=287
05/23/2022 07:01:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=289
05/23/2022 07:01:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/23/2022 07:01:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/23/2022 07:01:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=297
05/23/2022 07:01:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/23/2022 07:01:48 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6780375253549696 on epoch=299
05/23/2022 07:01:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 07:01:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/23/2022 07:01:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/23/2022 07:01:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
05/23/2022 07:02:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/23/2022 07:02:04 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.6693948412698413 on epoch=312
05/23/2022 07:02:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 07:02:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/23/2022 07:02:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/23/2022 07:02:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=322
05/23/2022 07:02:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=324
05/23/2022 07:02:19 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6705908674658674 on epoch=324
05/23/2022 07:02:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/23/2022 07:02:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=329
05/23/2022 07:02:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
05/23/2022 07:02:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 07:02:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=337
05/23/2022 07:02:34 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.710176282051282 on epoch=337
05/23/2022 07:02:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/23/2022 07:02:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/23/2022 07:02:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 07:02:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 07:02:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/23/2022 07:02:48 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6606803542287414 on epoch=349
05/23/2022 07:02:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/23/2022 07:02:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/23/2022 07:02:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/23/2022 07:02:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 07:03:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/23/2022 07:03:04 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7460176055510741 on epoch=362
05/23/2022 07:03:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7245767745767745 -> 0.7460176055510741 on epoch=362, global_step=1450
05/23/2022 07:03:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/23/2022 07:03:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 07:03:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 07:03:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 07:03:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/23/2022 07:03:19 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.6440451893541139 on epoch=374
05/23/2022 07:03:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/23/2022 07:03:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 07:03:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/23/2022 07:03:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/23/2022 07:03:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 07:03:34 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.687979094076655 on epoch=387
05/23/2022 07:03:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/23/2022 07:03:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 07:03:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/23/2022 07:03:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/23/2022 07:03:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/23/2022 07:03:49 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6045755319948868 on epoch=399
05/23/2022 07:03:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/23/2022 07:03:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/23/2022 07:03:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/23/2022 07:03:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 07:04:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 07:04:05 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6704833716168981 on epoch=412
05/23/2022 07:04:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/23/2022 07:04:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/23/2022 07:04:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 07:04:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/23/2022 07:04:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 07:04:18 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6354885057471265 on epoch=424
05/23/2022 07:04:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 07:04:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 07:04:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 07:04:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=434
05/23/2022 07:04:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=437
05/23/2022 07:04:33 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6318966616398546 on epoch=437
05/23/2022 07:04:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/23/2022 07:04:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/23/2022 07:04:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/23/2022 07:04:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 07:04:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/23/2022 07:04:48 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.6202362278881408 on epoch=449
05/23/2022 07:04:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 07:04:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 07:04:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/23/2022 07:04:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/23/2022 07:05:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 07:05:03 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6625000000000001 on epoch=462
05/23/2022 07:05:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/23/2022 07:05:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 07:05:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/23/2022 07:05:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 07:05:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 07:05:17 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6347743020068997 on epoch=474
05/23/2022 07:05:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 07:05:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 07:05:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/23/2022 07:05:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 07:05:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 07:05:33 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6367302052785924 on epoch=487
05/23/2022 07:05:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/23/2022 07:05:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=492
05/23/2022 07:05:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 07:05:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 07:05:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 07:05:48 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6582465319573509 on epoch=499
05/23/2022 07:05:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 07:05:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 07:05:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=507
05/23/2022 07:05:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 07:06:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 07:06:03 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.684009009009009 on epoch=512
05/23/2022 07:06:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 07:06:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=517
05/23/2022 07:06:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/23/2022 07:06:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 07:06:15 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 07:06:17 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7258935405994229 on epoch=524
05/23/2022 07:06:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 07:06:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 07:06:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/23/2022 07:06:27 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 07:06:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/23/2022 07:06:32 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.6495098039215687 on epoch=537
05/23/2022 07:06:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/23/2022 07:06:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/23/2022 07:06:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 07:06:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/23/2022 07:06:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/23/2022 07:06:46 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.646474358974359 on epoch=549
05/23/2022 07:06:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 07:06:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 07:06:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 07:06:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 07:06:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 07:07:01 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6367302052785924 on epoch=562
05/23/2022 07:07:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/23/2022 07:07:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/23/2022 07:07:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 07:07:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 07:07:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=574
05/23/2022 07:07:16 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6200581028167235 on epoch=574
05/23/2022 07:07:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 07:07:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 07:07:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 07:07:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 07:07:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 07:07:30 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7078489326765189 on epoch=587
05/23/2022 07:07:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 07:07:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/23/2022 07:07:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 07:07:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 07:07:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 07:07:45 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6703947368421053 on epoch=599
05/23/2022 07:07:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 07:07:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=604
05/23/2022 07:07:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 07:07:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 07:07:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 07:08:00 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6226516726516726 on epoch=612
05/23/2022 07:08:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 07:08:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 07:08:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 07:08:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 07:08:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 07:08:14 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6314716250777632 on epoch=624
05/23/2022 07:08:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 07:08:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 07:08:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 07:08:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 07:08:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 07:08:28 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6619565217391304 on epoch=637
05/23/2022 07:08:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/23/2022 07:08:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 07:08:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 07:08:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 07:08:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 07:08:43 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6963022185143988 on epoch=649
05/23/2022 07:08:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 07:08:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 07:08:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 07:08:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 07:08:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 07:08:57 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6053557516972151 on epoch=662
05/23/2022 07:08:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 07:09:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 07:09:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=669
05/23/2022 07:09:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/23/2022 07:09:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 07:09:10 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.664678464254286 on epoch=674
05/23/2022 07:09:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 07:09:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 07:09:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
05/23/2022 07:09:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 07:09:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/23/2022 07:09:24 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6890814777327936 on epoch=687
05/23/2022 07:09:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 07:09:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 07:09:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 07:09:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 07:09:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 07:09:38 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6740190979657934 on epoch=699
05/23/2022 07:09:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 07:09:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 07:09:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=707
05/23/2022 07:09:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 07:09:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 07:09:52 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6956209521733715 on epoch=712
05/23/2022 07:09:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 07:09:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 07:10:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 07:10:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 07:10:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=724
05/23/2022 07:10:06 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6858630952380953 on epoch=724
05/23/2022 07:10:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/23/2022 07:10:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
05/23/2022 07:10:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 07:10:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 07:10:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 07:10:21 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6835220673635308 on epoch=737
05/23/2022 07:10:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 07:10:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 07:10:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
05/23/2022 07:10:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=747
05/23/2022 07:10:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 07:10:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:10:34 - INFO - __main__ - Printing 3 examples
05/23/2022 07:10:34 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:10:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:10:34 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:10:34 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:10:34 - INFO - __main__ - Printing 3 examples
05/23/2022 07:10:34 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 07:10:34 - INFO - __main__ - ['others']
05/23/2022 07:10:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:10:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:10:34 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:10:35 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7116064116064116 on epoch=749
05/23/2022 07:10:35 - INFO - __main__ - save last model!
05/23/2022 07:10:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 07:10:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 07:10:35 - INFO - __main__ - Printing 3 examples
05/23/2022 07:10:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 07:10:35 - INFO - __main__ - ['others']
05/23/2022 07:10:35 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 07:10:35 - INFO - __main__ - ['others']
05/23/2022 07:10:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 07:10:35 - INFO - __main__ - ['others']
05/23/2022 07:10:35 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:10:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:10:43 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 07:10:53 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:10:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:10:54 - INFO - __main__ - Starting training!
05/23/2022 07:14:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/23/2022 07:14:41 - INFO - __main__ - Classification-F1 on test data: 0.2953
05/23/2022 07:14:42 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7460176055510741, test_performance=0.2953329703251377
05/23/2022 07:14:42 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/23/2022 07:14:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:14:43 - INFO - __main__ - Printing 3 examples
05/23/2022 07:14:43 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:14:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:14:43 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:14:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:14:43 - INFO - __main__ - Printing 3 examples
05/23/2022 07:14:43 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 07:14:43 - INFO - __main__ - ['others']
05/23/2022 07:14:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:14:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:14:43 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:15:01 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:15:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:15:02 - INFO - __main__ - Starting training!
05/23/2022 07:15:05 - INFO - __main__ - Step 10 Global step 10 Train loss 3.05 on epoch=2
05/23/2022 07:15:07 - INFO - __main__ - Step 20 Global step 20 Train loss 1.93 on epoch=4
05/23/2022 07:15:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.53 on epoch=7
05/23/2022 07:15:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.13 on epoch=9
05/23/2022 07:15:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=12
05/23/2022 07:15:15 - INFO - __main__ - Global step 50 Train loss 1.73 Classification-F1 0.35985444531864674 on epoch=12
05/23/2022 07:15:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.35985444531864674 on epoch=12, global_step=50
05/23/2022 07:15:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=14
05/23/2022 07:15:20 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=17
05/23/2022 07:15:22 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=19
05/23/2022 07:15:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=22
05/23/2022 07:15:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=24
05/23/2022 07:15:28 - INFO - __main__ - Global step 100 Train loss 0.99 Classification-F1 0.19879582267689683 on epoch=24
05/23/2022 07:15:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=27
05/23/2022 07:15:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/23/2022 07:15:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=32
05/23/2022 07:15:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=34
05/23/2022 07:15:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=37
05/23/2022 07:15:40 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.4413943355119826 on epoch=37
05/23/2022 07:15:40 - INFO - __main__ - Saving model with best Classification-F1: 0.35985444531864674 -> 0.4413943355119826 on epoch=37, global_step=150
05/23/2022 07:15:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=39
05/23/2022 07:15:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=42
05/23/2022 07:15:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=44
05/23/2022 07:15:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=47
05/23/2022 07:15:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=49
05/23/2022 07:15:53 - INFO - __main__ - Global step 200 Train loss 0.78 Classification-F1 0.4251290224650881 on epoch=49
05/23/2022 07:15:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=52
05/23/2022 07:15:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=54
05/23/2022 07:16:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=57
05/23/2022 07:16:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=59
05/23/2022 07:16:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.66 on epoch=62
05/23/2022 07:16:05 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.5538543328017012 on epoch=62
05/23/2022 07:16:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4413943355119826 -> 0.5538543328017012 on epoch=62, global_step=250
05/23/2022 07:16:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=64
05/23/2022 07:16:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=67
05/23/2022 07:16:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=69
05/23/2022 07:16:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=72
05/23/2022 07:16:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=74
05/23/2022 07:16:18 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.5332792207792207 on epoch=74
05/23/2022 07:16:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=77
05/23/2022 07:16:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=79
05/23/2022 07:16:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.63 on epoch=82
05/23/2022 07:16:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=84
05/23/2022 07:16:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=87
05/23/2022 07:16:31 - INFO - __main__ - Global step 350 Train loss 0.64 Classification-F1 0.5289172209903917 on epoch=87
05/23/2022 07:16:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=89
05/23/2022 07:16:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=92
05/23/2022 07:16:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=94
05/23/2022 07:16:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=97
05/23/2022 07:16:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=99
05/23/2022 07:16:43 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.4619050789529167 on epoch=99
05/23/2022 07:16:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=102
05/23/2022 07:16:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=104
05/23/2022 07:16:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=107
05/23/2022 07:16:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=109
05/23/2022 07:16:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=112
05/23/2022 07:16:56 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.4775461466637938 on epoch=112
05/23/2022 07:16:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=114
05/23/2022 07:17:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=117
05/23/2022 07:17:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=119
05/23/2022 07:17:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=122
05/23/2022 07:17:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
05/23/2022 07:17:09 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.622844827586207 on epoch=124
05/23/2022 07:17:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5538543328017012 -> 0.622844827586207 on epoch=124, global_step=500
05/23/2022 07:17:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=127
05/23/2022 07:17:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=129
05/23/2022 07:17:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=132
05/23/2022 07:17:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=134
05/23/2022 07:17:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
05/23/2022 07:17:22 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.5628288378288379 on epoch=137
05/23/2022 07:17:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=139
05/23/2022 07:17:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=142
05/23/2022 07:17:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=144
05/23/2022 07:17:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=147
05/23/2022 07:17:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=149
05/23/2022 07:17:34 - INFO - __main__ - Global step 600 Train loss 0.30 Classification-F1 0.5251157407407407 on epoch=149
05/23/2022 07:17:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=152
05/23/2022 07:17:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=154
05/23/2022 07:17:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=157
05/23/2022 07:17:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=159
05/23/2022 07:17:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=162
05/23/2022 07:17:47 - INFO - __main__ - Global step 650 Train loss 0.27 Classification-F1 0.6151602229188436 on epoch=162
05/23/2022 07:17:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=164
05/23/2022 07:17:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=167
05/23/2022 07:17:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/23/2022 07:17:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=172
05/23/2022 07:17:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/23/2022 07:18:00 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.5608743539778023 on epoch=174
05/23/2022 07:18:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=177
05/23/2022 07:18:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/23/2022 07:18:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/23/2022 07:18:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=184
05/23/2022 07:18:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/23/2022 07:18:12 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.5803971704972817 on epoch=187
05/23/2022 07:18:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
05/23/2022 07:18:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
05/23/2022 07:18:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=194
05/23/2022 07:18:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=197
05/23/2022 07:18:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=199
05/23/2022 07:18:25 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.513137315270936 on epoch=199
05/23/2022 07:18:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=202
05/23/2022 07:18:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
05/23/2022 07:18:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=207
05/23/2022 07:18:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=209
05/23/2022 07:18:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=212
05/23/2022 07:18:38 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.5321969696969697 on epoch=212
05/23/2022 07:18:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=214
05/23/2022 07:18:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=217
05/23/2022 07:18:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
05/23/2022 07:18:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/23/2022 07:18:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.06 on epoch=224
05/23/2022 07:18:52 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.5680329856900316 on epoch=224
05/23/2022 07:18:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=227
05/23/2022 07:18:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
05/23/2022 07:18:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=232
05/23/2022 07:19:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.19 on epoch=234
05/23/2022 07:19:04 - INFO - __main__ - Step 950 Global step 950 Train loss 1.94 on epoch=237
05/23/2022 07:19:05 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.548248494800219 on epoch=237
05/23/2022 07:19:07 - INFO - __main__ - Step 960 Global step 960 Train loss 1.34 on epoch=239
05/23/2022 07:19:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=242
05/23/2022 07:19:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=244
05/23/2022 07:19:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=247
05/23/2022 07:19:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/23/2022 07:19:18 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.5572507464434168 on epoch=249
05/23/2022 07:19:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=252
05/23/2022 07:19:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=254
05/23/2022 07:19:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=257
05/23/2022 07:19:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.06 on epoch=259
05/23/2022 07:19:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/23/2022 07:19:31 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.5396551724137931 on epoch=262
05/23/2022 07:19:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=264
05/23/2022 07:19:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=267
05/23/2022 07:19:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/23/2022 07:19:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/23/2022 07:19:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
05/23/2022 07:19:44 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.5057120524306287 on epoch=274
05/23/2022 07:19:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
05/23/2022 07:19:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=279
05/23/2022 07:19:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/23/2022 07:19:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
05/23/2022 07:19:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
05/23/2022 07:19:58 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.5292207792207793 on epoch=287
05/23/2022 07:20:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/23/2022 07:20:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=292
05/23/2022 07:20:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 07:20:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=297
05/23/2022 07:20:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=299
05/23/2022 07:20:12 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.5072519249382431 on epoch=299
05/23/2022 07:20:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/23/2022 07:20:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/23/2022 07:20:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/23/2022 07:20:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/23/2022 07:20:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=312
05/23/2022 07:20:25 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.503252571800959 on epoch=312
05/23/2022 07:20:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=314
05/23/2022 07:20:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
05/23/2022 07:20:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/23/2022 07:20:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=322
05/23/2022 07:20:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
05/23/2022 07:20:39 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.5991874417520969 on epoch=324
05/23/2022 07:20:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/23/2022 07:20:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/23/2022 07:20:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
05/23/2022 07:20:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=334
05/23/2022 07:20:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
05/23/2022 07:20:53 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.5184040869524741 on epoch=337
05/23/2022 07:20:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/23/2022 07:20:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=342
05/23/2022 07:21:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/23/2022 07:21:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/23/2022 07:21:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/23/2022 07:21:07 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.5072750386432254 on epoch=349
05/23/2022 07:21:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
05/23/2022 07:21:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=354
05/23/2022 07:21:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
05/23/2022 07:21:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=359
05/23/2022 07:21:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/23/2022 07:21:20 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.5612194257355547 on epoch=362
05/23/2022 07:21:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
05/23/2022 07:21:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
05/23/2022 07:21:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=369
05/23/2022 07:21:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/23/2022 07:21:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/23/2022 07:21:34 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.5685036945812807 on epoch=374
05/23/2022 07:21:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
05/23/2022 07:21:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/23/2022 07:21:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=382
05/23/2022 07:21:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
05/23/2022 07:21:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=387
05/23/2022 07:21:47 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.5745614035087718 on epoch=387
05/23/2022 07:21:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/23/2022 07:21:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/23/2022 07:21:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/23/2022 07:21:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=397
05/23/2022 07:22:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/23/2022 07:22:01 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.5647922294613062 on epoch=399
05/23/2022 07:22:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=402
05/23/2022 07:22:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/23/2022 07:22:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/23/2022 07:22:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
05/23/2022 07:22:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/23/2022 07:22:15 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5517416644076986 on epoch=412
05/23/2022 07:22:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=414
05/23/2022 07:22:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=417
05/23/2022 07:22:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=419
05/23/2022 07:22:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=422
05/23/2022 07:22:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/23/2022 07:22:28 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5136784511784511 on epoch=424
05/23/2022 07:22:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/23/2022 07:22:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 07:22:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=432
05/23/2022 07:22:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/23/2022 07:22:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
05/23/2022 07:22:42 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.5521551724137931 on epoch=437
05/23/2022 07:22:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/23/2022 07:22:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/23/2022 07:22:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
05/23/2022 07:22:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/23/2022 07:22:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/23/2022 07:22:55 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.5591397849462365 on epoch=449
05/23/2022 07:22:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/23/2022 07:23:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 07:23:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=457
05/23/2022 07:23:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
05/23/2022 07:23:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/23/2022 07:23:09 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.5549265922249793 on epoch=462
05/23/2022 07:23:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/23/2022 07:23:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=467
05/23/2022 07:23:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=469
05/23/2022 07:23:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=472
05/23/2022 07:23:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/23/2022 07:23:24 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.5640921409214092 on epoch=474
05/23/2022 07:23:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 07:23:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=479
05/23/2022 07:23:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/23/2022 07:23:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/23/2022 07:23:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
05/23/2022 07:23:38 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.5727737609689778 on epoch=487
05/23/2022 07:23:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/23/2022 07:23:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=492
05/23/2022 07:23:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/23/2022 07:23:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 07:23:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/23/2022 07:23:52 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.5117123617123617 on epoch=499
05/23/2022 07:23:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/23/2022 07:23:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=504
05/23/2022 07:23:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 07:24:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=509
05/23/2022 07:24:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/23/2022 07:24:06 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.532092907092907 on epoch=512
05/23/2022 07:24:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
05/23/2022 07:24:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/23/2022 07:24:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=519
05/23/2022 07:24:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/23/2022 07:24:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
05/23/2022 07:24:21 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.5312153817144742 on epoch=524
05/23/2022 07:24:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 07:24:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/23/2022 07:24:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=532
05/23/2022 07:24:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 07:24:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/23/2022 07:24:35 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.583566535917101 on epoch=537
05/23/2022 07:24:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 07:24:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/23/2022 07:24:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=544
05/23/2022 07:24:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=547
05/23/2022 07:24:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
05/23/2022 07:24:49 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.5407509589385651 on epoch=549
05/23/2022 07:24:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=552
05/23/2022 07:24:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 07:24:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
05/23/2022 07:24:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=559
05/23/2022 07:25:01 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 07:25:03 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5631493506493507 on epoch=562
05/23/2022 07:25:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 07:25:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/23/2022 07:25:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 07:25:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/23/2022 07:25:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 07:25:18 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5619083945696848 on epoch=574
05/23/2022 07:25:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 07:25:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=579
05/23/2022 07:25:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 07:25:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 07:25:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/23/2022 07:25:32 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.5879289215686274 on epoch=587
05/23/2022 07:25:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
05/23/2022 07:25:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 07:25:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 07:25:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=597
05/23/2022 07:25:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 07:25:46 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.5475517725517725 on epoch=599
05/23/2022 07:25:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 07:25:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 07:25:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/23/2022 07:25:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/23/2022 07:25:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/23/2022 07:26:01 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.5678710471952964 on epoch=612
05/23/2022 07:26:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
05/23/2022 07:26:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/23/2022 07:26:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/23/2022 07:26:11 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/23/2022 07:26:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/23/2022 07:26:15 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.5407509589385651 on epoch=624
05/23/2022 07:26:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/23/2022 07:26:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 07:26:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 07:26:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/23/2022 07:26:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/23/2022 07:26:29 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5261786713399615 on epoch=637
05/23/2022 07:26:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 07:26:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=642
05/23/2022 07:26:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 07:26:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/23/2022 07:26:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=649
05/23/2022 07:26:43 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5798929249119003 on epoch=649
05/23/2022 07:26:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 07:26:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 07:26:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 07:26:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 07:26:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/23/2022 07:26:58 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5851794391429073 on epoch=662
05/23/2022 07:27:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=664
05/23/2022 07:27:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 07:27:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/23/2022 07:27:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 07:27:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/23/2022 07:27:13 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.5947916666666666 on epoch=674
05/23/2022 07:27:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 07:27:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/23/2022 07:27:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 07:27:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 07:27:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 07:27:27 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5851794391429073 on epoch=687
05/23/2022 07:27:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 07:27:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 07:27:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/23/2022 07:27:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 07:27:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 07:27:42 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.5408377127127126 on epoch=699
05/23/2022 07:27:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/23/2022 07:27:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 07:27:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/23/2022 07:27:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/23/2022 07:27:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
05/23/2022 07:27:56 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5629105090311987 on epoch=712
05/23/2022 07:27:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 07:28:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/23/2022 07:28:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 07:28:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 07:28:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 07:28:11 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5886946386946387 on epoch=724
05/23/2022 07:28:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=727
05/23/2022 07:28:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/23/2022 07:28:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 07:28:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 07:28:23 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/23/2022 07:28:26 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.5929959371135841 on epoch=737
05/23/2022 07:28:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 07:28:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 07:28:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 07:28:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
05/23/2022 07:28:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 07:28:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:28:39 - INFO - __main__ - Printing 3 examples
05/23/2022 07:28:39 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:28:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:28:39 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:28:39 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:28:39 - INFO - __main__ - Printing 3 examples
05/23/2022 07:28:39 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 07:28:39 - INFO - __main__ - ['others']
05/23/2022 07:28:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:28:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:28:39 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:28:41 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5121562739209798 on epoch=749
05/23/2022 07:28:41 - INFO - __main__ - save last model!
05/23/2022 07:28:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 07:28:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 07:28:41 - INFO - __main__ - Printing 3 examples
05/23/2022 07:28:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 07:28:41 - INFO - __main__ - ['others']
05/23/2022 07:28:41 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 07:28:41 - INFO - __main__ - ['others']
05/23/2022 07:28:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 07:28:41 - INFO - __main__ - ['others']
05/23/2022 07:28:41 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:28:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:28:49 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 07:28:58 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:28:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:28:59 - INFO - __main__ - Starting training!
05/23/2022 07:33:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/23/2022 07:33:30 - INFO - __main__ - Classification-F1 on test data: 0.2321
05/23/2022 07:33:31 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.622844827586207, test_performance=0.23214166372587902
05/23/2022 07:33:31 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/23/2022 07:33:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:33:32 - INFO - __main__ - Printing 3 examples
05/23/2022 07:33:32 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:33:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:33:32 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:33:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:33:32 - INFO - __main__ - Printing 3 examples
05/23/2022 07:33:32 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/23/2022 07:33:32 - INFO - __main__ - ['others']
05/23/2022 07:33:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:33:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:33:32 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:33:50 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:33:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:33:51 - INFO - __main__ - Starting training!
05/23/2022 07:33:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.37 on epoch=2
05/23/2022 07:33:56 - INFO - __main__ - Step 20 Global step 20 Train loss 2.11 on epoch=4
05/23/2022 07:33:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.90 on epoch=7
05/23/2022 07:34:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.49 on epoch=9
05/23/2022 07:34:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.31 on epoch=12
05/23/2022 07:34:04 - INFO - __main__ - Global step 50 Train loss 2.04 Classification-F1 0.1967741935483871 on epoch=12
05/23/2022 07:34:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1967741935483871 on epoch=12, global_step=50
05/23/2022 07:34:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=14
05/23/2022 07:34:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=17
05/23/2022 07:34:11 - INFO - __main__ - Step 80 Global step 80 Train loss 1.03 on epoch=19
05/23/2022 07:34:14 - INFO - __main__ - Step 90 Global step 90 Train loss 1.07 on epoch=22
05/23/2022 07:34:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=24
05/23/2022 07:34:17 - INFO - __main__ - Global step 100 Train loss 1.03 Classification-F1 0.19444444444444445 on epoch=24
05/23/2022 07:34:19 - INFO - __main__ - Step 110 Global step 110 Train loss 1.02 on epoch=27
05/23/2022 07:34:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=29
05/23/2022 07:34:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=32
05/23/2022 07:34:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=34
05/23/2022 07:34:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=37
05/23/2022 07:34:30 - INFO - __main__ - Global step 150 Train loss 0.93 Classification-F1 0.5133620303111829 on epoch=37
05/23/2022 07:34:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1967741935483871 -> 0.5133620303111829 on epoch=37, global_step=150
05/23/2022 07:34:32 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=39
05/23/2022 07:34:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=42
05/23/2022 07:34:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/23/2022 07:34:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=47
05/23/2022 07:34:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=49
05/23/2022 07:34:42 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.42529124193883916 on epoch=49
05/23/2022 07:34:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=52
05/23/2022 07:34:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=54
05/23/2022 07:34:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
05/23/2022 07:34:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
05/23/2022 07:34:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=62
05/23/2022 07:34:55 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.5336601911810053 on epoch=62
05/23/2022 07:34:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5133620303111829 -> 0.5336601911810053 on epoch=62, global_step=250
05/23/2022 07:34:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.78 on epoch=64
05/23/2022 07:35:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=67
05/23/2022 07:35:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=69
05/23/2022 07:35:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=72
05/23/2022 07:35:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=74
05/23/2022 07:35:08 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.5287050352374832 on epoch=74
05/23/2022 07:35:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=77
05/23/2022 07:35:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=79
05/23/2022 07:35:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=82
05/23/2022 07:35:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.64 on epoch=84
05/23/2022 07:35:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=87
05/23/2022 07:35:21 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.49436045167752485 on epoch=87
05/23/2022 07:35:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=89
05/23/2022 07:35:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=92
05/23/2022 07:35:28 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=94
05/23/2022 07:35:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=97
05/23/2022 07:35:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=99
05/23/2022 07:35:34 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.57501665001665 on epoch=99
05/23/2022 07:35:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5336601911810053 -> 0.57501665001665 on epoch=99, global_step=400
05/23/2022 07:35:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.60 on epoch=102
05/23/2022 07:35:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=104
05/23/2022 07:35:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=107
05/23/2022 07:35:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=109
05/23/2022 07:35:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=112
05/23/2022 07:35:47 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.53056884635832 on epoch=112
05/23/2022 07:35:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=114
05/23/2022 07:35:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=117
05/23/2022 07:35:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=119
05/23/2022 07:35:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.64 on epoch=122
05/23/2022 07:35:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=124
05/23/2022 07:35:59 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.5303116415648385 on epoch=124
05/23/2022 07:36:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=127
05/23/2022 07:36:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=129
05/23/2022 07:36:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=132
05/23/2022 07:36:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=134
05/23/2022 07:36:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=137
05/23/2022 07:36:12 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.4431878306878307 on epoch=137
05/23/2022 07:36:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=139
05/23/2022 07:36:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=142
05/23/2022 07:36:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=144
05/23/2022 07:36:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=147
05/23/2022 07:36:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.54 on epoch=149
05/23/2022 07:36:25 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.46179921243859867 on epoch=149
05/23/2022 07:36:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
05/23/2022 07:36:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=154
05/23/2022 07:36:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=157
05/23/2022 07:36:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=159
05/23/2022 07:36:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=162
05/23/2022 07:36:38 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.5078763828763828 on epoch=162
05/23/2022 07:36:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=164
05/23/2022 07:36:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=167
05/23/2022 07:36:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=169
05/23/2022 07:36:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=172
05/23/2022 07:36:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=174
05/23/2022 07:36:51 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.5501373626373627 on epoch=174
05/23/2022 07:36:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=177
05/23/2022 07:36:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=179
05/23/2022 07:36:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=182
05/23/2022 07:37:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=184
05/23/2022 07:37:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=187
05/23/2022 07:37:04 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.548608895977317 on epoch=187
05/23/2022 07:37:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=189
05/23/2022 07:37:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=192
05/23/2022 07:37:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=194
05/23/2022 07:37:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=197
05/23/2022 07:37:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/23/2022 07:37:17 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.4768939393939394 on epoch=199
05/23/2022 07:37:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=202
05/23/2022 07:37:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=204
05/23/2022 07:37:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=207
05/23/2022 07:37:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=209
05/23/2022 07:37:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=212
05/23/2022 07:37:30 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.5464685314685314 on epoch=212
05/23/2022 07:37:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=214
05/23/2022 07:37:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=217
05/23/2022 07:37:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
05/23/2022 07:37:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=222
05/23/2022 07:37:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
05/23/2022 07:37:43 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.49344896331738436 on epoch=224
05/23/2022 07:37:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=227
05/23/2022 07:37:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=229
05/23/2022 07:37:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=232
05/23/2022 07:37:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=234
05/23/2022 07:37:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=237
05/23/2022 07:37:57 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.5555346276441612 on epoch=237
05/23/2022 07:37:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=239
05/23/2022 07:38:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=242
05/23/2022 07:38:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=244
05/23/2022 07:38:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
05/23/2022 07:38:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=249
05/23/2022 07:38:11 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.5370238095238095 on epoch=249
05/23/2022 07:38:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=252
05/23/2022 07:38:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
05/23/2022 07:38:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=257
05/23/2022 07:38:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=259
05/23/2022 07:38:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=262
05/23/2022 07:38:24 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.578081797235023 on epoch=262
05/23/2022 07:38:24 - INFO - __main__ - Saving model with best Classification-F1: 0.57501665001665 -> 0.578081797235023 on epoch=262, global_step=1050
05/23/2022 07:38:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=264
05/23/2022 07:38:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=267
05/23/2022 07:38:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=269
05/23/2022 07:38:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=272
05/23/2022 07:38:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=274
05/23/2022 07:38:37 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.5203763828763829 on epoch=274
05/23/2022 07:38:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=277
05/23/2022 07:38:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=279
05/23/2022 07:38:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=282
05/23/2022 07:38:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
05/23/2022 07:38:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=287
05/23/2022 07:38:50 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.4918018018018018 on epoch=287
05/23/2022 07:38:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=289
05/23/2022 07:38:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=292
05/23/2022 07:38:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=294
05/23/2022 07:39:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=297
05/23/2022 07:39:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=299
05/23/2022 07:39:04 - INFO - __main__ - Global step 1200 Train loss 0.12 Classification-F1 0.5262682987640543 on epoch=299
05/23/2022 07:39:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/23/2022 07:39:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/23/2022 07:39:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/23/2022 07:39:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=309
05/23/2022 07:39:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=312
05/23/2022 07:39:18 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.5270599472212376 on epoch=312
05/23/2022 07:39:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/23/2022 07:39:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=317
05/23/2022 07:39:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/23/2022 07:39:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=322
05/23/2022 07:39:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=324
05/23/2022 07:39:33 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.5083349044746104 on epoch=324
05/23/2022 07:39:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/23/2022 07:39:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/23/2022 07:39:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=332
05/23/2022 07:39:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/23/2022 07:39:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=337
05/23/2022 07:39:48 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.5315081908831908 on epoch=337
05/23/2022 07:39:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/23/2022 07:39:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=342
05/23/2022 07:39:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/23/2022 07:39:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=347
05/23/2022 07:40:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=349
05/23/2022 07:40:02 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.5781609195402299 on epoch=349
05/23/2022 07:40:02 - INFO - __main__ - Saving model with best Classification-F1: 0.578081797235023 -> 0.5781609195402299 on epoch=349, global_step=1400
05/23/2022 07:40:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/23/2022 07:40:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/23/2022 07:40:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/23/2022 07:40:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/23/2022 07:40:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/23/2022 07:40:17 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.5628294328484081 on epoch=362
05/23/2022 07:40:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=364
05/23/2022 07:40:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
05/23/2022 07:40:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/23/2022 07:40:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=372
05/23/2022 07:40:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=374
05/23/2022 07:40:32 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.5714285714285714 on epoch=374
05/23/2022 07:40:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
05/23/2022 07:40:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=379
05/23/2022 07:40:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=382
05/23/2022 07:40:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
05/23/2022 07:40:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/23/2022 07:40:47 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.5224911582670204 on epoch=387
05/23/2022 07:40:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/23/2022 07:40:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
05/23/2022 07:40:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
05/23/2022 07:40:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=397
05/23/2022 07:40:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=399
05/23/2022 07:41:02 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.544017094017094 on epoch=399
05/23/2022 07:41:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
05/23/2022 07:41:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/23/2022 07:41:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/23/2022 07:41:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 07:41:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=412
05/23/2022 07:41:17 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5359175928435891 on epoch=412
05/23/2022 07:41:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/23/2022 07:41:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/23/2022 07:41:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=419
05/23/2022 07:41:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/23/2022 07:41:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/23/2022 07:41:32 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.5498139287612971 on epoch=424
05/23/2022 07:41:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/23/2022 07:41:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 07:41:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=432
05/23/2022 07:41:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=434
05/23/2022 07:41:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=437
05/23/2022 07:41:47 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.5306001188354129 on epoch=437
05/23/2022 07:41:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/23/2022 07:41:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/23/2022 07:41:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/23/2022 07:41:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/23/2022 07:41:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=449
05/23/2022 07:42:02 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.5886425834701697 on epoch=449
05/23/2022 07:42:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5781609195402299 -> 0.5886425834701697 on epoch=449, global_step=1800
05/23/2022 07:42:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=452
05/23/2022 07:42:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
05/23/2022 07:42:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/23/2022 07:42:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 07:42:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/23/2022 07:42:18 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.58437619855363 on epoch=462
05/23/2022 07:42:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/23/2022 07:42:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=467
05/23/2022 07:42:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 07:42:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/23/2022 07:42:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/23/2022 07:42:33 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6132408067891939 on epoch=474
05/23/2022 07:42:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5886425834701697 -> 0.6132408067891939 on epoch=474, global_step=1900
05/23/2022 07:42:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 07:42:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=479
05/23/2022 07:42:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=482
05/23/2022 07:42:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 07:42:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/23/2022 07:42:49 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.5961448461448461 on epoch=487
05/23/2022 07:42:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/23/2022 07:42:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 07:42:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=494
05/23/2022 07:42:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/23/2022 07:43:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 07:43:05 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5741573614557487 on epoch=499
05/23/2022 07:43:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=502
05/23/2022 07:43:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/23/2022 07:43:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/23/2022 07:43:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=509
05/23/2022 07:43:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/23/2022 07:43:21 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.5841041998936736 on epoch=512
05/23/2022 07:43:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
05/23/2022 07:43:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/23/2022 07:43:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 07:43:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 07:43:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
05/23/2022 07:43:37 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.5720775113564487 on epoch=524
05/23/2022 07:43:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
05/23/2022 07:43:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 07:43:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/23/2022 07:43:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/23/2022 07:43:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/23/2022 07:43:53 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.5607815742312772 on epoch=537
05/23/2022 07:43:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/23/2022 07:43:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 07:44:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 07:44:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/23/2022 07:44:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 07:44:09 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.5611432546916418 on epoch=549
05/23/2022 07:44:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 07:44:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 07:44:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/23/2022 07:44:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 07:44:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 07:44:25 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.5476297411781283 on epoch=562
05/23/2022 07:44:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 07:44:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/23/2022 07:44:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/23/2022 07:44:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/23/2022 07:44:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=574
05/23/2022 07:44:42 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.5338630792484748 on epoch=574
05/23/2022 07:44:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 07:44:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 07:44:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 07:44:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
05/23/2022 07:44:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/23/2022 07:44:59 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5398879771344275 on epoch=587
05/23/2022 07:45:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 07:45:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 07:45:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/23/2022 07:45:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=597
05/23/2022 07:45:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
05/23/2022 07:45:16 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.574521072796935 on epoch=599
05/23/2022 07:45:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 07:45:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 07:45:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 07:45:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 07:45:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 07:45:32 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.578968253968254 on epoch=612
05/23/2022 07:45:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 07:45:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/23/2022 07:45:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=619
05/23/2022 07:45:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 07:45:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/23/2022 07:45:48 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.5888480392156863 on epoch=624
05/23/2022 07:45:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/23/2022 07:45:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
05/23/2022 07:45:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/23/2022 07:45:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 07:46:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 07:46:03 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.5806747924394984 on epoch=637
05/23/2022 07:46:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/23/2022 07:46:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/23/2022 07:46:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=644
05/23/2022 07:46:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=647
05/23/2022 07:46:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 07:46:20 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5689309056956116 on epoch=649
05/23/2022 07:46:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 07:46:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/23/2022 07:46:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 07:46:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
05/23/2022 07:46:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/23/2022 07:46:36 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.5881505766641297 on epoch=662
05/23/2022 07:46:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 07:46:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/23/2022 07:46:44 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 07:46:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/23/2022 07:46:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 07:46:53 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.5874529121993626 on epoch=674
05/23/2022 07:46:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/23/2022 07:46:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 07:47:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 07:47:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 07:47:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/23/2022 07:47:09 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6068627450980392 on epoch=687
05/23/2022 07:47:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 07:47:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/23/2022 07:47:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 07:47:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/23/2022 07:47:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 07:47:25 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.548887149132692 on epoch=699
05/23/2022 07:47:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/23/2022 07:47:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 07:47:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 07:47:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/23/2022 07:47:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 07:47:42 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.5486427604074663 on epoch=712
05/23/2022 07:47:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/23/2022 07:47:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=717
05/23/2022 07:47:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/23/2022 07:47:51 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 07:47:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 07:47:57 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.5860066833751045 on epoch=724
05/23/2022 07:48:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/23/2022 07:48:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
05/23/2022 07:48:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/23/2022 07:48:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 07:48:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=737
05/23/2022 07:48:14 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.6119047619047618 on epoch=737
05/23/2022 07:48:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/23/2022 07:48:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/23/2022 07:48:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/23/2022 07:48:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 07:48:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
05/23/2022 07:48:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:48:27 - INFO - __main__ - Printing 3 examples
05/23/2022 07:48:27 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:48:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:48:27 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:48:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:48:27 - INFO - __main__ - Printing 3 examples
05/23/2022 07:48:27 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 07:48:27 - INFO - __main__ - ['others']
05/23/2022 07:48:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:48:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:48:27 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:48:29 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.593568691844554 on epoch=749
05/23/2022 07:48:29 - INFO - __main__ - save last model!
05/23/2022 07:48:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 07:48:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 07:48:29 - INFO - __main__ - Printing 3 examples
05/23/2022 07:48:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 07:48:29 - INFO - __main__ - ['others']
05/23/2022 07:48:29 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 07:48:29 - INFO - __main__ - ['others']
05/23/2022 07:48:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 07:48:29 - INFO - __main__ - ['others']
05/23/2022 07:48:29 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:48:31 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:48:37 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 07:48:46 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:48:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:48:47 - INFO - __main__ - Starting training!
05/23/2022 07:54:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/23/2022 07:54:08 - INFO - __main__ - Classification-F1 on test data: 0.3227
05/23/2022 07:54:08 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.6132408067891939, test_performance=0.322682637130512
05/23/2022 07:54:08 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/23/2022 07:54:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:54:09 - INFO - __main__ - Printing 3 examples
05/23/2022 07:54:09 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:54:09 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:54:09 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 07:54:09 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 07:54:09 - INFO - __main__ - Printing 3 examples
05/23/2022 07:54:09 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 07:54:09 - INFO - __main__ - ['others']
05/23/2022 07:54:09 - INFO - __main__ - Tokenizing Input ...
05/23/2022 07:54:09 - INFO - __main__ - Tokenizing Output ...
05/23/2022 07:54:09 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 07:54:28 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 07:54:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 07:54:28 - INFO - __main__ - Starting training!
05/23/2022 07:54:31 - INFO - __main__ - Step 10 Global step 10 Train loss 2.75 on epoch=2
05/23/2022 07:54:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.50 on epoch=4
05/23/2022 07:54:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.99 on epoch=7
05/23/2022 07:54:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=9
05/23/2022 07:54:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=12
05/23/2022 07:54:42 - INFO - __main__ - Global step 50 Train loss 1.39 Classification-F1 0.33782505910165483 on epoch=12
05/23/2022 07:54:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.33782505910165483 on epoch=12, global_step=50
05/23/2022 07:54:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=14
05/23/2022 07:54:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=17
05/23/2022 07:54:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.68 on epoch=19
05/23/2022 07:54:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.67 on epoch=22
05/23/2022 07:54:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=24
05/23/2022 07:54:55 - INFO - __main__ - Global step 100 Train loss 0.70 Classification-F1 0.6246708502522456 on epoch=24
05/23/2022 07:54:55 - INFO - __main__ - Saving model with best Classification-F1: 0.33782505910165483 -> 0.6246708502522456 on epoch=24, global_step=100
05/23/2022 07:54:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=27
05/23/2022 07:55:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=29
05/23/2022 07:55:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=32
05/23/2022 07:55:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=34
05/23/2022 07:55:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=37
05/23/2022 07:55:08 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.5785136785136784 on epoch=37
05/23/2022 07:55:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=39
05/23/2022 07:55:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=42
05/23/2022 07:55:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=44
05/23/2022 07:55:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.42 on epoch=47
05/23/2022 07:55:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=49
05/23/2022 07:55:21 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.6295008912655973 on epoch=49
05/23/2022 07:55:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6246708502522456 -> 0.6295008912655973 on epoch=49, global_step=200
05/23/2022 07:55:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=52
05/23/2022 07:55:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=54
05/23/2022 07:55:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=57
05/23/2022 07:55:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=59
05/23/2022 07:55:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=62
05/23/2022 07:55:34 - INFO - __main__ - Global step 250 Train loss 0.34 Classification-F1 0.718280092839717 on epoch=62
05/23/2022 07:55:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6295008912655973 -> 0.718280092839717 on epoch=62, global_step=250
05/23/2022 07:55:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=64
05/23/2022 07:55:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=67
05/23/2022 07:55:42 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=69
05/23/2022 07:55:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=72
05/23/2022 07:55:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=74
05/23/2022 07:55:48 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.7137698412698413 on epoch=74
05/23/2022 07:55:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=77
05/23/2022 07:55:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=79
05/23/2022 07:55:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=82
05/23/2022 07:55:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=84
05/23/2022 07:56:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=87
05/23/2022 07:56:01 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.7324013564454332 on epoch=87
05/23/2022 07:56:01 - INFO - __main__ - Saving model with best Classification-F1: 0.718280092839717 -> 0.7324013564454332 on epoch=87, global_step=350
05/23/2022 07:56:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=89
05/23/2022 07:56:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=92
05/23/2022 07:56:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=94
05/23/2022 07:56:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.18 on epoch=97
05/23/2022 07:56:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.17 on epoch=99
05/23/2022 07:56:14 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.6857575757575757 on epoch=99
05/23/2022 07:56:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.15 on epoch=102
05/23/2022 07:56:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.09 on epoch=104
05/23/2022 07:56:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=107
05/23/2022 07:56:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=109
05/23/2022 07:56:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=112
05/23/2022 07:56:27 - INFO - __main__ - Global step 450 Train loss 0.12 Classification-F1 0.7535977535977536 on epoch=112
05/23/2022 07:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7324013564454332 -> 0.7535977535977536 on epoch=112, global_step=450
05/23/2022 07:56:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=114
05/23/2022 07:56:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=117
05/23/2022 07:56:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=119
05/23/2022 07:56:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=122
05/23/2022 07:56:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=124
05/23/2022 07:56:40 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.7667643832653566 on epoch=124
05/23/2022 07:56:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7535977535977536 -> 0.7667643832653566 on epoch=124, global_step=500
05/23/2022 07:56:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=127
05/23/2022 07:56:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=129
05/23/2022 07:56:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=132
05/23/2022 07:56:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=134
05/23/2022 07:56:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=137
05/23/2022 07:56:53 - INFO - __main__ - Global step 550 Train loss 0.07 Classification-F1 0.6753345210568211 on epoch=137
05/23/2022 07:56:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=139
05/23/2022 07:56:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.12 on epoch=142
05/23/2022 07:57:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=144
05/23/2022 07:57:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=147
05/23/2022 07:57:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=149
05/23/2022 07:57:07 - INFO - __main__ - Global step 600 Train loss 0.07 Classification-F1 0.7199037679587964 on epoch=149
05/23/2022 07:57:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=152
05/23/2022 07:57:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=154
05/23/2022 07:57:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/23/2022 07:57:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=159
05/23/2022 07:57:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=162
05/23/2022 07:57:20 - INFO - __main__ - Global step 650 Train loss 0.05 Classification-F1 0.6761017330277292 on epoch=162
05/23/2022 07:57:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
05/23/2022 07:57:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
05/23/2022 07:57:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
05/23/2022 07:57:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=172
05/23/2022 07:57:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=174
05/23/2022 07:57:33 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.672609126984127 on epoch=174
05/23/2022 07:57:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=177
05/23/2022 07:57:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=179
05/23/2022 07:57:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=182
05/23/2022 07:57:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=184
05/23/2022 07:57:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=187
05/23/2022 07:57:47 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.6530970625798211 on epoch=187
05/23/2022 07:57:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
05/23/2022 07:57:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/23/2022 07:57:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=194
05/23/2022 07:57:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=197
05/23/2022 07:57:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=199
05/23/2022 07:58:00 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.6783045807239356 on epoch=199
05/23/2022 07:58:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
05/23/2022 07:58:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=204
05/23/2022 07:58:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
05/23/2022 07:58:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/23/2022 07:58:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/23/2022 07:58:14 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.6845986670530282 on epoch=212
05/23/2022 07:58:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
05/23/2022 07:58:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=217
05/23/2022 07:58:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/23/2022 07:58:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
05/23/2022 07:58:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/23/2022 07:58:27 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6793386793386793 on epoch=224
05/23/2022 07:58:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
05/23/2022 07:58:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/23/2022 07:58:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/23/2022 07:58:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
05/23/2022 07:58:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=237
05/23/2022 07:58:41 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.6953703703703704 on epoch=237
05/23/2022 07:58:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/23/2022 07:58:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
05/23/2022 07:58:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
05/23/2022 07:58:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/23/2022 07:58:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=249
05/23/2022 07:58:54 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7064278858396505 on epoch=249
05/23/2022 07:58:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=252
05/23/2022 07:58:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
05/23/2022 07:59:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/23/2022 07:59:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/23/2022 07:59:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/23/2022 07:59:07 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.6861964945129246 on epoch=262
05/23/2022 07:59:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/23/2022 07:59:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=267
05/23/2022 07:59:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 07:59:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/23/2022 07:59:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/23/2022 07:59:21 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.688821843233608 on epoch=274
05/23/2022 07:59:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/23/2022 07:59:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 07:59:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/23/2022 07:59:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/23/2022 07:59:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=287
05/23/2022 07:59:34 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7232125173301645 on epoch=287
05/23/2022 07:59:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
05/23/2022 07:59:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/23/2022 07:59:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/23/2022 07:59:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/23/2022 07:59:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/23/2022 07:59:48 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.7228257763741636 on epoch=299
05/23/2022 07:59:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 07:59:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/23/2022 07:59:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/23/2022 07:59:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 08:00:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 08:00:01 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6797346716464363 on epoch=312
05/23/2022 08:00:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/23/2022 08:00:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/23/2022 08:00:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/23/2022 08:00:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=322
05/23/2022 08:00:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/23/2022 08:00:14 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6933922558922558 on epoch=324
05/23/2022 08:00:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/23/2022 08:00:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/23/2022 08:00:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 08:00:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/23/2022 08:00:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 08:00:28 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.6551861042183623 on epoch=337
05/23/2022 08:00:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=339
05/23/2022 08:00:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 08:00:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/23/2022 08:00:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=347
05/23/2022 08:00:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 08:00:43 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7168022949476026 on epoch=349
05/23/2022 08:00:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
05/23/2022 08:00:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 08:00:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 08:00:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 08:00:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
05/23/2022 08:00:57 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6703846703846703 on epoch=362
05/23/2022 08:00:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/23/2022 08:01:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 08:01:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 08:01:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 08:01:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=374
05/23/2022 08:01:11 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6703846703846703 on epoch=374
05/23/2022 08:01:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/23/2022 08:01:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/23/2022 08:01:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/23/2022 08:01:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/23/2022 08:01:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=387
05/23/2022 08:01:25 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7035851122058019 on epoch=387
05/23/2022 08:01:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/23/2022 08:01:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 08:01:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 08:01:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 08:01:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/23/2022 08:01:38 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7234262125902994 on epoch=399
05/23/2022 08:01:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/23/2022 08:01:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/23/2022 08:01:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 08:01:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 08:01:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 08:01:52 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7337498522982394 on epoch=412
05/23/2022 08:01:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/23/2022 08:01:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 08:02:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 08:02:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=422
05/23/2022 08:02:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/23/2022 08:02:06 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7007554945054946 on epoch=424
05/23/2022 08:02:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/23/2022 08:02:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/23/2022 08:02:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/23/2022 08:02:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 08:02:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/23/2022 08:02:20 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6769539897489081 on epoch=437
05/23/2022 08:02:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/23/2022 08:02:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/23/2022 08:02:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 08:02:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 08:02:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/23/2022 08:02:33 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7602475071225071 on epoch=449
05/23/2022 08:02:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 08:02:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 08:02:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 08:02:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/23/2022 08:02:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 08:02:47 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6674876847290641 on epoch=462
05/23/2022 08:02:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/23/2022 08:02:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/23/2022 08:02:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 08:02:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/23/2022 08:02:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/23/2022 08:03:00 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.7009803921568628 on epoch=474
05/23/2022 08:03:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/23/2022 08:03:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 08:03:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 08:03:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/23/2022 08:03:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/23/2022 08:03:13 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6857142857142857 on epoch=487
05/23/2022 08:03:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/23/2022 08:03:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 08:03:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 08:03:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 08:03:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 08:03:28 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7296685340802987 on epoch=499
05/23/2022 08:03:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/23/2022 08:03:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 08:03:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 08:03:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/23/2022 08:03:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 08:03:41 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.7160067222358657 on epoch=512
05/23/2022 08:03:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 08:03:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/23/2022 08:03:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 08:03:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 08:03:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 08:03:55 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7326720717612558 on epoch=524
05/23/2022 08:03:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
05/23/2022 08:04:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 08:04:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 08:04:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 08:04:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 08:04:08 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6725394749588297 on epoch=537
05/23/2022 08:04:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 08:04:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 08:04:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 08:04:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 08:04:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 08:04:21 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.745830150241915 on epoch=549
05/23/2022 08:04:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 08:04:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 08:04:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 08:04:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=559
05/23/2022 08:04:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 08:04:35 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7191208804112029 on epoch=562
05/23/2022 08:04:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 08:04:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 08:04:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 08:04:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/23/2022 08:04:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 08:04:48 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6881271588314501 on epoch=574
05/23/2022 08:04:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 08:04:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 08:04:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 08:04:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 08:05:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 08:05:01 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7678396871945259 on epoch=587
05/23/2022 08:05:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7667643832653566 -> 0.7678396871945259 on epoch=587, global_step=2350
05/23/2022 08:05:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 08:05:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 08:05:08 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 08:05:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 08:05:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 08:05:14 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7503971163245358 on epoch=599
05/23/2022 08:05:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 08:05:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 08:05:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 08:05:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 08:05:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/23/2022 08:05:27 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6657088122605364 on epoch=612
05/23/2022 08:05:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 08:05:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=617
05/23/2022 08:05:35 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 08:05:37 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=622
05/23/2022 08:05:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 08:05:41 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.7117857142857144 on epoch=624
05/23/2022 08:05:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 08:05:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 08:05:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 08:05:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 08:05:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 08:05:54 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6969960744154293 on epoch=637
05/23/2022 08:05:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 08:05:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 08:06:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/23/2022 08:06:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 08:06:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 08:06:08 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6788868857180052 on epoch=649
05/23/2022 08:06:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 08:06:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/23/2022 08:06:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 08:06:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 08:06:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 08:06:21 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6738899613899614 on epoch=662
05/23/2022 08:06:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 08:06:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 08:06:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 08:06:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=672
05/23/2022 08:06:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 08:06:35 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7131734006734007 on epoch=674
05/23/2022 08:06:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 08:06:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 08:06:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 08:06:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 08:06:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/23/2022 08:06:49 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.734714338162614 on epoch=687
05/23/2022 08:06:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 08:06:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 08:06:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 08:06:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/23/2022 08:07:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 08:07:03 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7465232683982684 on epoch=699
05/23/2022 08:07:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 08:07:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 08:07:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 08:07:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 08:07:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 08:07:16 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6810374743224108 on epoch=712
05/23/2022 08:07:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 08:07:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 08:07:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 08:07:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 08:07:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 08:07:29 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6821136942840796 on epoch=724
05/23/2022 08:07:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 08:07:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 08:07:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 08:07:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 08:07:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 08:07:42 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6810374743224108 on epoch=737
05/23/2022 08:07:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 08:07:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 08:07:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 08:07:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 08:07:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/23/2022 08:07:55 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7627058942583136 on epoch=749
05/23/2022 08:07:55 - INFO - __main__ - save last model!
05/23/2022 08:07:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 08:07:55 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 08:07:55 - INFO - __main__ - Printing 3 examples
05/23/2022 08:07:55 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:07:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:07:55 - INFO - __main__ - Printing 3 examples
05/23/2022 08:07:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:07:55 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:07:55 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:07:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:07:55 - INFO - __main__ - Printing 3 examples
05/23/2022 08:07:55 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:07:55 - INFO - __main__ - ['others']
05/23/2022 08:07:55 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:07:55 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:07:55 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:07:57 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:08:03 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 08:08:11 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:08:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:08:12 - INFO - __main__ - Starting training!
05/23/2022 08:09:57 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/23/2022 08:09:57 - INFO - __main__ - Classification-F1 on test data: 0.3173
05/23/2022 08:09:58 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.7678396871945259, test_performance=0.31728144680199344
05/23/2022 08:09:58 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/23/2022 08:09:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:09:59 - INFO - __main__ - Printing 3 examples
05/23/2022 08:09:59 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:09:59 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:09:59 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:09:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:09:59 - INFO - __main__ - Printing 3 examples
05/23/2022 08:09:59 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:09:59 - INFO - __main__ - ['others']
05/23/2022 08:09:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:09:59 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:09:59 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:10:17 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:10:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:10:18 - INFO - __main__ - Starting training!
05/23/2022 08:10:21 - INFO - __main__ - Step 10 Global step 10 Train loss 2.91 on epoch=2
05/23/2022 08:10:23 - INFO - __main__ - Step 20 Global step 20 Train loss 1.61 on epoch=4
05/23/2022 08:10:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.26 on epoch=7
05/23/2022 08:10:28 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=9
05/23/2022 08:10:30 - INFO - __main__ - Step 50 Global step 50 Train loss 0.86 on epoch=12
05/23/2022 08:10:31 - INFO - __main__ - Global step 50 Train loss 1.53 Classification-F1 0.21236263736263736 on epoch=12
05/23/2022 08:10:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21236263736263736 on epoch=12, global_step=50
05/23/2022 08:10:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.79 on epoch=14
05/23/2022 08:10:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=17
05/23/2022 08:10:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=19
05/23/2022 08:10:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=22
05/23/2022 08:10:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=24
05/23/2022 08:10:44 - INFO - __main__ - Global step 100 Train loss 0.72 Classification-F1 0.3334330545379138 on epoch=24
05/23/2022 08:10:44 - INFO - __main__ - Saving model with best Classification-F1: 0.21236263736263736 -> 0.3334330545379138 on epoch=24, global_step=100
05/23/2022 08:10:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=27
05/23/2022 08:10:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.64 on epoch=29
05/23/2022 08:10:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.61 on epoch=32
05/23/2022 08:10:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=34
05/23/2022 08:10:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=37
05/23/2022 08:10:57 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.7011701261701262 on epoch=37
05/23/2022 08:10:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3334330545379138 -> 0.7011701261701262 on epoch=37, global_step=150
05/23/2022 08:10:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=39
05/23/2022 08:11:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=42
05/23/2022 08:11:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=44
05/23/2022 08:11:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=47
05/23/2022 08:11:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=49
05/23/2022 08:11:09 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.5186666666666666 on epoch=49
05/23/2022 08:11:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=52
05/23/2022 08:11:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=54
05/23/2022 08:11:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=57
05/23/2022 08:11:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=59
05/23/2022 08:11:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=62
05/23/2022 08:11:22 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.5623249299719888 on epoch=62
05/23/2022 08:11:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=64
05/23/2022 08:11:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=67
05/23/2022 08:11:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=69
05/23/2022 08:11:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=72
05/23/2022 08:11:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=74
05/23/2022 08:11:35 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.7025373538165528 on epoch=74
05/23/2022 08:11:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7011701261701262 -> 0.7025373538165528 on epoch=74, global_step=300
05/23/2022 08:11:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=77
05/23/2022 08:11:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=79
05/23/2022 08:11:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=82
05/23/2022 08:11:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=84
05/23/2022 08:11:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=87
05/23/2022 08:11:47 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.7287010368663595 on epoch=87
05/23/2022 08:11:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7025373538165528 -> 0.7287010368663595 on epoch=87, global_step=350
05/23/2022 08:11:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=89
05/23/2022 08:11:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=92
05/23/2022 08:11:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=94
05/23/2022 08:11:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=97
05/23/2022 08:11:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=99
05/23/2022 08:12:00 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.7746212121212122 on epoch=99
05/23/2022 08:12:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7287010368663595 -> 0.7746212121212122 on epoch=99, global_step=400
05/23/2022 08:12:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=102
05/23/2022 08:12:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=104
05/23/2022 08:12:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.10 on epoch=107
05/23/2022 08:12:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.14 on epoch=109
05/23/2022 08:12:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=112
05/23/2022 08:12:13 - INFO - __main__ - Global step 450 Train loss 0.15 Classification-F1 0.7236541570033032 on epoch=112
05/23/2022 08:12:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=114
05/23/2022 08:12:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=117
05/23/2022 08:12:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=119
05/23/2022 08:12:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=122
05/23/2022 08:12:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=124
05/23/2022 08:12:26 - INFO - __main__ - Global step 500 Train loss 0.13 Classification-F1 0.7309027777777777 on epoch=124
05/23/2022 08:12:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.13 on epoch=127
05/23/2022 08:12:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=129
05/23/2022 08:12:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.09 on epoch=132
05/23/2022 08:12:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=134
05/23/2022 08:12:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=137
05/23/2022 08:12:40 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.7474025923866023 on epoch=137
05/23/2022 08:12:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.16 on epoch=139
05/23/2022 08:12:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=142
05/23/2022 08:12:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=144
05/23/2022 08:12:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=147
05/23/2022 08:12:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/23/2022 08:12:55 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.7472110215053762 on epoch=149
05/23/2022 08:12:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.08 on epoch=152
05/23/2022 08:12:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=154
05/23/2022 08:13:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=157
05/23/2022 08:13:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.06 on epoch=159
05/23/2022 08:13:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
05/23/2022 08:13:10 - INFO - __main__ - Global step 650 Train loss 0.08 Classification-F1 0.7473890692640693 on epoch=162
05/23/2022 08:13:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=164
05/23/2022 08:13:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=167
05/23/2022 08:13:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=169
05/23/2022 08:13:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=172
05/23/2022 08:13:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=174
05/23/2022 08:13:25 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.6942295016478578 on epoch=174
05/23/2022 08:13:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.04 on epoch=177
05/23/2022 08:13:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.09 on epoch=179
05/23/2022 08:13:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.07 on epoch=182
05/23/2022 08:13:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.13 on epoch=184
05/23/2022 08:13:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=187
05/23/2022 08:13:39 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.7682044592030361 on epoch=187
05/23/2022 08:13:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=189
05/23/2022 08:13:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/23/2022 08:13:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=194
05/23/2022 08:13:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=197
05/23/2022 08:13:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=199
05/23/2022 08:13:55 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.7887795895860412 on epoch=199
05/23/2022 08:13:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7746212121212122 -> 0.7887795895860412 on epoch=199, global_step=800
05/23/2022 08:13:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
05/23/2022 08:14:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
05/23/2022 08:14:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=207
05/23/2022 08:14:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
05/23/2022 08:14:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=212
05/23/2022 08:14:11 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.74243795420266 on epoch=212
05/23/2022 08:14:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=214
05/23/2022 08:14:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
05/23/2022 08:14:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=219
05/23/2022 08:14:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/23/2022 08:14:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/23/2022 08:14:26 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.7887795895860412 on epoch=224
05/23/2022 08:14:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/23/2022 08:14:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/23/2022 08:14:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=232
05/23/2022 08:14:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/23/2022 08:14:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/23/2022 08:14:41 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.727977427977428 on epoch=237
05/23/2022 08:14:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/23/2022 08:14:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=242
05/23/2022 08:14:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
05/23/2022 08:14:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=247
05/23/2022 08:14:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/23/2022 08:14:56 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.8123167155425219 on epoch=249
05/23/2022 08:14:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7887795895860412 -> 0.8123167155425219 on epoch=249, global_step=1000
05/23/2022 08:14:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/23/2022 08:15:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
05/23/2022 08:15:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/23/2022 08:15:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=259
05/23/2022 08:15:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=262
05/23/2022 08:15:12 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.7723801220575415 on epoch=262
05/23/2022 08:15:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=264
05/23/2022 08:15:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=267
05/23/2022 08:15:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/23/2022 08:15:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 08:15:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/23/2022 08:15:27 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.8086962057550293 on epoch=274
05/23/2022 08:15:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/23/2022 08:15:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=279
05/23/2022 08:15:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 08:15:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=284
05/23/2022 08:15:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/23/2022 08:15:43 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7932449494949495 on epoch=287
05/23/2022 08:15:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/23/2022 08:15:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/23/2022 08:15:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/23/2022 08:15:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/23/2022 08:15:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/23/2022 08:15:59 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.79171918767507 on epoch=299
05/23/2022 08:16:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 08:16:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/23/2022 08:16:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=307
05/23/2022 08:16:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 08:16:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/23/2022 08:16:14 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7535912698412699 on epoch=312
05/23/2022 08:16:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/23/2022 08:16:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
05/23/2022 08:16:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=319
05/23/2022 08:16:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/23/2022 08:16:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/23/2022 08:16:30 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7607323232323232 on epoch=324
05/23/2022 08:16:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/23/2022 08:16:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/23/2022 08:16:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/23/2022 08:16:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/23/2022 08:16:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 08:16:45 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.7772997835497836 on epoch=337
05/23/2022 08:16:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/23/2022 08:16:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=342
05/23/2022 08:16:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/23/2022 08:16:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/23/2022 08:16:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=349
05/23/2022 08:17:01 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.8095911678454374 on epoch=349
05/23/2022 08:17:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 08:17:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 08:17:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/23/2022 08:17:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 08:17:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 08:17:16 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7666865079365079 on epoch=362
05/23/2022 08:17:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=364
05/23/2022 08:17:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 08:17:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 08:17:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/23/2022 08:17:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/23/2022 08:17:32 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.8058718110900274 on epoch=374
05/23/2022 08:17:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/23/2022 08:17:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 08:17:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
05/23/2022 08:17:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 08:17:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/23/2022 08:17:48 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7521212121212122 on epoch=387
05/23/2022 08:17:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/23/2022 08:17:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/23/2022 08:17:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 08:17:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/23/2022 08:18:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/23/2022 08:18:03 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7586441336441337 on epoch=399
05/23/2022 08:18:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/23/2022 08:18:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 08:18:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=407
05/23/2022 08:18:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/23/2022 08:18:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/23/2022 08:18:19 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7887795895860412 on epoch=412
05/23/2022 08:18:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/23/2022 08:18:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/23/2022 08:18:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 08:18:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/23/2022 08:18:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/23/2022 08:18:35 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.8090861344537815 on epoch=424
05/23/2022 08:18:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 08:18:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 08:18:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 08:18:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/23/2022 08:18:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/23/2022 08:18:51 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7780934343434344 on epoch=437
05/23/2022 08:18:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 08:18:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/23/2022 08:18:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=444
05/23/2022 08:19:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/23/2022 08:19:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/23/2022 08:19:07 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.7747599247599247 on epoch=449
05/23/2022 08:19:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 08:19:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/23/2022 08:19:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 08:19:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 08:19:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 08:19:22 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.79171918767507 on epoch=462
05/23/2022 08:19:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/23/2022 08:19:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 08:19:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/23/2022 08:19:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/23/2022 08:19:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/23/2022 08:19:38 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.79171918767507 on epoch=474
05/23/2022 08:19:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 08:19:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 08:19:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 08:19:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 08:19:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 08:19:54 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.8268543702747557 on epoch=487
05/23/2022 08:19:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8123167155425219 -> 0.8268543702747557 on epoch=487, global_step=1950
05/23/2022 08:19:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 08:19:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 08:20:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 08:20:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 08:20:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 08:20:10 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7958628025990456 on epoch=499
05/23/2022 08:20:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/23/2022 08:20:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/23/2022 08:20:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/23/2022 08:20:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 08:20:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 08:20:26 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.8275889828071992 on epoch=512
05/23/2022 08:20:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8268543702747557 -> 0.8275889828071992 on epoch=512, global_step=2050
05/23/2022 08:20:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 08:20:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 08:20:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 08:20:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 08:20:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/23/2022 08:20:41 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.72750504000504 on epoch=524
05/23/2022 08:20:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 08:20:46 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/23/2022 08:20:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 08:20:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/23/2022 08:20:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 08:20:57 - INFO - __main__ - Global step 2150 Train loss 0.00 Classification-F1 0.8082505729564554 on epoch=537
05/23/2022 08:20:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 08:21:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 08:21:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 08:21:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 08:21:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 08:21:13 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7599789915966387 on epoch=549
05/23/2022 08:21:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 08:21:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 08:21:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 08:21:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 08:21:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 08:21:29 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7800067204301075 on epoch=562
05/23/2022 08:21:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 08:21:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 08:21:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 08:21:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 08:21:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 08:21:44 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7944614758128957 on epoch=574
05/23/2022 08:21:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 08:21:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/23/2022 08:21:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 08:21:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/23/2022 08:21:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=587
05/23/2022 08:21:59 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.8117647058823529 on epoch=587
05/23/2022 08:22:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 08:22:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 08:22:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 08:22:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 08:22:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/23/2022 08:22:14 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.808721725367263 on epoch=599
05/23/2022 08:22:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 08:22:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 08:22:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 08:22:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 08:22:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/23/2022 08:22:29 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7974750384024578 on epoch=612
05/23/2022 08:22:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 08:22:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 08:22:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=619
05/23/2022 08:22:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 08:22:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/23/2022 08:22:45 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7457837301587301 on epoch=624
05/23/2022 08:22:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 08:22:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 08:22:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 08:22:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 08:22:56 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 08:23:00 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7220932787376411 on epoch=637
05/23/2022 08:23:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 08:23:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 08:23:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/23/2022 08:23:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 08:23:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 08:23:16 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7851592851592852 on epoch=649
05/23/2022 08:23:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 08:23:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 08:23:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 08:23:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 08:23:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 08:23:31 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7864069264069264 on epoch=662
05/23/2022 08:23:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=664
05/23/2022 08:23:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 08:23:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 08:23:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 08:23:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 08:23:46 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7709839180427417 on epoch=674
05/23/2022 08:23:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 08:23:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 08:23:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 08:23:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 08:23:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 08:24:02 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7714040861099685 on epoch=687
05/23/2022 08:24:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 08:24:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 08:24:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 08:24:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 08:24:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 08:24:17 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7512554112554113 on epoch=699
05/23/2022 08:24:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 08:24:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 08:24:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 08:24:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 08:24:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/23/2022 08:24:32 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.8275445992179864 on epoch=712
05/23/2022 08:24:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 08:24:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 08:24:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 08:24:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 08:24:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 08:24:47 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7604636591478697 on epoch=724
05/23/2022 08:24:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 08:24:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 08:24:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 08:24:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 08:24:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/23/2022 08:25:03 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7788201487491548 on epoch=737
05/23/2022 08:25:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 08:25:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 08:25:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 08:25:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/23/2022 08:25:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 08:25:15 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:25:15 - INFO - __main__ - Printing 3 examples
05/23/2022 08:25:15 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:25:15 - INFO - __main__ - ['others']
05/23/2022 08:25:15 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:25:15 - INFO - __main__ - ['others']
05/23/2022 08:25:15 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:25:15 - INFO - __main__ - ['others']
05/23/2022 08:25:15 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:25:15 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:25:16 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:25:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:25:16 - INFO - __main__ - Printing 3 examples
05/23/2022 08:25:16 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:25:16 - INFO - __main__ - ['others']
05/23/2022 08:25:16 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:25:16 - INFO - __main__ - ['others']
05/23/2022 08:25:16 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:25:16 - INFO - __main__ - ['others']
05/23/2022 08:25:16 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:25:16 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:25:16 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:25:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.8117000117000117 on epoch=749
05/23/2022 08:25:18 - INFO - __main__ - save last model!
05/23/2022 08:25:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 08:25:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 08:25:18 - INFO - __main__ - Printing 3 examples
05/23/2022 08:25:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 08:25:18 - INFO - __main__ - ['others']
05/23/2022 08:25:18 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 08:25:18 - INFO - __main__ - ['others']
05/23/2022 08:25:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 08:25:18 - INFO - __main__ - ['others']
05/23/2022 08:25:18 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:25:20 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:25:26 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 08:25:34 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:25:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:25:35 - INFO - __main__ - Starting training!
05/23/2022 08:30:27 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/23/2022 08:30:27 - INFO - __main__ - Classification-F1 on test data: 0.1595
05/23/2022 08:30:27 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.8275889828071992, test_performance=0.15952913094386087
05/23/2022 08:30:27 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/23/2022 08:30:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:30:28 - INFO - __main__ - Printing 3 examples
05/23/2022 08:30:28 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:30:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:30:28 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:30:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:30:28 - INFO - __main__ - Printing 3 examples
05/23/2022 08:30:28 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:30:28 - INFO - __main__ - ['others']
05/23/2022 08:30:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:30:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:30:28 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:30:47 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:30:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:30:48 - INFO - __main__ - Starting training!
05/23/2022 08:30:50 - INFO - __main__ - Step 10 Global step 10 Train loss 3.10 on epoch=2
05/23/2022 08:30:53 - INFO - __main__ - Step 20 Global step 20 Train loss 1.89 on epoch=4
05/23/2022 08:30:55 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=7
05/23/2022 08:30:57 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=9
05/23/2022 08:30:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=12
05/23/2022 08:31:00 - INFO - __main__ - Global step 50 Train loss 1.70 Classification-F1 0.3523717932705518 on epoch=12
05/23/2022 08:31:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3523717932705518 on epoch=12, global_step=50
05/23/2022 08:31:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.99 on epoch=14
05/23/2022 08:31:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=17
05/23/2022 08:31:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.83 on epoch=19
05/23/2022 08:31:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.72 on epoch=22
05/23/2022 08:31:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=24
05/23/2022 08:31:13 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.436078431372549 on epoch=24
05/23/2022 08:31:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3523717932705518 -> 0.436078431372549 on epoch=24, global_step=100
05/23/2022 08:31:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.75 on epoch=27
05/23/2022 08:31:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=29
05/23/2022 08:31:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=32
05/23/2022 08:31:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=34
05/23/2022 08:31:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.60 on epoch=37
05/23/2022 08:31:25 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.5234179894179893 on epoch=37
05/23/2022 08:31:25 - INFO - __main__ - Saving model with best Classification-F1: 0.436078431372549 -> 0.5234179894179893 on epoch=37, global_step=150
05/23/2022 08:31:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=39
05/23/2022 08:31:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=42
05/23/2022 08:31:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=44
05/23/2022 08:31:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
05/23/2022 08:31:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.61 on epoch=49
05/23/2022 08:31:37 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.6002541866028709 on epoch=49
05/23/2022 08:31:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5234179894179893 -> 0.6002541866028709 on epoch=49, global_step=200
05/23/2022 08:31:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=52
05/23/2022 08:31:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=54
05/23/2022 08:31:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=57
05/23/2022 08:31:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=59
05/23/2022 08:31:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=62
05/23/2022 08:31:50 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.5697297297297297 on epoch=62
05/23/2022 08:31:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=64
05/23/2022 08:31:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=67
05/23/2022 08:31:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=69
05/23/2022 08:31:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=72
05/23/2022 08:32:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=74
05/23/2022 08:32:02 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.5085838779956428 on epoch=74
05/23/2022 08:32:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=77
05/23/2022 08:32:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=79
05/23/2022 08:32:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=82
05/23/2022 08:32:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=84
05/23/2022 08:32:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=87
05/23/2022 08:32:14 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.6926815140317588 on epoch=87
05/23/2022 08:32:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6002541866028709 -> 0.6926815140317588 on epoch=87, global_step=350
05/23/2022 08:32:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
05/23/2022 08:32:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
05/23/2022 08:32:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=94
05/23/2022 08:32:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=97
05/23/2022 08:32:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.29 on epoch=99
05/23/2022 08:32:27 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.6989233193277311 on epoch=99
05/23/2022 08:32:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6926815140317588 -> 0.6989233193277311 on epoch=99, global_step=400
05/23/2022 08:32:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=102
05/23/2022 08:32:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=104
05/23/2022 08:32:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=107
05/23/2022 08:32:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=109
05/23/2022 08:32:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=112
05/23/2022 08:32:39 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.7011363636363637 on epoch=112
05/23/2022 08:32:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6989233193277311 -> 0.7011363636363637 on epoch=112, global_step=450
05/23/2022 08:32:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=114
05/23/2022 08:32:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=117
05/23/2022 08:32:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=119
05/23/2022 08:32:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=122
05/23/2022 08:32:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=124
05/23/2022 08:32:51 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.7182765151515151 on epoch=124
05/23/2022 08:32:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7011363636363637 -> 0.7182765151515151 on epoch=124, global_step=500
05/23/2022 08:32:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=127
05/23/2022 08:32:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=129
05/23/2022 08:32:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=132
05/23/2022 08:33:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
05/23/2022 08:33:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=137
05/23/2022 08:33:04 - INFO - __main__ - Global step 550 Train loss 0.16 Classification-F1 0.73349464773269 on epoch=137
05/23/2022 08:33:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7182765151515151 -> 0.73349464773269 on epoch=137, global_step=550
05/23/2022 08:33:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.13 on epoch=139
05/23/2022 08:33:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=142
05/23/2022 08:33:11 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=144
05/23/2022 08:33:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.14 on epoch=147
05/23/2022 08:33:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.11 on epoch=149
05/23/2022 08:33:17 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.6790386084503731 on epoch=149
05/23/2022 08:33:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=152
05/23/2022 08:33:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/23/2022 08:33:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=157
05/23/2022 08:33:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.08 on epoch=159
05/23/2022 08:33:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=162
05/23/2022 08:33:30 - INFO - __main__ - Global step 650 Train loss 0.12 Classification-F1 0.683795357223349 on epoch=162
05/23/2022 08:33:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=164
05/23/2022 08:33:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=167
05/23/2022 08:33:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=169
05/23/2022 08:33:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=172
05/23/2022 08:33:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
05/23/2022 08:33:44 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.6990662931839402 on epoch=174
05/23/2022 08:33:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/23/2022 08:33:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=179
05/23/2022 08:33:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=182
05/23/2022 08:33:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.12 on epoch=184
05/23/2022 08:33:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=187
05/23/2022 08:33:57 - INFO - __main__ - Global step 750 Train loss 0.13 Classification-F1 0.7336265884652982 on epoch=187
05/23/2022 08:33:57 - INFO - __main__ - Saving model with best Classification-F1: 0.73349464773269 -> 0.7336265884652982 on epoch=187, global_step=750
05/23/2022 08:33:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/23/2022 08:34:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=192
05/23/2022 08:34:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.09 on epoch=194
05/23/2022 08:34:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
05/23/2022 08:34:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=199
05/23/2022 08:34:10 - INFO - __main__ - Global step 800 Train loss 0.11 Classification-F1 0.7033235581622679 on epoch=199
05/23/2022 08:34:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=202
05/23/2022 08:34:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=204
05/23/2022 08:34:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/23/2022 08:34:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/23/2022 08:34:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.11 on epoch=212
05/23/2022 08:34:23 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.7506336405529953 on epoch=212
05/23/2022 08:34:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7336265884652982 -> 0.7506336405529953 on epoch=212, global_step=850
05/23/2022 08:34:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
05/23/2022 08:34:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/23/2022 08:34:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/23/2022 08:34:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
05/23/2022 08:34:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/23/2022 08:34:37 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.7302330022918259 on epoch=224
05/23/2022 08:34:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=227
05/23/2022 08:34:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.06 on epoch=229
05/23/2022 08:34:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/23/2022 08:34:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/23/2022 08:34:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/23/2022 08:34:51 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7492943548387095 on epoch=237
05/23/2022 08:34:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/23/2022 08:34:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
05/23/2022 08:34:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/23/2022 08:35:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/23/2022 08:35:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/23/2022 08:35:06 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.7504337218758471 on epoch=249
05/23/2022 08:35:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/23/2022 08:35:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=254
05/23/2022 08:35:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/23/2022 08:35:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/23/2022 08:35:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
05/23/2022 08:35:20 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.7177118004704212 on epoch=262
05/23/2022 08:35:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/23/2022 08:35:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 08:35:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=269
05/23/2022 08:35:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/23/2022 08:35:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
05/23/2022 08:35:36 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.750733137829912 on epoch=274
05/23/2022 08:35:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7506336405529953 -> 0.750733137829912 on epoch=274, global_step=1100
05/23/2022 08:35:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/23/2022 08:35:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 08:35:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/23/2022 08:35:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
05/23/2022 08:35:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 08:35:52 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7961876832844575 on epoch=287
05/23/2022 08:35:52 - INFO - __main__ - Saving model with best Classification-F1: 0.750733137829912 -> 0.7961876832844575 on epoch=287, global_step=1150
05/23/2022 08:35:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/23/2022 08:35:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=292
05/23/2022 08:35:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=294
05/23/2022 08:36:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/23/2022 08:36:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/23/2022 08:36:08 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.7650140518084066 on epoch=299
05/23/2022 08:36:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 08:36:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/23/2022 08:36:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/23/2022 08:36:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=309
05/23/2022 08:36:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 08:36:24 - INFO - __main__ - Global step 1250 Train loss 0.04 Classification-F1 0.6997325574911781 on epoch=312
05/23/2022 08:36:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 08:36:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/23/2022 08:36:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/23/2022 08:36:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/23/2022 08:36:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=324
05/23/2022 08:36:39 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7287496287496288 on epoch=324
05/23/2022 08:36:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/23/2022 08:36:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/23/2022 08:36:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=332
05/23/2022 08:36:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 08:36:51 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/23/2022 08:36:55 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7575640046228282 on epoch=337
05/23/2022 08:36:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/23/2022 08:37:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/23/2022 08:37:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/23/2022 08:37:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/23/2022 08:37:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/23/2022 08:37:11 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.750366568914956 on epoch=349
05/23/2022 08:37:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=352
05/23/2022 08:37:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/23/2022 08:37:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/23/2022 08:37:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 08:37:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
05/23/2022 08:37:27 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.7465232683982684 on epoch=362
05/23/2022 08:37:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=364
05/23/2022 08:37:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/23/2022 08:37:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=369
05/23/2022 08:37:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/23/2022 08:37:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
05/23/2022 08:37:43 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7287031799899447 on epoch=374
05/23/2022 08:37:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=377
05/23/2022 08:37:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/23/2022 08:37:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/23/2022 08:37:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 08:37:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/23/2022 08:37:59 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7326378827393026 on epoch=387
05/23/2022 08:38:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/23/2022 08:38:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 08:38:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 08:38:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 08:38:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 08:38:15 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7965073529411765 on epoch=399
05/23/2022 08:38:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7961876832844575 -> 0.7965073529411765 on epoch=399, global_step=1600
05/23/2022 08:38:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/23/2022 08:38:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 08:38:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 08:38:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 08:38:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
05/23/2022 08:38:32 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7284518620002491 on epoch=412
05/23/2022 08:38:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=414
05/23/2022 08:38:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/23/2022 08:38:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 08:38:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 08:38:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 08:38:47 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.7960615280594996 on epoch=424
05/23/2022 08:38:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=427
05/23/2022 08:38:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 08:38:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/23/2022 08:38:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=434
05/23/2022 08:38:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/23/2022 08:39:04 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7771217357910906 on epoch=437
05/23/2022 08:39:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 08:39:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 08:39:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
05/23/2022 08:39:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/23/2022 08:39:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/23/2022 08:39:20 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.7304709401483596 on epoch=449
05/23/2022 08:39:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/23/2022 08:39:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=454
05/23/2022 08:39:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 08:39:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 08:39:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 08:39:36 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.7135161135161135 on epoch=462
05/23/2022 08:39:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=464
05/23/2022 08:39:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 08:39:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/23/2022 08:39:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 08:39:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 08:39:52 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.7605360325948561 on epoch=474
05/23/2022 08:39:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/23/2022 08:39:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 08:39:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 08:40:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/23/2022 08:40:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 08:40:08 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7712889643036702 on epoch=487
05/23/2022 08:40:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
05/23/2022 08:40:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 08:40:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 08:40:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 08:40:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 08:40:24 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.777917358165837 on epoch=499
05/23/2022 08:40:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 08:40:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=504
05/23/2022 08:40:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 08:40:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 08:40:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 08:40:41 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7485795454545454 on epoch=512
05/23/2022 08:40:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/23/2022 08:40:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 08:40:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
05/23/2022 08:40:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/23/2022 08:40:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 08:40:57 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7485795454545454 on epoch=524
05/23/2022 08:41:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 08:41:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 08:41:05 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 08:41:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/23/2022 08:41:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=537
05/23/2022 08:41:14 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7490196078431373 on epoch=537
05/23/2022 08:41:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/23/2022 08:41:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/23/2022 08:41:21 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 08:41:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/23/2022 08:41:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=549
05/23/2022 08:41:30 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7963848039215686 on epoch=549
05/23/2022 08:41:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 08:41:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/23/2022 08:41:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 08:41:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 08:41:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/23/2022 08:41:45 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7647751951422698 on epoch=562
05/23/2022 08:41:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 08:41:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 08:41:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 08:41:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
05/23/2022 08:41:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 08:42:01 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7620895865677649 on epoch=574
05/23/2022 08:42:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 08:42:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 08:42:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 08:42:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 08:42:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 08:42:17 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7805630371154565 on epoch=587
05/23/2022 08:42:20 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 08:42:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/23/2022 08:42:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 08:42:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/23/2022 08:42:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/23/2022 08:42:34 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7640446207808637 on epoch=599
05/23/2022 08:42:36 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 08:42:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 08:42:41 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 08:42:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/23/2022 08:42:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 08:42:50 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7640446207808637 on epoch=612
05/23/2022 08:42:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/23/2022 08:42:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 08:42:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 08:43:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/23/2022 08:43:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/23/2022 08:43:07 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7821206011730205 on epoch=624
05/23/2022 08:43:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 08:43:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 08:43:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 08:43:16 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/23/2022 08:43:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 08:43:22 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7718497189085425 on epoch=637
05/23/2022 08:43:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 08:43:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 08:43:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 08:43:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 08:43:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 08:43:38 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7657717680138426 on epoch=649
05/23/2022 08:43:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 08:43:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 08:43:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 08:43:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/23/2022 08:43:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 08:43:54 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7632393278628066 on epoch=662
05/23/2022 08:43:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 08:43:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 08:44:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/23/2022 08:44:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
05/23/2022 08:44:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/23/2022 08:44:11 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7948145044919239 on epoch=674
05/23/2022 08:44:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 08:44:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 08:44:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/23/2022 08:44:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 08:44:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 08:44:27 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.7643804112554113 on epoch=687
05/23/2022 08:44:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 08:44:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 08:44:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 08:44:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/23/2022 08:44:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/23/2022 08:44:43 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7612058080808081 on epoch=699
05/23/2022 08:44:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 08:44:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 08:44:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 08:44:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 08:44:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=712
05/23/2022 08:44:59 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7650140518084066 on epoch=712
05/23/2022 08:45:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=714
05/23/2022 08:45:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 08:45:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 08:45:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 08:45:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/23/2022 08:45:15 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.746373939922327 on epoch=724
05/23/2022 08:45:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 08:45:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 08:45:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 08:45:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 08:45:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 08:45:32 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.7787562532344317 on epoch=737
05/23/2022 08:45:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 08:45:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/23/2022 08:45:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 08:45:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 08:45:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 08:45:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:45:45 - INFO - __main__ - Printing 3 examples
05/23/2022 08:45:45 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:45:45 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:45:45 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:45:45 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:45:45 - INFO - __main__ - Printing 3 examples
05/23/2022 08:45:45 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:45:45 - INFO - __main__ - ['others']
05/23/2022 08:45:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:45:45 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:45:45 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:45:48 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7603354978354978 on epoch=749
05/23/2022 08:45:48 - INFO - __main__ - save last model!
05/23/2022 08:45:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 08:45:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 08:45:48 - INFO - __main__ - Printing 3 examples
05/23/2022 08:45:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 08:45:48 - INFO - __main__ - ['others']
05/23/2022 08:45:48 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 08:45:48 - INFO - __main__ - ['others']
05/23/2022 08:45:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 08:45:48 - INFO - __main__ - ['others']
05/23/2022 08:45:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:45:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:45:55 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 08:46:04 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:46:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:46:04 - INFO - __main__ - Starting training!
05/23/2022 08:51:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/23/2022 08:51:25 - INFO - __main__ - Classification-F1 on test data: 0.2594
05/23/2022 08:51:25 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7965073529411765, test_performance=0.25939605443626396
05/23/2022 08:51:25 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/23/2022 08:51:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:51:26 - INFO - __main__ - Printing 3 examples
05/23/2022 08:51:26 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:51:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:51:26 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 08:51:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 08:51:26 - INFO - __main__ - Printing 3 examples
05/23/2022 08:51:26 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/23/2022 08:51:26 - INFO - __main__ - ['others']
05/23/2022 08:51:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 08:51:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 08:51:26 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 08:51:45 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 08:51:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 08:51:46 - INFO - __main__ - Starting training!
05/23/2022 08:51:49 - INFO - __main__ - Step 10 Global step 10 Train loss 3.28 on epoch=2
05/23/2022 08:51:51 - INFO - __main__ - Step 20 Global step 20 Train loss 2.25 on epoch=4
05/23/2022 08:51:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.92 on epoch=7
05/23/2022 08:51:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=9
05/23/2022 08:51:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
05/23/2022 08:51:59 - INFO - __main__ - Global step 50 Train loss 2.02 Classification-F1 0.269484126984127 on epoch=12
05/23/2022 08:51:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.269484126984127 on epoch=12, global_step=50
05/23/2022 08:52:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
05/23/2022 08:52:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=17
05/23/2022 08:52:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=19
05/23/2022 08:52:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=22
05/23/2022 08:52:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=24
05/23/2022 08:52:12 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.22908677954103002 on epoch=24
05/23/2022 08:52:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.76 on epoch=27
05/23/2022 08:52:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=29
05/23/2022 08:52:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=32
05/23/2022 08:52:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=34
05/23/2022 08:52:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.71 on epoch=37
05/23/2022 08:52:24 - INFO - __main__ - Global step 150 Train loss 0.76 Classification-F1 0.6836667380595417 on epoch=37
05/23/2022 08:52:24 - INFO - __main__ - Saving model with best Classification-F1: 0.269484126984127 -> 0.6836667380595417 on epoch=37, global_step=150
05/23/2022 08:52:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=39
05/23/2022 08:52:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=42
05/23/2022 08:52:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=44
05/23/2022 08:52:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=47
05/23/2022 08:52:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.65 on epoch=49
05/23/2022 08:52:37 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5299205851619645 on epoch=49
05/23/2022 08:52:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=52
05/23/2022 08:52:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=54
05/23/2022 08:52:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=57
05/23/2022 08:52:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=59
05/23/2022 08:52:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=62
05/23/2022 08:52:50 - INFO - __main__ - Global step 250 Train loss 0.58 Classification-F1 0.7084048473500806 on epoch=62
05/23/2022 08:52:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6836667380595417 -> 0.7084048473500806 on epoch=62, global_step=250
05/23/2022 08:52:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=64
05/23/2022 08:52:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=67
05/23/2022 08:52:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=69
05/23/2022 08:52:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=72
05/23/2022 08:53:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=74
05/23/2022 08:53:02 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.666181857358328 on epoch=74
05/23/2022 08:53:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=77
05/23/2022 08:53:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=79
05/23/2022 08:53:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=82
05/23/2022 08:53:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=84
05/23/2022 08:53:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=87
05/23/2022 08:53:15 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.6911875511875513 on epoch=87
05/23/2022 08:53:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=89
05/23/2022 08:53:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=92
05/23/2022 08:53:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=94
05/23/2022 08:53:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=97
05/23/2022 08:53:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=99
05/23/2022 08:53:28 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.6784447253197253 on epoch=99
05/23/2022 08:53:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=102
05/23/2022 08:53:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=104
05/23/2022 08:53:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=107
05/23/2022 08:53:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
05/23/2022 08:53:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=112
05/23/2022 08:53:41 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.6486772486772486 on epoch=112
05/23/2022 08:53:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=114
05/23/2022 08:53:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
05/23/2022 08:53:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=119
05/23/2022 08:53:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=122
05/23/2022 08:53:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=124
05/23/2022 08:53:53 - INFO - __main__ - Global step 500 Train loss 0.31 Classification-F1 0.688967258794845 on epoch=124
05/23/2022 08:53:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=127
05/23/2022 08:53:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=129
05/23/2022 08:54:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=132
05/23/2022 08:54:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=134
05/23/2022 08:54:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=137
05/23/2022 08:54:06 - INFO - __main__ - Global step 550 Train loss 0.30 Classification-F1 0.7493585043988269 on epoch=137
05/23/2022 08:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7084048473500806 -> 0.7493585043988269 on epoch=137, global_step=550
05/23/2022 08:54:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=139
05/23/2022 08:54:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=142
05/23/2022 08:54:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=144
05/23/2022 08:54:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=147
05/23/2022 08:54:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=149
05/23/2022 08:54:19 - INFO - __main__ - Global step 600 Train loss 0.29 Classification-F1 0.682772834839265 on epoch=149
05/23/2022 08:54:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=152
05/23/2022 08:54:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=154
05/23/2022 08:54:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=157
05/23/2022 08:54:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=159
05/23/2022 08:54:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=162
05/23/2022 08:54:31 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.680011389688809 on epoch=162
05/23/2022 08:54:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=164
05/23/2022 08:54:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.18 on epoch=167
05/23/2022 08:54:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=169
05/23/2022 08:54:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=172
05/23/2022 08:54:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=174
05/23/2022 08:54:44 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6956129516613389 on epoch=174
05/23/2022 08:54:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=177
05/23/2022 08:54:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/23/2022 08:54:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=182
05/23/2022 08:54:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=184
05/23/2022 08:54:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=187
05/23/2022 08:54:57 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.6914029198635976 on epoch=187
05/23/2022 08:54:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=189
05/23/2022 08:55:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=192
05/23/2022 08:55:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
05/23/2022 08:55:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=197
05/23/2022 08:55:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=199
05/23/2022 08:55:10 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.6737575171079007 on epoch=199
05/23/2022 08:55:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.13 on epoch=202
05/23/2022 08:55:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
05/23/2022 08:55:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=207
05/23/2022 08:55:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.14 on epoch=209
05/23/2022 08:55:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=212
05/23/2022 08:55:22 - INFO - __main__ - Global step 850 Train loss 0.14 Classification-F1 0.7111996187363834 on epoch=212
05/23/2022 08:55:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=214
05/23/2022 08:55:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
05/23/2022 08:55:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=219
05/23/2022 08:55:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=222
05/23/2022 08:55:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
05/23/2022 08:55:35 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.660496423196652 on epoch=224
05/23/2022 08:55:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
05/23/2022 08:55:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
05/23/2022 08:55:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/23/2022 08:55:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=234
05/23/2022 08:55:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/23/2022 08:55:49 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6939495349421819 on epoch=237
05/23/2022 08:55:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/23/2022 08:55:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=242
05/23/2022 08:55:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=244
05/23/2022 08:55:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/23/2022 08:56:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/23/2022 08:56:02 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.7087847774244833 on epoch=249
05/23/2022 08:56:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/23/2022 08:56:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/23/2022 08:56:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=257
05/23/2022 08:56:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=259
05/23/2022 08:56:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=262
05/23/2022 08:56:15 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7423563719448035 on epoch=262
05/23/2022 08:56:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/23/2022 08:56:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
05/23/2022 08:56:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=269
05/23/2022 08:56:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=272
05/23/2022 08:56:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=274
05/23/2022 08:56:29 - INFO - __main__ - Global step 1100 Train loss 0.07 Classification-F1 0.6639875762859634 on epoch=274
05/23/2022 08:56:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/23/2022 08:56:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=279
05/23/2022 08:56:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=282
05/23/2022 08:56:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=284
05/23/2022 08:56:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 08:56:42 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6967908015078481 on epoch=287
05/23/2022 08:56:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/23/2022 08:56:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=292
05/23/2022 08:56:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/23/2022 08:56:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/23/2022 08:56:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=299
05/23/2022 08:56:56 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6935962404712405 on epoch=299
05/23/2022 08:56:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.10 on epoch=302
05/23/2022 08:57:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=304
05/23/2022 08:57:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/23/2022 08:57:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=309
05/23/2022 08:57:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=312
05/23/2022 08:57:10 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.700753083491461 on epoch=312
05/23/2022 08:57:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/23/2022 08:57:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=317
05/23/2022 08:57:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
05/23/2022 08:57:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/23/2022 08:57:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 08:57:24 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.7085013026189496 on epoch=324
05/23/2022 08:57:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
05/23/2022 08:57:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/23/2022 08:57:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=332
05/23/2022 08:57:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/23/2022 08:57:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/23/2022 08:57:38 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.6946065881549752 on epoch=337
05/23/2022 08:57:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/23/2022 08:57:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=342
05/23/2022 08:57:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/23/2022 08:57:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 08:57:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 08:57:52 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.713712757830405 on epoch=349
05/23/2022 08:57:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/23/2022 08:57:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=354
05/23/2022 08:57:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/23/2022 08:58:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 08:58:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=362
05/23/2022 08:58:07 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.713712757830405 on epoch=362
05/23/2022 08:58:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/23/2022 08:58:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
05/23/2022 08:58:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
05/23/2022 08:58:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=372
05/23/2022 08:58:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/23/2022 08:58:22 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.726853354978355 on epoch=374
05/23/2022 08:58:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=377
05/23/2022 08:58:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/23/2022 08:58:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/23/2022 08:58:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 08:58:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/23/2022 08:58:38 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7417292003985553 on epoch=387
05/23/2022 08:58:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=389
05/23/2022 08:58:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/23/2022 08:58:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 08:58:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/23/2022 08:58:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 08:58:53 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6580371154564703 on epoch=399
05/23/2022 08:58:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=402
05/23/2022 08:58:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 08:59:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/23/2022 08:59:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 08:59:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 08:59:08 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6938123219373219 on epoch=412
05/23/2022 08:59:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=414
05/23/2022 08:59:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 08:59:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/23/2022 08:59:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/23/2022 08:59:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 08:59:23 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6957798397136633 on epoch=424
05/23/2022 08:59:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 08:59:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 08:59:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 08:59:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/23/2022 08:59:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
05/23/2022 08:59:38 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.6945193355119825 on epoch=437
05/23/2022 08:59:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 08:59:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 08:59:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/23/2022 08:59:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/23/2022 08:59:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 08:59:54 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6741190182366653 on epoch=449
05/23/2022 08:59:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
05/23/2022 08:59:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 09:00:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=457
05/23/2022 09:00:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/23/2022 09:00:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=462
05/23/2022 09:00:09 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6734654234654234 on epoch=462
05/23/2022 09:00:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/23/2022 09:00:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 09:00:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/23/2022 09:00:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 09:00:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 09:00:25 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7039764157411216 on epoch=474
05/23/2022 09:00:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/23/2022 09:00:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 09:00:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/23/2022 09:00:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 09:00:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 09:00:41 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7313988095238095 on epoch=487
05/23/2022 09:00:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 09:00:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/23/2022 09:00:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/23/2022 09:00:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/23/2022 09:00:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/23/2022 09:00:57 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7035497835497837 on epoch=499
05/23/2022 09:00:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 09:01:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/23/2022 09:01:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/23/2022 09:01:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=509
05/23/2022 09:01:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/23/2022 09:01:13 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.7120159932659933 on epoch=512
05/23/2022 09:01:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/23/2022 09:01:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/23/2022 09:01:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=519
05/23/2022 09:01:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/23/2022 09:01:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 09:01:28 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.7087193272677144 on epoch=524
05/23/2022 09:01:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 09:01:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 09:01:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/23/2022 09:01:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/23/2022 09:01:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 09:01:44 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6896652367240602 on epoch=537
05/23/2022 09:01:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/23/2022 09:01:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/23/2022 09:01:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 09:01:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 09:01:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=549
05/23/2022 09:01:59 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.7616229485524617 on epoch=549
05/23/2022 09:01:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7493585043988269 -> 0.7616229485524617 on epoch=549, global_step=2200
05/23/2022 09:02:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
05/23/2022 09:02:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=554
05/23/2022 09:02:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 09:02:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 09:02:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 09:02:14 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.6896652367240602 on epoch=562
05/23/2022 09:02:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 09:02:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=567
05/23/2022 09:02:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/23/2022 09:02:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/23/2022 09:02:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/23/2022 09:02:30 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7475378787878788 on epoch=574
05/23/2022 09:02:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/23/2022 09:02:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 09:02:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 09:02:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=584
05/23/2022 09:02:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 09:02:45 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.7165966386554622 on epoch=587
05/23/2022 09:02:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 09:02:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/23/2022 09:02:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 09:02:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 09:02:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 09:03:00 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7100655539993775 on epoch=599
05/23/2022 09:03:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/23/2022 09:03:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 09:03:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 09:03:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 09:03:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 09:03:16 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7107356204130398 on epoch=612
05/23/2022 09:03:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 09:03:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/23/2022 09:03:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 09:03:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
05/23/2022 09:03:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/23/2022 09:03:31 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6940930494061424 on epoch=624
05/23/2022 09:03:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=627
05/23/2022 09:03:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 09:03:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=632
05/23/2022 09:03:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 09:03:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 09:03:47 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6734654234654234 on epoch=637
05/23/2022 09:03:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
05/23/2022 09:03:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 09:03:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 09:03:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/23/2022 09:03:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 09:04:03 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6452048768225238 on epoch=649
05/23/2022 09:04:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
05/23/2022 09:04:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 09:04:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 09:04:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 09:04:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 09:04:18 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7452380952380953 on epoch=662
05/23/2022 09:04:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 09:04:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/23/2022 09:04:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 09:04:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 09:04:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/23/2022 09:04:33 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6607277526395172 on epoch=674
05/23/2022 09:04:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=677
05/23/2022 09:04:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 09:04:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 09:04:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 09:04:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 09:04:49 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.675990675990676 on epoch=687
05/23/2022 09:04:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
05/23/2022 09:04:54 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 09:04:56 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 09:04:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 09:05:01 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 09:05:04 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6808982683982684 on epoch=699
05/23/2022 09:05:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 09:05:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 09:05:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 09:05:14 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/23/2022 09:05:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 09:05:19 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.7089522677757972 on epoch=712
05/23/2022 09:05:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/23/2022 09:05:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 09:05:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 09:05:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 09:05:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 09:05:35 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6742081447963801 on epoch=724
05/23/2022 09:05:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/23/2022 09:05:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 09:05:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 09:05:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/23/2022 09:05:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 09:05:50 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6910586349924586 on epoch=737
05/23/2022 09:05:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 09:05:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 09:05:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 09:05:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 09:06:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 09:06:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:06:03 - INFO - __main__ - Printing 3 examples
05/23/2022 09:06:03 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:06:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:06:03 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:06:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:06:03 - INFO - __main__ - Printing 3 examples
05/23/2022 09:06:03 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:06:03 - INFO - __main__ - ['sad']
05/23/2022 09:06:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:06:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:06:03 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:06:05 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7295319264069264 on epoch=749
05/23/2022 09:06:05 - INFO - __main__ - save last model!
05/23/2022 09:06:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 09:06:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 09:06:05 - INFO - __main__ - Printing 3 examples
05/23/2022 09:06:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 09:06:05 - INFO - __main__ - ['others']
05/23/2022 09:06:05 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 09:06:05 - INFO - __main__ - ['others']
05/23/2022 09:06:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 09:06:05 - INFO - __main__ - ['others']
05/23/2022 09:06:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:06:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:06:13 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 09:06:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:06:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:06:23 - INFO - __main__ - Starting training!
05/23/2022 09:10:26 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/23/2022 09:10:26 - INFO - __main__ - Classification-F1 on test data: 0.3156
05/23/2022 09:10:26 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.7616229485524617, test_performance=0.3155805176855274
05/23/2022 09:10:26 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/23/2022 09:10:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:10:27 - INFO - __main__ - Printing 3 examples
05/23/2022 09:10:27 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:10:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:10:27 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:10:27 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:10:27 - INFO - __main__ - Printing 3 examples
05/23/2022 09:10:27 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:10:27 - INFO - __main__ - ['sad']
05/23/2022 09:10:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:10:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:10:27 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:10:42 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:10:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:10:43 - INFO - __main__ - Starting training!
05/23/2022 09:10:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.14 on epoch=2
05/23/2022 09:10:49 - INFO - __main__ - Step 20 Global step 20 Train loss 1.71 on epoch=4
05/23/2022 09:10:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.16 on epoch=7
05/23/2022 09:10:53 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=9
05/23/2022 09:10:56 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=12
05/23/2022 09:10:57 - INFO - __main__ - Global step 50 Train loss 1.58 Classification-F1 0.22322540473225405 on epoch=12
05/23/2022 09:10:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22322540473225405 on epoch=12, global_step=50
05/23/2022 09:10:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.84 on epoch=14
05/23/2022 09:11:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.81 on epoch=17
05/23/2022 09:11:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=19
05/23/2022 09:11:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=22
05/23/2022 09:11:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=24
05/23/2022 09:11:10 - INFO - __main__ - Global step 100 Train loss 0.78 Classification-F1 0.4447307224776703 on epoch=24
05/23/2022 09:11:10 - INFO - __main__ - Saving model with best Classification-F1: 0.22322540473225405 -> 0.4447307224776703 on epoch=24, global_step=100
05/23/2022 09:11:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=27
05/23/2022 09:11:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=29
05/23/2022 09:11:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=32
05/23/2022 09:11:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=34
05/23/2022 09:11:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=37
05/23/2022 09:11:22 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.6234392419175028 on epoch=37
05/23/2022 09:11:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4447307224776703 -> 0.6234392419175028 on epoch=37, global_step=150
05/23/2022 09:11:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=39
05/23/2022 09:11:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=42
05/23/2022 09:11:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=44
05/23/2022 09:11:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=47
05/23/2022 09:11:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=49
05/23/2022 09:11:35 - INFO - __main__ - Global step 200 Train loss 0.43 Classification-F1 0.6173245614035088 on epoch=49
05/23/2022 09:11:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=52
05/23/2022 09:11:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=54
05/23/2022 09:11:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=57
05/23/2022 09:11:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=59
05/23/2022 09:11:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=62
05/23/2022 09:11:48 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.6717375366568914 on epoch=62
05/23/2022 09:11:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6234392419175028 -> 0.6717375366568914 on epoch=62, global_step=250
05/23/2022 09:11:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=64
05/23/2022 09:11:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=67
05/23/2022 09:11:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.33 on epoch=69
05/23/2022 09:11:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=72
05/23/2022 09:12:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=74
05/23/2022 09:12:01 - INFO - __main__ - Global step 300 Train loss 0.31 Classification-F1 0.6053904923599321 on epoch=74
05/23/2022 09:12:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=77
05/23/2022 09:12:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=79
05/23/2022 09:12:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=82
05/23/2022 09:12:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.20 on epoch=84
05/23/2022 09:12:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/23/2022 09:12:14 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.635989010989011 on epoch=87
05/23/2022 09:12:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=89
05/23/2022 09:12:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=92
05/23/2022 09:12:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.10 on epoch=94
05/23/2022 09:12:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=97
05/23/2022 09:12:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.17 on epoch=99
05/23/2022 09:12:27 - INFO - __main__ - Global step 400 Train loss 0.16 Classification-F1 0.6529203075255706 on epoch=99
05/23/2022 09:12:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=102
05/23/2022 09:12:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=104
05/23/2022 09:12:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=107
05/23/2022 09:12:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=109
05/23/2022 09:12:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.13 on epoch=112
05/23/2022 09:12:40 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.6971510340838833 on epoch=112
05/23/2022 09:12:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6717375366568914 -> 0.6971510340838833 on epoch=112, global_step=450
05/23/2022 09:12:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=114
05/23/2022 09:12:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=117
05/23/2022 09:12:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.15 on epoch=119
05/23/2022 09:12:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=122
05/23/2022 09:12:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=124
05/23/2022 09:12:53 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.6153339800443459 on epoch=124
05/23/2022 09:12:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.11 on epoch=127
05/23/2022 09:12:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=129
05/23/2022 09:13:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=132
05/23/2022 09:13:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.09 on epoch=134
05/23/2022 09:13:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.09 on epoch=137
05/23/2022 09:13:06 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.7041733870967742 on epoch=137
05/23/2022 09:13:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6971510340838833 -> 0.7041733870967742 on epoch=137, global_step=550
05/23/2022 09:13:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=139
05/23/2022 09:13:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.09 on epoch=142
05/23/2022 09:13:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=144
05/23/2022 09:13:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=147
05/23/2022 09:13:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=149
05/23/2022 09:13:20 - INFO - __main__ - Global step 600 Train loss 0.05 Classification-F1 0.7154134720057641 on epoch=149
05/23/2022 09:13:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7041733870967742 -> 0.7154134720057641 on epoch=149, global_step=600
05/23/2022 09:13:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=152
05/23/2022 09:13:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=154
05/23/2022 09:13:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=157
05/23/2022 09:13:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=159
05/23/2022 09:13:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=162
05/23/2022 09:13:33 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.7443518298007462 on epoch=162
05/23/2022 09:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7154134720057641 -> 0.7443518298007462 on epoch=162, global_step=650
05/23/2022 09:13:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
05/23/2022 09:13:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=167
05/23/2022 09:13:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=169
05/23/2022 09:13:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=172
05/23/2022 09:13:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=174
05/23/2022 09:13:46 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.686141208597604 on epoch=174
05/23/2022 09:13:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=177
05/23/2022 09:13:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=179
05/23/2022 09:13:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=182
05/23/2022 09:13:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=184
05/23/2022 09:13:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
05/23/2022 09:13:59 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.7220932787376411 on epoch=187
05/23/2022 09:14:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
05/23/2022 09:14:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=192
05/23/2022 09:14:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
05/23/2022 09:14:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/23/2022 09:14:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=199
05/23/2022 09:14:13 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.7235164286427644 on epoch=199
05/23/2022 09:14:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=202
05/23/2022 09:14:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=204
05/23/2022 09:14:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/23/2022 09:14:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=209
05/23/2022 09:14:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/23/2022 09:14:26 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.6757936507936508 on epoch=212
05/23/2022 09:14:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
05/23/2022 09:14:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=217
05/23/2022 09:14:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/23/2022 09:14:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.07 on epoch=222
05/23/2022 09:14:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/23/2022 09:14:40 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.7087847774244833 on epoch=224
05/23/2022 09:14:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=227
05/23/2022 09:14:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/23/2022 09:14:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
05/23/2022 09:14:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=234
05/23/2022 09:14:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=237
05/23/2022 09:14:53 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.7485282696489592 on epoch=237
05/23/2022 09:14:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7443518298007462 -> 0.7485282696489592 on epoch=237, global_step=950
05/23/2022 09:14:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/23/2022 09:14:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/23/2022 09:15:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=244
05/23/2022 09:15:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/23/2022 09:15:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/23/2022 09:15:07 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.7087961058549295 on epoch=249
05/23/2022 09:15:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/23/2022 09:15:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/23/2022 09:15:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/23/2022 09:15:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/23/2022 09:15:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/23/2022 09:15:21 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.7131410256410257 on epoch=262
05/23/2022 09:15:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
05/23/2022 09:15:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 09:15:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
05/23/2022 09:15:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 09:15:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/23/2022 09:15:35 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.7347222222222223 on epoch=274
05/23/2022 09:15:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/23/2022 09:15:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/23/2022 09:15:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 09:15:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/23/2022 09:15:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/23/2022 09:15:49 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7533424908424908 on epoch=287
05/23/2022 09:15:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7485282696489592 -> 0.7533424908424908 on epoch=287, global_step=1150
05/23/2022 09:15:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/23/2022 09:15:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/23/2022 09:15:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/23/2022 09:15:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/23/2022 09:16:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 09:16:03 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.7660709096192967 on epoch=299
05/23/2022 09:16:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7533424908424908 -> 0.7660709096192967 on epoch=299, global_step=1200
05/23/2022 09:16:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 09:16:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/23/2022 09:16:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/23/2022 09:16:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 09:16:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 09:16:17 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6731663929939793 on epoch=312
05/23/2022 09:16:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=314
05/23/2022 09:16:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/23/2022 09:16:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/23/2022 09:16:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/23/2022 09:16:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=324
05/23/2022 09:16:31 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.7466740988480118 on epoch=324
05/23/2022 09:16:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/23/2022 09:16:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/23/2022 09:16:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=332
05/23/2022 09:16:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=334
05/23/2022 09:16:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 09:16:44 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.7389949155840997 on epoch=337
05/23/2022 09:16:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=339
05/23/2022 09:16:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=342
05/23/2022 09:16:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/23/2022 09:16:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/23/2022 09:16:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/23/2022 09:16:58 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.7181341799855123 on epoch=349
05/23/2022 09:17:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 09:17:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/23/2022 09:17:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=357
05/23/2022 09:17:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 09:17:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 09:17:12 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6811388339920948 on epoch=362
05/23/2022 09:17:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/23/2022 09:17:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/23/2022 09:17:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 09:17:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/23/2022 09:17:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/23/2022 09:17:26 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7446774193548388 on epoch=374
05/23/2022 09:17:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/23/2022 09:17:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/23/2022 09:17:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 09:17:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/23/2022 09:17:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/23/2022 09:17:40 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.7305294795783926 on epoch=387
05/23/2022 09:17:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/23/2022 09:17:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 09:17:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 09:17:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 09:17:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 09:17:54 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7607526881720429 on epoch=399
05/23/2022 09:17:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 09:17:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/23/2022 09:18:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=407
05/23/2022 09:18:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=409
05/23/2022 09:18:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 09:18:08 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7084710408553665 on epoch=412
05/23/2022 09:18:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 09:18:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/23/2022 09:18:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 09:18:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/23/2022 09:18:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 09:18:22 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.7333998710831098 on epoch=424
05/23/2022 09:18:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 09:18:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/23/2022 09:18:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/23/2022 09:18:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 09:18:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/23/2022 09:18:36 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7524804496578691 on epoch=437
05/23/2022 09:18:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 09:18:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/23/2022 09:18:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=444
05/23/2022 09:18:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 09:18:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/23/2022 09:18:50 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7192553414327608 on epoch=449
05/23/2022 09:18:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 09:18:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/23/2022 09:18:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 09:18:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 09:19:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 09:19:03 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7772435897435896 on epoch=462
05/23/2022 09:19:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7660709096192967 -> 0.7772435897435896 on epoch=462, global_step=1850
05/23/2022 09:19:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/23/2022 09:19:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/23/2022 09:19:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 09:19:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 09:19:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/23/2022 09:19:16 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7096122047989498 on epoch=474
05/23/2022 09:19:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/23/2022 09:19:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 09:19:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=482
05/23/2022 09:19:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/23/2022 09:19:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 09:19:30 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7622493734335839 on epoch=487
05/23/2022 09:19:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 09:19:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 09:19:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 09:19:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 09:19:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 09:19:44 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7015873015873015 on epoch=499
05/23/2022 09:19:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 09:19:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 09:19:51 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 09:19:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/23/2022 09:19:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 09:19:58 - INFO - __main__ - Global step 2050 Train loss 0.00 Classification-F1 0.7404761904761905 on epoch=512
05/23/2022 09:20:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/23/2022 09:20:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/23/2022 09:20:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/23/2022 09:20:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 09:20:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=524
05/23/2022 09:20:11 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.720008728201524 on epoch=524
05/23/2022 09:20:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 09:20:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 09:20:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 09:20:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 09:20:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 09:20:24 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7770924604540903 on epoch=537
05/23/2022 09:20:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 09:20:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 09:20:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 09:20:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 09:20:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 09:20:37 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.7346829640947288 on epoch=549
05/23/2022 09:20:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 09:20:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 09:20:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 09:20:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 09:20:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 09:20:50 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.7404761904761905 on epoch=562
05/23/2022 09:20:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 09:20:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 09:20:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 09:21:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 09:21:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 09:21:03 - INFO - __main__ - Global step 2300 Train loss 0.00 Classification-F1 0.7466740988480118 on epoch=574
05/23/2022 09:21:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 09:21:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/23/2022 09:21:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 09:21:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 09:21:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 09:21:17 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7545916481892092 on epoch=587
05/23/2022 09:21:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 09:21:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 09:21:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 09:21:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 09:21:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 09:21:30 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.722172619047619 on epoch=599
05/23/2022 09:21:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 09:21:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 09:21:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 09:21:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/23/2022 09:21:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=612
05/23/2022 09:21:43 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6947375541125541 on epoch=612
05/23/2022 09:21:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 09:21:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 09:21:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 09:21:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 09:21:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 09:21:56 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.7087961058549295 on epoch=624
05/23/2022 09:21:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 09:22:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 09:22:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 09:22:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 09:22:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 09:22:09 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7768817204301075 on epoch=637
05/23/2022 09:22:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=639
05/23/2022 09:22:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=642
05/23/2022 09:22:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 09:22:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 09:22:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 09:22:22 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7606912442396314 on epoch=649
05/23/2022 09:22:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 09:22:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 09:22:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 09:22:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 09:22:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 09:22:35 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7095612753362557 on epoch=662
05/23/2022 09:22:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/23/2022 09:22:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 09:22:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 09:22:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 09:22:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 09:22:48 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7096122047989498 on epoch=674
05/23/2022 09:22:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 09:22:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 09:22:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 09:22:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 09:23:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 09:23:01 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7096122047989498 on epoch=687
05/23/2022 09:23:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 09:23:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 09:23:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 09:23:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 09:23:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 09:23:14 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.7039866118769883 on epoch=699
05/23/2022 09:23:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 09:23:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 09:23:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 09:23:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 09:23:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 09:23:27 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.7749136178861789 on epoch=712
05/23/2022 09:23:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 09:23:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 09:23:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 09:23:36 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 09:23:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 09:23:40 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7606912442396314 on epoch=724
05/23/2022 09:23:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 09:23:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 09:23:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 09:23:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 09:23:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/23/2022 09:23:53 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7166818688557819 on epoch=737
05/23/2022 09:23:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 09:23:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 09:24:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 09:24:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/23/2022 09:24:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 09:24:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:24:06 - INFO - __main__ - Printing 3 examples
05/23/2022 09:24:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:24:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:24:06 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:24:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:24:06 - INFO - __main__ - Printing 3 examples
05/23/2022 09:24:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:24:06 - INFO - __main__ - ['sad']
05/23/2022 09:24:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:24:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:24:06 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:24:06 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6940392940392941 on epoch=749
05/23/2022 09:24:06 - INFO - __main__ - save last model!
05/23/2022 09:24:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 09:24:06 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 09:24:06 - INFO - __main__ - Printing 3 examples
05/23/2022 09:24:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 09:24:06 - INFO - __main__ - ['others']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 09:24:06 - INFO - __main__ - ['others']
05/23/2022 09:24:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 09:24:06 - INFO - __main__ - ['others']
05/23/2022 09:24:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:24:08 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:24:14 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 09:24:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:24:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:24:22 - INFO - __main__ - Starting training!
05/23/2022 09:25:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/23/2022 09:25:58 - INFO - __main__ - Classification-F1 on test data: 0.2442
05/23/2022 09:25:58 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7772435897435896, test_performance=0.24419988151508595
05/23/2022 09:25:58 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/23/2022 09:25:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:25:59 - INFO - __main__ - Printing 3 examples
05/23/2022 09:25:59 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:25:59 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:25:59 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:25:59 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:25:59 - INFO - __main__ - Printing 3 examples
05/23/2022 09:25:59 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:25:59 - INFO - __main__ - ['sad']
05/23/2022 09:25:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:26:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:26:00 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:26:18 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:26:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:26:19 - INFO - __main__ - Starting training!
05/23/2022 09:26:22 - INFO - __main__ - Step 10 Global step 10 Train loss 3.09 on epoch=2
05/23/2022 09:26:24 - INFO - __main__ - Step 20 Global step 20 Train loss 1.95 on epoch=4
05/23/2022 09:26:26 - INFO - __main__ - Step 30 Global step 30 Train loss 1.32 on epoch=7
05/23/2022 09:26:28 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=9
05/23/2022 09:26:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=12
05/23/2022 09:26:32 - INFO - __main__ - Global step 50 Train loss 1.69 Classification-F1 0.2526371694381364 on epoch=12
05/23/2022 09:26:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2526371694381364 on epoch=12, global_step=50
05/23/2022 09:26:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=14
05/23/2022 09:26:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=17
05/23/2022 09:26:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.75 on epoch=19
05/23/2022 09:26:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.80 on epoch=22
05/23/2022 09:26:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
05/23/2022 09:26:44 - INFO - __main__ - Global step 100 Train loss 0.81 Classification-F1 0.5046044685990339 on epoch=24
05/23/2022 09:26:44 - INFO - __main__ - Saving model with best Classification-F1: 0.2526371694381364 -> 0.5046044685990339 on epoch=24, global_step=100
05/23/2022 09:26:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=27
05/23/2022 09:26:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.65 on epoch=29
05/23/2022 09:26:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=32
05/23/2022 09:26:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.56 on epoch=34
05/23/2022 09:26:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=37
05/23/2022 09:26:56 - INFO - __main__ - Global step 150 Train loss 0.66 Classification-F1 0.523936170212766 on epoch=37
05/23/2022 09:26:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5046044685990339 -> 0.523936170212766 on epoch=37, global_step=150
05/23/2022 09:26:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=39
05/23/2022 09:27:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=42
05/23/2022 09:27:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=44
05/23/2022 09:27:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.66 on epoch=47
05/23/2022 09:27:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=49
05/23/2022 09:27:08 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.6180643754656914 on epoch=49
05/23/2022 09:27:08 - INFO - __main__ - Saving model with best Classification-F1: 0.523936170212766 -> 0.6180643754656914 on epoch=49, global_step=200
05/23/2022 09:27:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=52
05/23/2022 09:27:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=54
05/23/2022 09:27:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=57
05/23/2022 09:27:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=59
05/23/2022 09:27:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=62
05/23/2022 09:27:21 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.7182081432081431 on epoch=62
05/23/2022 09:27:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6180643754656914 -> 0.7182081432081431 on epoch=62, global_step=250
05/23/2022 09:27:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=64
05/23/2022 09:27:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=67
05/23/2022 09:27:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=69
05/23/2022 09:27:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=72
05/23/2022 09:27:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=74
05/23/2022 09:27:33 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.5914141414141415 on epoch=74
05/23/2022 09:27:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=77
05/23/2022 09:27:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=79
05/23/2022 09:27:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=82
05/23/2022 09:27:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=84
05/23/2022 09:27:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=87
05/23/2022 09:27:45 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.6720899470899471 on epoch=87
05/23/2022 09:27:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=89
05/23/2022 09:27:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
05/23/2022 09:27:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=94
05/23/2022 09:27:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=97
05/23/2022 09:27:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=99
05/23/2022 09:27:58 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.6672077922077922 on epoch=99
05/23/2022 09:28:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=102
05/23/2022 09:28:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=104
05/23/2022 09:28:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/23/2022 09:28:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.18 on epoch=109
05/23/2022 09:28:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.31 on epoch=112
05/23/2022 09:28:10 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.6900545982768984 on epoch=112
05/23/2022 09:28:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=114
05/23/2022 09:28:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
05/23/2022 09:28:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=119
05/23/2022 09:28:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.15 on epoch=122
05/23/2022 09:28:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=124
05/23/2022 09:28:22 - INFO - __main__ - Global step 500 Train loss 0.17 Classification-F1 0.7262763966853782 on epoch=124
05/23/2022 09:28:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7182081432081431 -> 0.7262763966853782 on epoch=124, global_step=500
05/23/2022 09:28:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=127
05/23/2022 09:28:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.15 on epoch=129
05/23/2022 09:28:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=132
05/23/2022 09:28:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=134
05/23/2022 09:28:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.14 on epoch=137
05/23/2022 09:28:35 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.7051960277789223 on epoch=137
05/23/2022 09:28:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=139
05/23/2022 09:28:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.11 on epoch=142
05/23/2022 09:28:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=144
05/23/2022 09:28:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.10 on epoch=147
05/23/2022 09:28:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=149
05/23/2022 09:28:48 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.6572075349664483 on epoch=149
05/23/2022 09:28:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.13 on epoch=152
05/23/2022 09:28:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=154
05/23/2022 09:28:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
05/23/2022 09:28:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=159
05/23/2022 09:28:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.09 on epoch=162
05/23/2022 09:29:01 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.7060714285714286 on epoch=162
05/23/2022 09:29:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=164
05/23/2022 09:29:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
05/23/2022 09:29:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
05/23/2022 09:29:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=172
05/23/2022 09:29:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=174
05/23/2022 09:29:13 - INFO - __main__ - Global step 700 Train loss 0.06 Classification-F1 0.6744295982768984 on epoch=174
05/23/2022 09:29:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.06 on epoch=177
05/23/2022 09:29:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/23/2022 09:29:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.11 on epoch=182
05/23/2022 09:29:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=184
05/23/2022 09:29:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=187
05/23/2022 09:29:26 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.6814903846153846 on epoch=187
05/23/2022 09:29:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=189
05/23/2022 09:29:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/23/2022 09:29:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/23/2022 09:29:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
05/23/2022 09:29:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
05/23/2022 09:29:38 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.6694444444444444 on epoch=199
05/23/2022 09:29:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=202
05/23/2022 09:29:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
05/23/2022 09:29:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/23/2022 09:29:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/23/2022 09:29:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
05/23/2022 09:29:51 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.6658813747228381 on epoch=212
05/23/2022 09:29:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=214
05/23/2022 09:29:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/23/2022 09:29:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/23/2022 09:30:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=222
05/23/2022 09:30:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=224
05/23/2022 09:30:03 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.7181651069518716 on epoch=224
05/23/2022 09:30:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
05/23/2022 09:30:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=229
05/23/2022 09:30:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/23/2022 09:30:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
05/23/2022 09:30:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/23/2022 09:30:16 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.7145676691729324 on epoch=237
05/23/2022 09:30:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=239
05/23/2022 09:30:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/23/2022 09:30:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=244
05/23/2022 09:30:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
05/23/2022 09:30:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
05/23/2022 09:30:30 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.6770934959349594 on epoch=249
05/23/2022 09:30:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=252
05/23/2022 09:30:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=254
05/23/2022 09:30:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=257
05/23/2022 09:30:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/23/2022 09:30:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/23/2022 09:30:43 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.7176296933556461 on epoch=262
05/23/2022 09:30:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/23/2022 09:30:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 09:30:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=269
05/23/2022 09:30:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 09:30:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/23/2022 09:30:56 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.6721160184574818 on epoch=274
05/23/2022 09:30:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/23/2022 09:31:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 09:31:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/23/2022 09:31:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=284
05/23/2022 09:31:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/23/2022 09:31:08 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.7039969834087482 on epoch=287
05/23/2022 09:31:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 09:31:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/23/2022 09:31:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=294
05/23/2022 09:31:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/23/2022 09:31:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/23/2022 09:31:22 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.716210445268158 on epoch=299
05/23/2022 09:31:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 09:31:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/23/2022 09:31:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/23/2022 09:31:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/23/2022 09:31:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 09:31:37 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6731804087059329 on epoch=312
05/23/2022 09:31:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/23/2022 09:31:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/23/2022 09:31:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=319
05/23/2022 09:31:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/23/2022 09:31:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=324
05/23/2022 09:31:51 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.7160309996516894 on epoch=324
05/23/2022 09:31:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/23/2022 09:31:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
05/23/2022 09:31:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/23/2022 09:32:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 09:32:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 09:32:04 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.7024044795783926 on epoch=337
05/23/2022 09:32:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/23/2022 09:32:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 09:32:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=344
05/23/2022 09:32:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/23/2022 09:32:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 09:32:18 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.7002315795419244 on epoch=349
05/23/2022 09:32:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 09:32:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/23/2022 09:32:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 09:32:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 09:32:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 09:32:31 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.7283578490475042 on epoch=362
05/23/2022 09:32:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7262763966853782 -> 0.7283578490475042 on epoch=362, global_step=1450
05/23/2022 09:32:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 09:32:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
05/23/2022 09:32:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 09:32:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 09:32:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/23/2022 09:32:44 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6911363636363637 on epoch=374
05/23/2022 09:32:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/23/2022 09:32:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/23/2022 09:32:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
05/23/2022 09:32:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 09:32:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=387
05/23/2022 09:32:57 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6812371211269718 on epoch=387
05/23/2022 09:33:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/23/2022 09:33:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 09:33:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 09:33:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 09:33:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/23/2022 09:33:12 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.7094897941201775 on epoch=399
05/23/2022 09:33:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/23/2022 09:33:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 09:33:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/23/2022 09:33:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 09:33:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/23/2022 09:33:26 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.7029271038558964 on epoch=412
05/23/2022 09:33:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 09:33:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 09:33:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 09:33:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=422
05/23/2022 09:33:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 09:33:39 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6563779357897005 on epoch=424
05/23/2022 09:33:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 09:33:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/23/2022 09:33:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/23/2022 09:33:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 09:33:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 09:33:53 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6833596583596584 on epoch=437
05/23/2022 09:33:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 09:33:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=442
05/23/2022 09:34:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 09:34:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/23/2022 09:34:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/23/2022 09:34:09 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.703760162601626 on epoch=449
05/23/2022 09:34:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/23/2022 09:34:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 09:34:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 09:34:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 09:34:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 09:34:23 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7278961998189073 on epoch=462
05/23/2022 09:34:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/23/2022 09:34:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/23/2022 09:34:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 09:34:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/23/2022 09:34:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/23/2022 09:34:36 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7171743697478992 on epoch=474
05/23/2022 09:34:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/23/2022 09:34:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 09:34:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/23/2022 09:34:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/23/2022 09:34:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 09:34:50 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.7225715421303657 on epoch=487
05/23/2022 09:34:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/23/2022 09:34:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 09:34:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 09:34:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 09:35:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 09:35:04 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.7095987250735535 on epoch=499
05/23/2022 09:35:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/23/2022 09:35:09 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 09:35:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 09:35:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 09:35:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 09:35:18 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6929974329167877 on epoch=512
05/23/2022 09:35:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 09:35:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 09:35:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 09:35:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 09:35:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 09:35:32 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.755944055944056 on epoch=524
05/23/2022 09:35:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7283578490475042 -> 0.755944055944056 on epoch=524, global_step=2100
05/23/2022 09:35:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 09:35:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 09:35:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 09:35:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/23/2022 09:35:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 09:35:45 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7006473241873686 on epoch=537
05/23/2022 09:35:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 09:35:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 09:35:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/23/2022 09:35:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 09:35:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 09:36:00 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.7215233785822022 on epoch=549
05/23/2022 09:36:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 09:36:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 09:36:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/23/2022 09:36:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 09:36:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 09:36:14 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.7054233870967742 on epoch=562
05/23/2022 09:36:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 09:36:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 09:36:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 09:36:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 09:36:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 09:36:27 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.7068174962292609 on epoch=574
05/23/2022 09:36:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 09:36:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 09:36:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 09:36:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 09:36:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 09:36:41 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.7226107226107227 on epoch=587
05/23/2022 09:36:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 09:36:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 09:36:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 09:36:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 09:36:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 09:36:55 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.7068174962292609 on epoch=599
05/23/2022 09:36:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 09:36:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 09:37:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 09:37:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 09:37:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/23/2022 09:37:10 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.7373684210526316 on epoch=612
05/23/2022 09:37:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 09:37:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 09:37:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 09:37:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 09:37:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/23/2022 09:37:24 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6886086474501109 on epoch=624
05/23/2022 09:37:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 09:37:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/23/2022 09:37:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 09:37:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 09:37:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 09:37:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7226107226107227 on epoch=637
05/23/2022 09:37:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 09:37:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 09:37:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 09:37:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 09:37:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 09:37:52 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.7035513506101742 on epoch=649
05/23/2022 09:37:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 09:37:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 09:37:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 09:38:01 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 09:38:03 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 09:38:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6886086474501109 on epoch=662
05/23/2022 09:38:09 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/23/2022 09:38:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 09:38:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/23/2022 09:38:16 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 09:38:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 09:38:22 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7226107226107227 on epoch=674
05/23/2022 09:38:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 09:38:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 09:38:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 09:38:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 09:38:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 09:38:36 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.7160309996516894 on epoch=687
05/23/2022 09:38:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=689
05/23/2022 09:38:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 09:38:43 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 09:38:45 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 09:38:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 09:38:49 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7080415972056839 on epoch=699
05/23/2022 09:38:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=702
05/23/2022 09:38:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 09:38:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 09:38:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 09:39:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 09:39:03 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6886086474501109 on epoch=712
05/23/2022 09:39:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 09:39:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 09:39:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 09:39:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 09:39:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 09:39:17 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.7215233785822022 on epoch=724
05/23/2022 09:39:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=727
05/23/2022 09:39:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 09:39:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 09:39:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 09:39:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 09:39:30 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6886086474501109 on epoch=737
05/23/2022 09:39:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 09:39:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 09:39:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 09:39:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/23/2022 09:39:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
05/23/2022 09:39:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:39:43 - INFO - __main__ - Printing 3 examples
05/23/2022 09:39:43 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:39:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:39:43 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:39:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:39:43 - INFO - __main__ - Printing 3 examples
05/23/2022 09:39:43 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:39:43 - INFO - __main__ - ['sad']
05/23/2022 09:39:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:39:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:39:43 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:39:44 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7373684210526316 on epoch=749
05/23/2022 09:39:44 - INFO - __main__ - save last model!
05/23/2022 09:39:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 09:39:44 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 09:39:44 - INFO - __main__ - Printing 3 examples
05/23/2022 09:39:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 09:39:44 - INFO - __main__ - ['others']
05/23/2022 09:39:44 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 09:39:44 - INFO - __main__ - ['others']
05/23/2022 09:39:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 09:39:44 - INFO - __main__ - ['others']
05/23/2022 09:39:44 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:39:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:39:51 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 09:39:58 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:39:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:39:59 - INFO - __main__ - Starting training!
05/23/2022 09:42:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/23/2022 09:42:41 - INFO - __main__ - Classification-F1 on test data: 0.3012
05/23/2022 09:42:41 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.755944055944056, test_performance=0.30115703183065123
05/23/2022 09:42:41 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/23/2022 09:42:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:42:42 - INFO - __main__ - Printing 3 examples
05/23/2022 09:42:42 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:42:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:42:42 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:42:42 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:42:42 - INFO - __main__ - Printing 3 examples
05/23/2022 09:42:42 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:42:42 - INFO - __main__ - ['sad']
05/23/2022 09:42:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:42:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:42:43 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:43:01 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:43:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:43:02 - INFO - __main__ - Starting training!
05/23/2022 09:43:05 - INFO - __main__ - Step 10 Global step 10 Train loss 3.24 on epoch=2
05/23/2022 09:43:07 - INFO - __main__ - Step 20 Global step 20 Train loss 2.20 on epoch=4
05/23/2022 09:43:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.75 on epoch=7
05/23/2022 09:43:12 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=9
05/23/2022 09:43:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.03 on epoch=12
05/23/2022 09:43:15 - INFO - __main__ - Global step 50 Train loss 1.87 Classification-F1 0.2526371694381364 on epoch=12
05/23/2022 09:43:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2526371694381364 on epoch=12, global_step=50
05/23/2022 09:43:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=14
05/23/2022 09:43:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=17
05/23/2022 09:43:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=19
05/23/2022 09:43:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.80 on epoch=22
05/23/2022 09:43:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=24
05/23/2022 09:43:27 - INFO - __main__ - Global step 100 Train loss 0.87 Classification-F1 0.46818775297036164 on epoch=24
05/23/2022 09:43:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2526371694381364 -> 0.46818775297036164 on epoch=24, global_step=100
05/23/2022 09:43:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=27
05/23/2022 09:43:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.71 on epoch=29
05/23/2022 09:43:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=32
05/23/2022 09:43:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=34
05/23/2022 09:43:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=37
05/23/2022 09:43:40 - INFO - __main__ - Global step 150 Train loss 0.69 Classification-F1 0.4615231259968102 on epoch=37
05/23/2022 09:43:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=39
05/23/2022 09:43:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.70 on epoch=42
05/23/2022 09:43:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=44
05/23/2022 09:43:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=47
05/23/2022 09:43:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=49
05/23/2022 09:43:52 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.5451410056673214 on epoch=49
05/23/2022 09:43:52 - INFO - __main__ - Saving model with best Classification-F1: 0.46818775297036164 -> 0.5451410056673214 on epoch=49, global_step=200
05/23/2022 09:43:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=52
05/23/2022 09:43:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=54
05/23/2022 09:43:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=57
05/23/2022 09:44:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=59
05/23/2022 09:44:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=62
05/23/2022 09:44:04 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.6436015204307888 on epoch=62
05/23/2022 09:44:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5451410056673214 -> 0.6436015204307888 on epoch=62, global_step=250
05/23/2022 09:44:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=64
05/23/2022 09:44:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=67
05/23/2022 09:44:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=69
05/23/2022 09:44:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=72
05/23/2022 09:44:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=74
05/23/2022 09:44:16 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.6212622549019609 on epoch=74
05/23/2022 09:44:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=77
05/23/2022 09:44:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
05/23/2022 09:44:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=82
05/23/2022 09:44:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=84
05/23/2022 09:44:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=87
05/23/2022 09:44:29 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.670523531049847 on epoch=87
05/23/2022 09:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6436015204307888 -> 0.670523531049847 on epoch=87, global_step=350
05/23/2022 09:44:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=89
05/23/2022 09:44:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.31 on epoch=92
05/23/2022 09:44:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=94
05/23/2022 09:44:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=97
05/23/2022 09:44:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=99
05/23/2022 09:44:41 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.6925205566097407 on epoch=99
05/23/2022 09:44:41 - INFO - __main__ - Saving model with best Classification-F1: 0.670523531049847 -> 0.6925205566097407 on epoch=99, global_step=400
05/23/2022 09:44:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=102
05/23/2022 09:44:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=104
05/23/2022 09:44:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=107
05/23/2022 09:44:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=109
05/23/2022 09:44:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/23/2022 09:44:53 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.6968181818181818 on epoch=112
05/23/2022 09:44:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6925205566097407 -> 0.6968181818181818 on epoch=112, global_step=450
05/23/2022 09:44:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=114
05/23/2022 09:44:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=117
05/23/2022 09:45:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=119
05/23/2022 09:45:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=122
05/23/2022 09:45:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=124
05/23/2022 09:45:06 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.6577375762859634 on epoch=124
05/23/2022 09:45:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.14 on epoch=127
05/23/2022 09:45:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.17 on epoch=129
05/23/2022 09:45:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=132
05/23/2022 09:45:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=134
05/23/2022 09:45:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=137
05/23/2022 09:45:19 - INFO - __main__ - Global step 550 Train loss 0.15 Classification-F1 0.6826135460602727 on epoch=137
05/23/2022 09:45:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=139
05/23/2022 09:45:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
05/23/2022 09:45:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
05/23/2022 09:45:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=147
05/23/2022 09:45:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.06 on epoch=149
05/23/2022 09:45:31 - INFO - __main__ - Global step 600 Train loss 0.13 Classification-F1 0.7206314618005114 on epoch=149
05/23/2022 09:45:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6968181818181818 -> 0.7206314618005114 on epoch=149, global_step=600
05/23/2022 09:45:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/23/2022 09:45:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.13 on epoch=154
05/23/2022 09:45:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=157
05/23/2022 09:45:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=159
05/23/2022 09:45:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=162
05/23/2022 09:45:44 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.676984126984127 on epoch=162
05/23/2022 09:45:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=164
05/23/2022 09:45:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
05/23/2022 09:45:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=169
05/23/2022 09:45:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=172
05/23/2022 09:45:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=174
05/23/2022 09:45:56 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.7048965262379897 on epoch=174
05/23/2022 09:45:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=177
05/23/2022 09:46:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=179
05/23/2022 09:46:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=182
05/23/2022 09:46:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=184
05/23/2022 09:46:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=187
05/23/2022 09:46:09 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.7120980169256032 on epoch=187
05/23/2022 09:46:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
05/23/2022 09:46:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=192
05/23/2022 09:46:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=194
05/23/2022 09:46:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/23/2022 09:46:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.08 on epoch=199
05/23/2022 09:46:22 - INFO - __main__ - Global step 800 Train loss 0.07 Classification-F1 0.6952964991664682 on epoch=199
05/23/2022 09:46:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
05/23/2022 09:46:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=204
05/23/2022 09:46:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/23/2022 09:46:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=209
05/23/2022 09:46:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/23/2022 09:46:35 - INFO - __main__ - Global step 850 Train loss 0.05 Classification-F1 0.6613451732780227 on epoch=212
05/23/2022 09:46:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
05/23/2022 09:46:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
05/23/2022 09:46:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.10 on epoch=219
05/23/2022 09:46:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=222
05/23/2022 09:46:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/23/2022 09:46:48 - INFO - __main__ - Global step 900 Train loss 0.08 Classification-F1 0.6763641220864223 on epoch=224
05/23/2022 09:46:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=227
05/23/2022 09:46:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/23/2022 09:46:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/23/2022 09:46:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/23/2022 09:46:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
05/23/2022 09:47:01 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.7045380824652236 on epoch=237
05/23/2022 09:47:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=239
05/23/2022 09:47:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/23/2022 09:47:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/23/2022 09:47:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
05/23/2022 09:47:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=249
05/23/2022 09:47:14 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.739482021809608 on epoch=249
05/23/2022 09:47:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7206314618005114 -> 0.739482021809608 on epoch=249, global_step=1000
05/23/2022 09:47:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/23/2022 09:47:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=254
05/23/2022 09:47:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=257
05/23/2022 09:47:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=259
05/23/2022 09:47:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/23/2022 09:47:27 - INFO - __main__ - Global step 1050 Train loss 0.07 Classification-F1 0.667227485775873 on epoch=262
05/23/2022 09:47:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=264
05/23/2022 09:47:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
05/23/2022 09:47:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=269
05/23/2022 09:47:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 09:47:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/23/2022 09:47:40 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.7082368082368082 on epoch=274
05/23/2022 09:47:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=277
05/23/2022 09:47:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=279
05/23/2022 09:47:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 09:47:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=284
05/23/2022 09:47:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 09:47:52 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6947251258226868 on epoch=287
05/23/2022 09:47:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 09:47:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/23/2022 09:47:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 09:48:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/23/2022 09:48:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 09:48:05 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.6668372918372919 on epoch=299
05/23/2022 09:48:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/23/2022 09:48:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=304
05/23/2022 09:48:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/23/2022 09:48:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 09:48:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/23/2022 09:48:17 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6972221045496907 on epoch=312
05/23/2022 09:48:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 09:48:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=317
05/23/2022 09:48:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=319
05/23/2022 09:48:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/23/2022 09:48:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/23/2022 09:48:30 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6851546970875465 on epoch=324
05/23/2022 09:48:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/23/2022 09:48:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/23/2022 09:48:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=332
05/23/2022 09:48:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 09:48:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/23/2022 09:48:43 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6435574229691877 on epoch=337
05/23/2022 09:48:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/23/2022 09:48:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/23/2022 09:48:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/23/2022 09:48:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/23/2022 09:48:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=349
05/23/2022 09:48:56 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6803990364474236 on epoch=349
05/23/2022 09:48:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/23/2022 09:49:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 09:49:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=357
05/23/2022 09:49:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 09:49:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 09:49:08 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.7036490683229812 on epoch=362
05/23/2022 09:49:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 09:49:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 09:49:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 09:49:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/23/2022 09:49:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/23/2022 09:49:21 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.671072996826225 on epoch=374
05/23/2022 09:49:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/23/2022 09:49:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 09:49:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 09:49:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=384
05/23/2022 09:49:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/23/2022 09:49:33 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6803990364474236 on epoch=387
05/23/2022 09:49:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/23/2022 09:49:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 09:49:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 09:49:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=397
05/23/2022 09:49:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/23/2022 09:49:46 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6710317460317461 on epoch=399
05/23/2022 09:49:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 09:49:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/23/2022 09:49:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 09:49:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 09:49:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=412
05/23/2022 09:49:59 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6797619047619048 on epoch=412
05/23/2022 09:50:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 09:50:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/23/2022 09:50:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=419
05/23/2022 09:50:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=422
05/23/2022 09:50:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 09:50:11 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.7020325499927018 on epoch=424
05/23/2022 09:50:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 09:50:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 09:50:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/23/2022 09:50:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 09:50:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 09:50:24 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.7024044795783926 on epoch=437
05/23/2022 09:50:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 09:50:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 09:50:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/23/2022 09:50:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/23/2022 09:50:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 09:50:37 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.7036490683229812 on epoch=449
05/23/2022 09:50:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 09:50:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 09:50:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 09:50:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 09:50:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 09:50:49 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7171998732974343 on epoch=462
05/23/2022 09:50:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 09:50:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=467
05/23/2022 09:50:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 09:50:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 09:51:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 09:51:03 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6803990364474236 on epoch=474
05/23/2022 09:51:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 09:51:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 09:51:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/23/2022 09:51:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/23/2022 09:51:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 09:51:16 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.7170439455046231 on epoch=487
05/23/2022 09:51:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 09:51:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 09:51:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/23/2022 09:51:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/23/2022 09:51:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=499
05/23/2022 09:51:30 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7036490683229812 on epoch=499
05/23/2022 09:51:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 09:51:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/23/2022 09:51:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/23/2022 09:51:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/23/2022 09:51:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 09:51:43 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7171998732974343 on epoch=512
05/23/2022 09:51:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 09:51:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 09:51:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/23/2022 09:51:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 09:51:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 09:51:56 - INFO - __main__ - Global step 2100 Train loss 0.00 Classification-F1 0.7180294795783926 on epoch=524
05/23/2022 09:51:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 09:52:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 09:52:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 09:52:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/23/2022 09:52:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 09:52:09 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7036490683229812 on epoch=537
05/23/2022 09:52:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 09:52:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 09:52:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=544
05/23/2022 09:52:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 09:52:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 09:52:22 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6947251258226868 on epoch=549
05/23/2022 09:52:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/23/2022 09:52:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 09:52:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 09:52:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/23/2022 09:52:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 09:52:35 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6887782805429865 on epoch=562
05/23/2022 09:52:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 09:52:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=567
05/23/2022 09:52:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/23/2022 09:52:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 09:52:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 09:52:48 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6878595946387709 on epoch=574
05/23/2022 09:52:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 09:52:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/23/2022 09:52:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 09:52:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 09:52:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 09:53:00 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6878595946387709 on epoch=587
05/23/2022 09:53:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/23/2022 09:53:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 09:53:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 09:53:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=597
05/23/2022 09:53:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 09:53:13 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7020273220635019 on epoch=599
05/23/2022 09:53:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 09:53:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/23/2022 09:53:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
05/23/2022 09:53:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 09:53:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 09:53:26 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7127039062522933 on epoch=612
05/23/2022 09:53:29 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 09:53:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 09:53:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 09:53:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 09:53:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/23/2022 09:53:39 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6710317460317461 on epoch=624
05/23/2022 09:53:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 09:53:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 09:53:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 09:53:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 09:53:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 09:53:52 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6947251258226868 on epoch=637
05/23/2022 09:53:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 09:53:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 09:53:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 09:54:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 09:54:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 09:54:05 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6873873963846999 on epoch=649
05/23/2022 09:54:07 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 09:54:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 09:54:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 09:54:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 09:54:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 09:54:18 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6930304172951232 on epoch=662
05/23/2022 09:54:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 09:54:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 09:54:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
05/23/2022 09:54:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 09:54:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 09:54:31 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7155957841441712 on epoch=674
05/23/2022 09:54:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 09:54:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 09:54:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 09:54:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 09:54:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 09:54:44 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6874034749034749 on epoch=687
05/23/2022 09:54:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 09:54:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 09:54:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 09:54:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 09:54:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 09:54:57 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6874034749034749 on epoch=699
05/23/2022 09:54:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 09:55:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 09:55:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 09:55:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 09:55:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 09:55:10 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6731597774244833 on epoch=712
05/23/2022 09:55:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 09:55:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 09:55:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 09:55:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 09:55:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/23/2022 09:55:23 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.7168174962292609 on epoch=724
05/23/2022 09:55:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 09:55:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 09:55:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 09:55:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 09:55:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 09:55:36 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.703021978021978 on epoch=737
05/23/2022 09:55:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 09:55:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 09:55:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 09:55:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 09:55:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
05/23/2022 09:55:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:55:48 - INFO - __main__ - Printing 3 examples
05/23/2022 09:55:48 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:55:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:55:48 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:55:48 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:55:48 - INFO - __main__ - Printing 3 examples
05/23/2022 09:55:48 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:55:48 - INFO - __main__ - ['sad']
05/23/2022 09:55:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:55:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:55:48 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:55:49 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7234841628959277 on epoch=749
05/23/2022 09:55:49 - INFO - __main__ - save last model!
05/23/2022 09:55:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 09:55:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 09:55:49 - INFO - __main__ - Printing 3 examples
05/23/2022 09:55:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 09:55:49 - INFO - __main__ - ['others']
05/23/2022 09:55:49 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 09:55:49 - INFO - __main__ - ['others']
05/23/2022 09:55:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 09:55:49 - INFO - __main__ - ['others']
05/23/2022 09:55:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:55:51 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:55:56 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 09:56:04 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:56:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:56:05 - INFO - __main__ - Starting training!
05/23/2022 09:58:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/23/2022 09:58:09 - INFO - __main__ - Classification-F1 on test data: 0.1978
05/23/2022 09:58:10 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.739482021809608, test_performance=0.1978342006219844
05/23/2022 09:58:10 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/23/2022 09:58:10 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:58:10 - INFO - __main__ - Printing 3 examples
05/23/2022 09:58:10 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 09:58:10 - INFO - __main__ - ['sad']
05/23/2022 09:58:10 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 09:58:10 - INFO - __main__ - ['sad']
05/23/2022 09:58:10 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 09:58:10 - INFO - __main__ - ['sad']
05/23/2022 09:58:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:58:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:58:11 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 09:58:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 09:58:11 - INFO - __main__ - Printing 3 examples
05/23/2022 09:58:11 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/23/2022 09:58:11 - INFO - __main__ - ['sad']
05/23/2022 09:58:11 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/23/2022 09:58:11 - INFO - __main__ - ['sad']
05/23/2022 09:58:11 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/23/2022 09:58:11 - INFO - __main__ - ['sad']
05/23/2022 09:58:11 - INFO - __main__ - Tokenizing Input ...
05/23/2022 09:58:11 - INFO - __main__ - Tokenizing Output ...
05/23/2022 09:58:11 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 09:58:26 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 09:58:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 09:58:27 - INFO - __main__ - Starting training!
05/23/2022 09:58:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.59 on epoch=2
05/23/2022 09:58:32 - INFO - __main__ - Step 20 Global step 20 Train loss 2.50 on epoch=4
05/23/2022 09:58:35 - INFO - __main__ - Step 30 Global step 30 Train loss 2.21 on epoch=7
05/23/2022 09:58:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=9
05/23/2022 09:58:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
05/23/2022 09:58:40 - INFO - __main__ - Global step 50 Train loss 2.31 Classification-F1 0.16039850560398505 on epoch=12
05/23/2022 09:58:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16039850560398505 on epoch=12, global_step=50
05/23/2022 09:58:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.07 on epoch=14
05/23/2022 09:58:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=17
05/23/2022 09:58:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=19
05/23/2022 09:58:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=22
05/23/2022 09:58:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=24
05/23/2022 09:58:53 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.3314118629908104 on epoch=24
05/23/2022 09:58:53 - INFO - __main__ - Saving model with best Classification-F1: 0.16039850560398505 -> 0.3314118629908104 on epoch=24, global_step=100
05/23/2022 09:58:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=27
05/23/2022 09:58:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=29
05/23/2022 09:59:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.67 on epoch=32
05/23/2022 09:59:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.74 on epoch=34
05/23/2022 09:59:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=37
05/23/2022 09:59:06 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.48450292397660816 on epoch=37
05/23/2022 09:59:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3314118629908104 -> 0.48450292397660816 on epoch=37, global_step=150
05/23/2022 09:59:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.73 on epoch=39
05/23/2022 09:59:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.73 on epoch=42
05/23/2022 09:59:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.71 on epoch=44
05/23/2022 09:59:16 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=47
05/23/2022 09:59:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=49
05/23/2022 09:59:19 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.6231601731601731 on epoch=49
05/23/2022 09:59:19 - INFO - __main__ - Saving model with best Classification-F1: 0.48450292397660816 -> 0.6231601731601731 on epoch=49, global_step=200
05/23/2022 09:59:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=52
05/23/2022 09:59:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=54
05/23/2022 09:59:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=57
05/23/2022 09:59:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=59
05/23/2022 09:59:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=62
05/23/2022 09:59:32 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.588164708854364 on epoch=62
05/23/2022 09:59:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=64
05/23/2022 09:59:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=67
05/23/2022 09:59:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=69
05/23/2022 09:59:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=72
05/23/2022 09:59:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=74
05/23/2022 09:59:45 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.5999378158820883 on epoch=74
05/23/2022 09:59:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=77
05/23/2022 09:59:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=79
05/23/2022 09:59:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=82
05/23/2022 09:59:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=84
05/23/2022 09:59:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=87
05/23/2022 09:59:58 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.6099454163693478 on epoch=87
05/23/2022 10:00:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
05/23/2022 10:00:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=92
05/23/2022 10:00:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=94
05/23/2022 10:00:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=97
05/23/2022 10:00:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=99
05/23/2022 10:00:11 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.6022666961521451 on epoch=99
05/23/2022 10:00:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=102
05/23/2022 10:00:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=104
05/23/2022 10:00:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=107
05/23/2022 10:00:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
05/23/2022 10:00:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=112
05/23/2022 10:00:24 - INFO - __main__ - Global step 450 Train loss 0.33 Classification-F1 0.680622488728658 on epoch=112
05/23/2022 10:00:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6231601731601731 -> 0.680622488728658 on epoch=112, global_step=450
05/23/2022 10:00:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=114
05/23/2022 10:00:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=117
05/23/2022 10:00:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=119
05/23/2022 10:00:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.29 on epoch=122
05/23/2022 10:00:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.33 on epoch=124
05/23/2022 10:00:37 - INFO - __main__ - Global step 500 Train loss 0.32 Classification-F1 0.6147755753018911 on epoch=124
05/23/2022 10:00:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=127
05/23/2022 10:00:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=129
05/23/2022 10:00:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=132
05/23/2022 10:00:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=134
05/23/2022 10:00:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=137
05/23/2022 10:00:50 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.6210960869916727 on epoch=137
05/23/2022 10:00:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=139
05/23/2022 10:00:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.30 on epoch=142
05/23/2022 10:00:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=144
05/23/2022 10:01:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=147
05/23/2022 10:01:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=149
05/23/2022 10:01:03 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.6030787753697971 on epoch=149
05/23/2022 10:01:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
05/23/2022 10:01:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=154
05/23/2022 10:01:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=157
05/23/2022 10:01:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
05/23/2022 10:01:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=162
05/23/2022 10:01:16 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.5808892683892684 on epoch=162
05/23/2022 10:01:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=164
05/23/2022 10:01:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=167
05/23/2022 10:01:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=169
05/23/2022 10:01:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=172
05/23/2022 10:01:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=174
05/23/2022 10:01:29 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.6016617790811338 on epoch=174
05/23/2022 10:01:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=177
05/23/2022 10:01:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.14 on epoch=179
05/23/2022 10:01:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.13 on epoch=182
05/23/2022 10:01:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=184
05/23/2022 10:01:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=187
05/23/2022 10:01:42 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.6407130056323604 on epoch=187
05/23/2022 10:01:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=189
05/23/2022 10:01:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=192
05/23/2022 10:01:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=194
05/23/2022 10:01:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
05/23/2022 10:01:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=199
05/23/2022 10:01:56 - INFO - __main__ - Global step 800 Train loss 0.15 Classification-F1 0.6151386530843237 on epoch=199
05/23/2022 10:01:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=202
05/23/2022 10:02:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=204
05/23/2022 10:02:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
05/23/2022 10:02:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
05/23/2022 10:02:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.10 on epoch=212
05/23/2022 10:02:09 - INFO - __main__ - Global step 850 Train loss 0.12 Classification-F1 0.6434957725958567 on epoch=212
05/23/2022 10:02:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=214
05/23/2022 10:02:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=217
05/23/2022 10:02:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/23/2022 10:02:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.11 on epoch=222
05/23/2022 10:02:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=224
05/23/2022 10:02:22 - INFO - __main__ - Global step 900 Train loss 0.10 Classification-F1 0.6430849851902483 on epoch=224
05/23/2022 10:02:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
05/23/2022 10:02:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
05/23/2022 10:02:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=232
05/23/2022 10:02:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/23/2022 10:02:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=237
05/23/2022 10:02:35 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.6129713028906577 on epoch=237
05/23/2022 10:02:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.06 on epoch=239
05/23/2022 10:02:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
05/23/2022 10:02:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
05/23/2022 10:02:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.10 on epoch=247
05/23/2022 10:02:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=249
05/23/2022 10:02:49 - INFO - __main__ - Global step 1000 Train loss 0.08 Classification-F1 0.6416666666666666 on epoch=249
05/23/2022 10:02:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=252
05/23/2022 10:02:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=254
05/23/2022 10:02:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
05/23/2022 10:02:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/23/2022 10:03:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
05/23/2022 10:03:02 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.6572075349664483 on epoch=262
05/23/2022 10:03:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
05/23/2022 10:03:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
05/23/2022 10:03:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/23/2022 10:03:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=272
05/23/2022 10:03:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=274
05/23/2022 10:03:15 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.6416666666666666 on epoch=274
05/23/2022 10:03:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/23/2022 10:03:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/23/2022 10:03:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
05/23/2022 10:03:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
05/23/2022 10:03:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 10:03:29 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.6569738740791373 on epoch=287
05/23/2022 10:03:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=289
05/23/2022 10:03:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/23/2022 10:03:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/23/2022 10:03:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/23/2022 10:03:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/23/2022 10:03:42 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6569738740791373 on epoch=299
05/23/2022 10:03:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/23/2022 10:03:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/23/2022 10:03:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=307
05/23/2022 10:03:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/23/2022 10:03:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=312
05/23/2022 10:03:56 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.6426628551628552 on epoch=312
05/23/2022 10:03:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/23/2022 10:04:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/23/2022 10:04:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/23/2022 10:04:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/23/2022 10:04:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 10:04:10 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.6558284457478005 on epoch=324
05/23/2022 10:04:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=327
05/23/2022 10:04:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=329
05/23/2022 10:04:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
05/23/2022 10:04:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/23/2022 10:04:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/23/2022 10:04:23 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.6561783804430864 on epoch=337
05/23/2022 10:04:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/23/2022 10:04:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/23/2022 10:04:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/23/2022 10:04:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
05/23/2022 10:04:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/23/2022 10:04:37 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6854198916408669 on epoch=349
05/23/2022 10:04:37 - INFO - __main__ - Saving model with best Classification-F1: 0.680622488728658 -> 0.6854198916408669 on epoch=349, global_step=1400
05/23/2022 10:04:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 10:04:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/23/2022 10:04:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/23/2022 10:04:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/23/2022 10:04:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/23/2022 10:04:51 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.715821467995381 on epoch=362
05/23/2022 10:04:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6854198916408669 -> 0.715821467995381 on epoch=362, global_step=1450
05/23/2022 10:04:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/23/2022 10:04:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/23/2022 10:04:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/23/2022 10:05:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/23/2022 10:05:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/23/2022 10:05:05 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6860047846889952 on epoch=374
05/23/2022 10:05:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/23/2022 10:05:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/23/2022 10:05:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/23/2022 10:05:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/23/2022 10:05:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/23/2022 10:05:19 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6803508053508054 on epoch=387
05/23/2022 10:05:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/23/2022 10:05:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/23/2022 10:05:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=394
05/23/2022 10:05:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 10:05:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/23/2022 10:05:32 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6936887673729779 on epoch=399
05/23/2022 10:05:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/23/2022 10:05:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 10:05:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 10:05:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 10:05:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/23/2022 10:05:46 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.7011429879076938 on epoch=412
05/23/2022 10:05:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/23/2022 10:05:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=417
05/23/2022 10:05:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 10:05:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/23/2022 10:05:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 10:06:00 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6569738740791373 on epoch=424
05/23/2022 10:06:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 10:06:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 10:06:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/23/2022 10:06:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 10:06:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 10:06:14 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6701839826839827 on epoch=437
05/23/2022 10:06:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 10:06:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/23/2022 10:06:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/23/2022 10:06:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/23/2022 10:06:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/23/2022 10:06:28 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6702297702297703 on epoch=449
05/23/2022 10:06:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 10:06:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 10:06:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 10:06:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=459
05/23/2022 10:06:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 10:06:43 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.7161185432924564 on epoch=462
05/23/2022 10:06:43 - INFO - __main__ - Saving model with best Classification-F1: 0.715821467995381 -> 0.7161185432924564 on epoch=462, global_step=1850
05/23/2022 10:06:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/23/2022 10:06:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 10:06:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/23/2022 10:06:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 10:06:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 10:06:59 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.6789672117258324 on epoch=474
05/23/2022 10:07:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/23/2022 10:07:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 10:07:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/23/2022 10:07:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
05/23/2022 10:07:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/23/2022 10:07:13 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7085742291334396 on epoch=487
05/23/2022 10:07:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/23/2022 10:07:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/23/2022 10:07:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 10:07:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/23/2022 10:07:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 10:07:27 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6701839826839827 on epoch=499
05/23/2022 10:07:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
05/23/2022 10:07:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/23/2022 10:07:34 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 10:07:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 10:07:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/23/2022 10:07:41 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7085742291334396 on epoch=512
05/23/2022 10:07:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 10:07:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 10:07:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=519
05/23/2022 10:07:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/23/2022 10:07:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/23/2022 10:07:56 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6941919191919192 on epoch=524
05/23/2022 10:07:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 10:08:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 10:08:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/23/2022 10:08:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/23/2022 10:08:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=537
05/23/2022 10:08:11 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6928685114168985 on epoch=537
05/23/2022 10:08:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/23/2022 10:08:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 10:08:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/23/2022 10:08:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 10:08:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 10:08:25 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6788008158274639 on epoch=549
05/23/2022 10:08:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 10:08:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/23/2022 10:08:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 10:08:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/23/2022 10:08:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/23/2022 10:08:40 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.6949063224460087 on epoch=562
05/23/2022 10:08:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 10:08:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/23/2022 10:08:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 10:08:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 10:08:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 10:08:56 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6936887673729779 on epoch=574
05/23/2022 10:08:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/23/2022 10:09:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 10:09:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/23/2022 10:09:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 10:09:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/23/2022 10:09:11 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6570124320124321 on epoch=587
05/23/2022 10:09:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 10:09:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/23/2022 10:09:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/23/2022 10:09:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/23/2022 10:09:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/23/2022 10:09:26 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.680783397888661 on epoch=599
05/23/2022 10:09:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 10:09:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 10:09:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 10:09:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 10:09:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 10:09:41 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6934194455374123 on epoch=612
05/23/2022 10:09:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 10:09:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
05/23/2022 10:09:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 10:09:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/23/2022 10:09:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 10:09:56 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.6790603402445508 on epoch=624
05/23/2022 10:09:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 10:10:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 10:10:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 10:10:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/23/2022 10:10:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
05/23/2022 10:10:11 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6639072104814465 on epoch=637
05/23/2022 10:10:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 10:10:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 10:10:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 10:10:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 10:10:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/23/2022 10:10:27 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6940392940392941 on epoch=649
05/23/2022 10:10:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 10:10:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 10:10:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 10:10:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/23/2022 10:10:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/23/2022 10:10:41 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.66191534914361 on epoch=662
05/23/2022 10:10:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 10:10:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 10:10:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/23/2022 10:10:50 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/23/2022 10:10:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=674
05/23/2022 10:10:55 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.6786518435125246 on epoch=674
05/23/2022 10:10:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 10:11:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 10:11:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/23/2022 10:11:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 10:11:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 10:11:10 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6772727272727272 on epoch=687
05/23/2022 10:11:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 10:11:15 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 10:11:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 10:11:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 10:11:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 10:11:25 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6774602209384819 on epoch=699
05/23/2022 10:11:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 10:11:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/23/2022 10:11:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 10:11:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/23/2022 10:11:37 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/23/2022 10:11:39 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6291322835440483 on epoch=712
05/23/2022 10:11:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 10:11:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 10:11:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 10:11:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 10:11:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/23/2022 10:11:53 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6654305948423596 on epoch=724
05/23/2022 10:11:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/23/2022 10:11:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 10:12:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 10:12:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 10:12:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 10:12:08 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6786518435125246 on epoch=737
05/23/2022 10:12:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 10:12:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 10:12:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/23/2022 10:12:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 10:12:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 10:12:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:12:22 - INFO - __main__ - Printing 3 examples
05/23/2022 10:12:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:12:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:12:22 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:12:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:12:22 - INFO - __main__ - Printing 3 examples
05/23/2022 10:12:22 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:12:22 - INFO - __main__ - ['happy']
05/23/2022 10:12:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:12:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:12:22 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:12:24 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.669342037890425 on epoch=749
05/23/2022 10:12:24 - INFO - __main__ - save last model!
05/23/2022 10:12:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 10:12:24 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 10:12:24 - INFO - __main__ - Printing 3 examples
05/23/2022 10:12:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 10:12:24 - INFO - __main__ - ['others']
05/23/2022 10:12:24 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 10:12:24 - INFO - __main__ - ['others']
05/23/2022 10:12:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 10:12:24 - INFO - __main__ - ['others']
05/23/2022 10:12:24 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:12:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:12:31 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 10:12:41 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:12:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:12:41 - INFO - __main__ - Starting training!
05/23/2022 10:15:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/23/2022 10:15:02 - INFO - __main__ - Classification-F1 on test data: 0.1689
05/23/2022 10:15:02 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7161185432924564, test_performance=0.1688501839563393
05/23/2022 10:15:02 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/23/2022 10:15:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:15:03 - INFO - __main__ - Printing 3 examples
05/23/2022 10:15:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:15:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:15:03 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:15:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:15:03 - INFO - __main__ - Printing 3 examples
05/23/2022 10:15:03 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:15:03 - INFO - __main__ - ['happy']
05/23/2022 10:15:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:15:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:15:03 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:15:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:15:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:15:22 - INFO - __main__ - Starting training!
05/23/2022 10:15:25 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=2
05/23/2022 10:15:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.71 on epoch=4
05/23/2022 10:15:30 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=7
05/23/2022 10:15:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.91 on epoch=9
05/23/2022 10:15:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=12
05/23/2022 10:15:36 - INFO - __main__ - Global step 50 Train loss 1.50 Classification-F1 0.4502832803456428 on epoch=12
05/23/2022 10:15:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4502832803456428 on epoch=12, global_step=50
05/23/2022 10:15:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=14
05/23/2022 10:15:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.77 on epoch=17
05/23/2022 10:15:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.78 on epoch=19
05/23/2022 10:15:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=22
05/23/2022 10:15:48 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=24
05/23/2022 10:15:49 - INFO - __main__ - Global step 100 Train loss 0.81 Classification-F1 0.5799825174825175 on epoch=24
05/23/2022 10:15:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4502832803456428 -> 0.5799825174825175 on epoch=24, global_step=100
05/23/2022 10:15:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=27
05/23/2022 10:15:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=29
05/23/2022 10:15:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.74 on epoch=32
05/23/2022 10:15:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=34
05/23/2022 10:16:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=37
05/23/2022 10:16:02 - INFO - __main__ - Global step 150 Train loss 0.69 Classification-F1 0.6280566280566281 on epoch=37
05/23/2022 10:16:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5799825174825175 -> 0.6280566280566281 on epoch=37, global_step=150
05/23/2022 10:16:04 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=39
05/23/2022 10:16:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=42
05/23/2022 10:16:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=44
05/23/2022 10:16:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=47
05/23/2022 10:16:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=49
05/23/2022 10:16:15 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.6257880434782609 on epoch=49
05/23/2022 10:16:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=52
05/23/2022 10:16:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=54
05/23/2022 10:16:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=57
05/23/2022 10:16:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=59
05/23/2022 10:16:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=62
05/23/2022 10:16:28 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.5895658263305322 on epoch=62
05/23/2022 10:16:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=64
05/23/2022 10:16:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=67
05/23/2022 10:16:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=69
05/23/2022 10:16:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=72
05/23/2022 10:16:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=74
05/23/2022 10:16:41 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.6499554367201427 on epoch=74
05/23/2022 10:16:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6280566280566281 -> 0.6499554367201427 on epoch=74, global_step=300
05/23/2022 10:16:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=77
05/23/2022 10:16:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
05/23/2022 10:16:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=82
05/23/2022 10:16:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=84
05/23/2022 10:16:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=87
05/23/2022 10:16:54 - INFO - __main__ - Global step 350 Train loss 0.29 Classification-F1 0.61382098293863 on epoch=87
05/23/2022 10:16:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=89
05/23/2022 10:16:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=92
05/23/2022 10:17:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=94
05/23/2022 10:17:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=97
05/23/2022 10:17:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=99
05/23/2022 10:17:07 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.5839237967914439 on epoch=99
05/23/2022 10:17:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=102
05/23/2022 10:17:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=104
05/23/2022 10:17:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.15 on epoch=107
05/23/2022 10:17:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=109
05/23/2022 10:17:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=112
05/23/2022 10:17:20 - INFO - __main__ - Global step 450 Train loss 0.14 Classification-F1 0.6577380952380952 on epoch=112
05/23/2022 10:17:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6499554367201427 -> 0.6577380952380952 on epoch=112, global_step=450
05/23/2022 10:17:22 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=114
05/23/2022 10:17:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.14 on epoch=117
05/23/2022 10:17:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=119
05/23/2022 10:17:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=122
05/23/2022 10:17:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.12 on epoch=124
05/23/2022 10:17:33 - INFO - __main__ - Global step 500 Train loss 0.15 Classification-F1 0.5887709137709137 on epoch=124
05/23/2022 10:17:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=127
05/23/2022 10:17:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.10 on epoch=129
05/23/2022 10:17:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.10 on epoch=132
05/23/2022 10:17:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=134
05/23/2022 10:17:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.12 on epoch=137
05/23/2022 10:17:46 - INFO - __main__ - Global step 550 Train loss 0.10 Classification-F1 0.6282811669324828 on epoch=137
05/23/2022 10:17:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=139
05/23/2022 10:17:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=142
05/23/2022 10:17:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=144
05/23/2022 10:17:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=147
05/23/2022 10:17:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=149
05/23/2022 10:17:59 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.6149910688913247 on epoch=149
05/23/2022 10:18:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=152
05/23/2022 10:18:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.11 on epoch=154
05/23/2022 10:18:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=157
05/23/2022 10:18:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=159
05/23/2022 10:18:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.05 on epoch=162
05/23/2022 10:18:13 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.6032467532467533 on epoch=162
05/23/2022 10:18:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=164
05/23/2022 10:18:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=167
05/23/2022 10:18:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=169
05/23/2022 10:18:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=172
05/23/2022 10:18:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.08 on epoch=174
05/23/2022 10:18:28 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.6011695906432749 on epoch=174
05/23/2022 10:18:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=177
05/23/2022 10:18:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
05/23/2022 10:18:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=182
05/23/2022 10:18:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=184
05/23/2022 10:18:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.10 on epoch=187
05/23/2022 10:18:42 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.5826238390092879 on epoch=187
05/23/2022 10:18:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=189
05/23/2022 10:18:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=192
05/23/2022 10:18:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=194
05/23/2022 10:18:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=197
05/23/2022 10:18:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=199
05/23/2022 10:18:57 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.621952371952372 on epoch=199
05/23/2022 10:18:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=202
05/23/2022 10:19:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=204
05/23/2022 10:19:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
05/23/2022 10:19:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=209
05/23/2022 10:19:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=212
05/23/2022 10:19:11 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.5966137566137566 on epoch=212
05/23/2022 10:19:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
05/23/2022 10:19:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=217
05/23/2022 10:19:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/23/2022 10:19:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
05/23/2022 10:19:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/23/2022 10:19:26 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.6661390148598159 on epoch=224
05/23/2022 10:19:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6577380952380952 -> 0.6661390148598159 on epoch=224, global_step=900
05/23/2022 10:19:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
05/23/2022 10:19:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/23/2022 10:19:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/23/2022 10:19:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=234
05/23/2022 10:19:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.07 on epoch=237
05/23/2022 10:19:41 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.5941125367548783 on epoch=237
05/23/2022 10:19:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
05/23/2022 10:19:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=242
05/23/2022 10:19:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
05/23/2022 10:19:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/23/2022 10:19:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=249
05/23/2022 10:19:55 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.7123493623493624 on epoch=249
05/23/2022 10:19:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6661390148598159 -> 0.7123493623493624 on epoch=249, global_step=1000
05/23/2022 10:19:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
05/23/2022 10:20:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=254
05/23/2022 10:20:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=257
05/23/2022 10:20:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/23/2022 10:20:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/23/2022 10:20:09 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6443855095121972 on epoch=262
05/23/2022 10:20:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=264
05/23/2022 10:20:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=267
05/23/2022 10:20:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 10:20:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 10:20:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/23/2022 10:20:23 - INFO - __main__ - Global step 1100 Train loss 0.05 Classification-F1 0.6460901027077498 on epoch=274
05/23/2022 10:20:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/23/2022 10:20:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 10:20:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=282
05/23/2022 10:20:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/23/2022 10:20:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/23/2022 10:20:38 - INFO - __main__ - Global step 1150 Train loss 0.02 Classification-F1 0.7091103341103342 on epoch=287
05/23/2022 10:20:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=289
05/23/2022 10:20:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=292
05/23/2022 10:20:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=294
05/23/2022 10:20:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=297
05/23/2022 10:20:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=299
05/23/2022 10:20:52 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.6333953618274698 on epoch=299
05/23/2022 10:20:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 10:20:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=304
05/23/2022 10:20:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/23/2022 10:21:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 10:21:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=312
05/23/2022 10:21:06 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6046602787456447 on epoch=312
05/23/2022 10:21:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 10:21:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/23/2022 10:21:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/23/2022 10:21:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/23/2022 10:21:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/23/2022 10:21:20 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6293001258474404 on epoch=324
05/23/2022 10:21:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=327
05/23/2022 10:21:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=329
05/23/2022 10:21:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 10:21:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=334
05/23/2022 10:21:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 10:21:34 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.5805194805194804 on epoch=337
05/23/2022 10:21:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/23/2022 10:21:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
05/23/2022 10:21:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=344
05/23/2022 10:21:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 10:21:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=349
05/23/2022 10:21:48 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.6893225238813475 on epoch=349
05/23/2022 10:21:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 10:21:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 10:21:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 10:21:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 10:22:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 10:22:02 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6040204773705491 on epoch=362
05/23/2022 10:22:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 10:22:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/23/2022 10:22:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
05/23/2022 10:22:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 10:22:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/23/2022 10:22:15 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6820553539019963 on epoch=374
05/23/2022 10:22:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/23/2022 10:22:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 10:22:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 10:22:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/23/2022 10:22:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 10:22:29 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6279855561105562 on epoch=387
05/23/2022 10:22:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/23/2022 10:22:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 10:22:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 10:22:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 10:22:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 10:22:43 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6025075697445396 on epoch=399
05/23/2022 10:22:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 10:22:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=404
05/23/2022 10:22:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/23/2022 10:22:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/23/2022 10:22:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=412
05/23/2022 10:22:57 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6380370538063364 on epoch=412
05/23/2022 10:23:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/23/2022 10:23:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/23/2022 10:23:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=419
05/23/2022 10:23:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/23/2022 10:23:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 10:23:11 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6388615216201423 on epoch=424
05/23/2022 10:23:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/23/2022 10:23:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/23/2022 10:23:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/23/2022 10:23:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 10:23:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=437
05/23/2022 10:23:25 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6030787753697971 on epoch=437
05/23/2022 10:23:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/23/2022 10:23:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 10:23:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 10:23:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 10:23:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 10:23:38 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6795405030699149 on epoch=449
05/23/2022 10:23:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 10:23:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/23/2022 10:23:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 10:23:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=459
05/23/2022 10:23:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 10:23:52 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.6394291587602783 on epoch=462
05/23/2022 10:23:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/23/2022 10:23:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 10:24:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 10:24:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/23/2022 10:24:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/23/2022 10:24:06 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6474193548387096 on epoch=474
05/23/2022 10:24:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/23/2022 10:24:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 10:24:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=482
05/23/2022 10:24:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 10:24:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 10:24:20 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6588517155814152 on epoch=487
05/23/2022 10:24:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/23/2022 10:24:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 10:24:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 10:24:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 10:24:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/23/2022 10:24:34 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6820489844683393 on epoch=499
05/23/2022 10:24:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/23/2022 10:24:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 10:24:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/23/2022 10:24:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/23/2022 10:24:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 10:24:48 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.649369918699187 on epoch=512
05/23/2022 10:24:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
05/23/2022 10:24:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/23/2022 10:24:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/23/2022 10:24:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
05/23/2022 10:25:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/23/2022 10:25:02 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6610557184750734 on epoch=524
05/23/2022 10:25:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 10:25:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/23/2022 10:25:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 10:25:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
05/23/2022 10:25:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 10:25:15 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6054749172396232 on epoch=537
05/23/2022 10:25:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 10:25:20 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 10:25:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 10:25:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 10:25:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 10:25:29 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6596484570058923 on epoch=549
05/23/2022 10:25:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/23/2022 10:25:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 10:25:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 10:25:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 10:25:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 10:25:43 - INFO - __main__ - Global step 2250 Train loss 0.00 Classification-F1 0.593854045683314 on epoch=562
05/23/2022 10:25:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 10:25:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/23/2022 10:25:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 10:25:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 10:25:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 10:25:57 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.628609072087333 on epoch=574
05/23/2022 10:25:59 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 10:26:01 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 10:26:04 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 10:26:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 10:26:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 10:26:10 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.617365373732097 on epoch=587
05/23/2022 10:26:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 10:26:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 10:26:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 10:26:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 10:26:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 10:26:23 - INFO - __main__ - Global step 2400 Train loss 0.00 Classification-F1 0.628609072087333 on epoch=599
05/23/2022 10:26:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 10:26:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 10:26:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 10:26:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 10:26:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 10:26:37 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6458333333333334 on epoch=612
05/23/2022 10:26:39 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 10:26:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 10:26:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 10:26:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 10:26:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 10:26:50 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6505952380952381 on epoch=624
05/23/2022 10:26:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 10:26:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 10:26:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 10:26:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 10:27:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 10:27:03 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6402816736792893 on epoch=637
05/23/2022 10:27:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
05/23/2022 10:27:08 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 10:27:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 10:27:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 10:27:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/23/2022 10:27:17 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6452136327503974 on epoch=649
05/23/2022 10:27:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 10:27:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/23/2022 10:27:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 10:27:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 10:27:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/23/2022 10:27:30 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6542748273398429 on epoch=662
05/23/2022 10:27:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 10:27:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 10:27:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/23/2022 10:27:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 10:27:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 10:27:43 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.670571324819948 on epoch=674
05/23/2022 10:27:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 10:27:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 10:27:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 10:27:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 10:27:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 10:27:56 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6651409932659933 on epoch=687
05/23/2022 10:27:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 10:28:01 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=692
05/23/2022 10:28:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 10:28:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/23/2022 10:28:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
05/23/2022 10:28:09 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6651409932659933 on epoch=699
05/23/2022 10:28:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 10:28:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
05/23/2022 10:28:16 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 10:28:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 10:28:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 10:28:22 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6938188188188188 on epoch=712
05/23/2022 10:28:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/23/2022 10:28:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 10:28:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 10:28:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 10:28:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 10:28:35 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6865773559213425 on epoch=724
05/23/2022 10:28:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 10:28:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 10:28:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 10:28:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 10:28:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 10:28:48 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6803100356231286 on epoch=737
05/23/2022 10:28:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 10:28:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 10:28:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 10:28:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/23/2022 10:29:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 10:29:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:29:01 - INFO - __main__ - Printing 3 examples
05/23/2022 10:29:01 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:29:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:29:01 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:29:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:29:01 - INFO - __main__ - Printing 3 examples
05/23/2022 10:29:01 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:29:01 - INFO - __main__ - ['happy']
05/23/2022 10:29:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:29:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:29:01 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:29:01 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6802041605172536 on epoch=749
05/23/2022 10:29:01 - INFO - __main__ - save last model!
05/23/2022 10:29:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 10:29:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 10:29:01 - INFO - __main__ - Printing 3 examples
05/23/2022 10:29:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 10:29:01 - INFO - __main__ - ['others']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 10:29:01 - INFO - __main__ - ['others']
05/23/2022 10:29:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 10:29:01 - INFO - __main__ - ['others']
05/23/2022 10:29:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:29:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:29:09 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 10:29:20 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:29:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:29:20 - INFO - __main__ - Starting training!
05/23/2022 10:31:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/23/2022 10:31:05 - INFO - __main__ - Classification-F1 on test data: 0.3019
05/23/2022 10:31:05 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7123493623493624, test_performance=0.3019264079820776
05/23/2022 10:31:05 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/23/2022 10:31:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:31:06 - INFO - __main__ - Printing 3 examples
05/23/2022 10:31:06 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:31:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:31:06 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:31:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:31:06 - INFO - __main__ - Printing 3 examples
05/23/2022 10:31:06 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:31:06 - INFO - __main__ - ['happy']
05/23/2022 10:31:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:31:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:31:06 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:31:25 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:31:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:31:26 - INFO - __main__ - Starting training!
05/23/2022 10:31:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.08 on epoch=2
05/23/2022 10:31:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.93 on epoch=4
05/23/2022 10:31:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.33 on epoch=7
05/23/2022 10:31:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=9
05/23/2022 10:31:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.86 on epoch=12
05/23/2022 10:31:39 - INFO - __main__ - Global step 50 Train loss 1.64 Classification-F1 0.402311106355224 on epoch=12
05/23/2022 10:31:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.402311106355224 on epoch=12, global_step=50
05/23/2022 10:31:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=14
05/23/2022 10:31:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=17
05/23/2022 10:31:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.73 on epoch=19
05/23/2022 10:31:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=22
05/23/2022 10:31:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.77 on epoch=24
05/23/2022 10:31:52 - INFO - __main__ - Global step 100 Train loss 0.85 Classification-F1 0.5323787323787323 on epoch=24
05/23/2022 10:31:52 - INFO - __main__ - Saving model with best Classification-F1: 0.402311106355224 -> 0.5323787323787323 on epoch=24, global_step=100
05/23/2022 10:31:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.81 on epoch=27
05/23/2022 10:31:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.77 on epoch=29
05/23/2022 10:31:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=32
05/23/2022 10:32:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=34
05/23/2022 10:32:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=37
05/23/2022 10:32:05 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.5836526181353768 on epoch=37
05/23/2022 10:32:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5323787323787323 -> 0.5836526181353768 on epoch=37, global_step=150
05/23/2022 10:32:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=39
05/23/2022 10:32:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=42
05/23/2022 10:32:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=44
05/23/2022 10:32:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=47
05/23/2022 10:32:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=49
05/23/2022 10:32:18 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.6010888226258245 on epoch=49
05/23/2022 10:32:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5836526181353768 -> 0.6010888226258245 on epoch=49, global_step=200
05/23/2022 10:32:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.56 on epoch=52
05/23/2022 10:32:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=54
05/23/2022 10:32:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=57
05/23/2022 10:32:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=59
05/23/2022 10:32:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=62
05/23/2022 10:32:31 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.5758178053830227 on epoch=62
05/23/2022 10:32:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=64
05/23/2022 10:32:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=67
05/23/2022 10:32:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=69
05/23/2022 10:32:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=72
05/23/2022 10:32:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.35 on epoch=74
05/23/2022 10:32:44 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.6824175824175824 on epoch=74
05/23/2022 10:32:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6010888226258245 -> 0.6824175824175824 on epoch=74, global_step=300
05/23/2022 10:32:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=77
05/23/2022 10:32:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=79
05/23/2022 10:32:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=82
05/23/2022 10:32:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=84
05/23/2022 10:32:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.20 on epoch=87
05/23/2022 10:32:58 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.6000000000000001 on epoch=87
05/23/2022 10:33:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.33 on epoch=89
05/23/2022 10:33:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=92
05/23/2022 10:33:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=94
05/23/2022 10:33:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=97
05/23/2022 10:33:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=99
05/23/2022 10:33:11 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.6986554621848741 on epoch=99
05/23/2022 10:33:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6824175824175824 -> 0.6986554621848741 on epoch=99, global_step=400
05/23/2022 10:33:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=102
05/23/2022 10:33:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=104
05/23/2022 10:33:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=107
05/23/2022 10:33:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=109
05/23/2022 10:33:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=112
05/23/2022 10:33:25 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.6695746596747709 on epoch=112
05/23/2022 10:33:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=114
05/23/2022 10:33:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.12 on epoch=117
05/23/2022 10:33:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.14 on epoch=119
05/23/2022 10:33:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.14 on epoch=122
05/23/2022 10:33:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=124
05/23/2022 10:33:40 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.6847222222222222 on epoch=124
05/23/2022 10:33:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.12 on epoch=127
05/23/2022 10:33:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=129
05/23/2022 10:33:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=132
05/23/2022 10:33:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.16 on epoch=134
05/23/2022 10:33:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=137
05/23/2022 10:33:53 - INFO - __main__ - Global step 550 Train loss 0.12 Classification-F1 0.666540404040404 on epoch=137
05/23/2022 10:33:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=139
05/23/2022 10:33:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.07 on epoch=142
05/23/2022 10:34:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=144
05/23/2022 10:34:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=147
05/23/2022 10:34:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.10 on epoch=149
05/23/2022 10:34:07 - INFO - __main__ - Global step 600 Train loss 0.12 Classification-F1 0.6081655918181226 on epoch=149
05/23/2022 10:34:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=152
05/23/2022 10:34:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/23/2022 10:34:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.07 on epoch=157
05/23/2022 10:34:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=159
05/23/2022 10:34:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/23/2022 10:34:21 - INFO - __main__ - Global step 650 Train loss 0.09 Classification-F1 0.5804277244494636 on epoch=162
05/23/2022 10:34:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.09 on epoch=164
05/23/2022 10:34:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=167
05/23/2022 10:34:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=169
05/23/2022 10:34:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=172
05/23/2022 10:34:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
05/23/2022 10:34:36 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.6790948702713409 on epoch=174
05/23/2022 10:34:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=177
05/23/2022 10:34:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=179
05/23/2022 10:34:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=182
05/23/2022 10:34:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=184
05/23/2022 10:34:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
05/23/2022 10:34:50 - INFO - __main__ - Global step 750 Train loss 0.05 Classification-F1 0.6290846408493468 on epoch=187
05/23/2022 10:34:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
05/23/2022 10:34:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.09 on epoch=192
05/23/2022 10:34:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=194
05/23/2022 10:35:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
05/23/2022 10:35:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=199
05/23/2022 10:35:05 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.6302733214497921 on epoch=199
05/23/2022 10:35:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=202
05/23/2022 10:35:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=204
05/23/2022 10:35:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=207
05/23/2022 10:35:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/23/2022 10:35:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=212
05/23/2022 10:35:18 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.6192695689065925 on epoch=212
05/23/2022 10:35:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=214
05/23/2022 10:35:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=217
05/23/2022 10:35:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/23/2022 10:35:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=222
05/23/2022 10:35:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/23/2022 10:35:32 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.6300324675324676 on epoch=224
05/23/2022 10:35:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=227
05/23/2022 10:35:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/23/2022 10:35:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=232
05/23/2022 10:35:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=234
05/23/2022 10:35:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/23/2022 10:35:46 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.638095238095238 on epoch=237
05/23/2022 10:35:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/23/2022 10:35:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/23/2022 10:35:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/23/2022 10:35:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=247
05/23/2022 10:35:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=249
05/23/2022 10:36:00 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.6344967532467533 on epoch=249
05/23/2022 10:36:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.05 on epoch=252
05/23/2022 10:36:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=254
05/23/2022 10:36:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/23/2022 10:36:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=259
05/23/2022 10:36:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=262
05/23/2022 10:36:13 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6215686274509804 on epoch=262
05/23/2022 10:36:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/23/2022 10:36:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 10:36:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 10:36:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=272
05/23/2022 10:36:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=274
05/23/2022 10:36:27 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.6390720357454229 on epoch=274
05/23/2022 10:36:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/23/2022 10:36:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/23/2022 10:36:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=282
05/23/2022 10:36:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/23/2022 10:36:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=287
05/23/2022 10:36:41 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.6224399659752666 on epoch=287
05/23/2022 10:36:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 10:36:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/23/2022 10:36:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=294
05/23/2022 10:36:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/23/2022 10:36:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.04 on epoch=299
05/23/2022 10:36:55 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6036614247821144 on epoch=299
05/23/2022 10:36:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 10:37:00 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/23/2022 10:37:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/23/2022 10:37:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/23/2022 10:37:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/23/2022 10:37:09 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6525855492589364 on epoch=312
05/23/2022 10:37:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=314
05/23/2022 10:37:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/23/2022 10:37:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
05/23/2022 10:37:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/23/2022 10:37:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=324
05/23/2022 10:37:23 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.6363965469228627 on epoch=324
05/23/2022 10:37:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/23/2022 10:37:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/23/2022 10:37:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 10:37:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/23/2022 10:37:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=337
05/23/2022 10:37:37 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6048681071049841 on epoch=337
05/23/2022 10:37:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/23/2022 10:37:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 10:37:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/23/2022 10:37:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=347
05/23/2022 10:37:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
05/23/2022 10:37:51 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6497499404620147 on epoch=349
05/23/2022 10:37:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 10:37:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=354
05/23/2022 10:37:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 10:38:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 10:38:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 10:38:05 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6697492163009403 on epoch=362
05/23/2022 10:38:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 10:38:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/23/2022 10:38:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 10:38:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=372
05/23/2022 10:38:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=374
05/23/2022 10:38:20 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.6416666666666667 on epoch=374
05/23/2022 10:38:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=377
05/23/2022 10:38:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 10:38:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/23/2022 10:38:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/23/2022 10:38:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/23/2022 10:38:35 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6387271462639109 on epoch=387
05/23/2022 10:38:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/23/2022 10:38:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=392
05/23/2022 10:38:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=394
05/23/2022 10:38:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 10:38:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/23/2022 10:38:49 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.6658442982456141 on epoch=399
05/23/2022 10:38:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=402
05/23/2022 10:38:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 10:38:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/23/2022 10:38:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=409
05/23/2022 10:39:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/23/2022 10:39:03 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6759958133971291 on epoch=412
05/23/2022 10:39:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 10:39:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/23/2022 10:39:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/23/2022 10:39:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 10:39:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/23/2022 10:39:17 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.6591041021671826 on epoch=424
05/23/2022 10:39:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/23/2022 10:39:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=429
05/23/2022 10:39:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 10:39:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=434
05/23/2022 10:39:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 10:39:31 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.63879944577796 on epoch=437
05/23/2022 10:39:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/23/2022 10:39:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 10:39:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=444
05/23/2022 10:39:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/23/2022 10:39:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=449
05/23/2022 10:39:45 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6593120393120393 on epoch=449
05/23/2022 10:39:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 10:39:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=454
05/23/2022 10:39:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 10:39:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 10:39:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 10:39:59 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6180822649572649 on epoch=462
05/23/2022 10:40:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/23/2022 10:40:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 10:40:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 10:40:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/23/2022 10:40:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=474
05/23/2022 10:40:13 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6521990740740741 on epoch=474
05/23/2022 10:40:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 10:40:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 10:40:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 10:40:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=484
05/23/2022 10:40:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 10:40:28 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6863815284178187 on epoch=487
05/23/2022 10:40:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 10:40:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 10:40:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 10:40:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/23/2022 10:40:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 10:40:43 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6336460918114143 on epoch=499
05/23/2022 10:40:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
05/23/2022 10:40:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=504
05/23/2022 10:40:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 10:40:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 10:40:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 10:40:58 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6482793522267206 on epoch=512
05/23/2022 10:41:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/23/2022 10:41:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/23/2022 10:41:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/23/2022 10:41:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 10:41:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 10:41:12 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6315007855754885 on epoch=524
05/23/2022 10:41:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 10:41:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 10:41:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 10:41:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 10:41:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 10:41:26 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6470518680445151 on epoch=537
05/23/2022 10:41:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/23/2022 10:41:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 10:41:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 10:41:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/23/2022 10:41:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 10:41:41 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6159771411384315 on epoch=549
05/23/2022 10:41:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/23/2022 10:41:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 10:41:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 10:41:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 10:41:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/23/2022 10:41:55 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6155796536121079 on epoch=562
05/23/2022 10:41:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 10:42:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/23/2022 10:42:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 10:42:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 10:42:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 10:42:09 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6191347597597597 on epoch=574
05/23/2022 10:42:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 10:42:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 10:42:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 10:42:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 10:42:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 10:42:24 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6311144338118022 on epoch=587
05/23/2022 10:42:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/23/2022 10:42:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 10:42:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 10:42:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 10:42:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 10:42:37 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6455411255411255 on epoch=599
05/23/2022 10:42:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/23/2022 10:42:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 10:42:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 10:42:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 10:42:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 10:42:51 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6332211538461539 on epoch=612
05/23/2022 10:42:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 10:42:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 10:42:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 10:43:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 10:43:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 10:43:06 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6497546394320588 on epoch=624
05/23/2022 10:43:08 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 10:43:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 10:43:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 10:43:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 10:43:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 10:43:20 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6592899584076055 on epoch=637
05/23/2022 10:43:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 10:43:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 10:43:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/23/2022 10:43:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 10:43:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 10:43:35 - INFO - __main__ - Global step 2600 Train loss 0.00 Classification-F1 0.6288488843813387 on epoch=649
05/23/2022 10:43:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 10:43:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 10:43:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=657
05/23/2022 10:43:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 10:43:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 10:43:49 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6617577503974563 on epoch=662
05/23/2022 10:43:51 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 10:43:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 10:43:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 10:43:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/23/2022 10:44:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/23/2022 10:44:03 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.676009465686885 on epoch=674
05/23/2022 10:44:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 10:44:08 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 10:44:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 10:44:13 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 10:44:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 10:44:17 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6640427444322381 on epoch=687
05/23/2022 10:44:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 10:44:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 10:44:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 10:44:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 10:44:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 10:44:30 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6118160508296835 on epoch=699
05/23/2022 10:44:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/23/2022 10:44:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 10:44:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 10:44:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 10:44:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 10:44:44 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6328671328671328 on epoch=712
05/23/2022 10:44:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 10:44:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 10:44:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 10:44:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 10:44:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 10:44:58 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6330593185550082 on epoch=724
05/23/2022 10:45:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 10:45:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 10:45:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 10:45:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 10:45:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 10:45:13 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.6306786961286541 on epoch=737
05/23/2022 10:45:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 10:45:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 10:45:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 10:45:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 10:45:25 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=749
05/23/2022 10:45:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:45:26 - INFO - __main__ - Printing 3 examples
05/23/2022 10:45:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:45:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:45:26 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:45:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:45:26 - INFO - __main__ - Printing 3 examples
05/23/2022 10:45:26 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:45:26 - INFO - __main__ - ['happy']
05/23/2022 10:45:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:45:26 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:45:26 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:45:27 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6383000604960677 on epoch=749
05/23/2022 10:45:27 - INFO - __main__ - save last model!
05/23/2022 10:45:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 10:45:27 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 10:45:27 - INFO - __main__ - Printing 3 examples
05/23/2022 10:45:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 10:45:27 - INFO - __main__ - ['others']
05/23/2022 10:45:27 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 10:45:27 - INFO - __main__ - ['others']
05/23/2022 10:45:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 10:45:27 - INFO - __main__ - ['others']
05/23/2022 10:45:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:45:29 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:45:35 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 10:45:42 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:45:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:45:42 - INFO - __main__ - Starting training!
05/23/2022 10:47:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/23/2022 10:47:16 - INFO - __main__ - Classification-F1 on test data: 0.1573
05/23/2022 10:47:17 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.6986554621848741, test_performance=0.15732680023994872
05/23/2022 10:47:17 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/23/2022 10:47:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:47:18 - INFO - __main__ - Printing 3 examples
05/23/2022 10:47:18 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:47:18 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:47:18 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 10:47:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 10:47:18 - INFO - __main__ - Printing 3 examples
05/23/2022 10:47:18 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 10:47:18 - INFO - __main__ - ['happy']
05/23/2022 10:47:18 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:47:18 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:47:18 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 10:47:33 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 10:47:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 10:47:34 - INFO - __main__ - Starting training!
05/23/2022 10:47:37 - INFO - __main__ - Step 10 Global step 10 Train loss 2.99 on epoch=2
05/23/2022 10:47:40 - INFO - __main__ - Step 20 Global step 20 Train loss 2.25 on epoch=4
05/23/2022 10:47:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.60 on epoch=7
05/23/2022 10:47:44 - INFO - __main__ - Step 40 Global step 40 Train loss 1.29 on epoch=9
05/23/2022 10:47:47 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=12
05/23/2022 10:47:48 - INFO - __main__ - Global step 50 Train loss 1.82 Classification-F1 0.36248967019758094 on epoch=12
05/23/2022 10:47:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36248967019758094 on epoch=12, global_step=50
05/23/2022 10:47:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=14
05/23/2022 10:47:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=17
05/23/2022 10:47:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.89 on epoch=19
05/23/2022 10:47:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=22
05/23/2022 10:48:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=24
05/23/2022 10:48:01 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.49749231994538634 on epoch=24
05/23/2022 10:48:01 - INFO - __main__ - Saving model with best Classification-F1: 0.36248967019758094 -> 0.49749231994538634 on epoch=24, global_step=100
05/23/2022 10:48:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
05/23/2022 10:48:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=29
05/23/2022 10:48:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=32
05/23/2022 10:48:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.66 on epoch=34
05/23/2022 10:48:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
05/23/2022 10:48:14 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.4912655570550307 on epoch=37
05/23/2022 10:48:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.69 on epoch=39
05/23/2022 10:48:19 - INFO - __main__ - Step 170 Global step 170 Train loss 1.05 on epoch=42
05/23/2022 10:48:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=44
05/23/2022 10:48:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
05/23/2022 10:48:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
05/23/2022 10:48:27 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.48350743459439105 on epoch=49
05/23/2022 10:48:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.71 on epoch=52
05/23/2022 10:48:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=54
05/23/2022 10:48:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=57
05/23/2022 10:48:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=59
05/23/2022 10:48:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=62
05/23/2022 10:48:40 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.5007307464204016 on epoch=62
05/23/2022 10:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.49749231994538634 -> 0.5007307464204016 on epoch=62, global_step=250
05/23/2022 10:48:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=64
05/23/2022 10:48:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.68 on epoch=67
05/23/2022 10:48:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=69
05/23/2022 10:48:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.72 on epoch=72
05/23/2022 10:48:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.58 on epoch=74
05/23/2022 10:48:53 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.5363612836438924 on epoch=74
05/23/2022 10:48:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5007307464204016 -> 0.5363612836438924 on epoch=74, global_step=300
05/23/2022 10:48:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.59 on epoch=77
05/23/2022 10:48:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.69 on epoch=79
05/23/2022 10:49:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=82
05/23/2022 10:49:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=84
05/23/2022 10:49:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=87
05/23/2022 10:49:06 - INFO - __main__ - Global step 350 Train loss 0.61 Classification-F1 0.5892431071049842 on epoch=87
05/23/2022 10:49:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5363612836438924 -> 0.5892431071049842 on epoch=87, global_step=350
05/23/2022 10:49:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=89
05/23/2022 10:49:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=92
05/23/2022 10:49:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=94
05/23/2022 10:49:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=97
05/23/2022 10:49:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=99
05/23/2022 10:49:19 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.5654719569193254 on epoch=99
05/23/2022 10:49:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.62 on epoch=102
05/23/2022 10:49:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=104
05/23/2022 10:49:26 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=107
05/23/2022 10:49:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=109
05/23/2022 10:49:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=112
05/23/2022 10:49:32 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.5735913530031178 on epoch=112
05/23/2022 10:49:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.57 on epoch=114
05/23/2022 10:49:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.65 on epoch=117
05/23/2022 10:49:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=119
05/23/2022 10:49:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=122
05/23/2022 10:49:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=124
05/23/2022 10:49:45 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.5966202270381837 on epoch=124
05/23/2022 10:49:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5892431071049842 -> 0.5966202270381837 on epoch=124, global_step=500
05/23/2022 10:49:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=127
05/23/2022 10:49:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=129
05/23/2022 10:49:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=132
05/23/2022 10:49:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=134
05/23/2022 10:49:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=137
05/23/2022 10:49:58 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.6197363542337491 on epoch=137
05/23/2022 10:49:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5966202270381837 -> 0.6197363542337491 on epoch=137, global_step=550
05/23/2022 10:50:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=139
05/23/2022 10:50:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=142
05/23/2022 10:50:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=144
05/23/2022 10:50:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=147
05/23/2022 10:50:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=149
05/23/2022 10:50:11 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.6511067101584344 on epoch=149
05/23/2022 10:50:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6197363542337491 -> 0.6511067101584344 on epoch=149, global_step=600
05/23/2022 10:50:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=152
05/23/2022 10:50:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=154
05/23/2022 10:50:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=157
05/23/2022 10:50:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=159
05/23/2022 10:50:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=162
05/23/2022 10:50:24 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.6594153225806452 on epoch=162
05/23/2022 10:50:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6511067101584344 -> 0.6594153225806452 on epoch=162, global_step=650
05/23/2022 10:50:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=164
05/23/2022 10:50:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=167
05/23/2022 10:50:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=169
05/23/2022 10:50:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=172
05/23/2022 10:50:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=174
05/23/2022 10:50:37 - INFO - __main__ - Global step 700 Train loss 0.28 Classification-F1 0.7034944581280789 on epoch=174
05/23/2022 10:50:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6594153225806452 -> 0.7034944581280789 on epoch=174, global_step=700
05/23/2022 10:50:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=177
05/23/2022 10:50:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=179
05/23/2022 10:50:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=182
05/23/2022 10:50:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=184
05/23/2022 10:50:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=187
05/23/2022 10:50:50 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.6086693548387097 on epoch=187
05/23/2022 10:50:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=189
05/23/2022 10:50:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=192
05/23/2022 10:50:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=194
05/23/2022 10:50:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=197
05/23/2022 10:51:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=199
05/23/2022 10:51:03 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.5949143535883971 on epoch=199
05/23/2022 10:51:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
05/23/2022 10:51:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=204
05/23/2022 10:51:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=207
05/23/2022 10:51:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=209
05/23/2022 10:51:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=212
05/23/2022 10:51:15 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.5867885109864422 on epoch=212
05/23/2022 10:51:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=214
05/23/2022 10:51:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.14 on epoch=217
05/23/2022 10:51:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=219
05/23/2022 10:51:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/23/2022 10:51:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=224
05/23/2022 10:51:29 - INFO - __main__ - Global step 900 Train loss 0.12 Classification-F1 0.6692775923866023 on epoch=224
05/23/2022 10:51:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=227
05/23/2022 10:51:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=229
05/23/2022 10:51:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=232
05/23/2022 10:51:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=234
05/23/2022 10:51:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=237
05/23/2022 10:51:42 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.6260910815939279 on epoch=237
05/23/2022 10:51:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
05/23/2022 10:51:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.14 on epoch=242
05/23/2022 10:51:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=244
05/23/2022 10:51:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=247
05/23/2022 10:51:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/23/2022 10:51:56 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.590955459770115 on epoch=249
05/23/2022 10:51:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/23/2022 10:52:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=254
05/23/2022 10:52:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/23/2022 10:52:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/23/2022 10:52:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
05/23/2022 10:52:10 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.612869090288445 on epoch=262
05/23/2022 10:52:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=264
05/23/2022 10:52:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=267
05/23/2022 10:52:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/23/2022 10:52:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/23/2022 10:52:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
05/23/2022 10:52:24 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6096638655462185 on epoch=274
05/23/2022 10:52:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/23/2022 10:52:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/23/2022 10:52:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 10:52:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
05/23/2022 10:52:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
05/23/2022 10:52:39 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6047567472630409 on epoch=287
05/23/2022 10:52:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/23/2022 10:52:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=292
05/23/2022 10:52:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 10:52:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=297
05/23/2022 10:52:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
05/23/2022 10:52:54 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.5659663865546218 on epoch=299
05/23/2022 10:52:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=302
05/23/2022 10:52:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=304
05/23/2022 10:53:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/23/2022 10:53:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=309
05/23/2022 10:53:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=312
05/23/2022 10:53:10 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6224415204678363 on epoch=312
05/23/2022 10:53:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 10:53:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
05/23/2022 10:53:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
05/23/2022 10:53:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/23/2022 10:53:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/23/2022 10:53:25 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6360310798548094 on epoch=324
05/23/2022 10:53:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
05/23/2022 10:53:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=329
05/23/2022 10:53:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/23/2022 10:53:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/23/2022 10:53:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=337
05/23/2022 10:53:40 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.6076388888888888 on epoch=337
05/23/2022 10:53:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/23/2022 10:53:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/23/2022 10:53:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=344
05/23/2022 10:53:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/23/2022 10:53:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/23/2022 10:53:55 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6744212542851382 on epoch=349
05/23/2022 10:53:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/23/2022 10:54:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=354
05/23/2022 10:54:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/23/2022 10:54:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=359
05/23/2022 10:54:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=362
05/23/2022 10:54:10 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.5863095238095238 on epoch=362
05/23/2022 10:54:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=364
05/23/2022 10:54:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=367
05/23/2022 10:54:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=369
05/23/2022 10:54:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/23/2022 10:54:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/23/2022 10:54:26 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.6438591269841271 on epoch=374
05/23/2022 10:54:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/23/2022 10:54:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=379
05/23/2022 10:54:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/23/2022 10:54:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 10:54:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
05/23/2022 10:54:42 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6440503003003003 on epoch=387
05/23/2022 10:54:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/23/2022 10:54:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=392
05/23/2022 10:54:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=394
05/23/2022 10:54:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/23/2022 10:54:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=399
05/23/2022 10:54:56 - INFO - __main__ - Global step 1600 Train loss 0.03 Classification-F1 0.5886093073593074 on epoch=399
05/23/2022 10:54:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/23/2022 10:55:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=404
05/23/2022 10:55:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=407
05/23/2022 10:55:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/23/2022 10:55:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 10:55:12 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6322892469996129 on epoch=412
05/23/2022 10:55:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 10:55:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/23/2022 10:55:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/23/2022 10:55:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/23/2022 10:55:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=424
05/23/2022 10:55:27 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6083945696848923 on epoch=424
05/23/2022 10:55:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 10:55:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/23/2022 10:55:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 10:55:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/23/2022 10:55:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=437
05/23/2022 10:55:43 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.5895942982456142 on epoch=437
05/23/2022 10:55:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=439
05/23/2022 10:55:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 10:55:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=444
05/23/2022 10:55:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=447
05/23/2022 10:55:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 10:55:59 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5869902946515849 on epoch=449
05/23/2022 10:56:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/23/2022 10:56:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 10:56:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/23/2022 10:56:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 10:56:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 10:56:15 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6396462639109698 on epoch=462
05/23/2022 10:56:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/23/2022 10:56:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 10:56:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/23/2022 10:56:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 10:56:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 10:56:32 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6538645741231949 on epoch=474
05/23/2022 10:56:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=477
05/23/2022 10:56:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 10:56:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/23/2022 10:56:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
05/23/2022 10:56:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/23/2022 10:56:48 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6029411764705882 on epoch=487
05/23/2022 10:56:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 10:56:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 10:56:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/23/2022 10:56:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/23/2022 10:57:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/23/2022 10:57:05 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.5873539780189396 on epoch=499
05/23/2022 10:57:07 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/23/2022 10:57:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 10:57:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/23/2022 10:57:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/23/2022 10:57:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 10:57:22 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6341573631047315 on epoch=512
05/23/2022 10:57:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=514
05/23/2022 10:57:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/23/2022 10:57:29 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 10:57:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/23/2022 10:57:34 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
05/23/2022 10:57:40 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.6545410816823497 on epoch=524
05/23/2022 10:57:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=527
05/23/2022 10:57:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 10:57:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/23/2022 10:57:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=534
05/23/2022 10:57:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/23/2022 10:57:56 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.660795985060691 on epoch=537
05/23/2022 10:57:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=539
05/23/2022 10:58:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 10:58:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/23/2022 10:58:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/23/2022 10:58:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 10:58:12 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.6138701718145452 on epoch=549
05/23/2022 10:58:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 10:58:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 10:58:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 10:58:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.00 on epoch=559
05/23/2022 10:58:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/23/2022 10:58:28 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5731740669240669 on epoch=562
05/23/2022 10:58:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/23/2022 10:58:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/23/2022 10:58:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/23/2022 10:58:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 10:58:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/23/2022 10:58:43 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6459383753501401 on epoch=574
05/23/2022 10:58:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 10:58:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=579
05/23/2022 10:58:51 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/23/2022 10:58:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 10:58:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 10:58:59 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6250549899446959 on epoch=587
05/23/2022 10:59:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/23/2022 10:59:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/23/2022 10:59:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/23/2022 10:59:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 10:59:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 10:59:15 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6736733854380913 on epoch=599
05/23/2022 10:59:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 10:59:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/23/2022 10:59:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 10:59:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/23/2022 10:59:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 10:59:31 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6722875816993464 on epoch=612
05/23/2022 10:59:33 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 10:59:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 10:59:38 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 10:59:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 10:59:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 10:59:47 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6497300592128179 on epoch=624
05/23/2022 10:59:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 10:59:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
05/23/2022 10:59:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/23/2022 10:59:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 10:59:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 11:00:02 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.6305367867867868 on epoch=637
05/23/2022 11:00:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 11:00:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=642
05/23/2022 11:00:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
05/23/2022 11:00:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 11:00:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 11:00:18 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6364283638477186 on epoch=649
05/23/2022 11:00:20 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 11:00:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 11:00:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/23/2022 11:00:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/23/2022 11:00:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=662
05/23/2022 11:00:33 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6360310798548094 on epoch=662
05/23/2022 11:00:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 11:00:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 11:00:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 11:00:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 11:00:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=674
05/23/2022 11:00:50 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6520609318996415 on epoch=674
05/23/2022 11:00:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/23/2022 11:00:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=679
05/23/2022 11:00:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 11:00:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 11:01:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 11:01:05 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6125718390804598 on epoch=687
05/23/2022 11:01:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 11:01:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 11:01:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 11:01:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
05/23/2022 11:01:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 11:01:21 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.5856093833067517 on epoch=699
05/23/2022 11:01:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 11:01:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 11:01:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 11:01:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 11:01:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 11:01:38 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.6742295016478579 on epoch=712
05/23/2022 11:01:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 11:01:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 11:01:45 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 11:01:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 11:01:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 11:01:53 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6147342995169083 on epoch=724
05/23/2022 11:01:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/23/2022 11:01:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 11:02:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 11:02:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/23/2022 11:02:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=737
05/23/2022 11:02:09 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6622596153846153 on epoch=737
05/23/2022 11:02:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 11:02:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 11:02:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/23/2022 11:02:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 11:02:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 11:02:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:02:22 - INFO - __main__ - Printing 3 examples
05/23/2022 11:02:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:02:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:02:22 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:02:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:02:22 - INFO - __main__ - Printing 3 examples
05/23/2022 11:02:22 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 11:02:22 - INFO - __main__ - ['happy']
05/23/2022 11:02:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:02:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:02:23 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:02:25 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5946236559139786 on epoch=749
05/23/2022 11:02:25 - INFO - __main__ - save last model!
05/23/2022 11:02:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:02:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:02:25 - INFO - __main__ - Printing 3 examples
05/23/2022 11:02:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:02:25 - INFO - __main__ - ['others']
05/23/2022 11:02:25 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:02:25 - INFO - __main__ - ['others']
05/23/2022 11:02:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:02:25 - INFO - __main__ - ['others']
05/23/2022 11:02:25 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:02:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:02:33 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:02:38 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:02:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:02:39 - INFO - __main__ - Starting training!
05/23/2022 11:07:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/23/2022 11:07:30 - INFO - __main__ - Classification-F1 on test data: 0.1919
05/23/2022 11:07:31 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.7034944581280789, test_performance=0.19191198874383747
05/23/2022 11:07:31 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/23/2022 11:07:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:07:32 - INFO - __main__ - Printing 3 examples
05/23/2022 11:07:32 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:07:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:07:32 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:07:32 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:07:32 - INFO - __main__ - Printing 3 examples
05/23/2022 11:07:32 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/23/2022 11:07:32 - INFO - __main__ - ['happy']
05/23/2022 11:07:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:07:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:07:32 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:07:51 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:07:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:07:51 - INFO - __main__ - Starting training!
05/23/2022 11:07:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.40 on epoch=2
05/23/2022 11:07:56 - INFO - __main__ - Step 20 Global step 20 Train loss 2.51 on epoch=4
05/23/2022 11:07:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.92 on epoch=7
05/23/2022 11:08:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.63 on epoch=9
05/23/2022 11:08:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.33 on epoch=12
05/23/2022 11:08:05 - INFO - __main__ - Global step 50 Train loss 2.16 Classification-F1 0.3301175343018563 on epoch=12
05/23/2022 11:08:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3301175343018563 on epoch=12, global_step=50
05/23/2022 11:08:07 - INFO - __main__ - Step 60 Global step 60 Train loss 1.00 on epoch=14
05/23/2022 11:08:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.05 on epoch=17
05/23/2022 11:08:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=19
05/23/2022 11:08:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=22
05/23/2022 11:08:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.76 on epoch=24
05/23/2022 11:08:18 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.4586619054704161 on epoch=24
05/23/2022 11:08:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3301175343018563 -> 0.4586619054704161 on epoch=24, global_step=100
05/23/2022 11:08:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=27
05/23/2022 11:08:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.86 on epoch=29
05/23/2022 11:08:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=32
05/23/2022 11:08:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=34
05/23/2022 11:08:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.76 on epoch=37
05/23/2022 11:08:31 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.5294964684438368 on epoch=37
05/23/2022 11:08:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4586619054704161 -> 0.5294964684438368 on epoch=37, global_step=150
05/23/2022 11:08:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=39
05/23/2022 11:08:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=42
05/23/2022 11:08:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=44
05/23/2022 11:08:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=47
05/23/2022 11:08:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.73 on epoch=49
05/23/2022 11:08:44 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.6061976114215619 on epoch=49
05/23/2022 11:08:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5294964684438368 -> 0.6061976114215619 on epoch=49, global_step=200
05/23/2022 11:08:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.66 on epoch=52
05/23/2022 11:08:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=54
05/23/2022 11:08:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=57
05/23/2022 11:08:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=59
05/23/2022 11:08:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=62
05/23/2022 11:08:57 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5346206490560294 on epoch=62
05/23/2022 11:09:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.78 on epoch=64
05/23/2022 11:09:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=67
05/23/2022 11:09:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=69
05/23/2022 11:09:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=72
05/23/2022 11:09:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=74
05/23/2022 11:09:10 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.6063414254590725 on epoch=74
05/23/2022 11:09:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6061976114215619 -> 0.6063414254590725 on epoch=74, global_step=300
05/23/2022 11:09:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=77
05/23/2022 11:09:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.65 on epoch=79
05/23/2022 11:09:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=82
05/23/2022 11:09:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=84
05/23/2022 11:09:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=87
05/23/2022 11:09:23 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.5809497653051033 on epoch=87
05/23/2022 11:09:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=89
05/23/2022 11:09:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=92
05/23/2022 11:09:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=94
05/23/2022 11:09:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
05/23/2022 11:09:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=99
05/23/2022 11:09:36 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.5876840508036417 on epoch=99
05/23/2022 11:09:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=102
05/23/2022 11:09:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=104
05/23/2022 11:09:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=107
05/23/2022 11:09:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=109
05/23/2022 11:09:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=112
05/23/2022 11:09:50 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.6008563074352549 on epoch=112
05/23/2022 11:09:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=114
05/23/2022 11:09:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=117
05/23/2022 11:09:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=119
05/23/2022 11:10:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=122
05/23/2022 11:10:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=124
05/23/2022 11:10:03 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5862903225806452 on epoch=124
05/23/2022 11:10:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=127
05/23/2022 11:10:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=129
05/23/2022 11:10:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=132
05/23/2022 11:10:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=134
05/23/2022 11:10:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=137
05/23/2022 11:10:16 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.6150131497957585 on epoch=137
05/23/2022 11:10:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6063414254590725 -> 0.6150131497957585 on epoch=137, global_step=550
05/23/2022 11:10:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=139
05/23/2022 11:10:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.31 on epoch=142
05/23/2022 11:10:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=144
05/23/2022 11:10:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=147
05/23/2022 11:10:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=149
05/23/2022 11:10:29 - INFO - __main__ - Global step 600 Train loss 0.27 Classification-F1 0.6232142857142857 on epoch=149
05/23/2022 11:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6150131497957585 -> 0.6232142857142857 on epoch=149, global_step=600
05/23/2022 11:10:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=152
05/23/2022 11:10:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=154
05/23/2022 11:10:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=157
05/23/2022 11:10:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=159
05/23/2022 11:10:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
05/23/2022 11:10:42 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.6273477812177503 on epoch=162
05/23/2022 11:10:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6232142857142857 -> 0.6273477812177503 on epoch=162, global_step=650
05/23/2022 11:10:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=164
05/23/2022 11:10:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.17 on epoch=167
05/23/2022 11:10:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=169
05/23/2022 11:10:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=172
05/23/2022 11:10:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=174
05/23/2022 11:10:55 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.6026598026598027 on epoch=174
05/23/2022 11:10:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=177
05/23/2022 11:11:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
05/23/2022 11:11:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/23/2022 11:11:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=184
05/23/2022 11:11:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=187
05/23/2022 11:11:09 - INFO - __main__ - Global step 750 Train loss 0.16 Classification-F1 0.6662027956145603 on epoch=187
05/23/2022 11:11:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6273477812177503 -> 0.6662027956145603 on epoch=187, global_step=750
05/23/2022 11:11:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.11 on epoch=189
05/23/2022 11:11:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=192
05/23/2022 11:11:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=194
05/23/2022 11:11:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=197
05/23/2022 11:11:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.11 on epoch=199
05/23/2022 11:11:22 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.5746414948475504 on epoch=199
05/23/2022 11:11:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
05/23/2022 11:11:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=204
05/23/2022 11:11:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.12 on epoch=207
05/23/2022 11:11:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=209
05/23/2022 11:11:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=212
05/23/2022 11:11:36 - INFO - __main__ - Global step 850 Train loss 0.13 Classification-F1 0.6041303908950968 on epoch=212
05/23/2022 11:11:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=214
05/23/2022 11:11:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=217
05/23/2022 11:11:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
05/23/2022 11:11:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/23/2022 11:11:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=224
05/23/2022 11:11:50 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.6029591659048366 on epoch=224
05/23/2022 11:11:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
05/23/2022 11:11:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.05 on epoch=229
05/23/2022 11:11:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=232
05/23/2022 11:12:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=234
05/23/2022 11:12:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=237
05/23/2022 11:12:04 - INFO - __main__ - Global step 950 Train loss 0.10 Classification-F1 0.571956752689124 on epoch=237
05/23/2022 11:12:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.12 on epoch=239
05/23/2022 11:12:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=242
05/23/2022 11:12:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.07 on epoch=244
05/23/2022 11:12:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=247
05/23/2022 11:12:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
05/23/2022 11:12:17 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.6508534252258897 on epoch=249
05/23/2022 11:12:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
05/23/2022 11:12:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/23/2022 11:12:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=257
05/23/2022 11:12:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=259
05/23/2022 11:12:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=262
05/23/2022 11:12:32 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.628958503958504 on epoch=262
05/23/2022 11:12:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=264
05/23/2022 11:12:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=267
05/23/2022 11:12:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=269
05/23/2022 11:12:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/23/2022 11:12:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=274
05/23/2022 11:12:47 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.6810523460476348 on epoch=274
05/23/2022 11:12:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6662027956145603 -> 0.6810523460476348 on epoch=274, global_step=1100
05/23/2022 11:12:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=277
05/23/2022 11:12:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/23/2022 11:12:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/23/2022 11:12:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=284
05/23/2022 11:12:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=287
05/23/2022 11:13:02 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6003514956604201 on epoch=287
05/23/2022 11:13:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.09 on epoch=289
05/23/2022 11:13:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=292
05/23/2022 11:13:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/23/2022 11:13:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=297
05/23/2022 11:13:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 11:13:15 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6232142857142857 on epoch=299
05/23/2022 11:13:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/23/2022 11:13:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=304
05/23/2022 11:13:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=307
05/23/2022 11:13:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=309
05/23/2022 11:13:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=312
05/23/2022 11:13:29 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.5415607616602482 on epoch=312
05/23/2022 11:13:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/23/2022 11:13:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=317
05/23/2022 11:13:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=319
05/23/2022 11:13:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/23/2022 11:13:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/23/2022 11:13:44 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.6472027972027972 on epoch=324
05/23/2022 11:13:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=327
05/23/2022 11:13:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/23/2022 11:13:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
05/23/2022 11:13:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=334
05/23/2022 11:13:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=337
05/23/2022 11:13:59 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5948461091753775 on epoch=337
05/23/2022 11:14:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/23/2022 11:14:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/23/2022 11:14:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=344
05/23/2022 11:14:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=347
05/23/2022 11:14:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 11:14:14 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.632051282051282 on epoch=349
05/23/2022 11:14:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/23/2022 11:14:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/23/2022 11:14:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=357
05/23/2022 11:14:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/23/2022 11:14:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/23/2022 11:14:29 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.6097751710654936 on epoch=362
05/23/2022 11:14:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/23/2022 11:14:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=367
05/23/2022 11:14:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 11:14:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/23/2022 11:14:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=374
05/23/2022 11:14:44 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.6589644457291516 on epoch=374
05/23/2022 11:14:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/23/2022 11:14:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 11:14:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/23/2022 11:14:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/23/2022 11:14:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 11:14:59 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6224399058562472 on epoch=387
05/23/2022 11:15:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/23/2022 11:15:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
05/23/2022 11:15:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/23/2022 11:15:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 11:15:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
05/23/2022 11:15:14 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6261685072168943 on epoch=399
05/23/2022 11:15:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 11:15:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/23/2022 11:15:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/23/2022 11:15:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/23/2022 11:15:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=412
05/23/2022 11:15:30 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6469706368899917 on epoch=412
05/23/2022 11:15:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 11:15:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
05/23/2022 11:15:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/23/2022 11:15:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/23/2022 11:15:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 11:15:45 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6154344193817878 on epoch=424
05/23/2022 11:15:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/23/2022 11:15:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 11:15:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=432
05/23/2022 11:15:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 11:15:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
05/23/2022 11:16:00 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.631038221740309 on epoch=437
05/23/2022 11:16:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/23/2022 11:16:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/23/2022 11:16:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 11:16:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=447
05/23/2022 11:16:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/23/2022 11:16:15 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6020281895962476 on epoch=449
05/23/2022 11:16:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/23/2022 11:16:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 11:16:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/23/2022 11:16:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/23/2022 11:16:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=462
05/23/2022 11:16:30 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6324613716454324 on epoch=462
05/23/2022 11:16:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=464
05/23/2022 11:16:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 11:16:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/23/2022 11:16:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/23/2022 11:16:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
05/23/2022 11:16:45 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6322646103896103 on epoch=474
05/23/2022 11:16:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 11:16:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/23/2022 11:16:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/23/2022 11:16:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 11:16:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/23/2022 11:17:00 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6150793650793651 on epoch=487
05/23/2022 11:17:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/23/2022 11:17:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/23/2022 11:17:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/23/2022 11:17:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/23/2022 11:17:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
05/23/2022 11:17:15 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.6482419199153071 on epoch=499
05/23/2022 11:17:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=502
05/23/2022 11:17:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 11:17:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 11:17:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 11:17:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/23/2022 11:17:30 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.614696223316913 on epoch=512
05/23/2022 11:17:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 11:17:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 11:17:37 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/23/2022 11:17:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/23/2022 11:17:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/23/2022 11:17:44 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6006488499816139 on epoch=524
05/23/2022 11:17:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 11:17:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 11:17:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/23/2022 11:17:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/23/2022 11:17:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 11:17:59 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6006488499816139 on epoch=537
05/23/2022 11:18:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
05/23/2022 11:18:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 11:18:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/23/2022 11:18:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=547
05/23/2022 11:18:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 11:18:13 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6299277299277299 on epoch=549
05/23/2022 11:18:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/23/2022 11:18:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/23/2022 11:18:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 11:18:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 11:18:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/23/2022 11:18:28 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.5744851994851994 on epoch=562
05/23/2022 11:18:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 11:18:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/23/2022 11:18:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/23/2022 11:18:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=572
05/23/2022 11:18:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 11:18:42 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.5987037037037037 on epoch=574
05/23/2022 11:18:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/23/2022 11:18:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=579
05/23/2022 11:18:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/23/2022 11:18:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 11:18:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 11:18:57 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6227695675971539 on epoch=587
05/23/2022 11:18:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/23/2022 11:19:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/23/2022 11:19:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/23/2022 11:19:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/23/2022 11:19:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 11:19:11 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6008240297713981 on epoch=599
05/23/2022 11:19:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 11:19:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
05/23/2022 11:19:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/23/2022 11:19:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 11:19:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 11:19:26 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6466523007270036 on epoch=612
05/23/2022 11:19:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/23/2022 11:19:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/23/2022 11:19:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 11:19:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
05/23/2022 11:19:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 11:19:40 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.587666135034556 on epoch=624
05/23/2022 11:19:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/23/2022 11:19:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
05/23/2022 11:19:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/23/2022 11:19:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=634
05/23/2022 11:19:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=637
05/23/2022 11:19:54 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.5812324929971988 on epoch=637
05/23/2022 11:19:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/23/2022 11:19:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
05/23/2022 11:20:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 11:20:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 11:20:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 11:20:09 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5488811459543167 on epoch=649
05/23/2022 11:20:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=652
05/23/2022 11:20:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/23/2022 11:20:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/23/2022 11:20:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/23/2022 11:20:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 11:20:23 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6363143631436314 on epoch=662
05/23/2022 11:20:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=664
05/23/2022 11:20:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/23/2022 11:20:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/23/2022 11:20:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 11:20:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/23/2022 11:20:37 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6319444444444444 on epoch=674
05/23/2022 11:20:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/23/2022 11:20:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/23/2022 11:20:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/23/2022 11:20:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=684
05/23/2022 11:20:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 11:20:51 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6210454296661193 on epoch=687
05/23/2022 11:20:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/23/2022 11:20:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/23/2022 11:20:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/23/2022 11:21:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/23/2022 11:21:03 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/23/2022 11:21:06 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6028970146617205 on epoch=699
05/23/2022 11:21:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/23/2022 11:21:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/23/2022 11:21:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/23/2022 11:21:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 11:21:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 11:21:19 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.633826164874552 on epoch=712
05/23/2022 11:21:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/23/2022 11:21:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 11:21:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/23/2022 11:21:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/23/2022 11:21:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/23/2022 11:21:33 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.6525810755336617 on epoch=724
05/23/2022 11:21:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/23/2022 11:21:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/23/2022 11:21:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/23/2022 11:21:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/23/2022 11:21:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 11:21:47 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6340245775729647 on epoch=737
05/23/2022 11:21:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/23/2022 11:21:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/23/2022 11:21:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 11:21:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/23/2022 11:21:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
05/23/2022 11:22:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:22:01 - INFO - __main__ - Printing 3 examples
05/23/2022 11:22:01 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:22:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:22:01 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:22:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:22:01 - INFO - __main__ - Printing 3 examples
05/23/2022 11:22:01 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:22:01 - INFO - __main__ - ['others']
05/23/2022 11:22:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:22:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:22:01 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:22:02 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6183608058608059 on epoch=749
05/23/2022 11:22:02 - INFO - __main__ - save last model!
05/23/2022 11:22:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:22:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:22:02 - INFO - __main__ - Printing 3 examples
05/23/2022 11:22:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:22:02 - INFO - __main__ - ['others']
05/23/2022 11:22:02 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:22:02 - INFO - __main__ - ['others']
05/23/2022 11:22:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:22:02 - INFO - __main__ - ['others']
05/23/2022 11:22:02 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:22:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:22:09 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:22:16 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:22:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:22:17 - INFO - __main__ - Starting training!
05/23/2022 11:25:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/23/2022 11:25:02 - INFO - __main__ - Classification-F1 on test data: 0.1770
05/23/2022 11:25:03 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.6810523460476348, test_performance=0.1770327063155884
05/23/2022 11:25:03 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/23/2022 11:25:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:25:03 - INFO - __main__ - Printing 3 examples
05/23/2022 11:25:03 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:25:03 - INFO - __main__ - ['others']
05/23/2022 11:25:03 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:25:03 - INFO - __main__ - ['others']
05/23/2022 11:25:03 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:25:03 - INFO - __main__ - ['others']
05/23/2022 11:25:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:25:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:25:04 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:25:04 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:25:04 - INFO - __main__ - Printing 3 examples
05/23/2022 11:25:04 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:25:04 - INFO - __main__ - ['others']
05/23/2022 11:25:04 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:25:04 - INFO - __main__ - ['others']
05/23/2022 11:25:04 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:25:04 - INFO - __main__ - ['others']
05/23/2022 11:25:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:25:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:25:04 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:25:22 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:25:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:25:23 - INFO - __main__ - Starting training!
05/23/2022 11:25:26 - INFO - __main__ - Step 10 Global step 10 Train loss 2.82 on epoch=2
05/23/2022 11:25:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.52 on epoch=4
05/23/2022 11:25:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.19 on epoch=7
05/23/2022 11:25:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=9
05/23/2022 11:25:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=12
05/23/2022 11:25:36 - INFO - __main__ - Global step 50 Train loss 1.47 Classification-F1 0.28244033507191396 on epoch=12
05/23/2022 11:25:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.28244033507191396 on epoch=12, global_step=50
05/23/2022 11:25:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.85 on epoch=14
05/23/2022 11:25:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=17
05/23/2022 11:25:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.77 on epoch=19
05/23/2022 11:25:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.69 on epoch=22
05/23/2022 11:25:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=24
05/23/2022 11:25:49 - INFO - __main__ - Global step 100 Train loss 0.78 Classification-F1 0.404593837535014 on epoch=24
05/23/2022 11:25:49 - INFO - __main__ - Saving model with best Classification-F1: 0.28244033507191396 -> 0.404593837535014 on epoch=24, global_step=100
05/23/2022 11:25:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.70 on epoch=27
05/23/2022 11:25:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.66 on epoch=29
05/23/2022 11:25:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=32
05/23/2022 11:25:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.64 on epoch=34
05/23/2022 11:26:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=37
05/23/2022 11:26:02 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.643560606060606 on epoch=37
05/23/2022 11:26:03 - INFO - __main__ - Saving model with best Classification-F1: 0.404593837535014 -> 0.643560606060606 on epoch=37, global_step=150
05/23/2022 11:26:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.60 on epoch=39
05/23/2022 11:26:07 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=42
05/23/2022 11:26:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=44
05/23/2022 11:26:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=47
05/23/2022 11:26:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=49
05/23/2022 11:26:15 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.579040404040404 on epoch=49
05/23/2022 11:26:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=52
05/23/2022 11:26:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=54
05/23/2022 11:26:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=57
05/23/2022 11:26:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=59
05/23/2022 11:26:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=62
05/23/2022 11:26:28 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.6489028213166145 on epoch=62
05/23/2022 11:26:28 - INFO - __main__ - Saving model with best Classification-F1: 0.643560606060606 -> 0.6489028213166145 on epoch=62, global_step=250
05/23/2022 11:26:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=64
05/23/2022 11:26:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=67
05/23/2022 11:26:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=69
05/23/2022 11:26:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=72
05/23/2022 11:26:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=74
05/23/2022 11:26:41 - INFO - __main__ - Global step 300 Train loss 0.33 Classification-F1 0.6459013209013209 on epoch=74
05/23/2022 11:26:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.36 on epoch=77
05/23/2022 11:26:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=79
05/23/2022 11:26:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=82
05/23/2022 11:26:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=84
05/23/2022 11:26:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=87
05/23/2022 11:26:54 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.6856803327391564 on epoch=87
05/23/2022 11:26:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6489028213166145 -> 0.6856803327391564 on epoch=87, global_step=350
05/23/2022 11:26:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.20 on epoch=89
05/23/2022 11:26:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=92
05/23/2022 11:27:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=94
05/23/2022 11:27:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=97
05/23/2022 11:27:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=99
05/23/2022 11:27:07 - INFO - __main__ - Global step 400 Train loss 0.18 Classification-F1 0.6705389600126442 on epoch=99
05/23/2022 11:27:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=102
05/23/2022 11:27:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=104
05/23/2022 11:27:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=107
05/23/2022 11:27:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.13 on epoch=109
05/23/2022 11:27:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=112
05/23/2022 11:27:20 - INFO - __main__ - Global step 450 Train loss 0.15 Classification-F1 0.6587615576102418 on epoch=112
05/23/2022 11:27:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.14 on epoch=114
05/23/2022 11:27:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=117
05/23/2022 11:27:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.10 on epoch=119
05/23/2022 11:27:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=122
05/23/2022 11:27:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=124
05/23/2022 11:27:33 - INFO - __main__ - Global step 500 Train loss 0.11 Classification-F1 0.6363636363636364 on epoch=124
05/23/2022 11:27:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=127
05/23/2022 11:27:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.07 on epoch=129
05/23/2022 11:27:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.07 on epoch=132
05/23/2022 11:27:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.06 on epoch=134
05/23/2022 11:27:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.16 on epoch=137
05/23/2022 11:27:47 - INFO - __main__ - Global step 550 Train loss 0.09 Classification-F1 0.6478471978471978 on epoch=137
05/23/2022 11:27:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.09 on epoch=139
05/23/2022 11:27:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
05/23/2022 11:27:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=144
05/23/2022 11:27:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=147
05/23/2022 11:27:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=149
05/23/2022 11:28:00 - INFO - __main__ - Global step 600 Train loss 0.09 Classification-F1 0.6480676328502415 on epoch=149
05/23/2022 11:28:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.12 on epoch=152
05/23/2022 11:28:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=154
05/23/2022 11:28:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=157
05/23/2022 11:28:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=159
05/23/2022 11:28:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=162
05/23/2022 11:28:13 - INFO - __main__ - Global step 650 Train loss 0.08 Classification-F1 0.6610715382641148 on epoch=162
05/23/2022 11:28:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.06 on epoch=164
05/23/2022 11:28:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=167
05/23/2022 11:28:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=169
05/23/2022 11:28:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=172
05/23/2022 11:28:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=174
05/23/2022 11:28:27 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.6561378107430739 on epoch=174
05/23/2022 11:28:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=177
05/23/2022 11:28:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=179
05/23/2022 11:28:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=182
05/23/2022 11:28:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.06 on epoch=184
05/23/2022 11:28:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=187
05/23/2022 11:28:41 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.6722947191697192 on epoch=187
05/23/2022 11:28:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=189
05/23/2022 11:28:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=192
05/23/2022 11:28:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
05/23/2022 11:28:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=197
05/23/2022 11:28:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=199
05/23/2022 11:28:54 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.6577380952380952 on epoch=199
05/23/2022 11:28:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=202
05/23/2022 11:28:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
05/23/2022 11:29:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=207
05/23/2022 11:29:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=209
05/23/2022 11:29:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=212
05/23/2022 11:29:07 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.6567874692874692 on epoch=212
05/23/2022 11:29:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=214
05/23/2022 11:29:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/23/2022 11:29:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=219
05/23/2022 11:29:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/23/2022 11:29:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/23/2022 11:29:21 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6578571935108438 on epoch=224
05/23/2022 11:29:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=227
05/23/2022 11:29:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=229
05/23/2022 11:29:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/23/2022 11:29:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=234
05/23/2022 11:29:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.06 on epoch=237
05/23/2022 11:29:35 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.642918792918793 on epoch=237
05/23/2022 11:29:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=239
05/23/2022 11:29:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=242
05/23/2022 11:29:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=244
05/23/2022 11:29:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=247
05/23/2022 11:29:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.06 on epoch=249
05/23/2022 11:29:50 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.6316251316251316 on epoch=249
05/23/2022 11:29:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=252
05/23/2022 11:29:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=254
05/23/2022 11:29:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/23/2022 11:30:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=259
05/23/2022 11:30:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/23/2022 11:30:05 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.6557112478165109 on epoch=262
05/23/2022 11:30:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=264
05/23/2022 11:30:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=267
05/23/2022 11:30:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 11:30:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
05/23/2022 11:30:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=274
05/23/2022 11:30:18 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.61670328693355 on epoch=274
05/23/2022 11:30:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=277
05/23/2022 11:30:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 11:30:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=282
05/23/2022 11:30:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=284
05/23/2022 11:30:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=287
05/23/2022 11:30:32 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.6155772495755517 on epoch=287
05/23/2022 11:30:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=289
05/23/2022 11:30:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=292
05/23/2022 11:30:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 11:30:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=297
05/23/2022 11:30:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/23/2022 11:30:46 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6411624692874692 on epoch=299
05/23/2022 11:30:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 11:30:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=304
05/23/2022 11:30:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=307
05/23/2022 11:30:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=309
05/23/2022 11:30:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=312
05/23/2022 11:31:00 - INFO - __main__ - Global step 1250 Train loss 0.03 Classification-F1 0.6294419189156031 on epoch=312
05/23/2022 11:31:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/23/2022 11:31:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/23/2022 11:31:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/23/2022 11:31:10 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/23/2022 11:31:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/23/2022 11:31:14 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.6410785027083838 on epoch=324
05/23/2022 11:31:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/23/2022 11:31:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/23/2022 11:31:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 11:31:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/23/2022 11:31:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=337
05/23/2022 11:31:28 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.679703023181284 on epoch=337
05/23/2022 11:31:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/23/2022 11:31:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 11:31:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/23/2022 11:31:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/23/2022 11:31:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 11:31:41 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6654287010122251 on epoch=349
05/23/2022 11:31:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=352
05/23/2022 11:31:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=354
05/23/2022 11:31:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=357
05/23/2022 11:31:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/23/2022 11:31:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=362
05/23/2022 11:31:55 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.6199380565234224 on epoch=362
05/23/2022 11:31:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=364
05/23/2022 11:32:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=367
05/23/2022 11:32:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 11:32:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/23/2022 11:32:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/23/2022 11:32:09 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.6199380565234224 on epoch=374
05/23/2022 11:32:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=377
05/23/2022 11:32:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=379
05/23/2022 11:32:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=382
05/23/2022 11:32:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 11:32:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/23/2022 11:32:24 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6659606094388704 on epoch=387
05/23/2022 11:32:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=389
05/23/2022 11:32:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=392
05/23/2022 11:32:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=394
05/23/2022 11:32:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=397
05/23/2022 11:32:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/23/2022 11:32:38 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6182923707117255 on epoch=399
05/23/2022 11:32:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/23/2022 11:32:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/23/2022 11:32:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=407
05/23/2022 11:32:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/23/2022 11:32:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 11:32:52 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.61670328693355 on epoch=412
05/23/2022 11:32:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=414
05/23/2022 11:32:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=417
05/23/2022 11:33:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 11:33:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 11:33:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=424
05/23/2022 11:33:06 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6654287010122251 on epoch=424
05/23/2022 11:33:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/23/2022 11:33:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/23/2022 11:33:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/23/2022 11:33:15 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 11:33:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=437
05/23/2022 11:33:19 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.642918792918793 on epoch=437
05/23/2022 11:33:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/23/2022 11:33:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=442
05/23/2022 11:33:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 11:33:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=447
05/23/2022 11:33:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 11:33:32 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6422600696794245 on epoch=449
05/23/2022 11:33:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=452
05/23/2022 11:33:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 11:33:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=457
05/23/2022 11:33:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/23/2022 11:33:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=462
05/23/2022 11:33:45 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6413338187531735 on epoch=462
05/23/2022 11:33:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/23/2022 11:33:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/23/2022 11:33:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/23/2022 11:33:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 11:33:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 11:33:59 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6563609063609063 on epoch=474
05/23/2022 11:34:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 11:34:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 11:34:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 11:34:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/23/2022 11:34:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=487
05/23/2022 11:34:12 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6414390616693249 on epoch=487
05/23/2022 11:34:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 11:34:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 11:34:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=494
05/23/2022 11:34:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 11:34:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 11:34:26 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6414390616693249 on epoch=499
05/23/2022 11:34:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.00 on epoch=502
05/23/2022 11:34:31 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
05/23/2022 11:34:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/23/2022 11:34:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/23/2022 11:34:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/23/2022 11:34:40 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.6405128107430739 on epoch=512
05/23/2022 11:34:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/23/2022 11:34:45 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/23/2022 11:34:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 11:34:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=522
05/23/2022 11:34:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 11:34:54 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.61670328693355 on epoch=524
05/23/2022 11:34:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/23/2022 11:34:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/23/2022 11:35:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/23/2022 11:35:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 11:35:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 11:35:08 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6417281612527792 on epoch=537
05/23/2022 11:35:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/23/2022 11:35:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 11:35:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 11:35:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 11:35:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 11:35:22 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6417281612527792 on epoch=549
05/23/2022 11:35:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/23/2022 11:35:27 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 11:35:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 11:35:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 11:35:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 11:35:38 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6341291061879297 on epoch=562
05/23/2022 11:35:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/23/2022 11:35:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 11:35:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 11:35:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/23/2022 11:35:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/23/2022 11:35:53 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6414390616693249 on epoch=574
05/23/2022 11:35:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 11:35:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/23/2022 11:36:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 11:36:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 11:36:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 11:36:08 - INFO - __main__ - Global step 2350 Train loss 0.00 Classification-F1 0.6709744503862151 on epoch=587
05/23/2022 11:36:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 11:36:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 11:36:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/23/2022 11:36:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 11:36:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/23/2022 11:36:24 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6563609063609063 on epoch=599
05/23/2022 11:36:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=602
05/23/2022 11:36:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 11:36:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 11:36:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/23/2022 11:36:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 11:36:39 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.6642042858075465 on epoch=612
05/23/2022 11:36:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/23/2022 11:36:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 11:36:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 11:36:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/23/2022 11:36:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 11:36:53 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.6513307662501211 on epoch=624
05/23/2022 11:36:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 11:36:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/23/2022 11:37:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 11:37:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 11:37:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 11:37:08 - INFO - __main__ - Global step 2550 Train loss 0.00 Classification-F1 0.6417281612527792 on epoch=637
05/23/2022 11:37:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 11:37:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 11:37:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 11:37:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/23/2022 11:37:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 11:37:24 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6161170606416786 on epoch=649
05/23/2022 11:37:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/23/2022 11:37:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/23/2022 11:37:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 11:37:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 11:37:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 11:37:39 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6566697191697192 on epoch=662
05/23/2022 11:37:42 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/23/2022 11:37:44 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/23/2022 11:37:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/23/2022 11:37:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 11:37:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 11:37:54 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6175242949436497 on epoch=674
05/23/2022 11:37:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 11:37:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/23/2022 11:38:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 11:38:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 11:38:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 11:38:09 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6563609063609063 on epoch=687
05/23/2022 11:38:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 11:38:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 11:38:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 11:38:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 11:38:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 11:38:24 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6417281612527792 on epoch=699
05/23/2022 11:38:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 11:38:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 11:38:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 11:38:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 11:38:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 11:38:39 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.61726897889886 on epoch=712
05/23/2022 11:38:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/23/2022 11:38:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 11:38:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 11:38:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 11:38:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 11:38:54 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.6422600696794245 on epoch=724
05/23/2022 11:38:57 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/23/2022 11:38:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/23/2022 11:39:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 11:39:04 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 11:39:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=737
05/23/2022 11:39:09 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.646969696969697 on epoch=737
05/23/2022 11:39:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 11:39:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
05/23/2022 11:39:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/23/2022 11:39:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=747
05/23/2022 11:39:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 11:39:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:39:22 - INFO - __main__ - Printing 3 examples
05/23/2022 11:39:22 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:39:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:39:22 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:39:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:39:22 - INFO - __main__ - Printing 3 examples
05/23/2022 11:39:22 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:39:22 - INFO - __main__ - ['others']
05/23/2022 11:39:22 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:39:22 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:39:23 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:39:23 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6175242949436497 on epoch=749
05/23/2022 11:39:23 - INFO - __main__ - save last model!
05/23/2022 11:39:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:39:23 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:39:23 - INFO - __main__ - Printing 3 examples
05/23/2022 11:39:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:39:23 - INFO - __main__ - ['others']
05/23/2022 11:39:23 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:39:23 - INFO - __main__ - ['others']
05/23/2022 11:39:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:39:23 - INFO - __main__ - ['others']
05/23/2022 11:39:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:39:25 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:39:30 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:39:41 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:39:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:39:42 - INFO - __main__ - Starting training!
05/23/2022 11:41:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/23/2022 11:41:44 - INFO - __main__ - Classification-F1 on test data: 0.2177
05/23/2022 11:41:45 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.6856803327391564, test_performance=0.21767468913634216
05/23/2022 11:41:45 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/23/2022 11:41:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:41:46 - INFO - __main__ - Printing 3 examples
05/23/2022 11:41:46 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:41:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:41:46 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:41:46 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:41:46 - INFO - __main__ - Printing 3 examples
05/23/2022 11:41:46 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:41:46 - INFO - __main__ - ['others']
05/23/2022 11:41:46 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:41:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:41:46 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:42:04 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:42:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:42:05 - INFO - __main__ - Starting training!
05/23/2022 11:42:08 - INFO - __main__ - Step 10 Global step 10 Train loss 2.92 on epoch=2
05/23/2022 11:42:10 - INFO - __main__ - Step 20 Global step 20 Train loss 1.58 on epoch=4
05/23/2022 11:42:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.29 on epoch=7
05/23/2022 11:42:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=9
05/23/2022 11:42:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=12
05/23/2022 11:42:18 - INFO - __main__ - Global step 50 Train loss 1.53 Classification-F1 0.4668037602820212 on epoch=12
05/23/2022 11:42:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.4668037602820212 on epoch=12, global_step=50
05/23/2022 11:42:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=14
05/23/2022 11:42:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=17
05/23/2022 11:42:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.70 on epoch=19
05/23/2022 11:42:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.77 on epoch=22
05/23/2022 11:42:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=24
05/23/2022 11:42:31 - INFO - __main__ - Global step 100 Train loss 0.75 Classification-F1 0.40725940725940724 on epoch=24
05/23/2022 11:42:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=27
05/23/2022 11:42:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.69 on epoch=29
05/23/2022 11:42:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=32
05/23/2022 11:42:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.61 on epoch=34
05/23/2022 11:42:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=37
05/23/2022 11:42:43 - INFO - __main__ - Global step 150 Train loss 0.64 Classification-F1 0.669216397247543 on epoch=37
05/23/2022 11:42:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4668037602820212 -> 0.669216397247543 on epoch=37, global_step=150
05/23/2022 11:42:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=39
05/23/2022 11:42:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=42
05/23/2022 11:42:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=44
05/23/2022 11:42:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=47
05/23/2022 11:42:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=49
05/23/2022 11:42:55 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.4716142270861833 on epoch=49
05/23/2022 11:42:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=52
05/23/2022 11:43:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=54
05/23/2022 11:43:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=57
05/23/2022 11:43:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=59
05/23/2022 11:43:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=62
05/23/2022 11:43:08 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.6904337492572787 on epoch=62
05/23/2022 11:43:08 - INFO - __main__ - Saving model with best Classification-F1: 0.669216397247543 -> 0.6904337492572787 on epoch=62, global_step=250
05/23/2022 11:43:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=64
05/23/2022 11:43:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=67
05/23/2022 11:43:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=69
05/23/2022 11:43:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=72
05/23/2022 11:43:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=74
05/23/2022 11:43:20 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.666771877219285 on epoch=74
05/23/2022 11:43:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=77
05/23/2022 11:43:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=79
05/23/2022 11:43:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=82
05/23/2022 11:43:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=84
05/23/2022 11:43:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=87
05/23/2022 11:43:33 - INFO - __main__ - Global step 350 Train loss 0.33 Classification-F1 0.6323529411764706 on epoch=87
05/23/2022 11:43:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=89
05/23/2022 11:43:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=92
05/23/2022 11:43:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=94
05/23/2022 11:43:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=97
05/23/2022 11:43:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=99
05/23/2022 11:43:45 - INFO - __main__ - Global step 400 Train loss 0.29 Classification-F1 0.6739862257689083 on epoch=99
05/23/2022 11:43:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=102
05/23/2022 11:43:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.16 on epoch=104
05/23/2022 11:43:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=107
05/23/2022 11:43:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=109
05/23/2022 11:43:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=112
05/23/2022 11:43:58 - INFO - __main__ - Global step 450 Train loss 0.20 Classification-F1 0.709705753184014 on epoch=112
05/23/2022 11:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6904337492572787 -> 0.709705753184014 on epoch=112, global_step=450
05/23/2022 11:44:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=114
05/23/2022 11:44:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.11 on epoch=117
05/23/2022 11:44:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=119
05/23/2022 11:44:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=122
05/23/2022 11:44:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.15 on epoch=124
05/23/2022 11:44:10 - INFO - __main__ - Global step 500 Train loss 0.18 Classification-F1 0.67012012012012 on epoch=124
05/23/2022 11:44:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=127
05/23/2022 11:44:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=129
05/23/2022 11:44:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=132
05/23/2022 11:44:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=134
05/23/2022 11:44:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=137
05/23/2022 11:44:23 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.6461722488038277 on epoch=137
05/23/2022 11:44:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.11 on epoch=139
05/23/2022 11:44:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=142
05/23/2022 11:44:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=144
05/23/2022 11:44:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.16 on epoch=147
05/23/2022 11:44:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=149
05/23/2022 11:44:35 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.6783659730722155 on epoch=149
05/23/2022 11:44:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.10 on epoch=152
05/23/2022 11:44:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=154
05/23/2022 11:44:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=157
05/23/2022 11:44:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.13 on epoch=159
05/23/2022 11:44:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=162
05/23/2022 11:44:48 - INFO - __main__ - Global step 650 Train loss 0.11 Classification-F1 0.695133560670645 on epoch=162
05/23/2022 11:44:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.08 on epoch=164
05/23/2022 11:44:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.08 on epoch=167
05/23/2022 11:44:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.07 on epoch=169
05/23/2022 11:44:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.07 on epoch=172
05/23/2022 11:45:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=174
05/23/2022 11:45:01 - INFO - __main__ - Global step 700 Train loss 0.07 Classification-F1 0.7025906120023767 on epoch=174
05/23/2022 11:45:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=177
05/23/2022 11:45:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.15 on epoch=179
05/23/2022 11:45:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=182
05/23/2022 11:45:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=184
05/23/2022 11:45:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=187
05/23/2022 11:45:13 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.6631944444444444 on epoch=187
05/23/2022 11:45:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
05/23/2022 11:45:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=192
05/23/2022 11:45:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.04 on epoch=194
05/23/2022 11:45:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=197
05/23/2022 11:45:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=199
05/23/2022 11:45:26 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.6665701415701416 on epoch=199
05/23/2022 11:45:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=202
05/23/2022 11:45:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.09 on epoch=204
05/23/2022 11:45:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=207
05/23/2022 11:45:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=209
05/23/2022 11:45:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=212
05/23/2022 11:45:39 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.6588829259881892 on epoch=212
05/23/2022 11:45:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=214
05/23/2022 11:45:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=217
05/23/2022 11:45:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=219
05/23/2022 11:45:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=222
05/23/2022 11:45:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=224
05/23/2022 11:45:52 - INFO - __main__ - Global step 900 Train loss 0.04 Classification-F1 0.6689327485380118 on epoch=224
05/23/2022 11:45:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=227
05/23/2022 11:45:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=229
05/23/2022 11:45:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=232
05/23/2022 11:46:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=234
05/23/2022 11:46:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=237
05/23/2022 11:46:04 - INFO - __main__ - Global step 950 Train loss 0.04 Classification-F1 0.6899381868131869 on epoch=237
05/23/2022 11:46:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/23/2022 11:46:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=242
05/23/2022 11:46:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=244
05/23/2022 11:46:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/23/2022 11:46:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=249
05/23/2022 11:46:17 - INFO - __main__ - Global step 1000 Train loss 0.06 Classification-F1 0.737913067324832 on epoch=249
05/23/2022 11:46:17 - INFO - __main__ - Saving model with best Classification-F1: 0.709705753184014 -> 0.737913067324832 on epoch=249, global_step=1000
05/23/2022 11:46:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=252
05/23/2022 11:46:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=254
05/23/2022 11:46:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=257
05/23/2022 11:46:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/23/2022 11:46:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=262
05/23/2022 11:46:30 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.6835888307662501 on epoch=262
05/23/2022 11:46:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/23/2022 11:46:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=267
05/23/2022 11:46:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/23/2022 11:46:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=272
05/23/2022 11:46:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.10 on epoch=274
05/23/2022 11:46:43 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6606788855427693 on epoch=274
05/23/2022 11:46:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=277
05/23/2022 11:46:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=279
05/23/2022 11:46:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=282
05/23/2022 11:46:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=284
05/23/2022 11:46:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=287
05/23/2022 11:46:56 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.676002376002376 on epoch=287
05/23/2022 11:46:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=289
05/23/2022 11:47:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=292
05/23/2022 11:47:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
05/23/2022 11:47:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=297
05/23/2022 11:47:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=299
05/23/2022 11:47:09 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.6351262772315404 on epoch=299
05/23/2022 11:47:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=302
05/23/2022 11:47:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=304
05/23/2022 11:47:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/23/2022 11:47:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=309
05/23/2022 11:47:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 11:47:23 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.6739862257689083 on epoch=312
05/23/2022 11:47:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/23/2022 11:47:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=317
05/23/2022 11:47:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/23/2022 11:47:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=322
05/23/2022 11:47:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=324
05/23/2022 11:47:37 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.7116561784897024 on epoch=324
05/23/2022 11:47:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/23/2022 11:47:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=329
05/23/2022 11:47:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/23/2022 11:47:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=334
05/23/2022 11:47:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/23/2022 11:47:51 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.7390796703296704 on epoch=337
05/23/2022 11:47:51 - INFO - __main__ - Saving model with best Classification-F1: 0.737913067324832 -> 0.7390796703296704 on epoch=337, global_step=1350
05/23/2022 11:47:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/23/2022 11:47:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=342
05/23/2022 11:47:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/23/2022 11:48:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/23/2022 11:48:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=349
05/23/2022 11:48:05 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.6891462703962704 on epoch=349
05/23/2022 11:48:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=352
05/23/2022 11:48:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/23/2022 11:48:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/23/2022 11:48:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=359
05/23/2022 11:48:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/23/2022 11:48:19 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6956583377636009 on epoch=362
05/23/2022 11:48:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=364
05/23/2022 11:48:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=367
05/23/2022 11:48:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=369
05/23/2022 11:48:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=372
05/23/2022 11:48:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/23/2022 11:48:32 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.7261592574092575 on epoch=374
05/23/2022 11:48:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/23/2022 11:48:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/23/2022 11:48:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/23/2022 11:48:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=384
05/23/2022 11:48:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/23/2022 11:48:46 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.7256905539993775 on epoch=387
05/23/2022 11:48:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/23/2022 11:48:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 11:48:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/23/2022 11:48:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/23/2022 11:48:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 11:49:00 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6956583377636009 on epoch=399
05/23/2022 11:49:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/23/2022 11:49:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=404
05/23/2022 11:49:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=407
05/23/2022 11:49:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 11:49:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
05/23/2022 11:49:13 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.7095188595188595 on epoch=412
05/23/2022 11:49:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 11:49:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/23/2022 11:49:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 11:49:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 11:49:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=424
05/23/2022 11:49:26 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6996166887807755 on epoch=424
05/23/2022 11:49:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=427
05/23/2022 11:49:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 11:49:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=432
05/23/2022 11:49:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 11:49:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 11:49:40 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.7045380824652235 on epoch=437
05/23/2022 11:49:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/23/2022 11:49:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/23/2022 11:49:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/23/2022 11:49:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 11:49:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 11:49:54 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6970280422889856 on epoch=449
05/23/2022 11:49:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 11:49:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/23/2022 11:50:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 11:50:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 11:50:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/23/2022 11:50:07 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6829673423423424 on epoch=462
05/23/2022 11:50:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 11:50:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/23/2022 11:50:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/23/2022 11:50:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=472
05/23/2022 11:50:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/23/2022 11:50:21 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.7020833333333334 on epoch=474
05/23/2022 11:50:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=477
05/23/2022 11:50:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=479
05/23/2022 11:50:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/23/2022 11:50:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=484
05/23/2022 11:50:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/23/2022 11:50:34 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7390796703296704 on epoch=487
05/23/2022 11:50:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 11:50:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=492
05/23/2022 11:50:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 11:50:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/23/2022 11:50:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=499
05/23/2022 11:50:47 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.7193055555555555 on epoch=499
05/23/2022 11:50:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/23/2022 11:50:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/23/2022 11:50:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/23/2022 11:50:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/23/2022 11:51:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 11:51:01 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6999031670084301 on epoch=512
05/23/2022 11:51:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
05/23/2022 11:51:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/23/2022 11:51:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/23/2022 11:51:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=522
05/23/2022 11:51:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 11:51:14 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7246584637476478 on epoch=524
05/23/2022 11:51:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 11:51:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 11:51:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=532
05/23/2022 11:51:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/23/2022 11:51:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/23/2022 11:51:28 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7032837301587301 on epoch=537
05/23/2022 11:51:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 11:51:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/23/2022 11:51:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/23/2022 11:51:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/23/2022 11:51:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/23/2022 11:51:41 - INFO - __main__ - Global step 2200 Train loss 0.00 Classification-F1 0.6860795454545454 on epoch=549
05/23/2022 11:51:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
05/23/2022 11:51:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/23/2022 11:51:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 11:51:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/23/2022 11:51:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/23/2022 11:51:54 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7178354978354979 on epoch=562
05/23/2022 11:51:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 11:51:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.00 on epoch=567
05/23/2022 11:52:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/23/2022 11:52:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 11:52:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/23/2022 11:52:08 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6626305413469735 on epoch=574
05/23/2022 11:52:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 11:52:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/23/2022 11:52:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 11:52:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/23/2022 11:52:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/23/2022 11:52:21 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.737913067324832 on epoch=587
05/23/2022 11:52:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/23/2022 11:52:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/23/2022 11:52:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 11:52:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/23/2022 11:52:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/23/2022 11:52:35 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.7177421271538919 on epoch=599
05/23/2022 11:52:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/23/2022 11:52:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/23/2022 11:52:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/23/2022 11:52:45 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 11:52:47 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/23/2022 11:52:48 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.738333235392059 on epoch=612
05/23/2022 11:52:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/23/2022 11:52:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/23/2022 11:52:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/23/2022 11:52:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/23/2022 11:53:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/23/2022 11:53:02 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.738333235392059 on epoch=624
05/23/2022 11:53:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/23/2022 11:53:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=629
05/23/2022 11:53:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/23/2022 11:53:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/23/2022 11:53:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 11:53:15 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7185591206179442 on epoch=637
05/23/2022 11:53:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 11:53:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 11:53:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 11:53:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/23/2022 11:53:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 11:53:28 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7524480095068331 on epoch=649
05/23/2022 11:53:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7390796703296704 -> 0.7524480095068331 on epoch=649, global_step=2600
05/23/2022 11:53:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 11:53:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
05/23/2022 11:53:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 11:53:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/23/2022 11:53:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/23/2022 11:53:42 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.688399835458659 on epoch=662
05/23/2022 11:53:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 11:53:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/23/2022 11:53:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 11:53:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 11:53:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 11:53:55 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.738333235392059 on epoch=674
05/23/2022 11:53:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 11:54:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 11:54:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/23/2022 11:54:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/23/2022 11:54:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 11:54:09 - INFO - __main__ - Global step 2750 Train loss 0.00 Classification-F1 0.6968434343434343 on epoch=687
05/23/2022 11:54:11 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/23/2022 11:54:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/23/2022 11:54:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/23/2022 11:54:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 11:54:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 11:54:23 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.697845907404731 on epoch=699
05/23/2022 11:54:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 11:54:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/23/2022 11:54:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/23/2022 11:54:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 11:54:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/23/2022 11:54:36 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.697845907404731 on epoch=712
05/23/2022 11:54:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=714
05/23/2022 11:54:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/23/2022 11:54:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 11:54:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/23/2022 11:54:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/23/2022 11:54:50 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.738333235392059 on epoch=724
05/23/2022 11:54:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=727
05/23/2022 11:54:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 11:54:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=732
05/23/2022 11:55:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 11:55:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/23/2022 11:55:03 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.6994617224880383 on epoch=737
05/23/2022 11:55:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/23/2022 11:55:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 11:55:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/23/2022 11:55:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 11:55:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 11:55:17 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6778656126482213 on epoch=749
05/23/2022 11:55:17 - INFO - __main__ - save last model!
05/23/2022 11:55:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:55:17 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:55:17 - INFO - __main__ - Printing 3 examples
05/23/2022 11:55:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:55:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:55:17 - INFO - __main__ - Printing 3 examples
05/23/2022 11:55:17 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:55:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:55:17 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:55:17 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:55:17 - INFO - __main__ - Printing 3 examples
05/23/2022 11:55:17 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:55:17 - INFO - __main__ - ['others']
05/23/2022 11:55:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:55:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:55:17 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:55:19 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:55:25 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:55:36 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:55:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:55:36 - INFO - __main__ - Starting training!
05/23/2022 11:56:59 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/23/2022 11:56:59 - INFO - __main__ - Classification-F1 on test data: 0.3001
05/23/2022 11:56:59 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7524480095068331, test_performance=0.3001122498604652
05/23/2022 11:56:59 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/23/2022 11:57:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:57:00 - INFO - __main__ - Printing 3 examples
05/23/2022 11:57:00 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:57:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:57:00 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 11:57:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 11:57:00 - INFO - __main__ - Printing 3 examples
05/23/2022 11:57:00 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 11:57:00 - INFO - __main__ - ['others']
05/23/2022 11:57:00 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:57:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:57:01 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 11:57:16 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 11:57:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 11:57:17 - INFO - __main__ - Starting training!
05/23/2022 11:57:20 - INFO - __main__ - Step 10 Global step 10 Train loss 3.00 on epoch=2
05/23/2022 11:57:22 - INFO - __main__ - Step 20 Global step 20 Train loss 1.90 on epoch=4
05/23/2022 11:57:24 - INFO - __main__ - Step 30 Global step 30 Train loss 1.55 on epoch=7
05/23/2022 11:57:27 - INFO - __main__ - Step 40 Global step 40 Train loss 1.15 on epoch=9
05/23/2022 11:57:29 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=12
05/23/2022 11:57:30 - INFO - __main__ - Global step 50 Train loss 1.72 Classification-F1 0.37767094017094016 on epoch=12
05/23/2022 11:57:30 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.37767094017094016 on epoch=12, global_step=50
05/23/2022 11:57:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=14
05/23/2022 11:57:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=17
05/23/2022 11:57:37 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=19
05/23/2022 11:57:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=22
05/23/2022 11:57:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=24
05/23/2022 11:57:43 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.35935614210001576 on epoch=24
05/23/2022 11:57:45 - INFO - __main__ - Step 110 Global step 110 Train loss 0.77 on epoch=27
05/23/2022 11:57:48 - INFO - __main__ - Step 120 Global step 120 Train loss 0.78 on epoch=29
05/23/2022 11:57:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.70 on epoch=32
05/23/2022 11:57:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=34
05/23/2022 11:57:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.65 on epoch=37
05/23/2022 11:57:56 - INFO - __main__ - Global step 150 Train loss 0.72 Classification-F1 0.6266708437761069 on epoch=37
05/23/2022 11:57:56 - INFO - __main__ - Saving model with best Classification-F1: 0.37767094017094016 -> 0.6266708437761069 on epoch=37, global_step=150
05/23/2022 11:57:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=39
05/23/2022 11:58:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.61 on epoch=42
05/23/2022 11:58:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=44
05/23/2022 11:58:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=47
05/23/2022 11:58:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=49
05/23/2022 11:58:09 - INFO - __main__ - Global step 200 Train loss 0.61 Classification-F1 0.48886298886298885 on epoch=49
05/23/2022 11:58:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=52
05/23/2022 11:58:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=54
05/23/2022 11:58:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=57
05/23/2022 11:58:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=59
05/23/2022 11:58:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=62
05/23/2022 11:58:22 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.6660309996516893 on epoch=62
05/23/2022 11:58:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6266708437761069 -> 0.6660309996516893 on epoch=62, global_step=250
05/23/2022 11:58:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=64
05/23/2022 11:58:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=67
05/23/2022 11:58:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=69
05/23/2022 11:58:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=72
05/23/2022 11:58:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=74
05/23/2022 11:58:35 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.651316472214088 on epoch=74
05/23/2022 11:58:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=77
05/23/2022 11:58:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.36 on epoch=79
05/23/2022 11:58:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=82
05/23/2022 11:58:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=84
05/23/2022 11:58:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=87
05/23/2022 11:58:48 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.6474761747286775 on epoch=87
05/23/2022 11:58:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=89
05/23/2022 11:58:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=92
05/23/2022 11:58:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=94
05/23/2022 11:58:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=97
05/23/2022 11:59:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.32 on epoch=99
05/23/2022 11:59:01 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.708080808080808 on epoch=99
05/23/2022 11:59:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6660309996516893 -> 0.708080808080808 on epoch=99, global_step=400
05/23/2022 11:59:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=102
05/23/2022 11:59:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=104
05/23/2022 11:59:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=107
05/23/2022 11:59:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.31 on epoch=109
05/23/2022 11:59:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=112
05/23/2022 11:59:14 - INFO - __main__ - Global step 450 Train loss 0.28 Classification-F1 0.6701415701415702 on epoch=112
05/23/2022 11:59:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=114
05/23/2022 11:59:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.29 on epoch=117
05/23/2022 11:59:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=119
05/23/2022 11:59:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=122
05/23/2022 11:59:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=124
05/23/2022 11:59:27 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.7119949494949495 on epoch=124
05/23/2022 11:59:27 - INFO - __main__ - Saving model with best Classification-F1: 0.708080808080808 -> 0.7119949494949495 on epoch=124, global_step=500
05/23/2022 11:59:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=127
05/23/2022 11:59:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=129
05/23/2022 11:59:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=132
05/23/2022 11:59:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=134
05/23/2022 11:59:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=137
05/23/2022 11:59:40 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.6601515151515152 on epoch=137
05/23/2022 11:59:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=139
05/23/2022 11:59:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=142
05/23/2022 11:59:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=144
05/23/2022 11:59:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=147
05/23/2022 11:59:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=149
05/23/2022 11:59:53 - INFO - __main__ - Global step 600 Train loss 0.19 Classification-F1 0.669295705479916 on epoch=149
05/23/2022 11:59:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=152
05/23/2022 11:59:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.15 on epoch=154
05/23/2022 12:00:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=157
05/23/2022 12:00:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=159
05/23/2022 12:00:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=162
05/23/2022 12:00:06 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.7110792858075465 on epoch=162
05/23/2022 12:00:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=164
05/23/2022 12:00:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.13 on epoch=167
05/23/2022 12:00:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=169
05/23/2022 12:00:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.08 on epoch=172
05/23/2022 12:00:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=174
05/23/2022 12:00:19 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.6903846153846154 on epoch=174
05/23/2022 12:00:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=177
05/23/2022 12:00:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=179
05/23/2022 12:00:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=182
05/23/2022 12:00:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.09 on epoch=184
05/23/2022 12:00:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=187
05/23/2022 12:00:32 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.712815184602269 on epoch=187
05/23/2022 12:00:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7119949494949495 -> 0.712815184602269 on epoch=187, global_step=750
05/23/2022 12:00:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.08 on epoch=189
05/23/2022 12:00:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=192
05/23/2022 12:00:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=194
05/23/2022 12:00:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=197
05/23/2022 12:00:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.14 on epoch=199
05/23/2022 12:00:45 - INFO - __main__ - Global step 800 Train loss 0.10 Classification-F1 0.6289552710605342 on epoch=199
05/23/2022 12:00:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.08 on epoch=202
05/23/2022 12:00:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=204
05/23/2022 12:00:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.07 on epoch=207
05/23/2022 12:00:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=209
05/23/2022 12:00:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=212
05/23/2022 12:00:58 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.669724422880103 on epoch=212
05/23/2022 12:01:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.08 on epoch=214
05/23/2022 12:01:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=217
05/23/2022 12:01:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.07 on epoch=219
05/23/2022 12:01:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=222
05/23/2022 12:01:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.05 on epoch=224
05/23/2022 12:01:11 - INFO - __main__ - Global step 900 Train loss 0.06 Classification-F1 0.6847540818129053 on epoch=224
05/23/2022 12:01:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.07 on epoch=227
05/23/2022 12:01:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=229
05/23/2022 12:01:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.07 on epoch=232
05/23/2022 12:01:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
05/23/2022 12:01:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=237
05/23/2022 12:01:24 - INFO - __main__ - Global step 950 Train loss 0.07 Classification-F1 0.6717628107430739 on epoch=237
05/23/2022 12:01:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=239
05/23/2022 12:01:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=242
05/23/2022 12:01:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=244
05/23/2022 12:01:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=247
05/23/2022 12:01:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.08 on epoch=249
05/23/2022 12:01:37 - INFO - __main__ - Global step 1000 Train loss 0.09 Classification-F1 0.7093912093912094 on epoch=249
05/23/2022 12:01:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.07 on epoch=252
05/23/2022 12:01:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.05 on epoch=254
05/23/2022 12:01:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=257
05/23/2022 12:01:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=259
05/23/2022 12:01:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=262
05/23/2022 12:01:50 - INFO - __main__ - Global step 1050 Train loss 0.06 Classification-F1 0.6984126984126984 on epoch=262
05/23/2022 12:01:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/23/2022 12:01:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=267
05/23/2022 12:01:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=269
05/23/2022 12:02:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=272
05/23/2022 12:02:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.08 on epoch=274
05/23/2022 12:02:04 - INFO - __main__ - Global step 1100 Train loss 0.04 Classification-F1 0.6611374090714592 on epoch=274
05/23/2022 12:02:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/23/2022 12:02:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=279
05/23/2022 12:02:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=282
05/23/2022 12:02:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=284
05/23/2022 12:02:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
05/23/2022 12:02:17 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.6735918832693026 on epoch=287
05/23/2022 12:02:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=289
05/23/2022 12:02:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=292
05/23/2022 12:02:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/23/2022 12:02:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=297
05/23/2022 12:02:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/23/2022 12:02:31 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6954163343965976 on epoch=299
05/23/2022 12:02:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=302
05/23/2022 12:02:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/23/2022 12:02:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=307
05/23/2022 12:02:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/23/2022 12:02:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/23/2022 12:02:44 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.7093912093912094 on epoch=312
05/23/2022 12:02:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=314
05/23/2022 12:02:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
05/23/2022 12:02:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.05 on epoch=319
05/23/2022 12:02:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=322
05/23/2022 12:02:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 12:02:57 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.6857376857376857 on epoch=324
05/23/2022 12:03:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=327
05/23/2022 12:03:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=329
05/23/2022 12:03:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/23/2022 12:03:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=334
05/23/2022 12:03:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/23/2022 12:03:10 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6725225225225225 on epoch=337
05/23/2022 12:03:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=339
05/23/2022 12:03:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/23/2022 12:03:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=344
05/23/2022 12:03:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=347
05/23/2022 12:03:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/23/2022 12:03:24 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.6972454069228263 on epoch=349
05/23/2022 12:03:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=352
05/23/2022 12:03:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/23/2022 12:03:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=357
05/23/2022 12:03:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=359
05/23/2022 12:03:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/23/2022 12:03:38 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.689212037172189 on epoch=362
05/23/2022 12:03:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/23/2022 12:03:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=367
05/23/2022 12:03:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/23/2022 12:03:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/23/2022 12:03:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/23/2022 12:03:51 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7096115958345061 on epoch=374
05/23/2022 12:03:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=377
05/23/2022 12:03:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/23/2022 12:03:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=382
05/23/2022 12:04:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/23/2022 12:04:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=387
05/23/2022 12:04:05 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6961805555555556 on epoch=387
05/23/2022 12:04:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=389
05/23/2022 12:04:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/23/2022 12:04:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=394
05/23/2022 12:04:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/23/2022 12:04:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
05/23/2022 12:04:19 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.6656460656460657 on epoch=399
05/23/2022 12:04:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/23/2022 12:04:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/23/2022 12:04:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=407
05/23/2022 12:04:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/23/2022 12:04:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/23/2022 12:04:33 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6756390977443608 on epoch=412
05/23/2022 12:04:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=414
05/23/2022 12:04:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=417
05/23/2022 12:04:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/23/2022 12:04:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/23/2022 12:04:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/23/2022 12:04:46 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6948165869218501 on epoch=424
05/23/2022 12:04:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/23/2022 12:04:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/23/2022 12:04:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=432
05/23/2022 12:04:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 12:04:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/23/2022 12:05:00 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6656460656460657 on epoch=437
05/23/2022 12:05:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/23/2022 12:05:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/23/2022 12:05:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/23/2022 12:05:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/23/2022 12:05:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 12:05:14 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.6805844662381165 on epoch=449
05/23/2022 12:05:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/23/2022 12:05:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/23/2022 12:05:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/23/2022 12:05:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=459
05/23/2022 12:05:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/23/2022 12:05:29 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6947367352738196 on epoch=462
05/23/2022 12:05:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/23/2022 12:05:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/23/2022 12:05:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/23/2022 12:05:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/23/2022 12:05:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=474
05/23/2022 12:05:43 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6884731991268495 on epoch=474
05/23/2022 12:05:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/23/2022 12:05:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 12:05:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=482
05/23/2022 12:05:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=484
05/23/2022 12:05:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/23/2022 12:05:57 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.7185591206179442 on epoch=487
05/23/2022 12:05:57 - INFO - __main__ - Saving model with best Classification-F1: 0.712815184602269 -> 0.7185591206179442 on epoch=487, global_step=1950
05/23/2022 12:06:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/23/2022 12:06:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 12:06:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=494
05/23/2022 12:06:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/23/2022 12:06:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=499
05/23/2022 12:06:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.69012012012012 on epoch=499
05/23/2022 12:06:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=502
05/23/2022 12:06:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/23/2022 12:06:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 12:06:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/23/2022 12:06:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/23/2022 12:06:25 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.7087079057667293 on epoch=512
05/23/2022 12:06:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/23/2022 12:06:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/23/2022 12:06:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 12:06:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.00 on epoch=522
05/23/2022 12:06:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/23/2022 12:06:39 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.7128787878787879 on epoch=524
05/23/2022 12:06:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.00 on epoch=527
05/23/2022 12:06:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/23/2022 12:06:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/23/2022 12:06:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/23/2022 12:06:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/23/2022 12:06:53 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.7067574003057875 on epoch=537
05/23/2022 12:06:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=539
05/23/2022 12:06:57 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/23/2022 12:07:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=544
05/23/2022 12:07:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
05/23/2022 12:07:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 12:07:06 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7123495989304813 on epoch=549
05/23/2022 12:07:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 12:07:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=554
05/23/2022 12:07:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/23/2022 12:07:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 12:07:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/23/2022 12:07:20 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6948165869218501 on epoch=562
05/23/2022 12:07:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.00 on epoch=564
05/23/2022 12:07:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
05/23/2022 12:07:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 12:07:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/23/2022 12:07:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/23/2022 12:07:34 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.6566697191697192 on epoch=574
05/23/2022 12:07:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.00 on epoch=577
05/23/2022 12:07:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 12:07:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/23/2022 12:07:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/23/2022 12:07:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/23/2022 12:07:48 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6887663236988574 on epoch=587
05/23/2022 12:07:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/23/2022 12:07:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=592
05/23/2022 12:07:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/23/2022 12:07:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/23/2022 12:08:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 12:08:01 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.674520037278658 on epoch=599
05/23/2022 12:08:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/23/2022 12:08:06 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/23/2022 12:08:09 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/23/2022 12:08:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/23/2022 12:08:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/23/2022 12:08:15 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.688935384000799 on epoch=612
05/23/2022 12:08:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/23/2022 12:08:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/23/2022 12:08:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/23/2022 12:08:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 12:08:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/23/2022 12:08:30 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.708407605466429 on epoch=624
05/23/2022 12:08:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 12:08:34 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 12:08:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/23/2022 12:08:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 12:08:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/23/2022 12:08:43 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6948165869218501 on epoch=637
05/23/2022 12:08:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/23/2022 12:08:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/23/2022 12:08:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 12:08:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 12:08:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/23/2022 12:08:58 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.703028474903475 on epoch=649
05/23/2022 12:09:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/23/2022 12:09:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/23/2022 12:09:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 12:09:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/23/2022 12:09:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/23/2022 12:09:12 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.7223665223665223 on epoch=662
05/23/2022 12:09:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7185591206179442 -> 0.7223665223665223 on epoch=662, global_step=2650
05/23/2022 12:09:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/23/2022 12:09:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/23/2022 12:09:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/23/2022 12:09:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/23/2022 12:09:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/23/2022 12:09:27 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6805844662381165 on epoch=674
05/23/2022 12:09:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/23/2022 12:09:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 12:09:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 12:09:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/23/2022 12:09:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/23/2022 12:09:41 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.708407605466429 on epoch=687
05/23/2022 12:09:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 12:09:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 12:09:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/23/2022 12:09:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/23/2022 12:09:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 12:09:56 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6897447447447447 on epoch=699
05/23/2022 12:09:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/23/2022 12:10:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/23/2022 12:10:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=707
05/23/2022 12:10:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/23/2022 12:10:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 12:10:11 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.689212037172189 on epoch=712
05/23/2022 12:10:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 12:10:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/23/2022 12:10:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 12:10:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 12:10:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/23/2022 12:10:25 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6884731991268495 on epoch=724
05/23/2022 12:10:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 12:10:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
05/23/2022 12:10:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=732
05/23/2022 12:10:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/23/2022 12:10:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 12:10:40 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7364182364182364 on epoch=737
05/23/2022 12:10:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7223665223665223 -> 0.7364182364182364 on epoch=737, global_step=2950
05/23/2022 12:10:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/23/2022 12:10:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 12:10:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/23/2022 12:10:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/23/2022 12:10:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/23/2022 12:10:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 12:10:53 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:53 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:10:53 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 12:10:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 12:10:53 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:53 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:10:53 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 12:10:53 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6884518620002492 on epoch=749
05/23/2022 12:10:53 - INFO - __main__ - save last model!
05/23/2022 12:10:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 12:10:53 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 12:10:53 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 12:10:53 - INFO - __main__ - ['others']
05/23/2022 12:10:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:11:01 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 12:11:09 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 12:11:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 12:11:10 - INFO - __main__ - Starting training!
05/23/2022 12:13:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/23/2022 12:13:35 - INFO - __main__ - Classification-F1 on test data: 0.3336
05/23/2022 12:13:35 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7364182364182364, test_performance=0.3335801914619996
05/23/2022 12:13:35 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/23/2022 12:13:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 12:13:36 - INFO - __main__ - Printing 3 examples
05/23/2022 12:13:36 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:13:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:13:36 - INFO - __main__ - Loaded 64 examples from train data
05/23/2022 12:13:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/23/2022 12:13:36 - INFO - __main__ - Printing 3 examples
05/23/2022 12:13:36 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/23/2022 12:13:36 - INFO - __main__ - ['others']
05/23/2022 12:13:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:13:36 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:13:36 - INFO - __main__ - Loaded 64 examples from dev data
05/23/2022 12:13:55 - INFO - __main__ - load prompt embedding from ckpt
05/23/2022 12:13:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.92M parameters
05/23/2022 12:13:55 - INFO - __main__ - Starting training!
05/23/2022 12:13:58 - INFO - __main__ - Step 10 Global step 10 Train loss 3.28 on epoch=2
05/23/2022 12:14:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.16 on epoch=4
05/23/2022 12:14:03 - INFO - __main__ - Step 30 Global step 30 Train loss 1.81 on epoch=7
05/23/2022 12:14:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.39 on epoch=9
05/23/2022 12:14:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=12
05/23/2022 12:14:09 - INFO - __main__ - Global step 50 Train loss 1.96 Classification-F1 0.3503787878787879 on epoch=12
05/23/2022 12:14:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3503787878787879 on epoch=12, global_step=50
05/23/2022 12:14:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=14
05/23/2022 12:14:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=17
05/23/2022 12:14:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.98 on epoch=19
05/23/2022 12:14:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=22
05/23/2022 12:14:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=24
05/23/2022 12:14:22 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.20666666666666667 on epoch=24
05/23/2022 12:14:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=27
05/23/2022 12:14:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.76 on epoch=29
05/23/2022 12:14:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=32
05/23/2022 12:14:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=34
05/23/2022 12:14:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.68 on epoch=37
05/23/2022 12:14:35 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.4698750398043608 on epoch=37
05/23/2022 12:14:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3503787878787879 -> 0.4698750398043608 on epoch=37, global_step=150
05/23/2022 12:14:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=39
05/23/2022 12:14:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=42
05/23/2022 12:14:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.75 on epoch=44
05/23/2022 12:14:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=47
05/23/2022 12:14:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=49
05/23/2022 12:14:48 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.576796157059315 on epoch=49
05/23/2022 12:14:48 - INFO - __main__ - Saving model with best Classification-F1: 0.4698750398043608 -> 0.576796157059315 on epoch=49, global_step=200
05/23/2022 12:14:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=52
05/23/2022 12:14:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=54
05/23/2022 12:14:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=57
05/23/2022 12:14:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=59
05/23/2022 12:15:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.64 on epoch=62
05/23/2022 12:15:01 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.5522911497105045 on epoch=62
05/23/2022 12:15:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=64
05/23/2022 12:15:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=67
05/23/2022 12:15:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=69
05/23/2022 12:15:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=72
05/23/2022 12:15:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=74
05/23/2022 12:15:14 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.5328039927404719 on epoch=74
05/23/2022 12:15:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=77
05/23/2022 12:15:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=79
05/23/2022 12:15:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=82
05/23/2022 12:15:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=84
05/23/2022 12:15:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=87
05/23/2022 12:15:27 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.6232142857142857 on epoch=87
05/23/2022 12:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.576796157059315 -> 0.6232142857142857 on epoch=87, global_step=350
05/23/2022 12:15:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=89
05/23/2022 12:15:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=92
05/23/2022 12:15:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=94
05/23/2022 12:15:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=97
05/23/2022 12:15:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=99
05/23/2022 12:15:40 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.5139209701369412 on epoch=99
05/23/2022 12:15:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=102
05/23/2022 12:15:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=104
05/23/2022 12:15:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=107
05/23/2022 12:15:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=109
05/23/2022 12:15:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=112
05/23/2022 12:15:53 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.6017995570321152 on epoch=112
05/23/2022 12:15:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=114
05/23/2022 12:15:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=117
05/23/2022 12:16:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=119
05/23/2022 12:16:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=122
05/23/2022 12:16:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=124
05/23/2022 12:16:06 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.6335139318885449 on epoch=124
05/23/2022 12:16:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6232142857142857 -> 0.6335139318885449 on epoch=124, global_step=500
05/23/2022 12:16:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=127
05/23/2022 12:16:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=129
05/23/2022 12:16:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=132
05/23/2022 12:16:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=134
05/23/2022 12:16:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=137
05/23/2022 12:16:20 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.6305916305916306 on epoch=137
05/23/2022 12:16:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=139
05/23/2022 12:16:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=142
05/23/2022 12:16:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=144
05/23/2022 12:16:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.29 on epoch=147
05/23/2022 12:16:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=149
05/23/2022 12:16:33 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.6470588235294118 on epoch=149
05/23/2022 12:16:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6335139318885449 -> 0.6470588235294118 on epoch=149, global_step=600
05/23/2022 12:16:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=152
05/23/2022 12:16:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=154
05/23/2022 12:16:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=157
05/23/2022 12:16:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=159
05/23/2022 12:16:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=162
05/23/2022 12:16:46 - INFO - __main__ - Global step 650 Train loss 0.28 Classification-F1 0.5413919413919415 on epoch=162
05/23/2022 12:16:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=164
05/23/2022 12:16:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=167
05/23/2022 12:16:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=169
05/23/2022 12:16:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=172
05/23/2022 12:16:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.28 on epoch=174
05/23/2022 12:16:59 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.6629530600118836 on epoch=174
05/23/2022 12:16:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6470588235294118 -> 0.6629530600118836 on epoch=174, global_step=700
05/23/2022 12:17:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=177
05/23/2022 12:17:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.18 on epoch=179
05/23/2022 12:17:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=182
05/23/2022 12:17:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=184
05/23/2022 12:17:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.18 on epoch=187
05/23/2022 12:17:12 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.6697916666666666 on epoch=187
05/23/2022 12:17:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6629530600118836 -> 0.6697916666666666 on epoch=187, global_step=750
05/23/2022 12:17:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=189
05/23/2022 12:17:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=192
05/23/2022 12:17:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.16 on epoch=194
05/23/2022 12:17:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=197
05/23/2022 12:17:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.16 on epoch=199
05/23/2022 12:17:25 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.6988095238095238 on epoch=199
05/23/2022 12:17:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6697916666666666 -> 0.6988095238095238 on epoch=199, global_step=800
05/23/2022 12:17:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=202
05/23/2022 12:17:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=204
05/23/2022 12:17:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
05/23/2022 12:17:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
05/23/2022 12:17:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=212
05/23/2022 12:17:39 - INFO - __main__ - Global step 850 Train loss 0.15 Classification-F1 0.6785849823800488 on epoch=212
05/23/2022 12:17:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/23/2022 12:17:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=217
05/23/2022 12:17:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=219
05/23/2022 12:17:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.13 on epoch=222
05/23/2022 12:17:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=224
05/23/2022 12:17:52 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.6985923423423424 on epoch=224
05/23/2022 12:17:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=227
05/23/2022 12:17:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=229
05/23/2022 12:17:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=232
05/23/2022 12:18:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=234
05/23/2022 12:18:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=237
05/23/2022 12:18:05 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.6922171018945212 on epoch=237
05/23/2022 12:18:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=239
05/23/2022 12:18:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
05/23/2022 12:18:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=244
05/23/2022 12:18:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/23/2022 12:18:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=249
05/23/2022 12:18:18 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6594128690902884 on epoch=249
05/23/2022 12:18:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=252
05/23/2022 12:18:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=254
05/23/2022 12:18:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=257
05/23/2022 12:18:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=259
05/23/2022 12:18:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=262
05/23/2022 12:18:31 - INFO - __main__ - Global step 1050 Train loss 0.11 Classification-F1 0.6640633842671194 on epoch=262
05/23/2022 12:18:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=264
05/23/2022 12:18:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=267
05/23/2022 12:18:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=269
05/23/2022 12:18:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=272
05/23/2022 12:18:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
05/23/2022 12:18:44 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6475694444444444 on epoch=274
05/23/2022 12:18:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=277
05/23/2022 12:18:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=279
05/23/2022 12:18:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=282
05/23/2022 12:18:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=284
05/23/2022 12:18:57 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.07 on epoch=287
05/23/2022 12:18:58 - INFO - __main__ - Global step 1150 Train loss 0.07 Classification-F1 0.6505070993914807 on epoch=287
05/23/2022 12:19:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=289
05/23/2022 12:19:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=292
05/23/2022 12:19:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.06 on epoch=294
05/23/2022 12:19:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/23/2022 12:19:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=299
05/23/2022 12:19:11 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6640633842671194 on epoch=299
05/23/2022 12:19:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=302
05/23/2022 12:19:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=304
05/23/2022 12:19:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=307
05/23/2022 12:19:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=309
05/23/2022 12:19:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/23/2022 12:19:24 - INFO - __main__ - Global step 1250 Train loss 0.08 Classification-F1 0.6724979321753515 on epoch=312
05/23/2022 12:19:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/23/2022 12:19:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=317
05/23/2022 12:19:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=319
05/23/2022 12:19:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=322
05/23/2022 12:19:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=324
05/23/2022 12:19:37 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.6707885304659498 on epoch=324
05/23/2022 12:19:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=327
05/23/2022 12:19:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=329
05/23/2022 12:19:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=332
05/23/2022 12:19:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=334
05/23/2022 12:19:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=337
05/23/2022 12:19:50 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.6429743526517719 on epoch=337
05/23/2022 12:19:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=339
05/23/2022 12:19:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=342
05/23/2022 12:19:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=344
05/23/2022 12:20:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/23/2022 12:20:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=349
05/23/2022 12:20:04 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.656785160580227 on epoch=349
05/23/2022 12:20:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
05/23/2022 12:20:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/23/2022 12:20:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=357
05/23/2022 12:20:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=359
05/23/2022 12:20:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=362
05/23/2022 12:20:17 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.6475694444444444 on epoch=362
05/23/2022 12:20:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=364
05/23/2022 12:20:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=367
05/23/2022 12:20:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/23/2022 12:20:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/23/2022 12:20:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=374
05/23/2022 12:20:30 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.6566066066066065 on epoch=374
05/23/2022 12:20:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=377
05/23/2022 12:20:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=379
05/23/2022 12:20:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=382
05/23/2022 12:20:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=384
05/23/2022 12:20:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=387
05/23/2022 12:20:43 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.6678847296494356 on epoch=387
05/23/2022 12:20:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=389
05/23/2022 12:20:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=392
05/23/2022 12:20:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/23/2022 12:20:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=397
05/23/2022 12:20:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=399
05/23/2022 12:20:57 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.6707885304659498 on epoch=399
05/23/2022 12:20:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/23/2022 12:21:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/23/2022 12:21:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/23/2022 12:21:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/23/2022 12:21:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=412
05/23/2022 12:21:10 - INFO - __main__ - Global step 1650 Train loss 0.03 Classification-F1 0.6437732799212875 on epoch=412
05/23/2022 12:21:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/23/2022 12:21:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=417
05/23/2022 12:21:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=419
05/23/2022 12:21:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/23/2022 12:21:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=424
05/23/2022 12:21:23 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.69093505061247 on epoch=424
05/23/2022 12:21:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=427
05/23/2022 12:21:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=429
05/23/2022 12:21:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=432
05/23/2022 12:21:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=434
05/23/2022 12:21:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
05/23/2022 12:21:36 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6566066066066065 on epoch=437
05/23/2022 12:21:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/23/2022 12:21:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/23/2022 12:21:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=444
05/23/2022 12:21:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=447
05/23/2022 12:21:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/23/2022 12:21:51 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.69093505061247 on epoch=449
05/23/2022 12:21:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=452
05/23/2022 12:21:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.03 on epoch=454
05/23/2022 12:21:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=457
05/23/2022 12:22:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/23/2022 12:22:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/23/2022 12:22:06 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6822916666666667 on epoch=462
05/23/2022 12:22:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/23/2022 12:22:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/23/2022 12:22:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=469
05/23/2022 12:22:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/23/2022 12:22:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/23/2022 12:22:19 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.69093505061247 on epoch=474
05/23/2022 12:22:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/23/2022 12:22:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=479
05/23/2022 12:22:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=482
05/23/2022 12:22:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
05/23/2022 12:22:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/23/2022 12:22:33 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6761211205811486 on epoch=487
05/23/2022 12:22:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=489
05/23/2022 12:22:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/23/2022 12:22:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/23/2022 12:22:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=497
05/23/2022 12:22:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
05/23/2022 12:22:47 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.659601894384503 on epoch=499
05/23/2022 12:22:49 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=502
05/23/2022 12:22:52 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
05/23/2022 12:22:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/23/2022 12:22:57 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/23/2022 12:22:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/23/2022 12:23:01 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.6589262187088274 on epoch=512
05/23/2022 12:23:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=514
05/23/2022 12:23:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/23/2022 12:23:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=519
05/23/2022 12:23:10 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/23/2022 12:23:13 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
05/23/2022 12:23:14 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.69093505061247 on epoch=524
05/23/2022 12:23:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/23/2022 12:23:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/23/2022 12:23:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/23/2022 12:23:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
05/23/2022 12:23:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/23/2022 12:23:29 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6626984126984127 on epoch=537
05/23/2022 12:23:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/23/2022 12:23:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=542
05/23/2022 12:23:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/23/2022 12:23:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
05/23/2022 12:23:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=549
05/23/2022 12:23:44 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7032837301587301 on epoch=549
05/23/2022 12:23:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6988095238095238 -> 0.7032837301587301 on epoch=549, global_step=2200
05/23/2022 12:23:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=552
05/23/2022 12:23:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=554
05/23/2022 12:23:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.00 on epoch=557
05/23/2022 12:23:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/23/2022 12:23:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/23/2022 12:23:59 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7032837301587301 on epoch=562
05/23/2022 12:24:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/23/2022 12:24:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=567
05/23/2022 12:24:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/23/2022 12:24:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/23/2022 12:24:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=574
05/23/2022 12:24:12 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.6822916666666667 on epoch=574
05/23/2022 12:24:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/23/2022 12:24:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/23/2022 12:24:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/23/2022 12:24:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=584
05/23/2022 12:24:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/23/2022 12:24:26 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7032837301587301 on epoch=587
05/23/2022 12:24:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/23/2022 12:24:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=592
05/23/2022 12:24:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/23/2022 12:24:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/23/2022 12:24:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/23/2022 12:24:40 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.6947916666666667 on epoch=599
05/23/2022 12:24:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=602
05/23/2022 12:24:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/23/2022 12:24:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/23/2022 12:24:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/23/2022 12:24:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/23/2022 12:24:55 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7032837301587301 on epoch=612
05/23/2022 12:24:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/23/2022 12:25:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/23/2022 12:25:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/23/2022 12:25:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/23/2022 12:25:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/23/2022 12:25:10 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.689424648064354 on epoch=624
05/23/2022 12:25:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/23/2022 12:25:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/23/2022 12:25:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/23/2022 12:25:20 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/23/2022 12:25:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/23/2022 12:25:24 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7234546703296704 on epoch=637
05/23/2022 12:25:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7032837301587301 -> 0.7234546703296704 on epoch=637, global_step=2550
05/23/2022 12:25:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/23/2022 12:25:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/23/2022 12:25:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/23/2022 12:25:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/23/2022 12:25:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/23/2022 12:25:39 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6800646551724138 on epoch=649
05/23/2022 12:25:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/23/2022 12:25:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/23/2022 12:25:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/23/2022 12:25:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/23/2022 12:25:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/23/2022 12:25:53 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7032837301587301 on epoch=662
05/23/2022 12:25:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/23/2022 12:25:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/23/2022 12:26:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/23/2022 12:26:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/23/2022 12:26:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/23/2022 12:26:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7032837301587301 on epoch=674
05/23/2022 12:26:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/23/2022 12:26:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/23/2022 12:26:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/23/2022 12:26:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/23/2022 12:26:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/23/2022 12:26:27 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.689424648064354 on epoch=687
05/23/2022 12:26:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/23/2022 12:26:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/23/2022 12:26:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/23/2022 12:26:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/23/2022 12:26:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/23/2022 12:26:43 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7032837301587301 on epoch=699
05/23/2022 12:26:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/23/2022 12:26:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/23/2022 12:26:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/23/2022 12:26:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/23/2022 12:26:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/23/2022 12:27:00 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.689424648064354 on epoch=712
05/23/2022 12:27:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/23/2022 12:27:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/23/2022 12:27:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/23/2022 12:27:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/23/2022 12:27:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/23/2022 12:27:16 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.689424648064354 on epoch=724
05/23/2022 12:27:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/23/2022 12:27:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/23/2022 12:27:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/23/2022 12:27:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/23/2022 12:27:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/23/2022 12:27:32 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6627798724572918 on epoch=737
05/23/2022 12:27:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/23/2022 12:27:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/23/2022 12:27:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/23/2022 12:27:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/23/2022 12:27:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/23/2022 12:27:47 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6486461251167134 on epoch=749
05/23/2022 12:27:47 - INFO - __main__ - save last model!
05/23/2022 12:27:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 12:27:47 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 12:27:47 - INFO - __main__ - Printing 3 examples
05/23/2022 12:27:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 12:27:47 - INFO - __main__ - ['others']
05/23/2022 12:27:47 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 12:27:47 - INFO - __main__ - ['others']
05/23/2022 12:27:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 12:27:47 - INFO - __main__ - ['others']
05/23/2022 12:27:47 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:27:49 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:27:55 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 12:32:00 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-50prompt/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/23/2022 12:32:00 - INFO - __main__ - Classification-F1 on test data: 0.3401
05/23/2022 12:32:00 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7234546703296704, test_performance=0.340052250423813
